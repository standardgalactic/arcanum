

Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-1-
4419-1221-3, © Springer-Verlag New York 2010
Universitext
Series Editors
Sheldon Axler, Vincenzo Capasso, Carles Casacuberta, Angus MacIntyre,
Kenneth Ribet, Claude Sabbah, Endre Süli and Wojbor Woyczynski
For further volumes:http://www.springer.com/series/223

Wolfgang Rautenberg
A Concise Introduction to Mathematical
Logic

Wolfgang Rautenberg
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
raut@math.fu-berlin.de
ISBN 978-1-4419-1220-6
e-ISBN 978-1-4419-1221-3
Springer New York Dordrecht Heidelberg London Library of Congress Control
Number: 2009932782
© Springer-Verlag New York 2010
Mathematics Subject Classification (2000): 03-XX, 68N 17
All rights reserved. This work may not be translated or copied in whole or in part
without the written permission of the publisher (Springer Science+Business
Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief
excerpts in connection with reviews or scholarly analysis. Use in connection
with any form of information storage and retrieval, electronic adaptation,
computer software, or by similar or dissimilar methodology now known or
hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and
similar terms, even if they are not identified as such, is not to be taken as an
expression of opinion as to whether or not they are subject to proprietary rights.
Cover illustration: Photographer unknown. Courtesy of The Shelby White and
Leon Levy Archives Center, Institute for Advanced Study, Princeton, NJ, USA.
Printed on acid-free paper Springer is part of Springer Science+Business Media
(www.springer.com)

Foreword
The field of mathematical logic—evolving around the notions of logical validity,
provability, and computation—was created in the first half of the previous
century by a cohort of brilliant mathematicians and philosophers such as Frege,
Hilbert, Gödel, Turing, Tarski, Malcev, Gentzen, and some others. The
development of this discipline is arguably among the highest achievements of
science in the twentieth century: it expanded mathematics into a novel area of
applications, subjected logical reasoning and computability to rigorous analysis,
and eventually led to the creation of computers.
The textbook by Professor Wolfgang Rautenberg is a well-written
introduction to this beautiful and coherent subject. It contains classical material
such as logical calculi, beginnings of model theory, and Gödel's incompleteness
theorems, as well as some topics motivated by applications, such as a chapter on
logic programming. The author has taken great care to make the exposition
readable and concise; each section is accompanied by a good selection of
exercises.
A special word of praise is due for the author's presentation of Gödel's
second incompleteness theorem, in which the author has succeeded in giving an
accurate and simple proof of the derivability conditions and the provable ∑1-
completeness, a technically difficult point that is usually omitted in textbooks of
comparable level. This work can be recommended to all students who want to
learn the foundations of mathematical logic.
Lev Beklemishev Moscow

Preface
The third edition differs from the second mainly in that parts of the text have
been elaborated upon in more detail. Moreover, some new sections have been
added, for instance a separate section on Horn formulas in Chapter 4,
particularly interesting for logic programming. The book is aimed at students of
mathematics, computer science, and linguistics. It may also be of interest to
students of philosophy (with an adequate mathematical background) because of
the epistemological applications of Gödel's incompleteness theorems, which are
discussed in detail.
Although the book is primarily designed to accompany lectures on a
graduate level, most of the first three chapters are also readable by
undergraduates. The first hundred twenty pages cover sufficient material for an
undergraduate course on mathematical logic, combined with a due portion of set
theory. Only that part of set theory is included that is closely related to
mathematical logic. Some sections of Chapter 3 are partly descriptive, providing
a perspective on decision problems, on automated theorem proving, and on
nonstandard models.
Using this book for independent and individual study depends less on the
reader's mathematical background than on his (or her) ambition to master the
technical details. Suitable examples accompany the theorems and new notions
throughout. We always try to portray simple things simply and concisely and to
avoid excessive notation, which could divert the reader's mind from the
essentials. Line breaks in formulas have been avoided. To aid the student, the
indexes have been prepared very carefully. Solution hints to most exercises are
provided in an extra file ready for download from Springer's or the author's
website.
Starting from Chapter 4, the demands on the reader begin to grow. The
challenge can best be met by attempting to solve the exercises without recourse
to the hints. The density of information in the text is rather high; a newcomer
may need one hour for one page. Make sure to have paper and pencil at hand
when reading the text. Apart from sufficient training in logical (or mathematical)
deduction, additional prerequisites are assumed only for parts of Chapter 5,
namely some knowledge of classical algebra, and at the very end of the last
chapter some acquaintance with models of axiomatic set theory.
On top of the material for a one-semester lecture course on mathematical
logic, basic material for a course in logic for computer scientists is included in

Chapter 4 on logic programming. An effort has been made to capture some of
the interesting aspects of this discipline's logical foundations. The resolution
theorem is proved constructively. Since all recursive functions are computable in
PROLOG, it is not hard to deduce the undecidability of the existence problem
for successful resolutions.
Chapter 5 concerns applications of mathematical logic in mathematics itself.
It presents various methods of model construction and contains the basic
material for an introductory course on model theory. It contains in particular a
model-theoretic proof of quantifier eliminability in the theory of real closed
fields, which has a broad range of applications.
A special aspect of the book is the thorough treatment of Gödel's
incompleteness theorems in Chapters 6 and 7. Chapters 4 and 5 are not needed
here. 6.1 1 starts with basic recursion theory needed for the arithmetization of
syntax in 6.2 as well as in solving questions about decidability and
undecidability in 6.5. Defining formulas for arithmetical predicates are classified
early, to elucidate the close relationship between logic and recursion theory.
Along these lines, in 6.5 we obtain in one sweep Gödel's first incompleteness
theorem, the undecidability of the tautology problem by Church, and Tarski's
result on the nondefinability of truth, all of which are based on certain
diagonalization arguments. 6.6 includes among other things a sketch of the
solution to Hilbert's tenth problem.
Chapter 7 is devoted mainly to Gödel's second incompleteness theorem and
some of its generalizations. Of particular interest thereby is the fact that
questions about self-referential arithmetical statements are algorithmically
decidable due to Solovay's completeness theorem. Here and elsewhere, Peano
arithmetic (PA) plays a key role, a basic theory for the foundations of
mathematics and computer science, introduced already in 3.3. The chapter
includes some of the latest results in the area of selfreference not yet covered by
other textbooks.
Remarks in small print refer occasionally to notions that are undefined and
direct the reader to the bibliography, or will be introduced later. The
bibliography can represent an incomplete selection only. It lists most English
textbooks on mathematical logic and, in addition, some original papers mainly
for historical reasons. It also contains some titles treating biographical,
historical, and philosophical aspects of mathematical logic in more detail than
this can be done in the limited size of our book. Some brief historical remarks
are also made in the Introduction. Bibliographical entries are sorted
alphabetically by author names. This order may slightly diverge from the
alphabetic order of their citation labels.

The material contained in this book will remain with high probability the
subject of lectures on mathematical logic in the future. Its streamlined
presentation has allowed us to cover many different topics. Nonetheless, the
book provides only a selection of results and can at most accentuate certain
topics. This concerns above all Chapters 4, 5, 6, and 7, which go a step beyond
the elementary. Philosophical and foundational problems of mathematics are not
systematically discussed within the constraints of this book, but are to some
extent considered when appropriate.
The seven chapters of the book consist of numbered sections. A reference
like Theorem 5.4 is to mean Theorem 4 in Section 5 of a given chapter. In cross-
referencing from another chapter, the chapter number will be adjoined. For
instance, Theorem 6.5.4 means Theorem 5.4 in Chapter 6. You may find
additional information about the book or contact me on my website
www.math.fu-berlin.de/~raut. Please contact me if you propose improved
solutions to the exercises, which may afterward be included in the separate file
Solution Hints to the Exercises.
I would like to thank the colleagues who offered me helpful criticism along
the way. Useful for Chapter 7 were hints from Lev Beklemishev and Wilfried
Buchholz. Thanks also to Peter Agricola for his help in parts of the contents and
in technical matters, and to Michael Knoop and David Kramer for their thorough
reading of the manuscript and finding a number of mistakes.
Wolfgang Rautenberg June 2009

Introduction
Traditional logic as a part of philosophy is one of the oldest scientific disciplines.
It can be traced back to the Stoics and to Aristotle2 and is the root of what is
nowadays called philosophical logic. Mathematical logic, however, is a
relatively young discipline, having arisen from the endeavors of Peano, Frege,
and Russell to reduce mathematics entirely to logic. It steadily developed during
the twentieth century into a broad discipline with several subareas and numerous
applications in mathematics, computer science, linguistics, and philosophy.
One feature of modern logic is a clear distinction between object language
and metalanguage. The first is formalized or at least formalizable. The latter is,
like the language of this book, a kind of a colloquial language that differs from
author to author and depends also on the audience the author has in mind. It is
mixed up with semiformal elements, most of which have their origin in set
theory. The amount of set theory involved depends on one's objectives.
Traditional semantics and model theory as essential parts of mathematical logic
use stronger set-theoretic tools than does proof theory. In some model-theoretic
investigations these are often the strongest possible ones. But on average, little
more is assumed than knowledge of the most common set-theoretic terminology,
presented in almost every mathematical course or textbook for beginners. Much
of it is used only as a façon de parler.
The language of this book is similar to that common to almost all
mathematical disciplines. There is one essential difference though. In
mathematics, metalanguage and object language strongly interact with each
other, and the latter is semiformalized in the best of cases. This method has
proved successful. Separating object language and metalanguage is relevant only
in special context, for example in axiomatic set theory, where formalization is
needed to specify what certain axioms look like. Strictly formal languages are
met more often in computer science. In analyzing complex software or a
programming language, as in logic, formal linguistic entities are the central
objects of consideration.
The way of arguing about formal languages and theories is traditionally
called the metatheory. An important task of a metatheoretic analysis is to specify
procedures of logical inference by so-called logical calculi, which operate purely
syntactically. There are many different logical calculi. The choice may depend
on the formalized language, on the logical basis, and on certain aims of the
formalization. Basic metatheoretic tools are in any case the naive natural

numbers and inductive proof procedures. We will sometimes call them proofs by
metainduction, in particular when talking about formalized object theories that
speak about natural numbers. Induction can likewise be carried out on certain
sets of strings over a fixed alphabet, or on the system of rules of a logical
calculus.
The logical means of the metatheory are sometimes allowed or even
explicitly required to be different from those of the object language. But in this
book the logic of object languages, as well as that of the metalanguage, are
classical, twovalued logic. There are good reasons to argue that classical logic is
the logic of common sense. Mathematicians, computer scientists, linguists,
philosophers, physicists, and others are using it as a common platform for
communication.
It should be noticed that logic used in the sciences differs essentially from
logic used in everyday language, where logic is more an art than a serious task of
saying what follows from what. In everyday life, nearly every utterance depends
on the context. In most cases logical relations are only alluded to and rarely
explicitly expressed. Some basic assumptions of twovalued logic mostly fail, in
particular, a context-free use of the logical connectives. Problems of this type are
not dealt with here. To some extent, many-valued logic or Kripke semantics can
help to clarify the situation, and sometimes intrinsic mathematical methods must
be used in order to solve such problems. We shall use Kripke semantics here for
a different goal, though, the analysis of self-referential sentences in Chapter 7.
Let us add some historical remarks, which, of course, a newcomer may find
easier to understand after and not before reading at least parts of this book. In the
relatively short period of development of modern mathematical logic in the
twentieth century, some highlights may be distinguished, of which we mention
just a few. Many details on this development can be found in the excellent
biographies [Daw] and [FF] on Gödel and Tarski, the leading logicians in the last
century.
The first was the axiomatization of set theory in various ways. The most
important approaches are those of Zermelo (improved by Fraenkel and von
Neumann) and the theory of types by Whitehead and Russell. The latter was to
become the sole remnant of Frege's attempt to reduce mathematics to logic.
Instead it turned out that mathematics can be based entirely on set theory as a
first-order theory. Actually, this became more salient after the rest of the hidden
assumptions by Russell and others were removed from axiomatic set theory
around 1915; see [Hei]. For instance, the notion of an ordered pair, crucial for
reducing the notion of a function to set theory, is indeed a set-theoretic and not a
logical one.

Right after these axiomatizations were completed, Skolem discovered that
there are countable models of the set-theoretic axioms, a drawback to the hope
for an axiomatic characterization of a set. Just then, two distinguished
mathematicians, Hilbert and Brouwer, entered the scene and started their famous
quarrel on the foundations of mathematics. It is described in a comprehensive
manner for instance in [Kl2, Chapter IV] and need therefore not be repeated
here.
As a next highlight, Gödel proved the completeness of Hilbert's rules for
predicate logic, presented in the first modern textbook on mathematical logic,
[HA]. Thus, to some extent, a dream of Leibniz became real, namely to create an
ars inveniendi for mathematical truth. Meanwhile, Hilbert had developed his
view on a foundation of mathematics into a program. It aimed at proving the
consistency of arithmetic and perhaps the whole of mathematics including its
nonfinitistic set-theoretic methods by finitary means. But Gödel showed by his
incompleteness theorems in 1931 that Hilbert's original program fails or at least
needs thorough revision.
Many logicians consider these theorems to be the top highlights of
mathematical logic in the twentieth century. A consequence of these theorems is
the existence of consistent extensions of Peano arithmetic in which true and false
sentences live in peaceful coexistence with each other, called “dream theories” in
7.3. It is an intellectual adventure of holistic beauty to see wisdom from number
theory known for ages, such as the Chinese remainder theorem, simple
properties of prime numbers, and Euclid's characterization of coprimeness (page
249), unexpectedly assuming pivotal positions within the architecture of Gödel's
proofs. Gödel's methods were also basic for the creation of recursion theory
around 1936.
Church's proof of the undecidability of the tautology problem marks another
distinctive achievement. After having collected sufficient evidence by his own
investigations and by those of Turing, Kleene, and some others, Church
formulated his famous thesis (see 6.1), although in 1936 no computers in the
modern sense existed nor was it foreseeable that computability would ever play
the basic role it does today.
Another highlight of mathematical logic has its roots in the work of Tarski,
who proved first the undefinability of truth in formalized languages as explained
in 6.5, and soon thereafter started his fundamental work on decision problems in
algebra and geometry and on model theory, which ties logic and mathematics
closely together. See Chapter 5.
As already mentioned, Hilbert's program had to be revised. A decisive step
was undertaken by Gentzen, considered to be another groundbreaking

achievement of mathematical logic and the starting point of contemporary proof
theory. The logical calculi in 1.4 and 3.1 are akin to Gentzen's calculi of natural
deduction.
We further mention Gödel's discovery that it is not the axiom of choice (AC)
that creates the consistency problem in set theory. Set theory with AC and the
continuum hypothesis (CH) is consistent, provided set theory without AC and
CH is. This is a basic result of mathematical logic that would not have been
obtained without the use of strictly formal methods. The same applies to the
independence proof of AC and CH from the axioms of set theory by Cohen in
1963.
The above indicates that mathematical logic is closely connected with the
aim of giving mathematics a solid foundation. Nonetheless, we confine ourself
to logic and its fascinating interaction with mathematics, which characterizes
mathematical logic. History shows that it is impossible to establish a
programmatic view on the foundations of mathematics that pleases everybody in
the mathematical community. Mathematical logic is the right tool for treating the
technical problems of the foundations of mathematics, but it cannot solve its
epistemological problems.
Notation
We assume that the reader is familiar with the most basic mathematical
terminology and notation, in particular with the union, intersection, and
complementation of sets, denoted by ∪, ∩, and \, respectively. Here we
summarize only some notation that may differ slightly from author to author or
is specific for this book. ℕ, ℤ, ℚ, ℝ denote the sets of natural numbers
including 0, integers, rational, and real numbers, respectively, and ℕ+,ℚ+,ℝ+ the
sets of positive members of the corresponding sets. n, m, i, j, k always denote
natural numbers unless stated otherwise. Hence, extended notation like n ∈ ℕ is
mostly omitted.
In the following, M,N denote sets, M ⫅ N denotes inclusion, while M ⊂ N
means proper inclusion (i.e., M ⫅ N and M ≠ N). As a rule, we write M ⊂ N only
if the circumstance M ≠ N has to be emphasized. if M is fixed in a consideration
and N varies over subsets of M, then M\N may also be symbolized by \N or ¬N.
∅ denotes the empty set, and PM the power set (= set of all subsets) of M. If
one wants to emphasize that all elements of a set S are sets, S is also called a
system or family of sets. ⋃S denotes the union of S, that is, the set of elements
belonging to at least one M ∈ S, and ∩S stands for the intersection of a

nonempty system S, the set of elements belonging to all M ∈ S. If S = {Mi ∣ i ∈
I} then ∪S and ∩S are mostly denoted by ∪i∈I Mi and ∩i∈I Mi, respectively.
A relation between M and N is a subset of M × N, the set of ordered pairs (a,
b) with a ∈ M and b ∈ N. A precise definition of (a, b) is given on page 114.
Such a relation, f say, is said to be a function or mapping from M to N if for each
a ∈ M there is precisely one b ∈ N with (a, b) ∈ f. This b is denoted by f(a) or fa
or af and called the value of f at a. We denote a function f from M to N also by f
:M → N, or by f : x → t(x), provided f(x) = t(x) for some term t (see 2.2). ran f =
{fx ∣ x ∈ M} is called the range of f, and dom f = M its domain. idM denotes the
identical function on M, that is, idM(x) = x for all x ∈ M.
f :M → N is injective if fx = fy ⇒ x = y, for all x, y ∈ M, surjective if ran f =
N, and bijective if f is both injective and surjective. The reader should basically
be familiar with this terminology. The phrase “let f be a function from M to N” is
sometimes shortened to “let f :M → N.” The set of all functions from a set I to a
set M is denoted by MI. if f, g are functions with ran g ⫅ dom f then h: x →
f(g(x)) is called their composition (or product). It will preferably be written as h
= f ∘ g.
Let I and M be sets, f : I → M, and call I the index set. Then f will often be
denoted by (ai)i∈I and is named, depending on the context, an (indexed) family,
an I-tuple, or a sequence. If 0 is identified with ∅ and n > 0 with {0, 1, …, n -
1}, as is common in set theory, then Mn can be understood as the set of n-tuples
(ai)i<n = (a0, …, an−1) of length n whose members belong to M. In particular, M0
= {∅}. Also the set of sequences (a1, …, an) with ai ∈ M will frequently be
denoted by Mn. In concatenating finite sequences, which has an obvious
meaning, the empty sequence (i.e., ∅), plays the role of a neutral element. (a1,
…, an) will mostly be denoted by 
. Note that this is the empty sequence for n
= 0, similar to {a1, …, an} for n = 0 always being the empty set. fa means f(a1,
…, an) throughout.
If A is an alphabet, i.e., if the elements s ∈ A are symbols or at least named
symbols, then the sequence (s1, …, sn) ∈ An is written as s1 … sn and called a
string or a word over A. The empty sequence is called in this context the empty
string. A string consisting of a single symbol s is termed an atomic string. It will
likewise be denoted by s, since it will be clear from the context whether s means
a symbol or an atomic string.
Let ξη denote the concatenation of the strings ξ and η. If ξ = ξ1ηξ2 for some
strings ξ1, ξ2 and η≠= ∅ then η is called a segment (or substring) of ξ, termed a

proper segment in case η ≠= ξ. If ξ1 = ∅ then η is called an initial, if ξ2 = ∅, a
terminal segment of ξ.
Subsets P,Q,R, … ⫅ Mn are called n-ary predicates of M or n-ary relations.
A unary predicate will be identified with the corresponding subset of M. We may
write P
 for 
 ∈P, and ¬P
 for 
/∉ P. Metatheoretical predicates (or
properties) cast in words will often be distinguished from the surrounding text by
single quotes, for instance, if we speak of the syntactic predicate ‘The variable x
occurs in the formula α’. We can do so since quotes inside quotes will not occur
in this book. Single-quoted properties are often used in induction principles or
reflected in a theory, while ordinary (“double”) quotes have a stylistic function
only.
An n-ary operation of M is a function f :Mn → M. Since M0 = {∅}, a 0-ary
operation of M is of the form {(∅, c)}, with c ∈ M; it is denoted by c for short
and called a constant. Each operation f :Mn → M is uniquely described by the
graph of f, defined as graph 
.3 Both f
and graph f are essentially the same, but in most situations it is more convenient
to distinguish between them.
The most important operations are binary ones. The corresponding symbols
are mostly written between the arguments, as in the following listing of
properties of a binary operation ∘ on a set A. ∘ :A2 → A is
commutative if a ∘ b = b ∘ a for all a, b ∈ A
associative if a ∘ (b ∘ c) = (a ∘ b) ∘ c for all a, b, c ∈ A
idempotent if a ∘ a = a for all a ∈ A
invertible if for all a, b ∈ A there are x, y ∈ A with a ∘ x = b and y ∘ a =
b.
If H,Θ (read eta, theta) are expressions of our metalanguage, H ⇔ Θ stands for
‘H iff Θ’ which abbreviates ‘H if and only if Θ’. Similarly, H ⇒ Θ and H & Θ
mean ‘if H then Θ’ and ‘H and Θ’, respectively, and H∨Θ is to mean ‘H or Θ.’
This notation does not aim at formalizing the metalanguage but serves improved
organization of metatheoretic statements. We agree that⇒, ⇔, … separate
stronger than linguistic binding particles such as “there is” or “for all.”
Therefore, in the statement ‘X ⊢ α ⇔ X ⊨ α, for all X and all α’ (Theorem 1.4.6)
the comma should not be dropped; otherwise, some serious misunderstanding
may arise: ‘X ⊢ α for all X and all α ’ is simply false.
H :⇔ Θ means that the expression is defined by Θ. When integrating
formulas in the colloquial metalanguage, one may use certain abbreviating

notation. For instance, ‘α ≡ β and β ≡ γ’ is occasionally shortened to α ≡ β ≡ γ.
(‘the formulas α, β, and β, γ are equivalent’). This is allowed, since in this book
the symbol ≡ will never belong to the formal language from which the formulas
α, β, γ are taken. W.l.o.g. or w.l.o.g. is a colloquial shorthand of “without loss of
generality” used in mathematics.

Contents
Introduction Notation
1 Propositional Logic
1.1 Boolean Functions and Formulas
1.2 Semantic Equivalence and Normal Forms
1.3 Tautologies and Logical Consequence
1.4 A Calculus of Natural Deduction
1.5 Applications of the Compactness Theorem
1.6 Hilbert Calculi
2 First-Order Logic
2.1 Mathematical Structures
2.2 Syntax of First-Order Languages
2.3 Semantics of First-Order Languages
2.4 General Validity and Logical Equivalence
2.5 Logical Consequence and Theories
2.6 Explicit Definitions—Language Expansions
3 Complete Logical Calculi
3.1 A Calculus of Natural Deduction
3.2 The Completeness Proof
3.3 First Applications: Nonstandard Models

3.4 ZFC and Skolem's Paradox
3.5 Enumerability and Decidability
3.6 Complete Hilbert Calculi
3.7 First-Order Fragments
3.8 Extensions of First-Order Languages
4 Foundations of Logic Programming
4.1 Term Models and Herbrand's Theorem
4.2 Horn Formulas
4.3 Propositional Resolution
4.4 Horn Resolution
4.5 Unification
4.6 Logic Programming
4.7 A Proof of the Main Theorem
5 Elements of Model Theory
5.1 Elementary Extensions
5.2 Complete and κ-Categorical Theories
5.3 The Ehrenfeucht Game
5.4 Embedding and Characterization Theorems
5.5 Model Completeness
5.6 Quantifier Elimination
5.7 Reduced Products and Ultraproducts

6 Incompleteness and Undecidability
6.1 Recursive and Primitive Recursive Functions
6.2 Arithmetization
6.3 Representability of Arithmetical Predicates
6.4 The Representability Theorem
6.5 The Theorems of Gödel, Tarski, Church
6.6 Transfer by Interpretation
6.7 The Arithmetical Hierarchy
7 On the Theory of SelfReference
7.1 The Derivability Conditions
7.2 The Provable ∑1-Completeness
7.3 The Theorems of Gödel and Löb
7.4 The Provability Logic G
7.5 The Modal Treatment of SelfReference
7.6 A Bimodal Provability Logic for PA
7.7 Modal Operators in ZFC
Bibliography
Index of Terms and Names
Index of Symbols
Footnotes

1
2
3
This is to mean Section 6.1, more precisely, Section 1 in Chapter 6. All other boldface labels are to be
read accordingly throughout the book.
The Aristotelian syllogisms are easy but useful examples for inferences in a first-order language with
unary predicate symbols. One of these syllogisms serves as an example in Section 4.6 on logic
programming.
This means that the left-hand term graph f is defined by the right-hand term. A corresponding meaning
has := throughout, except in programs and flow diagrams, where x := t means the allocation of the value
of the term t to the variable x.

(1)
Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-
1-4419-1221-3_1, © Springer Science+Business Media, LLC 2010
1. Propositional Logic
Wolfgang Rautenberg
1  
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
Wolfgang Rautenberg
Email: raut@math.fu-berlin.de
Abstract
Propositional logic, by which we here mean two-valued propositional logic,
arises from analyzing connections of given sentences A, B, such as
These connection operations can be approximately described by two-valued
logic. There are other connections that have temporal or local features, for
instance, first A then B or here A there B, as well as unary modal operators like it
is necessarily true that, whose analysis goes beyond the scope of two-valued
logic. These operators are the subject of temporal, modal, or other subdisciplines
of many-valued or nonclassical logic. Furthermore, the connections that we
began with may have a meaning in other versions of logic that two-valued logic
only incompletely captures. This pertains in particular to their meaning in natural
or everyday language, where meaning may strongly depend on context.
Propositional logic, by which we here mean two-valued propositional logic,
arises from analyzing connections of given sentences A, B, such as
These connection operations can be approximately described by two-valued
logic. There are other connections that have temporal or local features, for
instance, first A then B or here A there B, as well as unary modal operators like it
is necessarily true that, whose analysis goes beyond the scope of two-valued
logic. These operators are the subject of temporal, modal, or other subdisciplines
of many-valued or nonclassical logic. Furthermore, the connections that we

began with may have a meaning in other versions of logic that two-valued logic
only incompletely captures. This pertains in particular to their meaning in natural
or everyday language, where meaning may strongly depend on context.
In two-valued propositional logic such phenomena are set aside. This
approach not only considerably simplifies matters, but has the advantage of
presenting many concepts, for instance those of consequence, rule induction, or
resolution, on a simpler and more perspicuous level. This will in turn save a lot
of writing in Chapter 2 when we consider the corresponding concepts in the
framework of predicate logic.
We will not consider everything that would make sense in two-valued
propositional logic, such as two-valued fragments and problems of definability
and interpolation. The reader is referred instead to [KK] or [Ra1]. We will
concentrate our attention more on propositional calculi. While there exists a
multitude of applications of propositional logic, we will not consider technical
applications such as the designing of Boolean circuits and problems of
optimization. These topics have meanwhile been integrated into computer
science. Rather, some useful applications of the propositional compactness
theorem are described comprehensively.
1.1 Boolean Functions and Formulas
Two-valued logic is based on two foundational principles: the principle of
bivalence, which allows only two truth values, namely true and false, and the
principle of extensionality, according to which the truth value of a connected
sentence depends only on the truth values of its parts, not on their meaning.
Clearly, these principles form only an idealization of the actual relationships.
Questions regarding degrees of truth or the sense-content of sentences are
ignored in two-valued logic. Despite this simplification, or indeed because of it,
such a method is scientifically successful. One does not even have to know
exactly what the truth values true and false actually are. Indeed, in what follows
we will identify them with the two symbols 1 and 0. Of course, one could have
chosen any other apt symbols such as ⊤ and ⊥ or t and f. The advantage here is
that all conceivable interpretations of true and false remain open, including those
of a purely technical nature, for instance the two states of a gate in a Boolean
circuit.
According to the meaning of the word and, the conjunction A and B of
sentences A, B, in formalized languages written as A ∧ B or A & B, is true if and
only if A, B are both true and is false otherwise. So conjunction corresponds to a
binary function or operation over the set {0, 1} of truth values, named the ∧-

function and denoted by ∧. It is given by its value matrix 
, where, in
general, 
 represents the value matrix or truth table of a binary
function ∘ with arguments and values in {0, 1}. The delimiters of these small
matrices will usually be omitted.
A function f: {0, 1} n → { 0, 1} is called an n-ary Boolean function or truth
function. Since there are 2 n n-tuples of 0, 1, it is easy to see that the number of
n-ary Boolean functions is 
. We denote their totality by Bn. While B 2 has
24 = 16 members, there are only four unary Boolean functions. One of these is
negation, denoted by ¬and defined by ¬1 = 0 and ¬0 = 1. B 0 consists just of the
constants 0 and 1.
The first column of the table below contains the common binary connections
with examples of their instantiation in English. The second column lists some of
its traditional symbols, which also denote the corresponding truth function, and
the third its truth table. Disjunction is the inclusive or and is to be distinguished
from the exclusive disjunction. The latter corresponds to addition modulo 2 and
is therefore given the symbol +. In Boolean circuits the functions +, ↓, ↑ are
often denoted by xor, nor, and nand; the latter is also known as the Sheffer
function. Recall our agreement in the section Notation that the symbols &, ∨,  
⇒  , and   ⇔   will be used only on the metatheoretic level.
A connected sentence and its corresponding truth function need not be
denoted by the same symbol; for example, one might take ∧ for conjunction and
et as the corresponding truth function. But in doing so one would only be
creating extra notation, but no new insights. The meaning of a symbol will
always be clear from the context: if α, β are sentences of a formal language, then
α ∧ β denotes their conjunction; if a, b are truth values, then a ∧ b just denotes a
truth value. Occasionally, we may want to refer to the symbols 
themselves, setting their meaning temporarily aside. Then we talk of the
connectives or truth functors 
Sentences formed using connectives given in the table are said to be logically
equivalent if their corresponding truth tables are identical. This is the case, for
example, for the sentences A provided B and A or not B, which represent the
converse implication, denoted by A ← B.1 It does not appear in the table, since it
arises by swapping A, B in the implication. This and similar reasons explain why
only a few of the sixteen binary Boolean functions require notation. Another

example of logical equivalent sentences are if A and B then C, and if B then C
provided A.
compound sentence conjunction
symbol truth table
A and B; A as well as B disjunction
∧, &
A or B implication
∨, ∨
if A then B; B provided A equivalence
→,   ⇒  
A if and only if B; A iff B exclusive disjunction
↔,   ⇔  
either A or B but not both nihilation
+
neither A nor B incompatibility
↓
not at once A and B
↑
In order to recognize and describe logical equivalence of compound
sentences it is useful to create a suitable formalism or a formal language. The
idea is basically the same as in arithmetic, where general statements are more
clearly expressed by means of certain formulas. As with arithmetical terms, we
consider propositional formulas as strings of signs built in given ways from basic
symbols. Among these basic symbols are variables, for our purposes called
propositional variables, the set of which is denoted by PV. Traditionally, these
are symbolized by 
 However, our numbering of the variables below
begins with p 1 rather than with p 0, enabling us later on to represent Boolean
functions more conveniently. Further, we use certain logical signs such as 
, similar to the signs 
 of arithmetic. Finally, parentheses (, ) will
serve as technical aids, although these are dispensable, as will be seen later on.
Each time a propositional language is in question, the set of its logical
symbols, called the logical signature, and the set of its variables must be given

in advance. For instance, it is crucial in some applications of propositional logic
in Section 1.5 for PV to be an arbitrary set, and not a countably infinite one as
indicated previously. Put concretely, we define a propositional language ℱ of
formulas built up from the symbols 
 inductively as
follows:
The atomic strings 
 are formulas, called prime formulas, also called
atomic formulas, or simply prime.
If the strings α, β are formulas, then so too are the strings (α ∧ β), (α  ∨  β),
and ¬α.
This is a recursive (somewhat sloppily also called inductive) definition in the
set of strings on the alphabet of the mentioned symbols, that is, only those
strings gained using (F1) or (F2) are in this context formulas. Stated set-
theoretically, 
 is the smallest (i.e., the intersection) of all sets of strings S built
from the aforementioned symbols with the properties
Example. (p 1 ∧ (p 2  ∨   ¬p 1)) is a formula. On the other hand, its initial
segment (p 1 ∧ (p 2  ∨   ¬p 1) is not, because a closing parenthesis is missing. It
is intuitively clear and will rigorously be proved on the next page that the
number of left parentheses occurring in a formula coincides with the number of
its right parentheses.
Remark 1. (f1) and (f2) are set-theoretic translations of (F1) and (F2). Some
authors like to add a third condition to (F1), (F2), namely (F3): No other strings
than those obtained by (F1) and (F2) are formulas in this context. But this at
most underlines that (F1), (F2) are the only formula-building rules; (F3) follows
from our definition, as its set-theoretic translation by (f1), (f2) indicates. Note
that we do not strictly distinguish between the symbol p i and the prime formula
or atomic string p i . Note also that in the formula definition parentheses are
needed only for binary connectives, not if a formula starts with ¬. By a slightly
more involved definition at least the outermost parentheses in formulas of the
form (α ∘ β) with a binary connective ∘ could be saved. Howsoever propositional
formulas are defined, what counts is their unique readability, see page 6.
The formulas defined by (F1), (F2) are called Boolean formulas, because
they are obtained using the Boolean signature { ∧, ∨, ¬}. Should further
connectives belong to the logical signature, for example → or ↔, (F2) of the
above definition must be augmented accordingly. But unless stated otherwise, (α
→ β) and (α ↔ β) are here just abbreviations; the first is ¬(α ∧ ¬β), the second is
((α → β) ∧ (β → α)).

Occasionally, it is useful to have symbols in the logical signature for always
false and always true, ⊥ and ⊤ respectively, say, called falsum and verum and
sometimes also denoted by 0 and 1. These are to be regarded as supplementary
prime formulas, and clause (F1) should be altered accordingly. However, we
prefer to treat ⊥ and ⊤ as abbreviations: ⊥ : = (p 1 ∧ ¬p 1) and 
.
For the time being we let 
 be the set of all Boolean formulas, although
everything said about 
 holds correspondingly for any propositional language.
Propositional variables will henceforth be denoted by 
, formulas by 
, prime formulas also by π, and sets of propositional formulas by
X, Y, Z, where these letters may also be indexed.
For the reason of parenthesis economy in formulas, we set some conventions
similar to those used in writing arithmetical terms.
1. The outermost parentheses in a formula may be omitted (if there are
any). For example, (p  ∨  q) ∧ ¬p may be written in place of ((p  ∨  q) ∧
¬p). Note that (p  ∨  q) ∧ ¬p is not itself a formula but denotes the
formula ((p  ∨  q) ∧ ¬p).
 
2. In the order ¬, ∧, ∨, →, ↔, each connective binds more strongly than
those following it. Thus, one may write p  ∨  q ∧ ¬p instead of p  ∨  (q
∧ ¬p), which means (p  ∨  (q ∧ ¬p)) by convention 1.
 
3. By the multiple use of → we associate to the right. So p → q → p is to
mean p → (q → p). Multiple occurrences of other binary connectives are
associated to the left, for instance, p ∧ q ∧ ¬p means (p ∧ q) ∧ ¬p. In
place of α0 ∧ ⋯ ∧ α n and α0 ∨ ⋯ ∨ α n we may write 
 and 
, respectively.
 
Also, in arithmetic, one normally associates to the left. An exception is the
term 
, where traditionally association to the right is used, that is, 
equals 
. Association to the right has some advantages in writing
tautologies in which → occurs several times; for instance in the examples of
tautologies listed in 1.3 on page 16.
The above conventions are based on a reliable syntax in the framework of
which intuitively clear facts, such as the identical number of left and right
parentheses in a formula, are rigorously provable. These proofs are generally
carried out using induction on the construction of a formula. To make this clear
we denote by 
 that a property  holds for a string φ. For example, let  mean

the property ‘φ is a formula that has equally many right-and left-hand
parentheses’.  is trivially valid for prime formulas, and if 
, 
 then clearly
also 
, and 
. From this we may conclude that  applies to
all formulas, our reasoning being a particularly simple instance of the following
Principle of formula induction. Let  be a property of strings that satisfies the
conditions
(o)
 for all prime formulas π,
 
(s)
 , for all 
. 
Then 
 holds for all formulas φ.
The justification of this principle is straightforward. The set S of all strings
with property  has, thanks to (o) and (s), the properties (f1) and (f2) on page 4.
But 
 is the smallest such set. Therefore, 
. In words,  applies to all
formulas φ. Clearly, if other connectives are involved, condition (s) must
accordingly be modified.
It is intuitively clear and easily confirmed inductively on φ that a compound
Boolean formula φ (i.e., φ is not prime) is of the form φ = ¬α or φ = (α ∧ β) or φ
= (α  ∨  β) for suitable 
. Moreover, this decomposition is unique. For
instance, (α ∧ β) cannot at the same time be written (α′  ∨  β′) with perhaps
different formulas α′, β′. Thus, compound formulas have the unique readability
property, more precisely, the Unique formula reconstruction property. Each
compound formula 
 is either of the form ¬α or (α ∘ β) for some uniquely
determined formulas 
 , where ∘ is either ∧ or ∨.
This property is less obvious than it might seem. Nonetheless, the proof is
left as an exercise (Exercise 4) in order to maintain the flow of things. It may be
a surprise to the novice that for the unique formula reconstruction, parentheses
are dispensable throughout. Indeed, propositional formulas, like arithmetical
terms, can be written without any parentheses; this is realized in Polish notation
(= PN), also called prefix notation, once widely used in the logic literature. The
idea consists in altering (F2) as follows: if α,β are formulas then so too are ∧αβ,
∨ αβ, and ¬α. Similar to PN is RPN (reverse Polish notation), still used in some
programming languages like PostScript. RPN differs from PN only in that a
connective is placed after the arguments. For instance, (p ∧ (q  ∨   ¬p)) is
written in RPN as 
. Reading PN or RPN requires more effort due to the

high density of information; but by the same token it can be processed very fast
by a computer or a high-tech printer getting its job as a PostScript program. The
only advantage of the parenthesized version is that its decoding is somewhat
easier for our eye through the dilution of information.
Intuitively it is clear what a subformula of a formula φ is; for example, (q ∧
¬p) is a subformula of (p  ∨  (q ∧ ¬p)). All the same, for some purposes it is
convenient to characterize the set 
 of all subformulas of φ inductively:
Thus, a formula is always regarded as a subformula of itself. The above is a
typical example of a recursive definition on the construction of formulas.
Another example of such a definition is the rank degφ of a formula φ, which
provides a sometimes more convenient measure of the complexity of φ than its
length as a string and occasionally simplifies inductive arguments. Intuitively,
degφ is the highest number of nested connectives in φ. Let degπ = 0 for prime
formulas π, and if degα and degβ are already defined, then 
 and 
. Here ∘ denotes any binary connective. We will not
give here a general formulation of this definition procedure because it is very
intuitive and similar to the well-known procedure of recursive definitions on ℕ.
It has been made sufficiently clear by the preceding examples. Its justification is
based on the unique reconstruction property and insofar not quite trivial, in
contrast to the proof procedure by induction on formulas that immediately
follows from the definition of propositional formulas.
If a property is to be proved by induction on the construction of formulas φ,
we will say that it is a proof by induction on φ. Similarly, the recursive
construction of a function f on 
 will generally be referred to as defining f by
recursion on φ, often somewhat sloppily paraphrased as defining f by induction
on φ. Examples are 
 and {rk}. Others will follow.
Since the truth value of a connected sentence depends only on the truth
values of its constituent parts, we may assign to every propositional variable of α
a truth value rather than a sentence, thereby evaluating α, i.e., calculating a truth
value. Similarly, terms are evaluated in, say, the arithmetic of real numbers,
whose value is then a real (= real number). An arithmetical term t in the
variables 
 describes an n-ary function whose arguments and values are
reals, while a formula φ in 
 describes an n-ary Boolean function. To be
precise, a propositional valuation, or alternatively, a (propositional) model, is a

mapping 
 that can also be understood as a mapping from the set of
prime formulas to {0, 1}. We can extend this to a mapping from the whole of 
to {0, 1} (likewise denoted by w) according to the stipulations
2
By the value wφ of a formula φ under a valuation 
 we mean the
value given by this extension. We could denote the extended mapping by 
, say,
but it is in fact not necessary to distinguish it symbolically from 
because the latter determines the extension uniquely. Similarly, we keep the
same symbol if an operation in  extends to a larger domain. If the logical
signature contains further connectives, for example 
, then ( ∗ ) must be
supplemented accordingly, with w(α → β) = wα → wβ in the example. However,
if 
 is defined as in the Boolean case, then this equation must be provable.
Indeed, it is provable, because from our definition of α → β it follows that
for any w. A corresponding remark could be made with respect to ↔ and to ⊤
and ⊥. Always w ⊤ = 1 and w ⊥ = 0 by our definition of ⊤, ⊥, in accordance
with the meaning of these symbols. However, if these or similar symbols belong
to the logical signature, then suitable equations must be added to the definition
of w.
Let 
 denote the set of all formulas of 
 in which at most the variables 
 occur (n > 0). Then it can easily be seen that wα for the formula 
 depends only on the truth values of 
. In other words, 
satisfies for all valuations w, w′,
The simple proof of 
 follows from induction on the construction of formulas
in 
, observing that these are closed under the operations ¬, ∧, ∨. Clearly, 
holds for 
, and if 
 is valid for α, β ∈ ℱn, then also for ¬α, α ∧ β, and α 
∨  β. It is then clear that each 
 defines or represents an n-ary Boolean
function according to the following Definition. 
 represents the function 
 (or f is represented by α) whenever 
 for all
valuations w.
Because wα for α ∈ ℱn is uniquely determined by 
, α represents
precisely one function 
, sometimes written as α(n). For instance, both p 1
∧ p 2 and ¬( ¬p 1  ∨   ¬p 2) represent the ∧ -function, as can easily be illustrated

using a table. Similarly, ¬p 1  ∨  p 2 and ¬(p 1 ∧ ¬p 2) represent the →-function,
and p 1  ∨  p 2, ¬( ¬p 1 ∧ ¬p 2), (p 1 → p 2) → p 2 all represent the ∨ -function.
Incidentally, the last formula shows that the ∨ -connective can be expressed
using implication alone.
There is a caveat though: since α = p 1  ∨  p 2, for instance, belongs not only
to ℱ 2 but to ℱ 3 as well, α also represents the Boolean function 
. However, the third argument is only “fictional,” or put
another way, the function f is not essentially ternary.
In general we say that an operation f M n → M is essentially n-ary if f has no
fictional arguments, where the ith argument of f is called fictional whenever for
all 
 and all x i ′ ∈ M,
Identity and the ¬-function are the essentially unary Boolean functions, and out
of the sixteen binary functions, only ten are essentially binary, as is seen in
scrutinizing the possible truth tables.
Remark 2. If a n denotes temporarily the number of all n-ary Boolean
functions and e n the number of all essentially n-ary Boolean functions, it is not
particularly difficult to prove that 
. Solving for e n results in 
. However, we will not make use of these equations. These
become important only in a more specialized study of Boolean functions.
1.2 Exercises
1.
 is called linear if 
 for suitable
coefficients 
. Here + denotes exclusive disjunction
(addition modulo 2) and the not written multiplication is conjunction
(i.e., a i x i = x i for a i = 1 and a i x i = 0 for a i = 0). (a) Show that the
above representation of a linear function f is unique. (b) Determine the
number of n-ary linear Boolean functions. (c) Prove that each formula α
in ¬, + (i.e., α is a formula of the logical signature 
) represents a
linear Boolean function.
 
2. Verify that a compound Boolean formula φ is either of the form φ = ¬α
or else φ = (α ∧ β) or φ = (α  ∨  β) for suitable formulas α, β (this is the  

easy part of the unique reconstruction property).
3. Prove that a proper initial segment of a formula φ is never a formula.
Equivalently: If αξ = βη with 
 and arbitrary strings ξ, η, then α =
β. The same holds for formulas in PN, but not in RPN.
 
4. Prove (with Exercise 3) the second more difficult part of the unique
reconstruction property, the claim of uniqueness.
 
1.3 Semantic Equivalence and Normal Forms
Throughout this chapter w will always denote a propositional valuation.
Formulas α, β are called (logically or semantically) equivalent, and we write α ≡
β, when wα = wβ for all valuations w. For example α ≡ ¬ ¬α. Obviously, α ≡ β iff
for any n such that α, β ∈ ℱn, both formulas represent the same n-ary Boolean
function. It follows that at most 
 formulas in ℱn can be pairwise
inequivalent, since there are no more than 
 n-ary Boolean functions.
In arithmetic one writes simply s = t to express that the terms s, t represent
the same function. For example, 
 expresses the
equality of values of the left-and right-hand terms for all x, y ∈ ℝ. This way of
writing is permissible because formal syntax plays a minor role in arithmetic. In
formal logic, however, as is always the case when syntactic considerations are to
the fore, one uses the equality sign in messages like α = β only for the syntactic
identity of the strings α and β. Therefore, the equivalence of formulas must be
denoted differently. Clearly, for all formulas α, β, γ the following equivalences
hold:
Furthermore, α  ∨   ¬α ≡ ⊤, α ∧ ¬α ≡ ⊥, and α ∧ ⊤ ≡ α  ∨   ⊥ ≡ α. It is also
useful to list certain equivalences for formulas containing →, for example the
frequently used α → β ≡ ¬α  ∨  β ( ≡ ¬(α ∧ ¬β), and the important
To generalize: α1 →⋯ → α n ≡ α1 ∧ ⋯ ∧ α n − 1 → α n . Further, we mention

the “left distributivity” of implication with respect to ∧ and ∨, namely
Should the symbol → lie to the right then the following are valid:
Remark 1. These last two logical equivalences are responsible for a curious
phenomenon in everyday language. For example, the two sentences A: Students
and pensioners pay half price, B: Students or pensioners pay half price evidently
have the same meaning. How to explain this? Let student and pensioner be
abbreviated by S, P, and pay half price by H. Then
express somewhat more precisely the factual content of A and B,
respectively. Now, according to our truth tables, the formulas α and β are simply
logically equivalent. The everyday-language statements A and B of α and β
obscure the structural difference of α and β through an apparently synonymous
use of the words and and or.
Obviously, ≡ is an equivalence relation, that is,
Moreover, ≡ is a congruence relation on ℱ,3 i.e., for all α, α′, β, β′,
For this reason the replacement theorem holds: α ≡ α′  ⇒   φ ≡ φ′, where φ′ is
obtained from φ by replacing one or several of the possible occurrences of the
subformula α in φ by α′. For instance, by replacing the subformula ¬p  ∨   ¬q by
the equivalent formula ¬(p ∧ q) in φ = ( ¬p  ∨   ¬q) ∧ (p  ∨  q) we obtain φ′ =
¬(p ∧ q) ∧ (p  ∨  q), which is equivalent to φ. A similar replacement theorem
also holds for arithmetical terms and is constantly used in their manipulation.
This mostly goes unnoticed, because = is written instead of ≡, and the
replacement for = is usually correctly applied. The simple inductive proof of the
replacement theorem will be given in a somewhat broader context in 2.4.
Furnished with the equivalences ¬ ¬α ≡ α, ¬(α ∧ β) ≡ ¬α  ∨   ¬β, and ¬(α  ∨
β) ≡ ¬α ∧ ¬β, and using replacement it is easy to construct for each formula an
equivalent formula in which ¬stands only in front of variables. For example, ¬(p
∧ q  ∨  r) ≡ ¬(p ∧ q) ∧ ¬r ≡ ( ¬p  ∨   ¬q) ∧ ¬r is obtained in this way. This
observation follows also from Theorem 2.1.
It is always something of a surprise to the newcomer that independent of its
arity, every Boolean function can be represented by a Boolean formula. While
this can be proved in various ways, we take the opportunity to introduce certain
normal forms and therefore begin with the following Definition. Prime formulas

and negations of prime formulas are called literals. A disjunction α1 ∨ ⋯ ∨ α n ,
where each α i is a conjunction of literals, is called a disjunctive normal form, a
DNF for short. A conjunction β1 ∧ ⋯ ∧ β n , where every β i is a disjunction of
literals, is called a conjunctive normal form, a CNF for short.
Example 1. The formula p  ∨  (q ∧ ¬p) is a DNF; p  ∨  q is at once a DNF
and a CNF; p  ∨   ¬(q ∧ ¬p) is neither a DNF nor a CNF.
Theorem 2.1 states that every Boolean function is represented by a Boolean
formula, indeed by a DNF, and also by a CNF. It would suffice to show that for
given n there are at least 
 pairwise inequivalent DNFs (resp. CNFs).
However, we present instead a constructive proof whereby for a Boolean
function given in tabular form a representing DNF (resp. CNF) can explicitly be
written down. In Theorem 2.1 we temporarily use the following notation: p 1 : =
p and p 0 : = ¬p. With this stipulation, 
 iff wp 1 = x 1 and wp 2 =
x 2. More generally, induction on n ≥ 1 easily shows that for all x1, …, xn ∈ { 0,
1},
Theorem 2.1.
Every Boolean function f with f ∈ Bn (n > 0) is representable by a DNF , namely
by
4
At the same time, f is representable by the CNF
Proof. By the definition of α f , the following equivalences hold for an
arbitrary valuation w:
Thus, 
. From this equivalence, and because there are only
two truth values, 
 follows immediately. The representability proof of f
by β f runs analogously; alternatively, Theorem 2.4 below may be used.
Example 2. For the exclusive-or function +, the construction of α f in
Theorem 2.1 gives the representing DNF p 1 ∧ ¬p 2  ∨   ¬p 1 ∧ p 2, because (1,

0), (0, 1) are the only pairs for which + has the value 1. The CNF given by the
theorem, on the other hand, is (p 1  ∨  p 2) ∧ ( ¬p 1  ∨   ¬p 2); the equivalent
formula (p 1  ∨  p 2) ∧ ¬(p 1 ∧ p 2) makes the meaning of the exclusive-or
compound particularly intuitive.
p 1 ∧ p 2  ∨   ¬p 1 ∧ p 2  ∨   ¬p 1 ∧ ¬p 2 is the DNF given by Theorem 2.1
for the Boolean function →. It is longer than the formula ¬p 1  ∨  p 2, which is
also a representing DNF. But the former is distinctive in that each of its disjuncts
contains each variable occurring in the formula exactly once. A DNF of n
variables with the analogous property is called canonical. The notion of
canonical CNF is correspondingly explained. For instance, the function ↔ is
represented by the canonical CNF ( ¬p 1 ∨ p 2) ∧ (p 1 ∨ ¬p 2) according to
Theorem 2.1, which always provides canonical normal forms as representing
formulas.
Since each formula represents a certain Boolean function, Theorem 2.1
immediately implies the following fact, which has also a (more lengthy)
syntactical proof with the replacement theorem mentioned on page 11.
Corollary 2.2.
Each φ ∈ ℱ is equivalent to a DNF and to a CNF .
Functional completeness. A logical signature is called functional complete if
every Boolean function is representable by a formula in this signature. Theorem
2.1 shows that ¬, ∧,   ∨  } is functional complete. Because of p  ∨  q ≡ ¬( ¬p ∧
¬q) and p ∧ q ≡ ¬( ¬p  ∨   ¬q), one can further leave aside ∨, or alternatively ∧.
This observation is the content of
Corollary 2.3.
Both {¬, ∧} and {¬, ∨ } are functional complete.
Therefore, to show that a logical signature L is functional complete, it is
enough to represent ¬, ∧ or else ¬,   ∨   by formulas in L. For example, because
¬p ≡ p → 0 and p ∧ q ≡ ¬(p → ¬q), the signature { →, 0} is functional
complete. On the other hand, { →, ∧,   ∨  }, and a fortiori { →}, are not. Indeed,
wφ = 1 for any formula φ in →, ∧,   ∨   and any valuation w such that wp = 1 for
all p. This can readily be confirmed by induction on φ. Thus, never ¬p ≡ φ for
any such formula φ.
It is noteworthy that the signature containing only ↓ is functional complete:
from the truth table for ↓ we get ¬p ≡ p↓p as well as p ∧ q ≡ ¬p↓ ¬q. Likewise
for {↑}, because ¬p ≡ p↑p and p  ∨  q ≡ ¬p↑ ¬q. That {↑} must necessarily be
functional complete once we know that {↓} is will become obvious in the

discussion of the duality theorem below. Even up to term equivalence, there still
exist infinitely many signatures. Here signatures are called term equivalent if the
formulas of these signatures represent the same Boolean functions as in Exercise
2, for instance.
Define inductively on the formulas from ℱ a mapping δ : ℱ → ℱ by
 is called the dual formula of α and is obtained from α simply by
interchanging ∧ and ∨. Obviously, for a DNF α, 
 is a CNF, and vice versa.
Define the dual of f ∈ Bn by 
 with 
. Clearly 
 since 
. Note that 
, 
, 
, 
, but 
. In other words, ¬is self-dual. One may check by
going through all truth tables that essentially binary self-dual Boolean functions
do not exist. But it was Dedekind who discovered the interesting ternary self-
dual function
The above notions of duality are combined in the following
Theorem 2.4 (The duality principle for two-valued logic).
If α represents the function f then 
 represents the dual function 
 .
Proof by induction on α. Trivial for α = p. Let α, β represent 
, and in view of the induction hypothesis, 
 represents 
. This function is just the dual of f because
The induction step for ∨ is similar. Now let α represent f. Then ¬α represents 
 . By the induction hypothesis, 
 represents 
. Thus 
 represents 
, which coincides with 
 because of 
).
For example, we know that ↔ is represented by p ∧ q  ∨   ¬p ∧ ¬q. Hence, by
Theorem 2.4, 
 is represented by (p ∨ q) ∧ ( ¬p ∨ ¬q). More
generally, if a canonical DNF α represents f ∈ Bn, then the canonical CNF 

represents 
. Thus, if every f ∈ Bn is representable by a DNF then every f
must necessarily be representable by a CNF, since 
 maps Bn bijectively
onto itself as follows from 
. Note also that Dedekind’s just defined
ternary self-dual function d 3 shows in view of Theorem 2.4 that p ∧ q  ∨  p ∧ r 
∨  q ∧ r ≡ (p  ∨  q) ∧ (p  ∨  r) ∧ (q  ∨  r).
Remark 2. { ∧, ∨, 0, 1} is maximally functional incomplete, that is, if f is
any Boolean function not representable by a formula in ∧, ∨, 0, 1, then { ∧, ∨,
0, 1, f} is functional complete (Exercise 4). As was shown by E. Post (1920),
there are up to term equivalence only five maximally functional incomplete
logical signatures: besides { ∧, ∨, 0, 1} only { →, ∧ }, the dual of this, { ↔, ¬,
and {d 3, ¬. The formulas of the last one represent just the self-dual Boolean
functions. Since ¬p ≡ 1 + p, the signature {0, 1, +, ⋅ } is functional complete,
where ⋅ is written in place of ∧. The deeper reason is that {0, 1, +, ⋅ } is at the
same time the extralogical signature of fields (see 2.1). Functional completeness
in the two-valued case just derives from the fact that for a finite field, each
operation on its domain is represented by a suitable polynomial. We mention
also that for any finite set M of truth values considered in many-valued logics
there is a generalized two-argument Sheffer function, by which every operation
on M can be obtained, similarly to ↑ in the two-valued case.
Exercises
1. Verify the logical equivalences
  (p → q 1) ∧ ( ¬p → q 2) ≡ p ∧ q 1  ∨   ¬p ∧ q 2,
p 1 ∧ q 1 → p 2  ∨  q 2 ≡ (p 1 → p 2)  ∨  (q 1 → q 2).
 
2. Show that the signatures { +, 1}, { +, ¬, { ↔, 0}, and { ↔, ¬are all term
equivalent. The formulas of each of these signatures represent precisely
the linear Boolean functions.
 
3. Show that the formulas in ∧, ∨, 0, 1 represent exactly the monotonic
Boolean functions. These are the constants from B 0, and for n > 0 the f
∈ Bn such that for all i with 1 ≤ i ≤ n,
 

(.)
4.
Show that the logical signature { ∧, ∨, 0, 1} is maximally
functional incomplete.
 
5.
If one wants to prove Corollary 2.2 syntactically with the
properties of ≡ (page 10) one needs generalizations of the
distributivity, e.g., 
. Verify the
latter.
 
1.4 Tautologies and Logical Consequence
Instead of wα = 1 we prefer from now on to write w ⊨ α and read this wsatisfies
α. Further, if X is a set of formulas, we write w ⊨ X if w ⊨ α for all α ∈ X and
say that w is a (propositional) model for X. A given α (resp. X) is called
satisfiable if there is some w with w ⊨ α (resp. w ⊨ X). ⊨, called the
satisfiability relation, evidently has the following properties:
One can define the satisfiability relation w ⊨ α for a given w PV → { 0, 1}
also inductively on α, according to the clauses just given. This approach is
particularly useful for extending the satisfiability conditions in 2.3.
It is obvious that w PV → { 0, 1} will be uniquely determined by setting
down in advance for which variables w ⊨ p should be valid. Likewise the
notation w ⊨ α for α ∈ ℱn is already meaningful when w is defined only for 
. One could extend such a w to a global valuation by setting, for
instance, wp = 0 for all unmentioned variables p.
For formulas containing other connectives the satisfaction conditions are to
be formulated accordingly. For example, we expect
If → is taken to be a primitive connective, ( ∗ ) is required. However, we
defined → in such a way that ( ∗ ) is provable.
Definition. α is called logically valid or a (two-valued) tautology, in short ⊨
α, whenever w ⊨ α for all valuations w. A formula not satisfiable at all, i.e. w⊭
α for all w, is called a contradiction.
Examples. p  ∨   ¬p is a tautology and so is α  ∨   ¬α for every formula α,
the so-called law of the excluded middle or the tertium non datur. On the other
hand, α ∧ ¬α and α ↔ ¬α are always contradictions. The following tautologies
in → are mentioned in many textbooks on logic. Remember our agreement about
association to the right in formulas in which → repeatedly occurs.

It will later turn out that all tautologies in → alone are derivable (in a sense
still to be explained) from the last three formulas.
Clearly, it is decidable whether a formula α is a tautology, in that one tries
out the valuations of the variables of α. Unfortunately, no essentially more
efficient method is known; such a method exists only for formulas of a certain
form. We will have a somewhat closer look at this problem in 4.3. Various
questions such as checking the equivalence of formulas can be reduced to a
decision about whether a formula is a tautology. For notice the obvious
equivalence of α ≡ β and ⊨ α ↔ β.
Basic in propositional logic is the following
Definition. α is a logical consequence of X, written X ⊨ α, if w ⊨ α for
every model w of X. In short, w ⊨ X ⇒ w ⊨ α, for all valuations w.
While we use ⊨ both as the symbol for logical consequence (which is a
relation between sets of formulas X and formulas α) and the satisfiability
property, it will always be clear from the context what ⊨ actually means.
Evidently, α is a tautology iff ∅ ⊨ α, so that ⊨ α can be regarded as an
abbreviation for ∅ ⊨ α.
In this book, X ⊨ α, β will always mean ‘X ⊨ α and X ⊨ β’. More generally,
X ⊨ Y is always to mean ‘X ⊨ β for all β ∈ Y ’. We also write throughout α1, …,
α n ⊨ β in place of {α1, …, α n } ⊨ β, and more briefly, X, α ⊨ β in place of X ∪{
α} ⊨ β.
Examples of logical consequence. (a) α, β ⊨ α ∧ β and α ∧ β ⊨ α, β. This
is evident from the truth table of ∧. (b) α, α → β ⊨ β, because 
according to the truth table of →.(c) 
 for each α. Indeed, X ⊨
⊥ = p 1 ∧ ¬p 1 obviously means that X is unsatisfiable (has no model), as e.g. X
= p 2, ¬p 2}.(d) 
. In order to see this let w ⊨ X. If
w ⊨ α then X, α ⊨ β and hence w ⊨ β, and if w⊭ α (i.e., w ⊨ ¬α) then w ⊨ β
clearly follows from X, ¬α ⊨ β. Note that (d) reflects our case distinction made
in the naive metatheory while proving (d).
Example (a) could also be stated as 
. The property
exemplified by (b) is called the modus ponens when formulated as a rule of
inference, as will be done in 1.6. Example (d) is another formulation of the

(.)
(.)
often-used procedure of proof by cases: In order to conclude a sentence β from a
set of premises X it suffices to show it to be a logical consequence both under an
additional supposition and under its negation. This is generalized in Exercise 3.
Important are the following general and obvious properties of ⊨ :
Useful for many purposes is also the closure of the logical
consequence relation under substitution, which generalizes the fact that from p 
∨   ¬p all tautologies of the form α  ∨   ¬α arise from substituting α for p.
Definition. A (propositional) substitution is a mapping σ : PV → ℱ that is
extended in a natural way to a mapping σ : ℱ → ℱ as follows:
Thus, like valuations, substitutions are considered as operations on the whole
of ℱ. For example, if 
 for some fixed p and 
 otherwise, then 
arises from φ by substituting α for p at all occurrences of p in φ. From p  ∨   ¬p
arises in this way the schema α  ∨   ¬α. For X ⊆ ℱ let 
. The
observation 
 turns out to be the special instance X = ∅ of the
useful property (S) 
 (substitution invariance).
In order to verify (S), define 
 for a given valuation w in such a way that 
. We first prove by induction on α that
If α is prime, ( ∗ ) certainly holds. As regards the induction step, note that
The reasoning for ∨ and ¬is analogous and so ( ∗ ) holds. Now let X ⊨
α and 
. By ( ∗ ), we get 
. Thus 
, and again by ( ∗ ), 
. This confirms (S). Another important property of ⊨ that is not so
easily obtained will be proved in 1.4, namely (F) X ⊨ α  ⇒   X 0 ⊨ α for some
finite subset X 0 ⊆ X.
⊨ shares the properties (R), (M), (T), and (S) with almost all classical and
nonclassical (many-valued) propositional consequence relations. This is to mean
a relation ⊢ between sets of formulas and formulas of an arbitrary propositional
language ℱ that has the properties corresponding to (R), (M), (T), and (S). These
properties are the starting point for a general and strong theory of logical

systems created by Tarski, which underpins nearly all logical systems considered
in the literature. Should ⊢ satisfy the property corresponding to (F) then ⊢ is
called finitary.
Remark 1. Sometimes (S) is not demanded in defining a consequence
relation, and if (S) holds, one speaks of a structural consequence relation. We
omit this refinement. Notions such as tautology, consistency, maximal
consistency, and so on can be used with reference to any consequence relation ⊢
in an arbitrary propositional language ℱ. For instance, a set of formulas X is
called consistent in ⊢ whenever X⊬ α for some α, and maximally consistent if X
is consistent but has no proper consistent extension. ⊢ itself is called consistent
if X⊬ α for some X and α (this is equivalent to not ⊢ α for all α). Here as
always, ⊢ α stands for ∅ ⊢ α. If ℱ contains ¬then the consistency of X is often
defined by X ⊢ α, ¬α for no α. But the aforementioned definition has the
advantage of being completely independent of any assumption concerning the
occurring connectives. Another example of a general definition is this: A
formula set X is called deductively closed in ⊢ provided X ⊢ α ⇒ α ∈ X, for all
α ∈ ℱ. Because of (R), this condition can be replaced by X ⊢ α ⇔ α ∈ X.
Examples in ⊨ are the set of all tautologies and the whole of ℱ. The intersection
of a family of deductively closed sets is again deductively closed. Hence, each 
 is contained in a smallest deductively closed set, called the deductive
closure of X in ⊢. It equals {α ∈ ℱ∣ X ⊢ α}, as is easily seen. The notion of a
consequence relation can also be defined in terms of properties of the deductive
closure. We mention that (F) holds not just for our relation ⊨ that is given by a
two-valued matrix, but for the consequence relation of any finite logical matrix
in any propositional language. This is stated and at once essentially generalized
in Exercise 3 in 5.7 as an application of the ultraproduct theorem.
A special property of the consequence relation ⊨, easily provable, is (D) X, α
⊨ β  ⇒   X ⊨ α → β, called the (semantic) deduction theorem for propositional
logic. To see this suppose X, α ⊨ β and let w be a model for X. If w ⊨ α then by
the supposition, w ⊨ β, hence w ⊨ α → β. If w⊭ α then w ⊨ α → β as well.
Hence X ⊨ α → β in any case. This proves (D). As is immediately seen, the
converse of (D) holds as well, that is, one may replace ⇒ in (D) by ⇔. Iterated
application of this simple observation yields
In this way, β’s being a logical consequence of a finite set of premises is
transformed into a tautology. Using (D) it is easy to obtain tautologies. For
instance, to prove ⊨ p → q → p, it is enough to verify p ⊨ q → p, for which it
in turn suffices to show that p, q ⊨ p, and this is trivial.
Remark 2. By some simple applications of (D) each of the tautologies in the

examples on page 16 can be obtained, except the formula of Peirce. As we shall
see in Chapter 2, all properties of ⊨ derived above and in the exercises will
carry over to the consequence relation of a first-order language.
Exercises
1. Use the deduction theorem as in the text in order to prove
(a) ⊨ (p → q → r) → (p → q) → (p → r),(b) ⊨ (p → q) → (q → r)
→ (p → r).
 
2. Suppose that X ⊨ α → β. Prove that X ⊨ (γ → α) → (γ → β).
 
3. Verify the (rule of) disjunctive case distinction: if X, α ⊨ γ and X, β ⊨ γ
then X, α  ∨  β ⊨ γ. This implication is traditionally written more
suggestively as
 
4.
Verify the rules of contraposition (notation as in Exercise 3):
 
5. Let ⊢ be a consequence relation and let X be maximally consistent in ⊢
(see Remark 1). Show that X is deductively closed in ⊢.
 
1.5 A Calculus of Natural Deduction
We will now define a derivability relation ⊢ by means of a calculus operating
solely with some structural rules. ⊢ turns out to be identical to the consequence
relation ⊨. The calculus ⊢ is of the so-called Gentzen type and its rules are
given with respect to pairs (X, α) of formulas X and formulas α. Another calculus
for ⊨, of the Hilbert type, will be considered in 1.6. In distinction to [Ge], we do
not require that X be finite; our particular goals here make such a restriction
dispensable. If ⊢ applies to the pair (X, α) then we write X ⊢ α and say that α is
derivable or provable from X (made precise below); otherwise we write X⊬ α.
Following [Kl1], Gentzen’s name for (X, α), Sequenz, is translated as

sequent. The calculus is formulated in terms of ∧, ¬and encompasses the six
rules below, called the basic rules. How to operate with these rules will be
explained afterwards. The choice of { ∧, ¬as the logical signature is a matter of
convenience and justified by its functional completeness. The other standard
connectives are introduced by the definitions
⊤, ⊥ are defined as on page 5. Of course, one could choose any other functional
complete signature and adapt the basic rules correspondingly. But it should be
observed that a complete calculus in ¬, ∧, ∨, →, say, must also include basic
rules concerning ∨ and →, which makes induction arguments on the basic rules
of the calculus more lengthy.
Each of the basic rules below has certain premises and a conclusion. Only (IS)
has no premises. It allows the derivation of all sequents α ⊢ α. These are called
the initial sequents, because each derivation must start with these. (MR), the
monotonicity rule, could be weakened. It becomes even provable if all pairs (X,
α) with α ∈ X are called initial sequents.
Here and in the following X ⊢ α, β is to mean X ⊢ α and X ⊢ β. This
convention is important, since X ⊢ α, β has another meaning in Gentzen calculi
that operate with pairs of sets of formulas. The rules ( ∧ 1) and ( ¬1) actually
have two premises, just like ( ¬2). Note further that ( ∧ 2) really consists of two
subrules corresponding to the conclusions X ⊢ α and X ⊢ β. In ( ¬2), X, α means
X ∪{ α}, and this abbreviated form will always be used when there is no risk of
misunderstanding.
α1, …, α n ⊢ β stands for {α1, …, α n } ⊢ β; in particular, α ⊢ β for 
,
and ⊢ α for ∅ ⊢ α, just as with ⊨.
X ⊢ α (read from “X is provable or derivable α”) is to mean that the sequent
(X, α) can be obtained after a stepwise application of the basic rules. We can
make this idea of “stepwise application” of the basic rules rigorous and formally
precise (intelligible to a computer, so to speak) in the following way: a
derivation is to mean a finite sequence 
 of sequents such that every S i
is either an initial sequent or is obtained through the application of some basic
rule to preceding elements in the sequence. Thus, from X is derivable α if there is
a derivation 
 with S n = (X, α). A simple example with the end sequent
α, β ⊢ α ∧ β, or minutely ({α, β}, α ∧ β), is the derivation

Here (MR) was applied twice, followed by an application of ( ∧ 1). Not
shorter would be complete derivation of the sequent (∅, ⊤ ), i.e., a proof of ⊢
⊤. In this example both ( ¬1) and ( ¬2) are essentially involved.
Useful for shortening lengthy derivations is the derivation of additional rules,
which will be illustrated with the examples to follow. The second example, a
generalization of the first, is the often-used proof method reductio ad absurdum:
α is proved from X by showing that the assumption ¬α leads to a contradiction.
The other examples are given with respect to the defined →-connective. Hence,
for instance, the →-elimination mentioned below runs in the original language 
.
Examples of derivable rules

Remark 1. The example of →-introduction is nothing other than the
syntactic form of the deduction theorem that was semantically formulated in the
previous section. The deduction theorem also holds for intuitionistic logic.
However, it is not in general true for all logical systems dealing with implication,
thus indicating that the deduction theorem is not an inherent property of every
meaningful conception of implication. For instance, the deduction theorem does
not hold for certain formal systems of relevance logic that attempt to model
implication as a cause-and-effect relation.
A simple application of the →-elimination and the cut rule is a proof of the
detachment rule 
.
Indeed, the premise X ⊢ α → β yields X, α ⊢ β by →-elimination, and since
X ⊢ α, it follows X ⊢ β by the cut rule. Applying detachment on X = { α, α →
β}, we obtain α, α → β ⊢ β. This collection of sequents is known as modus

ponens. It will be more closely considered in 1.6.
Many properties of ⊢ are proved through rule induction, which we describe
after introducing some convenient terminology. We identify a property  of
sequents with the set of all pairs (X, α) to which  applies. In this sense the
logical consequence relation ⊨ is the property that applies to all pairs (X, α) with
X ⊨ α.
All the rules considered here are of the form
and are referred to as Gentzen-style rules. We say that  is closed underR
when 
 implies 
. For a rule without premises, i.e., n
= 0, this is just to mean 
. For instance, consider the above already
mentioned property 
. This property is closed under each basic rule of
⊢. In detail this means α ⊨ α, X ⊨ α ⇒ X′ ⊨ α for X′ ⊇ X, X ⊨ α, β ⇒ X ⊨ α ∧
β, etc.
From the latter we may conclude that  applies to all provable sequents; in
other words, ⊢ is (semantically) sound. What we need here to verify this
conclusion is the following easily justifiable Principle of rule induction. Let 
 be a property closed under all basic rules of   ⊢. Then X ⊢ α
implies 
 .
Proof by induction on the length of a derivation of S = (X, α). If the length is
1, 
 holds since S must be an initial sequent. Now let 
 be a
derivation of the sequent S : = S n . By the induction hypothesis we have 
 for
all i < n. If S is an initial sequent then 
 holds by assumption. Otherwise S has
been obtained by the application of a basic rule on some of the S i for i < n. But
then 
 holds, because  is closed under all basic rules.
As already remarked, the property X ⊨ α is closed under all basic rules.
Therefore, the principle of rule induction immediately yields the soundness of
the calculus, that is, ⊢   ⊆   ⊨. More explicitly,
There are several equivalent definitions of ⊢. A purely set-theoretic one is
the following: ⊢ is the smallest of all relations 
 that are closed under
all basic rules. ⊢ is equally the smallest consequence relation closed under the
rules ( ∧ 1) through ( ¬2). The equivalence proofs of such definitions are wordy
but not particularly contentful. We therefore do not elaborate further, because we
henceforth use only rule induction. Using rule induction one can also prove 

, and in particular the following theorem, for which the
soundness of ⊢ is irrelevant.
Theorem 4.1 (Finiteness theorem for ⊢ ).
If X ⊢ α then there is a finite subset X 0 ⊆ X with X 0 ⊢ α.
Proof. Let 
 be the property ‘X 0 ⊢ α for some finite X 0 ⊆ X’. We will
show that  is closed under all basic rules. Certainly, 
 holds for X = { α},
with X 0 = X so that  is closed under (MI). If X has a finite subset X 0 such that
X 0 ⊢ α, then so too does every set X′ such that X′ ⊇ X. Hence  is closed under
(MR). Let 
, 
, with, say, X 1 ⊢ α, X 2 ⊢ β for finite X 1, X 2 ⊆ X.
Then we also have X 0 ⊢ α, β for X 0 = X 1 ∪X 2 by (MR). Hence X 0 ⊢ α ∧ β by
( ∧ 1). Thus 
 holds, and  is closed under ( ∧ 1). Analogously one
shows the same for all remaining basic rules of ⊢ so that rule induction can be
applied.
Of great significance is the notion of formal consistency. It fully determines
the derivability relation, as the lemma to come shows. It will turn out that
consistent formalizes adequately the notion satisfiable. The proof of this
adequacy is the clue to the completeness problem.
Definition. X ⊆ ℱ is called inconsistent (in our calculus ⊢ ) if X ⊢ α for all
α ∈ ℱ, and otherwise consistent. X is called maximally consistent if X is
consistent but each Y ⊃ X is inconsistent.
The inconsistency of X can be identified by the derivability of a single
formula, namely ⊥ ( = p 1 ∧ ¬p 1), because X ⊢ ⊥ implies X ⊢ p 1, ¬p 1 by ( ∧
2), hence X ⊢ α for all α by ( ¬1). Conversely, when X is inconsistent then in
particular X ⊢ ⊥. Thus, X ⊢ ⊥ may be read as ‘X is inconsistent’, and X⊬ ⊥
as ‘X is consistent’. From this it easily follows that X is maximally consistent iff
either α ∈ X or ¬α ∈ X for each α. The latter is necessary, for if α, ¬α∉X then
both X, α ⊢ ⊥ and X, ¬α ⊢ ⊥, hence X ⊢ ⊥ by ( ¬2). This contradicts the
consistency of X. Sufficiency is obvious. Most important is the following lemma,
in which the properties C+ and C− can also be understood each as a pair of
provable rules.
Lemma 4.2.
The derivability relation ⊢ has the properties

Proof. Suppose that X ⊢ α. Then clearly X, ¬α ⊢ α and since certainly X, ¬α ⊢
¬α, we have X, ¬α ⊢ β for all β by ( ¬1), in particular X, ¬α ⊢ ⊥. Conversely,
let X, ¬α ⊢ ⊥ be the case, so that in particular X, ¬α ⊢ α, and thus X ⊢ α by ¬-
elimination on page 21. Property C− is proved completely analogously.
The claim ⊨   ⊆   ⊢, not yet proved, is equivalent to X⊬ α ⇒ X⊭ α, for all
X and α. But so formulated it becomes apparent what needs to be done to obtain
the proof. Since X⊬ α is by C+ equivalent to the consistency of X′ : = X ∪ ¬α},
and likewise X⊭ α to the satisfiability of X′, we need only show that consistent
sets are satisfiable. To this end we state the following lemma, whose proof,
exceptionally, jumps ahead of matters in that it uses Zorn’s lemma from 2.1
(page 46).
Lemma 4.3 (Lindenbaum’s theorem).
Every consistent set X ⊆ ℱ can be extended to a maximally consistent set X′⊇ X.
Proof. Let H be the set of all consistent 
, partially ordered with respect to
⊆. H≠∅, because X ∈ H. Let K ⊆ H be a chain, i.e., Y ⊆ Z or Z ⊆ Y, for all Y, Z
∈ K. Claim: U : = ⋃K is an upper bound for K. Since Y ∈ K ⇒ Y ⊆ U, we have
to show that U is consistent. Assume that U ⊢ ⊥. Then U 0 ⊢ ⊥ for some finite
. If, say, α i ∈ Y i ∈ K, and Y is the biggest of the sets Y 0,
…, Y n , then α i ∈ Y for all i ≤ n, hence also Y ⊢ ⊥ by (MR). This contradicts Y
∈ H and confirms the claim. By Zorn’s lemma, H has a maximal element X′,
which is necessarily a maximally consistent extension of X.
Remark 2. The advantage of this proof is that it is free of assumptions
regarding the cardinality of the language, while Lindenbaum’s original
construction was based on countable languages ℱ and runs as follows: Let X 0 :
= X ⊆ ℱ be consistent and let 
 be an enumeration of ℱ. Put 
 if this set is consistent and 
 otherwise. Then 
 is a maximally consistent extension of X, as can be easily verified.
In this proof, Zorn’s lemma, which is equivalent to the axiom of choice, is not
required.
Lemma 4.4.
A maximally consistent set X ⊆ ℱ has the property [¬] X ⊢¬α  ⇔  X ⊬ α, for
arbitrary α.
Proof. If X ⊢ ¬α, then X ⊢ α cannot hold due to the consistency of X. If, on the

other hand, X⊬ α, then X, ¬α is a consistent extension of X according by C+. But
then ¬α ∈ X, because X is maximally consistent. Consequently X ⊢ ¬α.
Lemma 4.5.
A maximally consistent set X is satisfiable.
Proof. Define w by w ⊨ p  ⇔   X ⊢ p. We will show that for all α, ( ∗ ) X ⊢ α 
⇔   w ⊨ α.
For prime formulas this is trivial. Further,
By ( ∗ ), w is a model for X, thereby completing the proof.
Only the properties [ ∧ ] X ⊢ α ∧ β ⇔ X ⊢ α, β and [ ¬] from Lemma 4.4
are used in the simple model construction in Lemma 4.5, which reveals the
requirements for propositional model construction in the base 
. Since
maximally consistent sets X are deductively closed (Exercise 5 in 1.3), these
requirements may also be stated as ( ∧ ) α ∧ β ∈ X ⇔ α, β ∈ X ; ( ¬) ¬α ∈ X ⇔
α∉X.
Lemma 4.3 and Lemma 4.5 confirm the equivalence of the consistency and
the satisfiability of a set of formulas. From this fact we easily obtain the main
result of the present section.
Theorem 4.6 (Completeness theorem).
X ⊢ α ⇔ X ⊨ α, for all formula sets X and formulas α.
Proof. The direction ⇒ is the soundness of ⊢. Conversely, X⊬ α implies that X,
¬α is consistent. Let Y be a maximally consistent extension of X, ¬α according to
Lemma 4.3. By Lemma 4.5, Y is satisfiable, hence also X, ¬α. Therefore X⊭ α.
An immediate consequence of Theorem 4.6 is the finiteness property (F)
mentioned in 1.3, which is almost trivial for ⊢ but not for ⊨ :
Theorem 4.7 (Finiteness theorem for ⊨ ).
If X ⊨ α, then so too X 0 ⊨ α for some finite subset X 0 of X.
This is clear because the finiteness theorem holds for ⊢ (Theorem 4.1),
hence also for ⊨. A further highly interesting consequence of the completeness

theorem is
Theorem 4.8 (Propositional compactness theorem).
A set X of propositional formulas is satisfiable if each finite subset of X is
satisfiable.
This theorem holds because if X is unsatisfiable, i.e., if X ⊨ ⊥, then, by
Theorem 4.7, we also know that X 0 ⊨ ⊥ for some finite X 0 ⊆ X, thus proving
the claim indirectly. Conversely, one easily obtains Theorem 4.7 from Theorem
4.8; both theorems are directly derivable from one another. Because Theorem 4.6
makes no assumptions regarding the cardinality of the set of variables, the
compactness theorem following from it is likewise valid without the respective
restrictions. This means that Theorem 4.8 has many useful applications, as the
next section will illustrate.
Let us notice that there are direct proofs of Theorem 4.8 or appropriate
reformulations that have nothing to do with a logical calculus. For example, the
theorem is equivalent to 
 for some finite X 0 ⊆
X,
where Mdα denotes the set of all models of α. In this formulation the
compactness of a certain naturally arising topological space is claimed. The
points of this space are the valuations of the variables, hence the name
“compactness theorem.” More on this can be found in [RS].
Another approach to completeness (probably the simplest one) is provided
by Exercises 3 and 4. This approach makes some elegant use of substitutions,
hence is called the completeness proof by the substitution method. This method
is explained in the Solution Hints (and in more detail in [Ra3]). It yields the
maximality of the derivability relation ⊢ (see Exercise 3), a much stronger result
than its semantic completeness. This result yields not only the Theorems 4.6,
4.7, and 4.8 in one go, but also some further remarkable properties: Neither new
tautologies nor new Hilbert style rules can consistently be adjoined to the
calculus ⊢. These properties (discussed in detail, e.g., in [Ra1]) are known under
the names Post completeness and structural completeness of ⊢, respectively.
Exercises
1. Prove using Theorem 4.7: if X ∪ ¬α∣ α ∈ Y is inconsistent and Y is
nonempty, then there exist formulas 
 such that X ⊢ α0  ∨  
⋯  ∨  α n .
 
2. Augment the signature ¬, ∧ } by   ∨   and prove the completeness of the
calculus obtained by supplementing the basic rules used so far with the  

(.)
rules 
3. Let ⊢ be a finitary consistent consequence relation in ℱ{ ∧, ¬with the
properties ( ∧ 1) through ( ¬2). Show that ⊢ is maximal (or maximally
consistent). This means that each consequence relation 
 in ℱ{ ∧,
¬is inconsistent, i.e., ⊢ ′α for all α.
 
4. Show by referring to Exercise 3: there is exactly one (consistent)
consequence relation in ℱ{ ∧, ¬satisfying ( ∧ 1)–( ¬2). This clearly
entails the completeness of ⊢.
 
1.6 Applications of the Compactness Theorem
Theorem 4.8 is very useful in carrying over certain properties of finite structures
to infinite ones. This section presents some typical examples. While these could
also be treated with the compactness theorem of first-order logic in 3.3, the
examples demonstrate how the consistency of certain sets of first-order
sentences can also be obtained in propositional logic. This approach to
consistency is also useful also for Herbrand’s theorem and related results
concerning logic programming.
1. Every set M can be (totally) ordered. 5
This means that there is an irreflexive, transitive, and connex relation < on
M. For finite M this follows easily by induction on the number of elements of M.
The claim is obvious when M = ∅ or is a singleton. Let now M = N ∪a} with an
n-element set N and a∉N, so that M has n + 1 elements. Then we clearly get an
order on M from that for N by “setting a to the end,” that is, defining x < a for all
x ∈ N.
Now let M be any set. We consider for every pair (a, b) ∈ M ×M a
propositional variable p ab . Let X be the set consisting of the formulas
From w ⊨ X we obtain an order <, simply by putting a < b  ⇔   w ⊨ p ab .
w ⊨ ¬p aa says the same thing as a≮a. Analogously, the remaining formulas of
X reflect transitivity and connexity. Thus, according to Theorem 4.8, it suffices
to show that every finite subset X 0 ⊆ X has a model. In X 0 only finitely many
variables occur. Hence, there are finite sets M 1 ⊆ M and X 1 ⊇ X 0, where X 1 is

(.)
given exactly as X except that a, b, c now run through the finite set M 1 instead
of M. But X 1 is satisfiable, because if < orders the finite set M 1 and w is defined
by w ⊨ p ab iff a < b, then w is clearly a model for X 1, hence also for X 0.
2. The four-color theorem for infinite planar graphs.
A simple graph is a pair (V, E) with an irreflexive symmetrical relation E ⊆
V 2. The elements of V are called points or vertices. It is convenient to identify E
with the set of all unordered pairs 
 such that aEb and to call these pairs the
edges of (V, E). If {a, b} ∈ E then we say that a, b are neighbors. (V, E) is said
to be k-colorable if V can be decomposed into kcolor classesC 1, …, C k ≠∅, 
, with C i ∩ C j = ∅ for i≠j, such that neighboring points do not
carry the same color; in other words, if a, b ∈ C i then {a, b}∉E for 
.
The figure shows the smallest four-colorable graph that is not three-
colorable; all its points neighbor each other. We will show that a graph (V, E) is
k-colorable if every finite subgraph (V 0, E 0) is k-colorable. E 0 consists of the
edges {a, b} ∈ E with a, b ∈ V 0. To prove our claim consider the following set
X of formulas built from the variables p a, i for a ∈ V and 1 ≤ i ≤ k:
The first formula states that every point belongs to at least one color class; the
second ensures their disjointedness, and the third that no neighboring points have
the same color. Once again it is enough to construct some w ⊨ X. Defining then
the C i by a ∈ C i ⇔ w ⊨ p a, i proves that (V, E) is k-colorable. We must
therefore satisfy each finite X 0 ⊆ X. Let (V 0, E 0) be the finite subgraph of (V,
E) of all the points that occur as indices in the variables of X 0. The assumption
on (V 0, E 0) obviously ensures the satisfiability of X 0 for reasons analogous to
those given in Example 1, and this is all we need to show. The four-color
theorem says that every finite planar graph is four-colorable. Hence, the same
holds for all graphs whose finite subgraphs are planar. These cover in particular
all planar graphs embeddable in the real plane.
3. Knig’s tree lemma. There are several versions of this lemma. For
simplicity, ours refers to a directed tree. This is a pair 
 with an irreflexive

(.)
relation 
 such that for a certain point c, the root of the tree, and any other
point a there is precisely one path connecting c with a. This is a sequence 
with a 0 = c, a n = a, and a i ◃a i + 1 for all i < n. From the uniqueness of a path
connecting c with any other point it follows that each b≠c has exactly one
predecessor in (V, ◃), that is, there is precisely one a with a◃b. Hence the name
tree.
Knig’s lemma then reads as follows: If every a ∈ V has only finitely many
successors and V contains arbitrarily long finite paths, then there is an infinite
path through V starting at c. By such a path we mean a sequence 
 such that
c 0 = c and c k ◃c k + 1 for each k. In order to prove the lemma we define the
“layer” S k inductively by S 0 = c} and S k + 1 = b ∈ V∣ { there is some $a ∈ Sk$
with $a ◃ b$}}. Since every point has only finitely many successors, each S k is
finite, and since there are arbitrarily long paths c◃a 1 ◃⋯ ◃a k and a k ∈ S k , no S
k is empty. Now let p a for each a ∈ V be a propositional variable, and let X
consist of the formulas
Suppose that w ⊨ X. Then by the formulas under (A), for every k there
is precisely one a ∈ S k with w ⊨ p a , denoted by c k . In particular, c 0 = c.
Moreover, c k ◃c k + 1 for all k. Indeed, if a is the predecessor of 
, then w
⊨ p a in view of (B), hence necessarily a = c k . Thus, 
 is a path of the type
sought. Again, every finite subset X 0 ⊆ X is satisfiable; for if X 0 contains
variables with indices up to at most the layer S n , then X 0 is a subset of a finite
set of formulas X 1 that is defined as X, except that k runs only up to n, and for
this case the claim is obvious.
4. The marriage problem (in linguistic guise).
Let N≠∅ be a set of words or names (in speech) with meanings in a set M. A
name ν ∈ N can be a synonym (i.e., it shares its meaning with other names in N),
or a homonym (i.e., it can have several meanings), or even both. We proceed
from the plausible assumption that each name ν has finitely many meanings only
and that k names have at least k meanings. It is claimed that a pairing-off exists;
that is, an injection f N → M that associates to each ν one of its original
meanings.
For finite N, the claim will be proved by induction on the number n of
elements of N. It is trivial for n = 1. Now let n > 1 and assume that the claim

holds for all k-element sets of names whenever 0 < k < n.
Case 1: For each k (0 < k < n): k names in N have at least k + 1 distinct
meanings. Then to an arbitrarily chosen ν from N, assign one of its meanings a
to it so that from the names out of N ∖ {ν} any k names still have at least k
meanings ≠a. By the induction hypothesis there is a pairing-off for N ∖ {ν} that
together with the ordered pair 
 yields a pairing-off for the whole of N.
Case 2: There is some k-element K ⊆ N (0 < k < n) such that the set M K of
meanings of the ν ∈ K has only k members. Every ν ∈ K can be assigned its
meaning from M K by the induction hypothesis. From the names in N ∖ K any i
names (i ≤ n − k) still have i meanings not in M K , as is not hard to see. By the
induction hypothesis there is also a pairing-off for N ∖ K with a set of values
from M ∖ M K . Joining the two obviously results in a pairing-off for the whole
of N.
We will now prove the claim for arbitrary sets of names N: assign to each
pair (ν, a) ∈ N ×M a variable 
 and consider the set of formulas
Assume that w ⊨ X. Then to each ν there is exactly one 
 with 
, so
that 
 is a pairing-off for N. Such a model w exists by Theorem
4.8, for in a finite set X 0 ⊆ X occur only finitely many names as indices and the
case of finitely many names has just been treated.
5. The ultrafilter theorem.
This theorem is of fundamental significance in topology (from which it
originally stems), model theory, set theory, and elsewhere. Let I be any
nonempty set. A nonempty collection of sets 
 is called a filter on I if for
all M, N ⊆ I hold the conditions (a) M, N ∈ F ⇒ M ∩ N ∈ F, (b) M ∈ F & M ⊆
N ⇒ N ∈ F.
Since F≠∅, (b) shows that always I ∈ F. As is easily verified, (a) and (b)
together are equivalent to just a single condition, namely to
For fixed K ⊆ I, {J ⊆ I∣ J ⊇ K} is a filter, the principal filter generated by K.
This is a proper filter provided K≠∅, which in general is to mean a filter with
∅∉F. Another example on an infinite I is the set of all cofinite subsets M ⊆ I,
i.e., ¬M ( = I ∖ M) is finite. This holds because M 1 ∩ M 2 is cofinite iff M 1, M 2
are both cofinite, so that ( ∩ ) is satisfied.
A filter F is said to be an ultrafilter on I provided it satisfies, in addition,

Ultrafilters on an infinite set I containing all cofinite subsets are called
nontrivial. That such ultrafilters exist will be shown below. It is nearly
impossible to describe them more closely. Roughly speaking, “we know they
exist but we cannot see them.” A trivial ultrafilter on I contains at least one finite
subset. {J ⊆ I∣ i 0 ∈ J} is an example for each i 0 ∈ I. This is a principal
ultrafilter. All trivial ultrafilters are of this form, Exercise 3. Thus, trivial and
principal ultrafilters coincide. In particular, each ultrafilter on a finite set I is
trivial in this sense.
Each proper filter F obviously satisfies the assumption of the following theorem
and can thereby be extended to an ultrafilter.
Theorem 5.1 (Ultrafilter theorem).
Every subset 
 can be extended to an ultrafilter U on a set I, provided M 0
∩⋯ ∩ M n ≠∅ for all n and all 
 .
Proof. Consider along with the propositional variables 
 for J ⊆ I 
 (M, N ⊆ I, J ∈ F).
Let w ⊨ X. Then ( ∩ ), ( ¬) are valid for 
; hence U is an
ultrafilter such that F ⊆ U. It therefore suffices to show that every finite subset
of X has a model, for which it is in turn enough to prove the ultrafilter theorem
for finite F. But this is easy: Let 
, D : = M 0 ∩ ⋯ ∩ M n , and i
0 ∈ D. Then U = J ⊆ I∣ i 0 ∈ J} is an ultrafilter containing F.
Exercises
1. Prove (using the compactness theorem) that every partial order ≤ 0 on a
set M can be extended to a total order ≤ on M.
 
2. Let F be a proper filter on I (≠∅). Show that F is an ultrafilter iff it
satisfies ( ∪): M ∪N ∈ F  ⇔   M ∈ F or N ∈ F.
 
3. Let I be an infinite set. Show that an ultrafilter U on I is trivial iff there is
an i 0 ∈ I such that U = J ⊆ I∣ i 0 ∈ J}.
 
1.7 Hilbert Calculi
In a certain sense the simplest logical calculi are so-called Hilbert calculi. They

(.)
are based on tautologies selected to play the role of logical axioms; this selection
is, however, rather arbitrary and depends considerably on the logical signature.
They use rules of inference such as, for example, modus ponens MP: α, α → β ⁄
β.6 An advantage of these calculi consists in the fact that formal proofs, defined
below as certain finite sequences, are immediately rendered intuitive. This
advantage will pay off above all in the arithmetization of proofs in 6.2.
In the following we consider such a calculus with MP as the only rule of
inference; we denote this calculus for the time being by 
, in order to
distinguish it from the calculus ⊢ of 1.4. The logical signature contains just
¬and ∧, the same as for ⊢. In the axioms of 
, however, we will also use
implication defined by α → β : = ¬(α ∧ ¬β), thus considerably shortening the
writing down of the axioms.
The logical axiom scheme of our calculus consists of the set Λ of all formulas
of the following form (not forgetting the right association of parentheses in Λ1,
Λ2, and Λ4):
Λ consists only of tautologies. Moreover, all formulas derivable from Λ
using MP are tautologies as well, because ⊨ α, α → β implies ⊨ β. We will
show that all 2-valued tautologies are provable from Λ by means of MP. To this
aim we first define the notion of a proof from X ⊆ ℱ in 
.
Definition. A proof from X (in 
) is a sequence Φ = (φ0, …, φn) such that
for every k ≤ n either φ k ∈ X ∪Λ or there exist indices i, j < k such that φ j = φ i
→ φ k (i.e., φ k results from applying MP to terms of Φ preceding φ k ). A proof
(φ0, …, φn) with φ n = α is called a proof of α from X of length n + 1. Whenever
such a proof exists we write 
 and say that α is provable or
derivablefromX.
Example. (p, q, p → q → p ∧ q, q → p ∧ q, p ∧ q) is a proof of p ∧ q from
the set X = p, q}. The last two terms in the proof sequence derive with MP from
the previous ones, which are members of X ∪Λ.
Since a proof contains only finitely many formulas, the preceding definition
leads immediately to the finiteness theorem for ∣ ∼, formulated correspondingly
to Theorem 4.1. Every proper initial segment of a proof is obviously a proof
itself. Moreover, concatenating proofs of α and α → β and tacking on β to the
resulting sequence will produce a proof for β, as is plain to see. This observation
implies

In short, the set of all formulas derivable from X is closed under MP. In applying
the property ( ∗ ) we will often say “MP yields …” It is easily seen that X ∣ ∼ α
iff α belongs to the smallest set containing X ∪Λ and closed under MP. For the
arithmetization of proofs and for automated theorem proving, however, it is
more appropriate to base derivability on the finitary notion of a proof that was
given in the last definition. Fortunately, the following theorem relieves us of the
necessity to verify a property of formulas α derivable from a given formula set X
each time by induction on the length of a proof of α from X.
Theorem 6.1 (Induction principle for ∣ ∼ ).
Let X be given and let  be a property of formulas. Then  holds for all α with X
∣ ∼ α, provided (o)  holds for all α ∈ X ∪ Λ, (s) 
 and 
 imply 
 ,
for all α,β.
Proof by induction on the length n of a proof Φ of α from X. If α ∈ X ∪Λ then 
 holds by (o), which applies in particular if n = 1. If α∉X ∪Λ then n > 1 and
Φ contains members α i and α j = α i → α both having proofs of length < n.
Hence, it holds 
 and 
 by the induction hypothesis, and so 
 according to
(s). _
An application of Theorem 6.1 is the proof of ∣ ∼ ⊆   ⊨, or more explicitly,
X ∣ ∼ α  ⇒   X ⊨ α (soundness).
To see this let 
 be the property ‘X ⊨ α’ for fixed X. Certainly, X ⊨ α holds
for α ∈ X. The same is true for α ∈ Λ. Thus, 
 for all α ∈ X ∪Λ, and (o) is
confirmed. Now let X ⊨ α, α → β; then so too X ⊨ β, thus confirming the
inductive step (s) in Theorem 6.1. Consequently, 
 (that is, X ⊨ α) holds for all
α with X ∣ ∼ α.
Unlike the proof of completeness for ⊢, the one for ∣ ∼ requires a whole
series of derivations to be undertaken. This is in accordance with the nature of
things. To get Hilbert calculi up and running one must often begin with drawn-
out derivations. In the derivations below we shall use without further comment
the monotonicity (M) (page 17, with ∣ ∼ for ⊨ ). (M) is obvious, for a proof in ∣
∼ from X is also a proof from X′ ⊇ X. Moreover, ∣ ∼ is a consequence relation
(as is every Hilbert calculus, based on Hilbert style rules). For example, if X ∣ ∼
Y ∣ ∼ α, we construct a proof of α from X by replacing each φ ∈ Y occurring in a
proof of α from Y by a proof of φ from X. This confirms the transitivity (T).
Lemma 6.2.
(a) X ∣ ∼ α →¬β  ⇒  X ∣ ∼ β →¬α, (b) ∣ ∼ α → β → α, (c) ∣ ∼ α → α, (d) ∣ ∼ α

→¬¬α, (e) ∣ ∼ β →¬β → α.
Proof. (a): Clearly X ∣ ∼ (α → ¬β) → β → ¬α by Axiom Λ4. From this and from
X ∣ ∼ α → ¬β the claim is derived by MP. (b): By Λ3, ∣ ∼ β ∧ ¬α → ¬α, and so
with (a), ∣ ∼ α → ¬(β ∧ ¬α) = α → β → α.
(c): From 
 in Λ1 we obtain
which yields the claim by applying (b) and MP twice; (d) then follows from
(a) using ∣ ∼ ¬α → ¬α. (e): Due to ∣ ∼ ¬β ∧ ¬α → ¬β and (a), we get ∣ ∼ β →
¬( ¬β ∧ ¬α) = β → ¬β → α.
Clearly, ∣ ∼ satisfies the rules ( ∧ 1) and ( ∧ 2) of 1.4, in view of Λ2, Λ3.
Part (e) of Lemma 6.2 yields X ∣ ∼ β, ¬β ⇒ X ∣ ∼ α, so that ∣ ∼ satisfies also
rule ( ¬1). After some preparation we will show that rule ( ¬2) holds for ∣ ∼ as
well, thereby obtaining the desired completeness result. A crucial step in this
direction is
Lemma 6.3 (Deduction theorem).
X,α ∣ ∼ γ implies X ∣ ∼ α → γ.
Proof by induction in ∣ ∼ with a given set X, α. Let X, α ∣ ∼ γ, and let 
now mean ‘X ∣ ∼ α → γ’. To prove (o) in Theorem 6.1, let γ ∈ Λ ∪X ∪{ α}. If γ
= α then clearly X ∣ ∼ α → γ by Lemma 6.2(c). If γ ∈ X ∪Λ then certainly X ∣ ∼
γ. Because also X ∣ ∼ γ → α → γ by Lemma 6.2(b), MP yields X ∣ ∼ α → γ, thus
proving (o). To show (s) let X, α ∣ ∼ β and X, α ∣ ∼ β → γ, so that X ∣ ∼ α → β,
α → β → γ by the induction hypothesis. Applying MP to Λ1 twice yields X ∣ ∼ α
→ γ, thus confirming (s). Therefore, by Theorem 6.1, 
 for all γ, which
completes the proof. _
Lemma 6.4.
∣ ∼ ¬¬α → α.
Proof. By Λ3 and MP, ¬ ¬α ∧ ¬α ∣ ∼ ¬α, ¬ ¬α. Choose any τ with ∣ ∼ τ. The
already verified rule ( ¬1) clearly yields ¬ ¬α ∧ ¬α ∣ ∼ ¬τ, and in view of
Lemma 6.3, ∣ ∼ ¬ ¬α ∧ ¬α → ¬τ. From Lemma 6.2(a) it follows that ∣ ∼ τ → ¬(
¬ ¬α ∧ ¬α). But ∣ ∼ τ, hence using MP we obtain ∣ ∼ ¬( ¬ ¬α ∧ ¬α) and the
latter formula is just ¬ ¬α → α.
Lemma 6.3 and Lemma 6.4 are preparations for the next lemma, which is
decisive in proving the completeness of ∣ ∼.
Lemma 6.5.

∣ ∼ satisfies also rule (¬2) of the calculus ⊢.
Proof. Let X, β ∣ ∼ α and X, ¬β ∣ ∼ α; then X, β ∣ ∼ ¬ ¬α and X, ¬β ∣ ∼ ¬ ¬α by
Lemma 6.2(d). Hence, X ∣ ∼ β → ¬ ¬α, ¬β → ¬ ¬α (Lemma 6.3), and so X ∣ ∼
¬α → ¬β and X ∣ ∼ ¬α → ¬ ¬β by Lemma 6.2(a). Thus, MP yields X, ¬α ∣ ∼ ¬β,
¬ ¬β, whence X, ¬α ∣ ∼ ¬τ by ( ¬1), with τ as in Lemma 6.4. Therefore X ∣ ∼ ¬α
→ ¬τ, due to Lemma 6.3, and hence X ∣ ∼ τ → ¬ ¬α by Lemma 6.2(a). Since X ∣
∼ τ it follows that X ∣ ∼ ¬ ¬α and so eventually X ∣ ∼ α by Lemma 6.4.
Theorem 6.6 (Completeness theorem).
∣ ∼ = ⊨.
Proof. Clearly, ∣ ∼ ⊆ ⊨. Now, by what was said already on page 34 and by the
lemma above, ∣ ∼ satisfies all basic rules of ⊢. Therefore, ⊢ ⊆ ∣ ∼. Since ⊢   =
⊨ (Theorem 4.6), we obtain also ⊨   ⊆ ∣ ∼.
This theorem implies in particular ∣ ∼ φ ⇔ ⊨ φ. In short, using MP one
obtains from the axiom system Λ exactly the two-valued tautologies.
Remark 1. It may be something of a surprise that Λ1–Λ4 are sufficient to
obtain all propositional tautologies, because these axioms and all formulas
derivable from them using MP are collectively valid in intuitionistic and minimal
logic. That Λ permits the derivation of all two-valued tautologies is based on the
fact that → was defined. Had → been considered as a primitive connective, this
would no longer have been the case. To see this, alter the interpretation of ¬by
setting 
. While one here indeed obtains the value 1 for every
valuation of the axioms of Λ and formulas derived from them using MP, one
does not do so for ¬ ¬p → p, which therefore cannot be derived. Modifying the
two-valued matrix or using many-valued logical matrices is a widely applied
method to obtain independence results for logical axioms.
Thus, we have seen that there are very different calculi for deriving
tautologies or to recover other properties of the semantic relation ⊨. We have
studied here to some extend Gentzen-style and Hilbert-style calculi and this will
be done also for first-order logic in Chapter 2. In any case, logical calculi and
their completeness proofs depend essentially on the logical signature, as can be
seen, for example, from Exercise 1.
Besides Gentzen-and Hilbert-style calculi there are still other types of logical
calculi, for example various tableau calculi, which are above all significant for
their generalizations to nonclassical logical systems. Related to tableau calculi is
the resolution calculus dealt with in 4.3.
Using Hilbert-style calculi one can axiomatize 2-valued logic in other logical

signatures and functional incomplete fragments. For instance, the fragment in ∧,
∨, which, while having no tautologies, contains a lot of interesting Hilbert-style
rules. Proving that this fragment is axiomatizable by finitely many such rules is
less easy as might be expected. At least nine Hilbert rules are required. Easier is
the axiomatization of the well-known →-fragment in Exercise 3, less easy that
of the ∨ -fragment in Exercise 4. Each of the infinitely many fragments of two-
valued logic with or without tautologies is axiomatizable by a calculus using
only finitely many Hilbert-style rules of its respective language, as was shown in
[HeR].
Remark 2. The calculus in Exercise 4 that treats the fragment in ∨ alone, is
based solely on unary rules. This fact considerably simplifies the matter, but the
completeness proof is nevertheless nontrivial. For instance, the indispensable
rule (αβ)γ ⁄ α(βγ) is derivable in this calculus, since a tricky application of the
rules (3) and (4) yields (αβ)γ ⊢ γ(αβ) ⊢ (γα)β ⊢ β(γα) ⊢ (βγ)α ⊢ α(βγ). Much
easier would be a completeness proof of this fragment with respect to the
Gentzen-style rules ( ∨ 1) and ( ∨ 2) from Exercise 2 in 1.4.
Exercises
1. Prove the completeness of the Hilbert calculus ⊢ in ℱ{ →, ⊥ } with
MP as the sole rule of inference, the definition ¬α : = α → ⊥, and the
axioms A1: α → β → α, A2: (α → β → γ) → (α → β) → α → γ, and A3:
¬ ¬α → α.
 
2. Let ⊢ be a finitary consequence relation and let X⊬ φ. Use Zorn’s
lemma to prove that there is a φ-maximalY ⊇ X, that is, Y⊬ φ but Y, α ⊢
φ whenever α∉Y. Such a Y is deductively closed but need not be
maximally consistent.
 
3. Let ⊢ denote the calculus in ℱ{ →} with the rule of inference MP, the
axioms A1, A2 from Exercise 1, and ((α → β) → α) → α (the Peirce
axiom). Verify that (a) a φ-maximal set X is maximally consistent, (b) ⊢
is a complete calculus in the propositional language ℱ{ →}.
 
4. Show the completeness of the calculus ⊢ in ℱ{ ∨ } with the four unary
Hilbert-style rules below. The writing of ∨ has been omitted:
 

1
2
3
4
References
KK.
G. KREISEL, J.-L. KRIVINE, Elements of Mathematical Logic, North-Holland 1971.
Ra1. W. RAUTENBERG, Klassische und Nichtklassische Aussagenlogik, Vieweg 1979.
Ge.
G. GENTZEN, The Collected Papers of Gerhard Gentzen (editor M. E. SZABO), North-Holland 1969.
RS.
H. RASIOWA, R. SIKORSKI, The Mathematics of Metamathematics, Warschau 1963, 3⟨{ rd}⟩ ed. Polish
Scientific Publ. 1970.
HeR. B. HERRMANN, W. RAUTENBERG, Finite replacement and finite Hilbert-style axiomatizability,
Zeitsch. Math. Logik Grundlagen Math. 38 (1982), 327–344.
Kl1.
S. KLEENE, Introduction to Metamathematics, Amsterdam 1952, 2⟨{ nd}⟩ ed. Wolters-Noordhoff
1988.
Ra3. _________ , A note on completeness and maximality in propositional logic, Reports on Mathematical
Logic 21 (1987), 3–8.
Footnotes
Converse implication is used in the programming language PROLOG, see 4.6.
We often use ( ∗ ) or ( ⋆ ) as a temporary label for a condition (or property) that we refer back to in the
text following the labeled condition.
This concept, stemming originally from geometry, is meaningfully defined in every algebraic structure
and is one of the most important and most general mathematical concepts; see 2.1. The definition is
equivalent to the condition
The disjuncts of αf can be arranged, for instance, according to the
lexicographical order of the n-tuples (x1,…,xn) ∈{ 0, 1}n. If the disjunction is
empty (that is, if f does not take the value 1) let αf be ⊥ (= p1 ∧¬p1). Thus,
the empty disjunction is ⊥. Similarly, the empty conjunction equals ⊤ (=
¬⊥). These conventions correspond to those in arithmetic, where the empty
sum is 0 and the empty product is 1.

5
6
Unexplained notions are defined in 2.1. Our first application is interesting
because in set theory the compactness theorem is weaker than the axiom of
choice (AC) which is equivalent to the statement that every set can be well-
ordered. Thus, the ordering principle is weaker than AC since it follows from
the compactness theorem.
Putting it crudely, this notation should express the fact that β is held to be
proved from a formula set X when α and α → β are provable from X. Modus
ponens is an example of a binary Hilbert-style rule; for a general definition of
this type of rule see, for instance, [Ra1].

(1)
Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-
1-4419-1221-3_2, © Springer Science+Business Media, LLC 2010
2. First-Order Logic
Wolfgang Rautenberg
1  
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
Wolfgang Rautenberg
Email: raut@math.fu-berlin.de
Abstract
Mathematics and some other disciplines such as computer science often consider
domains of individuals in which certain relations and operations are singled out.
When using the language of propositional logic, our ability to talk about the
properties of such relations and operations is very limited. Thus, it is necessary
to refine our linguistic means of expression, in order to procure new possibilities
of description. To this end, one needs not only logical symbols but also variables
for the individuals of the domain being considered, as well as a symbol for
equality and symbols for the relations and operations in question. First-order
logic, sometimes called also predicate logic, is the part of logic that subjects
properties of such relations and operations to logical analysis.
Linguistic particles such as “for all” and “there exists” (called quantifiers)
play a central role here, whose analysis should be based on a well prepared
semantic background. Hence, we first consider mathematical structures and
classes of structures. Some of these are relevant both to logic (in particular
model theory) and to computer science. Neither the newcomer nor the advanced
student needs to read all of 2.1, with its mathematical flavor, at once. The first
five pages should suffice. The reader may continue with 2.2 and later return to
what is needed.
Mathematics and some other disciplines such as computer science often consider
domains of individuals in which certain relations and operations are singled out.

When using the language of propositional logic, our ability to talk about the
properties of such relations and operations is very limited. Thus, it is necessary
to refine our linguistic means of expression, in order to procure new possibilities
of description. To this end, one needs not only logical symbols but also variables
for the individuals of the domain being considered, as well as a symbol for
equality and symbols for the relations and operations in question. First-order
logic, sometimes called also predicate logic, is the part of logic that subjects
properties of such relations and operations to logical analysis.
Linguistic particles such as “for all” and “there exists” (called quantifiers)
play a central role here, whose analysis should be based on a well prepared
semantic background. Hence, we first consider mathematical structures and
classes of structures. Some of these are relevant both to logic (in particular
model theory) and to computer science. Neither the newcomer nor the advanced
student needs to read all of 2.1, with its mathematical flavor, at once. The first
five pages should suffice. The reader may continue with 2.2 and later return to
what is needed.
Next we home in on the most important class of formal languages, the first-
order languages, also called elementary languages. Their main characteristic is a
restriction of the quantification possibilities. We discuss in detail the semantics
of these languages and arrive at a notion of logical consequence from arbitrary
premises. In this context, the notion of a formalized theory is made more precise.
Finally, we treat the introduction of new notions by explicit definitions and
other expansions of a language, for instance by Skolem functions. Not until
Chapter 3 do we talk about methods of formal logical deduction. While a
multitude of technical details have to be considered in this chapter, nothing is
especially profound. Anyway, most of it is important for the undertakings of the
subsequent chapters.
2.1 Mathematical Structures
By a structure 
 we understand a nonempty set A together with certain
distinguished relations and operations of A, as well as certain constants
distinguished therein. The set A is also termed the domain of 
, or its universe.
The distinguished relations, operations, and constants are called the (basic)
relations, operations, and constants of 
. A finite structure is one with a finite
domain. An easy example is ({0, 1}, ∧, ∨, ¬). Here ∧, ∨, ¬have their usual
meanings on the domain {0, 1}, and no distinguished relations or constants
occur. An infinite structure has an infinite domain. 
 is an

example with the domain ℕ; here <, +, ⋅, 0, 1 have again their ordinary meaning.
Without having to say so every time, for a structure 
 the corresponding
letter A will always denote the domain of 
; similarly B denotes the domain of 
, etc. If 
 contains no operations or constants, then 
 is also called a
relational structure. If 
 has no relations it is termed an algebraic structure, or
simply an algebra. For example, 
 is a relational structure, whereas 
is an algebraic structure, the additive group  (it is customary to use here the
symbol  as well). Also the set of propositional formulas from 1.1 can be
understood as an algebra, equipped with the operations 
, 
, and α↦ ¬α. Thus, one may speak of the formula algebra 
whenever it is useful to do so.
Despite our interest in specific structures, whole classes of structures are also
often considered, for instance the classes of groups, rings, fields, vector spaces,
Boolean algebras, and so on. Even when initially just a single structure is
viewed, call it the paradigm structure, one often needs to talk about similar
structures in the same breath, in one language, so to speak. This can be achieved
by setting aside the concrete meaning of the relation and operation symbols in
the paradigm structure and considering the symbols in themselves, creating
thereby a formal language that enables one to talk at once about all structures
relevant to a topic. Thus, one distinguishes in this context clearly between
denotation and what is denoted. To emphasize this distinction, for instance for 
, it is better to write 
, where 
, and 
 mean the relation, operation, and constant denoted by +, <, and 0 in 
. Only
if it is clear from the context what these symbols denote may the superscripts be
omitted. In this way we are free to talk on the one hand about the structure 
,
and on the other hand about the symbols +, <, 0.
A finite or infinite set L resulting in this way, consisting of relation,
operation, and constant symbols of given arity, is called an extralogical
signature. For the class of all groups (see page 15), L = { ∘, e} exemplifies a
favored signature; that is, one often considers groups as structures of the form
(G, ∘, e), where ∘ denotes the group operation and e the unit element. But one
can also define groups as structures of the signature { ∘ }, because e is definable
in terms of ∘, as we shall see later. Of course, instead of ∘, another operation
symbol could be chosen such as ⋅, ∗, or +. The latter is mainly used in
connection with commutative groups. In this sense, the actual appearance of a
symbol is less important; what matters is its arity. r ∈ L always means that r is a
relation symbol, and f ∈ L that f is an operation symbol, each time of some arity

n > 0, which of course depends on the symbols r and f, respectively.1
An L-structure is a pair 
, where 
 contains for every r ∈ L a
relation 
 on A of the same arity as r, for every f ∈ L an operation 
 on A of
the arity of f, and for every c ∈ L a constant 
. We may omit the
superscripts, provided it is clear from the context which operation or relation on
A is meant. We occasionally shorten also the notation of structures. For instance,
we sometimes speak of the ring  or the field  provided there is no danger of
misunderstanding.
Every structure is an L-structure for a certain signature, namely that
consisting of the symbols for its relations, functions, and constants. But this does
not make the name L-structure superfluous. Basic concepts, such as isomorphism
and substructure, each refer to structures of the same signature. From 2.2 on,
once the first-order language  belonging to L has been defined, L-structures
will mostly be called 
-structures. We then also often say that r, f, or c belongs
to  instead of L.
If A ⊆ B and f is an n-ary operation on B then A is closed under f, briefly f-
closed, if 
 ∈ A for all 
 ∈ A n . If n = 0, i.e., if f is a constant c, this simply
means c ∈ A. The intersection of any nonempty family of f-closed subsets of B is
itself f-closed. Accordingly, we can talk of the smallest (the intersection) of all f-
closed subsets of B that contain a given subset E ⊆ B. All of this extends in a
natural way if f is here replaced by an arbitrary family of operations of B.
Example. For a given positive m, the set 
 of integers
divisible by m is closed in  under +, −, and ⋅, and is in fact the smallest such
subset of  containing m.
The restriction of an n-ary relation r B ⊆ B n to a subset A ⊆ B is r A = r B ∩
A n . For instance, the restriction of the standard order of  to 
 is the standard
order of 
. Only because of this fact can the same symbol be used to denote
these relations. The restriction f A of an operation f B on B to a set A ⊆ B is
defined analogously whenever A is f-closed. Simply let 
 for 
.
For instance, addition in  is the restriction of addition in  to 
, or addition in 
 is an extension of this operation in . Again, only this state of affairs allows
us to denote the two operations by the same symbol.
Let  be an L-structure and let A ⊆ B be nonempty and closed under all
operations of 
; this will be taken to include 
 for constant symbols c ∈

L. To such a subset A corresponds in a natural way an L-structure 
,
where 
 and 
 for r, f ∈ L are the restrictions of 
 respectively 
 to A.
Finally, let 
 for c ∈ L. The structure 
 so defined is then called a
substructure of 
, and  is called an extension of 
, in symbols 
. This is
a certain abuse of ⊆ but it does not cause confusion, since the arguments
indicate what is meant.
 implies A ⊆ B but not conversely, in general. For example, 
 is a substructure of 
 since 
 is closed under
addition in  and 0 has the same meaning in 
 and 
. Here we dropped the
superscripts for <, +, and 0 because there is no risk of misunderstanding.
A nonempty subset G of the domain B of a given L-structure 
 defines a
smallest substructure 
 of  containing G. The domain of 
 is the smallest
subset of B containing G and closed under all operations of B. 
 is called the
substructure generated from G in . For instance, 
 (= 
) is the
domain of the substructure generated from {3} in 
, since 3ℕ contains 0
and 3, is closed under +, and is clearly the smallest such subset of . A structure 
 is called finitely generated if for some finite G ⊆ A the substructure generated
from G in 
 coincides with 
. For instance, 
 is finitely generated by
G = { 1}.
If 
 is an L-structure and L sb0 ⊆ L then the L sb0-structure 
 with domain
A and where 
 for all symbols ζ ∈ L sb0 is termed the L 0 -reduct of 
,
and 
 is called an L-expansion of 
. For instance, the group 
 is the { +,
0}-reduct of the ordered ring 
. The notions reduct and substructure
must clearly be distinguished. A reduct of 
 has always the same domain as 
,
while the domain of a substructure of 
 is as a rule a proper subset of A.
Below we list some frequently cited properties of a binary relation ◃ in a set
A. It is convenient to write a◃b instead of (a, b) ∈ ◃, and a⋪b for (a, b)∉ ◃. Just
as a < b < c often stands for a < b & b < c, we write a◃b◃c for a◃b & b◃c. In the
listing below, ‘for all a’ and ‘there exists an a’ respectively mean ‘for all a ∈ A’
and ‘there exists some a ∈ A’. The relation ◃ ⊆ A 2 is called
Reflexive, transitive, and symmetric relations are also called equivalence
relations. These are often denoted by ∼, ≈, ≡, ≃, or similar symbols. Such a
relation generates a partition of its domain whose parts, consisting of mutually
equivalent elements, are called equivalence classes.

reflexive
if a◃a for all a,
irreflexive
if a⋪a for all a,
symmetric
if a◃b  ⇒   b◃a, for all a, b,
antisymmetric if a◃b◃a  ⇒   a = b, for all a, b,
transitive
if a◃b◃c  ⇒   a◃c, for all a, b, c,
connex
if a = b or a◃b or b◃a, for all a, b.
We now present an overview of classes of structures to which we will later
refer, mainly in Chapter 5. Hence, for the time being, the beginner may skip the
following and jump to 2.2.
1. Graphs, partial orders, and orders. A relational structure (A, ◃) with
some relation ◃  ⊆ A 2 is often termed a (directed) graph. If ◃ is irreflexive and
transitive we usually write < for ◃ and speak of a (strict) partial order or a
partially ordered set, also called a poset for short. If we define x ≤ y by x < y or x
= y, then ≤ is reflexive, transitive, and antisymmetric, called a reflexive partial
order, the one that belongs to <. If one starts with a reflexive partial order on A
and defines x < y by x ≤ y & x≠y, then (A, < ) is clearly a poset.
A connex partial order 
 is called a total or linear order, also termed
an ordered or a strictly ordered set. , , 
, 
 are examples with respect to
their standard orders. Here we follow the traditional habit of referring to ordered
sets by their domains only.
Let U be a nonempty subset of some ordered set A such that for all a, b ∈ A, 
. Such a U is called an initial segment of A. In addition, let V :
= A ∖ U≠∅. Then the pair (U, V ) is called a cut. The cut is said to be a gap if U
has no largest and V no smallest element. However, if Uhas a largest element a,
and V a smallest element b, then (U, V ) is called a jump. b is in this case called
the immediate successor of a, and athe immediate predecessor of b, because then
there is no element from A between a and b. An infinite ordered set without gaps
and jumps, like 
, is said to be continuously ordered. Such a set is easily seen to
be densely ordered, i.e., between any two elements lies another one.
A totally ordered subset K of a partially ordered set H is called a chain in H.
Such a K is said to be bounded (to the above) if there is some b ∈ H with a ≤ b
for all a ∈ K. Call c ∈ Hmaximal in H if no a ∈ H exists with a > c. An infinite
partial order need not have a maximal element, nor need all chains be bounded,
as is seen by the example (ℕ, < ). With these notions, a basic mathematical tool
can now be stated: Zorn’s lemma. If every chain in a nonempty poset H is
bounded then H has a maximal element.
A (totally) ordered set A is well-ordered if every nonempty subset of A has a

smallest element; equivalently, there are no infinite decreasing sequences a sb0 >
a sb1 > ⋯ of elements from A. Clearly, every finite ordered set is well-ordered.
The simplest example of an infinite well-ordered set is  together with its
standard order.
2. Groupoids, semigroups, and groups. Algebras 
 with an
operation ∘   A 2 → A are termed groupoids. If ∘ is associative then 
 is called a
semigroup, and if ∘ is additionally invertible, then 
 is said to be a group. It is
provable that a group (G, ∘ ) in this sense contains exactly one unit element, that
is, an element e such that 
 for all x ∈ G, also called a neutral
element. A well-known example is the group of bijections of a set M. If the
group operation ∘ is commutative, we speak of a commutative or abelian group.
Here are some examples of semigroups that are not groups: (a) the set of
strings on some alphabet A with respect to concatenation, the word-semigroup or
free semigroup generated from A. (b) the set M M of mappings from M to itself
with respect to composition. (c) 
 and 
; these two are commutative
semigroups. With the exception of (M M , ∘ ), all mentioned examples of
semigroups are regular, which is to mean 
, and 
, for all x, y, z.
Substructures of semigroups are again semigroups. Substructures of groups
are in general only semigroups, as seen from 
. Not so in the
signature 
, where e denotes the unit element and x − 1 the inverse of x.
Here all substructures are indeed subgroups. The reason is that in 
, the
group axioms can be written as universally quantified equations, where for
brevity, we omit the writing of “for all x, y, z,” namely as 
. These equations certainly retain their
validity in the transition to substructures. We mention that from the last three
equations, e ∘ x = x and 
 are derivable, although ∘ is not supposed to
be commutative.
Ordered semigroups and groups possess along with ∘ some order, with
respect to which ∘ is monotonic in both arguments, like 
. A
commutative ordered semigroup (A, +, 0, ≤ ) with zero element 0, which at the
same time is the smallest element in A, and where a ≤ b iff there is some c with 
, is called a domain of magnitude. Everyday examples are the domains
of length, mass, money, etc.

(1)
3. Rings and fields. These belong to the most commonly known structures.
Below we list the axioms for the theory T F of fields in +, ⋅, 0, 1. A field is a
model of T F. A ring is a model of the axiom system T R for rings that derives
from T F by dropping the constant 1 from the signature and the axioms N×, C×,
and I× from T F. Here are the axioms of T F:
In view of { C}×, axiom D′ is dispensable for T F but not for T R. When
removing I+ from T R, we obtain the theory of semirings. A well-known example
is 
. A commutative ring that has a unit element 1 but no zero-divisor
(i.e., ¬ ∃x ∃y(x, y≠0 ∧ x ⋅ y = 0) is called an integral domain. A typical example
is 
.
Let 
 be any fields with 
. We call a ∈ K′ ∖ Kalgebraic or
transcendental on 
, depending on whether a is a zero of a polynomial with
coefficients in K or not. If every polynomial of degree ≥ 1 with coefficients in K
breaks down into linear factors, as is the case for the field of complex numbers,
then 
 is called algebraically closed, in short, 
 is a.c. These fields will be
more closely inspected in 3.3 and Chapter 5. Each field 
 has a smallest subfield
, called a prime field. One says that 
 has characteristic 0 or p (a prime
number), depending on whether 
 is isomorphic to the field 
 or the finite field
of p elements. No other prime fields exist. It is not hard to show that 
 has the
characteristic p iff the sentence 
 holds in 
.
Rings, fields, etc. may also be ordered, whereby the usual monotonicity laws
are required. For example, 
 is the ordered ring of integers and 
 the ordered semiring of natural numbers.
4. Semilattices and lattices. 
 is called a semilattice if ∘ is
associative, commutative, and idempotent. An example is ({0, 1}, ∘ ) with ∘ = ∧.
If we define a ≤ b : ⇔ a ∘ b = a then ≤ is a reflexive partial order on A.
Reflexivity holds, since a ∘ a = a. As can be easily verified, a ∘ b is in fact the
infimum of a, b with respect to ≤, a ∘ b = inf{a, b}, that is, a ∘ b ≤ a, b, and c ≤
a, b ⇒ c ≤ a∘b, for all a, b, c ∈ A.

 is called a lattice if (A, ∩ ) and (A, ∪) are both semilattices and
the following so-called absorption laws hold: a ∩ (a ∪b) = a and a ∪(a ∩ b) =
a. These imply 
. As above, a ≤ b : ⇔ a ∩ b = a defines a
partial order such that a ∩ b = inf{a, b}. In addition, one has a ∪b = sup{a, b}
(the supremum of a, b), which is to mean a, b ≤ a ∪b, and a, b ≤ c  ⇒   a ∪b ≤ c,
for all c ∈ A. If 
 satisfies, moreover, the distributive lawsx ∩ (y ∪c) = (x ∩ y)
∪(x ∩ c) and x ∪(y ∩ c) = (x ∪y) ∩ (x ∪c), then 
 is termed a distributive
lattice. For instance, the power set 
 with the operations ∩ and ∪for ∩ and
∪respectively is a distributive lattice, as is every nonempty family of subsets of
M closed under ∩ and ∪, a so-called lattice of sets. Another important example
is 
. Here gcd(a, b) and lcm(a, b) denote the greatest common divisor
and the least common multiple of 
.
5. Boolean algebras. An algebra 
 where (A, ∩, ∪) is a
distributive lattice and in which at least the equations
are valid is called a Boolean algebra. The paradigm structure is the two-
element Boolean algebra 2 : = ({0, 1}, ∧, ∨, ¬), with ∩, ∪interpreted as ∧, ∨,
respectively. One defines the constants 0 and 1 by 0 : = a ∩ ¬a for any a ∈ A and
1 : = ¬0. There are many ways to characterize Boolean algebras 
, for instance,
by saying that 
 satisfies all equations valid in 2. The signature can also be
variously selected. For example, the signature ∧, ∨, ¬is well suited to deal
algebraically with two-valued propositional logic. Terms of this signature are, up
to the denotation of variables, precisely the Boolean formulas from 1.1, and a
valid logical equivalence α ≡ β corresponds to the equation α = β, valid in 2.
Further examples of Boolean algebras are the algebras of sets 
.
Here A consists of a nonempty system of subsets of a set I, closed under ∩, ∪,
and ¬(complementation in I). These are the most general examples; a famous
theorem, Stone’s representation theorem, says that each Boolean algebra is
isomorphic to an algebra of sets.
6. Logical L-matrices. These are structures 
, where L
contains only operation symbols (the “logical” symbols) and D denotes a unary
predicate, the set of distinguished values of 
. Best known is the two-valued
Boolean matrix 
 with 
. The consequence relation 
 in
the propositional language 
 of signature L is defined as in the two-valued case:

Let 
 and φ ∈ ℱ. Then 
 if 
 for every 
 with 
 (
). In words, if the values of all α ∈ X are
distinguished, then so too is the value of φ.
Homomorphisms and isomorphisms. The following notions are important
for both mathematical and logical investigations. Much of the material presented
here will be needed in Chapter 5. In the following definition, n ( > 0) denotes as
always the arity of f or r.
Definition. Let 
 be L-structures and 
 (strictly speaking 
) a mapping such that for all f, c, r ∈ L and 
,
Then h is called a homomorphism. If the third condition in (H) is replaced by
the stronger condition 
  2then h is said to
be a strong homomorphism (for algebras, the word “strong” is dispensable). An
injective strong homomorphism 
 is called an embedding of 
 into 
. If,
in addition, h is bijective then h is called an isomorphism, and in case 
, an
automorphism.
An embedding or isomorphism 
 satisfies 
. Indeed,
since 
, (S) yields 
. 
 are said to be isomorphic, in symbols 
, if there is an isomorphism
from 
 to . It is readily verified that ≃ is reflexive, symmetric, and transitive,
hence an equivalence relation on the class of all 
-structures.
Examples 1. (a) A valuation w considered in 1.1 can be regarded as a
homomorphism of the propositional formula algebra ℱ into the two-element
Boolean algebra 2. Such a w ℱ → 2 is necessarily onto.
(b) Let 
 be a word semigroup with the concatenation operation ∗
and 
 the additive semigroup of natural numbers, considered as L-structures for
L = { ∘} with 
 and 
. Let lh(ξ) denote the length of a word or
string 
. Then ξ↦lh(ξ) is a homomorphism since 
, for
all ξ, η ∈ A. If 
 is generated from a single letter, lh is evidently bijective, hence
an isomorphism.
(c) The mapping a↦(a, 0) from  to 
 (= set of complex numbers,
understood as ordered pairs of real numbers) is a good example of an embedding
of the field 
 into the field 
. Nonetheless, in this case, we are used to saying

that 
 is a subfield of , and that 
 is a subset of .
(d) Let 
 be the ordered additive group of real numbers and 
 the multiplicative group of positive reals. Then for any 
there is precisely one isomorphism 
 such that η1 = b, namely η x↦b x ,
the exponential function expb to the base b. It is even possible to define expb as
this isomorphism, by first proving that—up to isomorphism—there is only one
continuously ordered abelian group (first noticed in [Ta2] though not explicitly
put into words).
(e) The algebras 
 and 
 are only apparently
different, but are in fact isomorphic, with the isomorphism δ where δ0 = 1, δ1 =
0. Thus, since 
 is a group, 
 is a group as well, which is not obvious at first
glance. By adjoining the unary predicate D = { 1}, 
 and 
 become
(nonisomorphic) logical matrices. These actually define the two “dual”
fragmentary two-valued logics for the connectives either … or …, and … if and
only if …, which have many properties in common.
Congruences. A congruence relation (or simply a congruence) in a structure
 of signature L is an equivalence relation ≈ in A such that for all n > 0, all f ∈ L
of arity n, and all 
,
Here 
 means a i ≈ b i for i = 1, …, n. A trivial example is the identity in 
. If 
 is a homomorphism then 
, defined by a ≈ h b ⇔ ha =
hb, is a congruence in 
, called the kernel of h. Let A′ be the set of equivalence
classes a ⁄ ≈ : = { x ∈ A∣ a ≈ x} for a ∈ A, also called the congruence classes of
≈, and set 
 for 
. Define 
and let 
. These definitions are sound, that is, independent
of the choice of the n-tuple  of representatives. Then A′ becomes an L-structure 
, the factor structure of 
 modulo ≈, denoted by 
. Interesting, in
particular for Chapter 5, is the following very general and easily provable
Homomorphism theorem. Let 
 be L-structure and ≈ a congruence in 
 .
Then k a↦a⁄≈ is a strong homomorphism from 
 onto 
 , the canonical
homomorphism. Conversely, if 
 is a strong homomorphism from 
 onto
an  -structure 
 with kernel ≈ then ı : a⁄≈↦ha is an isomorphism from 
 to

 , and h = ı∘ k.
Proof. We omit here the superscripts for f and r just for the sake of legibility.
Clearly, 
, and 
 by definition. Hence k is
what we claimed. The definition of ı is sound, and ı is bijective since 
. Furthermore, ı is an isomorphism because
 and 
 ⁄ ≈ ⇔ 
 ⇔ r ı(  ⁄ ≈ ).
Finally, h is the composition ı∘k by the definitions of ı and k.
Remark. For algebras 
, this theorem is the usual homomorphism theorem
of universal algebra. 
 is then named the factor algebra. The theorem covers
groups, rings, etc. In groups, the kernel of a homomorphism is already
determined by the congruence class of the unit element, called a normal
subgroup, in rings by the congruence class of 0, called an ideal. Hence, in
textbooks on basic algebra the homomorphism theorem is separately formulated
for groups and rings, but is easily derivable from the general theorem present
here.
Direct products. These provide the basis for many constructions of new
structures, especially in 5.7. A well-known example is the n-dimensional vector
group 
. This is the n-fold direct product of the group 
 with
itself. The addition in 
 is defined componentwise, as is also the case in the
following
Definition. Let 
 be a nonempty family of L-structures. The direct
product 
 is the structure defined as follows: Its domain is 
,
called the direct product of the sets A i. The elements 
 of B are
functions defined on I with a i ∈ A i for each i ∈ I. Relations and operations in 
are defined componentwise, that is,
where 
 (here the superscripts count the components)
with 
 for ν = 1, …, n, and 
.
Whenever 
 for all 
, then 
 is denoted by 
 and called a

direct power of the structure 
. Note that 
 is embedded in 
 by the
mapping 
, where 
 denotes the I-tuple with the constant value a,
that is, 
. For 
, the product 
 is also written as
. If 
 one mostly writes 
 for 
.
Examples 2. (a) Let I = { 1, 2}, 
, and 
. Then 
, for all 
. Note that if 
 are
ordered sets then 
 is only a partial order. The deeper reason for this observation
will become clear in Chapter 5.(b) Let 
 be a direct power of the two-
element Boolean algebra 2. The elements a ∈ B are I-tuples of 0 and 1. These
uniquely correspond to the subsets of I via the mapping 
. As a matter of fact, ı is an isomorphism from 
 to 
, as can readily
be verified; Exercise 4.
Exercises
1. Show that there are (up to isomorphism) exactly five two-element proper
groupoids. Here a groupoid (H, ⋅ ) is termed proper if the operation ⋅ is
essentially binary.
 
2. ≈ ( ⊆ A 2) is termed Euclidean if a ≈ b & a ≈ c  ⇒   b ≈ c, for all a, b, c
∈ A. Show that ≈ is an equivalence relation in A if and only if ≈ is
reflexive and Euclidean.
 
3. Prove that an equivalence relation ≈ on an algebraic L-structure 
 is a
congruence iff for all f ∈ L of arity n, all 
, and all 
 with a ≈ a′, 
.
 
4. Prove in detail that 
 for a nonempty index set I. Prove
the corresponding statement for any subalgebra of 2 I .
 
5. Show that 
 with ha = a j is a homomorphism for each j ∈
I.
 

2.2 Syntax of First-Order Languages
Standard mathematical language enables us to talk precisely about structures,
such as the field of real numbers. However, for logical (and metamathematical)
issues it is important to delimit the theoretical framework to be considered; this
is achieved most simply by means of a formalization. In this way one obtains an
object language; that is, the formalized elements of the language, such as the
components of a structure, are objects of our consideration. To formalize
interesting properties of a structure in this language, one requires at least
variables for the elements of its domain, called individual variables. Further are
required sufficiently many logical symbols, along with symbols for the
distinguished relations, functions, and constants of the structure. These
extralogical symbols constitute the signature L of the formal language that we
are going to define.
In this manner one arrives at the first-order languages, also termed
elementary languages. Nothing is lost in terms of generality if the set of
variables is the same for all elementary languages; we denote this set by { Var}
and take it to consist of the countably many symbols 
 Two such
languages therefore differ only in the choice of their extralogical symbols.
Variables for subsets of the domain are consciously excluded, since languages
containing variables both for individuals and sets of these individuals—second-
order languages, discussed in 3.8—have different semantic properties from those
investigated here.
We first determine the alphabet, the set of basic symbols of a first-order
language determined by a signature L. It includes, of course, the already
specified variables 
 In what follows, these will mostly be denoted by x,
y, z, u, v, though sometimes other letters with or without indices may serve the
same purpose. The boldface printed original variables are useful in writing down
a formula in the variables 
, for these can then be denoted, for instance,
by v sb1, …, v n, or by 
.
Further, the logical symbols ∧ (and), ¬(not),  (for all), the equality sign =,
and, of course, all extralogical symbols from L should belong to the alphabet.
Note that the boldface symbol = is taken as a basic symbol; simply taking =
could lead to unintended mix-ups with the metamathematical use of the equality
symbol = (in Chapter 4 also identity-free languages without = will be
considered). Finally, the parentheses (,) are included in the alphabet. Other
symbols are introduced by definition, e.g., ∨, →, ↔ are defined as in 1.4 and the
symbols ∃(there exists) and ∃! (there exists exactly one) will be defined later. Let

 denote the set of all strings made up of symbols that belong to the alphabet
of 
.
From the set 
 of all strings we pick out the meaningful ones, namely
terms and formulas, according to certain rules. A term, under an interpretation of
the language, will always denote an element of a domain, provided an
assignment of the occurring variables to elements of that domain has been given.
In order to keep the syntax as simple as possible, terms will be understood as
certain parenthesis-free strings, although this kind of writing may look rather
unusual at the first glance.
Terms in L:
(T1) Variables and constants, considered as atomic strings, are terms, also
called prime terms.
(T1) If f ∈ L is n-ary and 
 are terms, then ft sb1⋯t n is a term.
This is a recursive definition of the set of terms as a subset of 
. Any string
that is not generated by (T1) and (T2) is not a term in this context (cf. the related
definition of 
 in 1.1). Parenthesis-free term notation simplifies the syntax, but
for binary operations we proceed differently in practice and write, for example,
the term 
. The reason is that a high density of information in the
notation complicates reading. Our brain does not process information
sequentially like a computer. Officially, terms are parenthesis-free, and the
parenthesized notation is just an alternative way of rewriting terms. Similarly to
the unique reconstruction property of propositional formulas in 1.1, here the
unique term reconstruction property holds, that is, ft 1 ⋯t n = fs 1 ⋯s n implies s
i = t i for i = 1,…,n (t i, s i terms),
which immediately follows from the unique term concatenation property
t 1 ⋯t n = s 1 ⋯s m implies n = m and t i = s i for i = 1,…,n.
The latter is shown in Exercise 2. 
 denotes the set of all terms of a
given signature L. Variable-free terms, which can exist only with the availability
of constant symbols, are called constant terms or ground terms, mainly in logic
programming. With the operations given in 
 by setting 
, 
 forms an algebra, the term algebra. From the
definition of terms immediately follows the useful Principle of proof by term
induction. Let  be a property of strings such that  holds for all prime terms,
and for each n > 0 and each n-ary function symbol f, the assumptions 
 imply 
 . Then all terms have the property  .

Indeed, 
 is by definition the smallest set of strings satisfying the conditions
of this principle, and hence a subset of the set of all strings with the property .
A simple application of term induction is the proof that each compound term t is
a function term in the sense that t = ft sb1⋯t n for some n-ary function symbol f
and some terms t sb1, …, t n. Simply consider the property ‘t is either prime or a
function term’. Term induction can also be executed on certain subsets of 
, for
instance on ground terms.
We also have at our disposal a definition principle by term recursion which,
rather than defining it generally, we present through examples. The set { var} t
of variables occurring in a term t is recursively defined by
{ var} t, and even { var} ξ for any 
, can also be defined explicitly using
concatenation. { var} ξ is the set of all x ∈ { Var} for which there are strings 
with 
. The notion of a subterm of a term can also be defined recursively.
Again, we can also do it more briefly using concatenation. Definition by term
induction should more precisely be called definition by term recursion. But most
authors are sloppy in this respect.
We now define recursively those strings of the alphabet of L to be called
formulas, also termed (first-order) expressions or well-formed formulas.
Formulas in L:
(F1) If s, t are terms, then the string s = t is a formula.
(F2) If 
 are terms and r ∈ L is n-ary, then rt sb1⋯t n is a formula.
(F3) If α, β are formulas and x is a variable, then (α ∧ β), ¬α, and 
 are
formulas.
Any string not generated according to (F1), (F2), (F3) is in this context not a
formula. Other logical symbols serve throughout merely as abbreviations,
namely ∃xα : = ¬ ∀x ¬α, (α  ∨  β) : = ¬( ¬α ∧ ¬β), and as in 1.1, (α → β) : = ¬(α
∧ ¬β), and (α ↔ β) : = ((α → β) ∧ (β → α)). In addition, s≠t will throughout be
written for ¬ s = t. The formulas ∀xα and ∃xα are said to arise from α by
quantification.
Examples. (a) 
 (more explicitly, 
) is a
formula, expressing ‘for all x there exists a y such that 
’. Here we
assume tacitly that x, y denote distinct variables. The same is assumed in all of
the following whenever this can be made out from the context.
(b) ∀x ∀x x = y is a formula, since repeated quantification of the same
variable is not forbidden. ∀z x = y is a formula also if z≠x, y, although z does

then not appear in the formula x = y.
Example (b) indicates that the grammar of our formal language is more
liberal than one might expect. This will spare us a lot of writing. The formulas
∀x ∀x x = y and ∃x ∀x x = y both have the same meaning as 
. These three
formulas are logically equivalent (in a sense still to be defined), as are ∀z x = y
and x = y. It would be to our disadvantage to require any restriction here. In spite
of this liberality, the formula syntax corresponds roughly to the syntax of natural
language.
The formulas procured by (F1) and (F2) are said to be prime or atomic
formulas, or simply called prime. As in propositional logic, prime formulas and
their negations are called literals.
Prime formulas of the form s = t are called equations. These are the only
prime formulas if L contains no relation symbols, in which case L is called an
algebraic signature. Prime formulas that are not equations begin with a relation
symbol, although in practice a binary symbol tends to separate the two
arguments as, for example, in x ≤ y. The official notation is, however, that of
clause (F2). The unique term concatenation property clearly implies the unique
prime formula reconstruction property
The set of all formulas in L is denoted by 
. If L = { ∈ } or L = { ∘ } then 
 is
also denoted by ℒ ∈ or ℒ∘, respectively. If L is more complex, e.g. L = { ∘, e},
we write 
. The case L = ∅ is also permitted; it defines the language of
pure identity, denoted by ℒ = = = =.
Instead of terms, formulas, and structures of signature L, we will talk of 
-
terms (writing 
 for 
), 
-formulas, and -structures respectively. We also
omit the prefix if 
 has been given earlier and use the same conventions of
parenthesis economy as in 1.1. We will also allow ourselves other informal aids
in order to increase readability. For instance, variously shaped brackets may be
used as in ∀x ∃y ∀z[z ∈ y ↔ ∃u(z ∈ u ∧ u ∈ x)]. Even verbal descriptions
(partial or complete) are permitted, as long as the intended formula is uniquely
recognizable.
The strings ∀x and ∃x (read “for all x” respectively “there is an x”) are called
prefixes. Also concatenations of these such as ∀x ∃y are prefixes. No other
prefixes are considered here. Formulas in which 
 do not occur are termed
quantifier-free or open. These are the Boolean combinations of prime formulas.
Generally, the Boolean combinations of formulas from a set X ⊆ ℒ are the ones
generated by ¬, ∧ (and ∨ ) from those of X.

X, Y, Z always denote sets of formulas, 
 denote formulas, and
s, t terms, while Φ, Ψ are reserved to denote finite sequences of formulas and
formal proofs. Substitutions (to be defined below) will be denoted by σ, τ, ω, ρ,
and ι.
Principles of proof by formula induction and of definition by formula
induction (more precisely formula recursion) also exist for first-order and other
formal languages. After the explanation of these principles for propositional
languages in 1.1, it suffices to present here some examples, adhering to the
maxim verba docent, exempla trahunt. Formula recursion is based on the
unique formula reconstruction, which is similar to the corresponding property in
1.1: Each composed φ ∈ ℒ can uniquely be written as φ = ¬α, φ = (α ∧ β), or
∀xα for some α, β ∈ ℒ and x ∈ { Var}. A simple example of a recursive
definition is rkφ, the rank of a formula φ. Starting with rkπ = 0 for prime
formulas π it is defined as on page 8, with the additional clause 
.
Functions on 
 are sometimes defined by recursion on rkφ, not on φ, as for
instance on page 60.
Useful for some purposes is also the quantifier rank, qrφ. It represents a
measure of nested quantifiers in φ. For prime π let qrπ = 0, and let 
.
Note that 
. A subformula of a formula is defined
analogously to the definition in 1.1. Hence, we need say no more on this. We
write x ∈ { bnd} φ (or x occurs bound in φ) if φ contains the prefix ∀x. In
subformulas of φ of the form ∀xα, the formula α is called the scope of ∀x. The
same prefix can occur repeatedly and with nested scopes in φ, as for instance in
∀x( ∀x x = 0 ∧ x < y). In practice we avoid this way of writing, though for a
computer this would pose no problem.
Intuitively, the formulas (a) 
 and (b) 
 are different
in that in every context with a given meaning for + and 0, the former is either
true or false, whereas in (b) the variable x is waiting to be assigned a value. One
also says that all variables in (a) are bound, while (b) contains the “free” variable
x. The syntactic predicate ‘x occurs free in φ’, or ‘x ∈ { free} φ’ is defined
inductively: Let { free} α = { var} α for prime formulas α ({ var} α was defined
on page 32), and
For instance, 
, while 
equals {x, y}. As the last formula shows, x can occur both free and bound in a
formula. This too will be avoided in practice whenever possible. In some proof-

theoretically oriented presentations, even different symbols are chosen for free
and bound variables. Each of these approaches has its advantages and its
disadvantages.
Formulas without free variables are called sentences, or closed formulas. 
 and 
 are examples. Throughout take
ℒ0 to denote the set of all sentences of 
. More generally, let 
 be the set of all
formulas φ such that 
. Clearly, 
 and 
.
At this point we meet a for the remainder of the book valid
Convention. As long as not otherwise stated, the notation φ = φ(x) means
that the formula φ contains at most x as a free variable; more generally, 
 or φ = φ(
) is to mean 
, where 
stand for arbitrary but distinct variables. Not all of these variables need actually
occur in φ. Further, t = t(
) for terms t is to be read completely analogously.
The term ft sb1⋯t n is often denoted by f 
, the prime formula rt sb1⋯t n by
. Here  denotes the string concatenation t sb1⋯t n. Fortunately, 
 behaves
exactly like the sequence (t sb1, …, t n) as was pointed out already; it has the
unique term concatenation property, see page 31.
Substitutions. We begin with the substitution 
 of some term t for a single
variable x, called a simple substitution. Put intuitively, 
 (also denoted by φx(t)
and read “φ t for x”) is the formula that results from replacing all free
occurrences of x in φ by the term t. This intuitive characterization is made
precise recursively, first for terms by
where, for brevity, t i ′ stands for t i , and next for formulas as follows:
Then also 
, and the corresponding holds for ∨, while 
 for y = x, and ∃y(α  ) otherwise. Simple substitutions are special
cases of so-called simultaneous substitutions

For brevity, this will be written 
 or 
 or just φ(  ), provided there is no
danger of misunderstanding. Here the variables x i are simultaneously replaced
by the terms t i at free occurrences. Simultaneous substitutions easily generalize
to global substitutions σ. Such a σ assigns to every variable x a term 
. It
extends to the whole of 
 by the clauses 
 and 
, and
subsequently to  by recursion on rkφ, so that σ is defined for the whole of 
: 
, 
, 
, 
, and 
, where τ is defined by 
 and 
 for y≠x. 3
These clauses cover also the case of a simultaneous substitution, because 
can be identified with the global substitution σ such that 
 for i = 1, …, n
and 
 otherwise. In other words, a simultaneous substitution can be
understood as a global substitution σ such that 
 for almost all variables x,
i.e., with the exception of finitely many. The identical substitution, always
denoted by ι, is defined by 
 for all x; hence 
 and 
 for all
terms t and formulas φ.
Clearly, a global substitution yields locally, i.e. with respect to individual
formulas, the same as a suitable simultaneous substitution. Moreover, it will turn
out below that simultaneous substitutions are products of simple ones.
Nonetheless, a separate study of simultaneous substitutions is useful mainly for
Chapter 4.
It always holds that 
, whereas the compositions 
 and 
are distinct, in general. Let us elaborate by explaining the difference between 
 and 
. For example, if one wants to swap x sb1, x sb2 at
their free occurrences in φ then the desired formula is 
, but not, in general, 
 (choose for instance 
). Rather 
 for any 
, as is readily shown by induction on φ after first treating
terms. We recommend to carry out this induction in detail. In the same way we
obtain

(1)
).  
This formula shows that a simultaneous substitution is a suitable product
(composition) of simple substitutions. Conversely, it can be shown that each
such product can be written as a single simultaneous substitution. In some cases
(1) can be simplified. Useful, for example, is the following equation which holds
in particular when all terms t i are variable-free:
(2)
 (x i∉{ var} t j for i≠j).
Getting on correctly with substitutions is not altogether simple; it
requires practice, because our ability to regard complex strings is not
especially trustworthy. A computer is not only much faster but also
more reliable in this respect.
 
Exercises
1. Show by term induction that a terminal segment of a term t is a
concatenation s sb1⋯s m of terms s i for some m ≥ 1. Thus, a symbol in t
is at each position in t the initial symbol of a unique subterm s of t. The
uniqueness of s is an easy consequence of Exercise 2(a).
 
2. Let  be a first-order language, 
, and 
 the property ‘No proper
initial segment of 
 is a term, nor is t a proper initial segment of a
term from 
 ’. Prove (a) 
 for all 
, hence 
 for all 
 and arbitrary 
, and (b) the unique term concatenation
property (page 31).
 
3. Prove (a) No proper initial segment of a formula φ is a formula. (b) The
unique formula reconstruction property stated on page 36. (c) 
 and 
. (c) easily yields (d) 
, for all 
.
 
4. Prove 
 for x∉{ free} φ, and 
 for y∉{ var} φ. It can be
shown that these restrictions are indispensable, provided t≠x.
 

5. Let 
 be a nonempty formula set and 
}.
Show that a Boolean combination of formulas from X is equivalent to a
disjunction of conjunctions of formulas from X ∗.
 
2.3 Semantics of First-Order Languages
Intuitively it is clear that the formula 
 can be allocated a truth value
in the domain 
 only if to the free variable x there corresponds a value in 
.
Thus, along with an interpretation of the extralogical symbols, a truth value
allocation for a formula φ requires a valuation of at least the variables occurring
free in φ. However, it is technically more convenient to work with a global
assignment of values to all variables, even if in a concrete case only the values of
finitely many variables are needed. We therefore begin with the following
Definition. A model 
 is a pair 
 consisting of an 
-structure 
 and a
valuationw { Var} → A, w x↦x w . We denote 
, and x w also by 
, and 
, respectively. The domain of 
 will also called the
domain of 
.
Models are sometimes called interpretations, occasionally also 
-models if
the connection to 
 is to be highlighted. Some authors identify models with
structures from the outset. This also happens in 2.5, where we are talking about
models of theories. The notion of a model is to be maintained sufficiently
flexible in logic and mathematics.
A model 
 allocates in a natural way to every term t a value in A, denoted
by 
 or 
 or just by t w . Clearly, for prime terms the value is already given
by 
. This evaluation extends to compound terms by term induction as follows:
, where 
 abbreviates here the sequence 
.
If the context allows we neglect the superscripts and retain just an imaginary
distinction between symbols and their interpretation. For instance, if 
 and x w = 2, say, we write somewhat sloppily 
.
The value of t under 
 depends only on the meaning of the symbols that
effectively occur in t; using induction on t, the following slightly more general
claim is obtained: if 
 and 
 are models with the same

(.)
domain such that 
 for all x ∈ V and 
 for all remaining
symbols ζ occurring in t, then 
. Clearly, 
 may simply be denoted
by 
, provided the term t contains no variables.
We now are going to define a satisfiability relation ⊨ between models 
 and formulas φ, using induction on φ as in 1.3. We read 
 as
 satisfies φ, or 
 is a model for φ.
Sometimes 
 is written instead of 
. A similar notation, just as
frequently encountered, is introduced later. Each of these notations has its
advantages, depending on the context. If 
 for all φ ∈ X we write 
and call 
 a model for X. For the formulation of the satisfaction clauses below
(taken from [Ta1]) we consider for given 
, x ∈ { Var}, and a ∈ A
also the model ℳax (generalized to 
 below). ℳax differs from ℳ only in
that the variable x receives the value a ∈ A instead of 
. Thus, 
with x w′ = a and y w′ = y w otherwise. The satisfaction clauses then look as
follows:
Remark 1. The last satisfaction clause can be stated differently if a name for
each a ∈ A, say a, is available in the signature: 
 for all a ∈
A. This assumption permits the definition of the satisfaction relation for
sentences using induction on sentences while bypassing arbitrary formulas. If
not every a ∈ A has a name in L, one could “fill up” L in advance by adjoining to
L a name a for each a. But expanding the language is not always wanted and
does not really simplify the matter.
ℳax is slightly generalized to 
, which
differs from ℳ in the values of a sequence x sb1, …, x n of distinct variables. This
and writing 
 for 
 permits a short notation of a useful
generalization of the last clause above, namely

The definitions of α  ∨  β, α → β, and α ↔ β from page 33 readily imply the
additional clauses 
 iff 
 or 
, 
 iff 
, and analogously for ↔. Clearly, if ∨, →, ↔ were treated as
independent connectives, these equivalences would have to be added to the
above ones. Further, the definition of ∃xφ in 2.2 corresponds to its intended
meaning, because 
. Indeed, whenever 
 then 
 does not hold for all a; hence there is some
a ∈ A such that ℳax⊭ ¬φ, or equivalently, ℳax ⊨ φ. And this chain of
reasoning is obviously reversible.
Example 1. 
 for arbitrary 
, provided x∉{ var} t. Indeed, ℳax
⊨ x = t with 
, since 
 in view of x∉{ var} t. The
assumption x∉{ var} t is essential. For instance, 
 holds only if the
function 
 has a fixed point.
We now introduce several fundamental notions that will be treated more
systematically in 2.4 and 2.5, once certain necessary preparations have been
completed.
Definition. A formula or set of formulas in 
 is termed satisfiable if it has a
model. 
 is called generally valid, logically valid, or a tautology, in short, ⊨
φ, if 
 for every model 
. Formulas α, β are called (logically or
semantically) equivalent, in symbols, α ≡ β, if 
, for each 
-
model 
.
Further, let 
 (read φ holds in 
 or 
 satisfies φ) if 
 for all w 
{ Var} → A. One writes 
 in case 
 for all φ ∈ X. Finally, let X ⊨ φ
(read from X follows φ, or φ is a consequence of X) if every model 
 of X also
satisfies the formula φ, i.e., 
.
As in Chapter 1, ⊨ denotes both the satisfaction and the consequence
relation. Here, as there, we write 
 for {φ1, …, φn} ⊨ φ. Note that
in addition, ⊨ denotes the validity relation in structures, which is illustrated by
the following Example 2. We show that 
, where the domain of 
contains at least two elements. Indeed, let 
 and let a ∈ A be given
arbitrarily. Then there exists some b ∈ A with a≠b. Hence, 
, and so ℳax ⊨ ∃y x≠y. Since a was arbitrary, 
. Clearly the actual values of w are irrelevant in this argument.

Hence 
 for all w, that is, 
.
Here some care is needed. While 
 or 
 for all formulas, 
or 
 (the law of the excluded middle for validity in structures) is in general
correct only for sentences φ, as Theorem 3.1 will show. If 
 contains more than
one element, then, for example, neither 
 nor 
. Indeed, x = y is
falsified by any w such that x w ≠y w , and x≠y by any w with x w = y w . This is
one of the reasons why models were not simply identified with structures.
For 
 let φg be the sentence 
, where 
 is an
enumeration of { free} φ according to index size, say. φg is called the
generalized of φ, also called its universal closure. For φ ∈ ℒ0 clearly φg = φ.
From the definitions immediately results
(1)
,  
and more generally, 
. (1) explains why φ and
φg are often notionally identified, and the information that formally runs φg is
often shortened to φ. It must always be clear from the context whether our eye is
on validity in a structure, or on validity in a model with its fixed valuation. Only
in the first case can a generalization (or globalization) of the free variables be
thought of as carried out. However, independent of this discussion, ⊨ φ  ⇔   ⊨
φg always holds.
Even after just these incomplete considerations it is already clear that
numerous properties of structures and whole systems of axioms can adequately
be described by first-order formulas and sentences. Thus, for example, an axiom
system for groups in ∘, e, − 1, mentioned already in 2.1, can be formulated as
follows:
Precisely, the sentences that follow from these axioms form the elementary
group theory in ∘ ,e, −1. It will be denoted by 
. In the sense elaborated in
Exercise 3 in 2.6 an equivalent formulation of the theory of groups in ∘, e,
denoted by T G, is obtained if the third 
-axiom is replaced by ∀x ∃y x∘y =
e. Let us mention that ∀xe∘x = x and ∀x ∃y y∘x = e are provable in T G and also
in 
.
An axiom system for ordered sets can also easily be provided, in that one
formalizes the properties of being irreflexive, transitive, and connex. Here and
elsewhere, ∀x sb1⋯x nφ stands for 
:

In writing down these and other axioms the outer -prefixes are very often
omitted so as to save on writing, and we think implicitly of the generalization of
variables as having been carried out. This kind of economical writing is
employed also in the formulation of (1) above, which strictly speaking runs ‘for
all 
 ’.
For sentences α of a given language it is intuitively clear that the values of
the variables of w for the relation 
 are irrelevant. The precise proof is
extracted from the following theorem for V = ∅. Thus, either 
 for all
w and hence 
, or else 
 for no w, i.e., 
 for all w, and
hence 
. Sentences therefore obey the already-cited tertium non datur.
Theorem 3.1 (Coincidence theorem).
Let 
 , { free} φ ⊆ V, and 
 be models on the same domain A such
that x ℳ = x ℳ′ for all x ∈ V, and 
 for all extralogical symbols ζ
occurring in φ. Then 
 .
Proof by induction on φ. Let 
 be prime, so that 
. As was
mentioned earlier, the value of a term t depends only on the meaning of the
symbols occurring in t. But in view of the suppositions, these meanings are the
same in 
 and 
. Therefore, 
 (i.e., 
 for i = 1, …, n),
and so 
. For equations t sb1 = t sb2 one
reasons analogously. Further, the induction hypothesis for α, β yields 
. In the same way one obtains 
. By the induction step on  it becomes clear that the
induction hypothesis needs to be skillfully formulated. It must be given with
respect to any pair 
 of models and any subset V of 
.
Therefore let a ∈ A and ℳax ⊨ φ. Since for V′ : = V ∪{ x} certainly { free}
φ ⊆ V′ and the models ℳax, ℳ′ a x coincide for all y ∈ V′ (although in general 
), by the induction hypothesis ℳax ⊨ φ ⇔ ℳ′ a x ⊨ φ, for each a ∈
A. This clearly implies
It follows from this theorem that an 
-model 
 of φ for the case
that 
 can be completely arbitrarily expanded to an 
-model 

(.)
 of φ, i.e., arbitrarily fixing 
 for ζ ∈ L′ ∖ L gives 
 by the above theorem with V = { Var}. This readily implies
that the consequence relation 
 with respect to 
 is a conservative extension
of ⊨ ℒ in that 
, for all sets 
 and all 
. Hence, there is
no need here for using indices. In particular, the satisfiability or general validity
of φ depends only on the symbols effectively occurring in φ.
Another application of Theorem 3.1 is the following fact, which justifies the
already mentioned “omission of superfluous quantifiers.”
(2) ∀xφ ≡ φ ≡ ∃xφ whenever x∉{ free} φ. 
Indeed, x∉{ free} φ implies 
 (here a ∈ A is arbitrary)
according to Theorem 3.1; choose 
 and V = { free} φ. Therefore,
Very important for the next theorem and elsewhere is
(3) If 
, 
 and w { Var} → A then 
. 
This is clear for prime terms, and the induction hypothesis 
 for 
 together with 
 imply
For 
 and 
 let 
, or more suggestively 
 denote
the value of 
. Then (3) can somewhat more simply be written as
(4)
 and 
 imply 
 for all 
. 
Thus, along with the basic functions, also the so-called term functions 
 are the restrictions to their counterparts in . Clearly, if n = 0 or t is
variable-free, one may write 
 for 
. Note that in these cases 
whenever 
, according to (4).

By Theorem 3.1 the satisfaction of φ in 
 depends only on the values of
the x ∈ { free} φ. Let 
 4 and 
. Then the statement
 for a valuation w with 
can more suggestively be expressed by writing
without mentioning w as a global valuation. Such notation also makes sense
if w is restricted to a valuation on 
. One may accordingly extend the
concept of a model and call a pair 
 a model for a formula φ(  ) whenever 
, in particular if φ ∈ ℒn. We return to this extended concept in 4.1.
Until then we use it only for n = 0. That is, besides 
 also the structure 
 itself is occasionally called a model for a set S ⊆ ℒ0 of sentences, provided 
.
As above let 
. Then 
 is called the
predicate defined by the formula φ in the structure 
. For instance, the ≤ -
predicate in 
 is defined by 
, but also by several other
formulas.
More generally, a predicate P ⊆ A n is termed (explicitly or elementarily or
first-order) definable in 
 if there is some 
 with 
, and φ is
called a defining formula for P. Analogously, 
 is called definable in 
if 
 for some 
. One often talks in this case of explicit
definability of f in 
, to distinguish it from other kinds of definability. Much
information is gained from the knowledge of which sets, predicates, or functions
are definable in a structure. For instance, the sets definable in 
 are the
eventually periodic ones (periodic from some number on). Thus, ⋅ cannot
explicitly be defined by +, 0, 1 because the set of square numbers is not
eventually periodic.
 and 
 do not imply 
, in general. For instance,
let 
, 
, and 
. Then 
, while 
contains all pairs 
. As the next theorem will show, 
holds in general only for open formulas φ, and is even characteristic for 

provided A ⊆ B. Clearly, A ⊆ B is much weaker a condition than 
:
Theorem 3.2 (Substructure theorem).
For structures 
 such that A ⊆ B the following conditions are equivalent:
Proof. (i) ⇒ (ii): It suffices to prove that 
, with 
 and
, where w { Var} → A. In view of (3) the claim is obvious for prime
formulas, and the induction steps for ∧, ¬are carried out just as in Theorem 3.1.
(ii) ⇒ (iii): Trivial. (iii) ⇒ (i): By (iii), 
.
Analogously,
for all 
, b ∈ A. These conclusions state precisely that 
.
(i)
 
(ii)
, for all open 
 and all 
,
(iii)
, for all prime formulas 
 and 
.
Let α be of the form 
 with open β, where 
 may also be the empty
prefix. Then α is a universal or  -formula (spoken “A-formula”), and for α ∈
ℒ0 also a universal or  -sentence. A simple example is 
, which holds
in 
 iff A contains precisely one element. Dually, ∃ β with β open is termed an
∃-formula, and an ∃-sentence whenever ∃\vec{x}β ∈ ℒ0. Examples are the
“how-many sentences”
∃sb1 : = ∃v sb0 v sb0 = v sb0; 
 (n > 1).
∃n states ‘there exist at least n elements’, ¬ ∃n + 1 thus that ‘there exist at
most n elements’, and 
 says ‘there exist exactly n elements’.
Since ∃sb1 is a tautology, it is convenient to set ⊤ : = ∃sb1, and 
 in
all first-order languages with equality. Clearly, equivalent definitions of ⊤, ⊥
may be used as well.
Corollary 3.3.
Let 
 . Then every  -sentence 
 valid in  is also satisfied in 
 .
Dually, every ∃-sentence ∃  β valid in 
 is also valid in  .

Proof. Let 
 and 
. Then 
, hence 
 by Theorem 3.2. 
 was arbitrary and therefore 
. Now let 
. Then 
 for
some 
, hence 
 by Theorem 3.2, and consequently 
.
We now formulate a generalization of certain individual often-used
arguments about the invariance of properties under isomorphisms:
Theorem 3.4 (Invariance theorem).
Let 
 be isomorphic structures of signature L and let 
 be an
isomorphism. Then for all 
In particular 
 , for all sentences φ of  .
Proof. It is convenient to reformulate the claim as
This is easily confirmed by induction on φ after first proving 
inductively on t. This proof clearly includes the case φ ∈ ℒ0.
Thus, for example, it is once and for all clear that the isomorphic image of a
group is a group even if we know at first only that it is a groupoid. Simply let α
in the theorem run through all axioms of group theory. Another application: Let ı
be an isomorphism of the group 
 onto the group 
 and let e
and e′ denote their unit elements, not named in the signature. We claim that
nonetheless ıe = e′, using the fact that the unit element of a group is the only
solution of x∘x = x (Example 2, page 91). Thus, since 
, we get 
 by Theorem 3.4, hence ıe = e′. Theorem 3.4, incidentally, holds
for formulas of higher order as well. For instance, the property of being a
continuously ordered set (formalizable in a second-order language, see 3.8) is
likewise invariant under isomorphism.
-structures 
 are termed elementarily equivalent if 
, for
all α ∈ ℒ0. One then writes 
. We consider this important notion in 3.3 and
more closely in 5.1. Theorem 3.4 states in particular that 
. The
question immediately arises whether the converse of this also holds. For infinite
structures the answer is negative (see 3.3), for finite structures affirmative; a
finite structure of a finite signature can, up to isomorphism, even be described by
a single sentence. For example, the 2-element group ({0, 1}, + ) is up to
isomorphism well determined by the following sentence, which tells us precisely

how + operates:
We now investigate the behavior of the satisfaction relation under substitution.
The definition of φ  in 2.2 pays no attention to collision of variables, which is
taken to mean that some variables of the substitution term t fall into the scope of
quantifiers after the substitution has been performed. In this case 
 does
not necessarily imply 
, although this might have been expected. In other
words, ∀xφ ⊨ φ  is not unrestrictedly correct. For instance, if φ = ∃y x≠y then
certainly 
 whenever 
 has at least two elements, but 
 ( = ∃y y≠y) is certainly false. Analogously φ  ⊨ ∃xφ is not correct, in
general. For example, choose ∀y x = y for φ and y for t.
One could forcibly obtain ∀xφ ⊨ φ  without any limitation by renaming
bound variables by a suitable modification of the inductive definition of φ  in
the quantifier step. However, such measures are rather unwieldy for the
arithmetization of proof method in 6.2. It is therefore preferable to put up with
minor restrictions when we are formulating rules of deduction later. The
restrictions we will use are somewhat stronger than they need to be but can be
handled more easily; they look as follows: Call φ,  collision-free if y∉{ bnd} φ
for all y ∈ { var} t distinct from x. We need not require x∉{ bnd} φ because t is
substituted only at free occurrences of x in φ, that is, x cannot fall after
substitution within the scope of a prefix ∀x, even if x ∈ { var} t. For collision-
free φ,  we always get 
 by Corollary 3.6 below.
If σ is a global substitution (see 2.2) then φ, σ are termed collision-free if 
 are collision-free for every x ∈ { Var}. If 
, this condition clearly
need be checked only for the pairs 
 with x ∈ { var}  and x ∈ { free} φ.
For 
 put 
 with 
 for x ∈ { Var}, so
that 
. This equation reproduces itself to
(5)
 for all terms t. 

(.)
Indeed, 
 for = 
 in view of the induction hypothesis 
 (
). Notice
that ℳσ coincides with 
 for the case σ =  .
Theorem 3.5 (Substitution theorem).
Let 
 be a model and σ a global substitution. Then holds for all φ such that φ,σ
are collision-free,
(6)   
. 
In particular, 
 , provided 
 are collision-free.
Proof by induction on φ. In view of (5), we obtain
Prime formulas 
 are treated analogously. The induction steps for ∧, ¬in the
proof of (6) are harmless. Only the -step is interesting. The reader should recall
the definition of 
 page 40 and realize that the induction hypothesis refers
to an arbitrary global substitution τ.
We show that 
. Since 
 (hence 
 for every y)
are collision-free, we have 
 if y≠x, and since 
 we get in this
case 
. But also in the
case y = x we have 
.
Corollary 3.6.

For all φ and 
 such that 
 are collision-free, the following properties hold:
(a) 
 , in particular 
 , (b) 
 , (c) 
 ,
provided 
 are collision-free.
Proof. Let 
, so that 
 for all 
. In particular, 
.
Therefore, 
 by Theorem 3.5. (b) follows easily from 
. This
holds by (a), for 
 and 
. (c): Let 
, so
that 
 and 
 by the theorem. Clearly, then also 
.
Hence 
.
Remark 2. The identical substitution ι is obviously collision-free with every
formula. Thus, 
 is always the case, while 
 is correct in
general only if t contains at most the variable x, since φ,  are then collision-
free. Theorem 3.5 and Corollary 3.6 are easily strengthened. Define inductively
a ternary predicate ‘t is free for x in φ’, which intuitively is to mean that no free
occurrence in φ of the variable x lies within the scope of a prefix ∀y whenever y
∈ { var} t. In this case Theorem 3.5 holds for σ =  as well, so that nothing
needs to be changed in the proofs based on this theorem if one works with ‘t is
free for x in φ’, or simply reads “φ,  are collision-free” as “t is free for x in φ.”
Though collision-freeness is somewhat cruder and slightly more restrictive, it is
for all that more easily manageable, which will pay off, for example, in 6.2,
where proofs will be arithmetized. Once one has become accustomed to the
required caution, it is allowable not always to state explicitly the restrictions
caused by collisions of variables, but rather to assume them tacitly.
Theorem 3.5 also shows that the quantifier “there exists exactly one,”
denoted by ∃!, is correctly defined by 
 with
y∉{ var} φ. Indeed, it is easily seen that 
 means just 
. In short, ℳax ⊨ φ for at most one a. Putting
everything together, 
 iff there is precisely one a ∈ A with ℳax ⊨ φ.
An example is 
 for arbitrary 
 and x∉{ var} t. In other words, ∃! x

x = t is a tautology. Half of this, namely 
, was shown in Example 1, and
 is obvious. There are various equivalent definitions
of ∃! xφ. For example, a short and catchy formula is ∃x ∀y(φ  ↔ x = y), where
y∉{ var} φ. The equivalence proof is left to the reader.
Exercises
1. Let X ⊨ φ and x∉{ free} X. Show that X ⊨ ∀xφ.
 
2. Prove that 
, which is obviously equivalent to 
.
 
3. Suppose 
 results from 
 by adjoining a constant symbol a for some a
∈ A. Prove 
 for α = α(x), by first verifying 
. This is easily generalized to the case of more than one
free variable in α.
 
4. Show that (a) A conjunction of the ∃i and their negations is equivalent to
∃n ∧ ¬ ∃m for suitable n, m ( ∃n ∧ ¬ ∃sb0 ≡ ∃n, ∃sb1 ∧ ¬ ∃m ≡ ¬ ∃m). (b)
A Boolean combination of the ∃i is equivalent to 
 or to 
, with k sb0 < ⋯ < k n < k. Note that 
 equals ∃sb =
0 ( ≡ ⊥ ) for 
 and 
 for n > 0.
 
2.4 General Validity and Logical Equivalence
From the perspective of predicate logic α  ∨   ¬α (α ∈ ℒ) is a trivial example of
a tautology, because it results by inserting α for p from the propositional
tautology p  ∨   ¬p. Every propositional tautology provides generally valid -
formulas by the insertion of -formulas for the propositional variables. But
there are tautologies not arising in this way. ∀x(x < x  ∨  x≮x) is an example,
though it has still a root in propositional logic. Tautologies without a such a root
are ∃x x = x and ∃x x = t for x∉{ var} t. The former arises from the convention
that structures are always nonempty, the latter from the restriction to totally
defined basic operations. A particularly interesting tautology is given by the
following Example 1 (Russell’s antinomy). We will show that the “Russellian
set” u, consisting of all sets not containing themselves as a member, does not

exist which clearly follows from ⊨ ¬ ∃u ∀x(x ∈ u ↔ x∉ x). We start with ∀x(x
∈ u ↔ x∉ x) ⊨ u ∈ u ↔ u∉ u. This holds by Corollary 3.6(a). Clearly, u ∈ u ↔
u∉ u is unsatisfiable. Hence, the same holds for ∀x(x ∈ u ↔ x∉ x), and thus for
∃u ∀x(x ∈ u ↔ x∉ x). Consequently, ⊨ ¬ ∃u ∀x(x ∈ u ↔ x∉ x).
Note that we need not assume in the above argument that ∈ means
membership. The proof of ⊨ ¬ ∃u ∀x(x ∈ u ↔ x∉ x) need not be related to set
theory at all. Hence, our example represents rather a logical paradox than a set-
theoretic antinomy. What looks like an antinomy here is the expectation that ∃u
∀x(x ∈ u ↔ x∉ x) should hold in set theory if ∈ is to mean membership and
Cantor’s definition of a set is taken literally.
The satisfaction clause for α → β easily yields α ⊨ β  ⇔   ⊨ α → β, a special
case of X, α ⊨ β  ⇔   X ⊨ α → β. This can be very useful in checking whether
formulas given in implicative form are tautologies, as was mentioned already in
1.3. For instance, from ∀xα ⊨ α  (which holds for collision-free α,  ) we
immediately get 
.
As in propositional logic, α ≡ β is again equivalent to ⊨ α ↔ β. By inserting 
-formulas for the variables of a propositional equivalence one automatically
procures one of predicate logic. Thus, for instance, 
, because
certainly 
. Since every -formula results from the insertion of
propositionally irreducible -formulas in a formula of propositional logic, one
also sees that every 
-formula can be converted into a conjunctive normal form.
But there are also numerous other equivalences, for example 
 and 
. The first of these means just ¬ ∀xα ≡ ¬ ∀x ¬ ¬α ( = ∃x ¬α),
obtained by replacing α by the equivalent formula ¬ ¬α under the prefix ∀x. This
is a simple application of Theorem 4.1 below with ≡ for ≈.
As in propositional logic, semantic equivalence is an equivalence relation in 
 and, moreover, a congruence in 
. Speaking more generally, an equivalence
relation ≈ in  satisfying the congruence property
is termed a congruence in 
. Its most important property is expressed by
Theorem 4.1 (Replacement theorem).
Let ≈ be a congruence in 
 and α ≈ α′. If φ′ results from φ by replacing the
formula α at one or more of its occurrences in φ by the formula α′, then φ ≈ φ′.
Proof by induction on φ. Suppose φ is a prime formula. Both for φ = α and φ≠α,
φ ≈ φ′ clearly holds. Now let φ = φsb1 ∧ φsb2. In case φ = α holds trivially φ ≈ φ′.

(.)
(.)
Otherwise φ′ = φsb1 ′ ∧ φsb2 ′, where φsb1 ′, φsb2 ′ result from φsb1, φsb1 by
possible replacements. By the induction hypothesis φsb1 ≈ φsb1 ′ and φsb2 ≈ φsb2 ′.
Hence, 
 according to CP above. The induction steps for
¬,  follow analogously.
This theorem will constantly be used, mainly with ≡ for ≈, without actually
specifically being cited, just as in the arithmetical rearrangement of terms, where
the laws of arithmetic used are hardly ever named explicitly. The theorem readily
implies that CP is provable for all defined connectives such as → and ∃. For
example, 
, because 
.
First-order languages have a finer structure than those of propositional logic.
There are consequently further interesting congruences in 
. In particular,
formulas α, β are equivalent in an  -structure 
, in symbols 
, if 
, for all w. Hence, in 
 the formulas x < y and 
 are equivalent. The proof of CP for 
 is very simple and
is therefore left to the reader.
Clearly, 
 is equivalent to 
. Because of 
, properties
such as 
 carry over from 
 to 
. But there are often new
interesting equivalences in certain structures. For instance, there are structures in
which every formula is equivalent to a formula without quantifiers, as we will
see in 5.6.
A very important fact with an almost trivial proof is that the intersection of a
family of congruences is itself a congruence. Consequently, for any class 
of 
-structures, 
 is necessarily a congruence. For the
class 
 of all 
-structures, 
 equals the logical equivalence 
, which in this
section we deal with exclusively. Below we list its most important features; these
should be committed to memory, since they will continually be applied.
If x does not occur free in the formula β, then also
The simple proofs are left to the reader. (7) and (8) were stated in (2) in 2.3.
Only (9) and (10) look at first sight surprising. But in practice these equivalences

(.)
are very frequently used. For instance, consider for a fixed set of formulas X the
evidently true metalogical assertion ‘for all α: if X ⊨ α, ¬α then X ⊨ ∀x x≠x’.
This clearly states the same as ‘If there is some α such that X ⊨ α, ¬α then X ⊨
∀x x≠x’.
Remark. In everyday speech variables tend to remain unquantified, partly
because in some cases the same meaning results from quantifying with “there
exists a” as with “for all.” For instance, consider the following three sentences,
which obviously tell us the same thing, and of which the last two correspond to
the logical equivalence (9):
If a lawyer finds a loophole in the law it must be changed.
If there is a lawyer who finds a loophole in the law it must be changed.
For all lawyers: if one of them finds a loophole in the law then it must be
changed.
Often, the type of quantification in linguistic bits of information can be made
out only from the context, and this leads not all too seldom to unintentional (or
intentional) misunderstandings. “Logical relations in language are almost always
just alluded to, left to guesswork, and not actually expressed” (G. Frege).
Let x, y be distinct variables and 
. One of the most important logical
equivalences is renaming of bound variables (in short, bound renaming), stated
in
(11)
(a)
), 
).
 
(b) follows from (a) by rearranging equivalently. Note that
y∉{ var} α is equivalent to y∉{ free} α and 
collision-free. Writing 
 for 
, (a) derives as
follows:
 
 
(12) and (13) below are also noteworthy. According to (13), substitutions are

completely described up to logical equivalence by so-called free renamings
(substitutions of the form 
). (13) also embraces the case 
. In (12) and
(13) we tacitly assume that 
 are collision-free.
(12)
 (
).
 
(13)
). 
Proof of (12): 
 by Corollary
3.6. Conversely, let 
. If 
 then clearly 
. Hence also 
, since 
. Thus, 
 for any a ∈ A, i.e., 
. This proves the left equivalence in (12). The right
equivalence reduces to the left one because
Item (13) is proved similarly. Note that 
 by
Corollary 3.6 and Exercise 4 in 2.2.
With the above equivalences we can now regain an equivalent formula
starting with any formula in which all quantifiers are standing at the beginning.
But this result requires both quantifiers  and , in the following denoted by 
A formula of the form 
 with an open formula β is termed a
prenex formula or a prenex normal form, in short, a PNF. β is called the kernel of
α. W.l.o.g. 
 are distinct and x i occurs free in β since we may drop
“superfluous quantifiers,” see (2) page 53. Prenex normal forms are very
important for classifying definable number-theoretic predicates in 6.3, and for
other purposes. The already mentioned - and ∃-formulas are the simplest
examples.
Theorem 4.2 (on the prenex normal form).
Every formula φ is equivalent to a formula in prenex normal form that can
effectively be constructed from φ.
Proof. Without loss of generality let φ contain only the logical symbols 

(besides = ). For each prefix 
 in φ consider the number of symbols ¬or ∧
occurring to the left of Qx. Let sφ be the sum of these numbers, summed over all
prefixes occurring in φ. Clearly, φ is a PNF iff sφ = 0. Let sφ≠0. Then φ contains
some prefix Qx and ¬or ∧ stands immediately in front of Qx. A successive
application of either ¬ ∀xα ≡ ∃x ¬α, ¬ ∃xα ≡ ∀x ¬α, or β ∧ Qxα ≡ Qy( b ∧ α )
(y∉{ var} α, β),
inside φ obviously reduces sφ stepwise.
Example 2. 
 is a PNF for 
. And
 for 
, provided 
; if not, a bound renaming will help. An equivalent PNF for this
formula with minimal quantifier rank is 
.
The formula 
 from Example 2 may be abbreviated by
( ∀x≠0) ∃y x ⋅ y = 1. More generally, we shall often write ( ∀x≠t)α for ∀x(x≠t →
α) and ( ∃x≠t)α for ∃x(x≠t ∧ α). A similar notation is used for ≤, <, ∈ and their
negations. For instance, ( ∀x ≤ t)α and ( ∃x ≤ t)α are to mean ∀x(x ≤ t → α) and
∃x(x ≤ t ∧ α), respectively. For any binary relation symbol ◃, the “prefixes” (
∀y◃x) and ( ∃y◃x) are related to each other, as are  and ∃, see Exercise 2.
Exercises
1. Let α ≡ β. Prove that 
 (α,  and 
 collision-free).
 
2. Prove that 
 and 
. Here ◃
represents any binary relation symbol.
 
3. Show by means of bound renaming that both the conjunction and the
disjunction of -formulas α, β is equivalent to some -formula. Prove
the same for -formulas.
 
4. Show that every formula 
 is equivalent to some 
 built up
from literals by means of ∧, ∨, and .
 
5. Let P be a unary predicate symbol. Prove that 
 is a
tautology.
 

6. Call 
 tautologically equivalent if ⊨ α ⇔ ⊨ β. Confirm that the
following (in general not logically equivalent) formulas are
tautologically equivalent: α, ∀xα, and 
, where the constant symbol c
does not occur in α.
 
2.5 Logical Consequence and Theories
Whenever 
, the language 
 is called an expansion or extension of  and 
 a reduct or restriction of 
. Recall the insensitivity of the consequence
relation to extensions of a first-order language, mentioned in 2.3. Theorem 3.1
yields that establishing X ⊨ α does not depend on the language to which the set
of formulas X and the formula α belong. For this reason, indices for ⊨, such as 
, are dispensable.
Because of the unaltered satisfaction conditions for ∧ and ¬, all properties of
the propositional consequence gained in 1.3 carry over to the first-order logical
consequence relation. These include general properties such as, for example, the
reflexivity and transitivity of ⊨, and the semantic counterparts of the rules ( ∧
1), ( ∧ 2), ( ¬1), ( ¬2) from 1.4, for instance the counterpart of ( ∧ 1), 
. 5
In addition, Gentzen-style properties such as the deduction theorem
automatically carry over. But there are also completely new properties. Some of
these will be elevated to basic rules of a logical calculus for first-order languages
in 3.1, to be found among the following ones: Some properties of the predicate
logical consequence relation.
(a) follows from 
, for ⊨ is transitive. Similarly, (b) follows
from 
, stated in Corollary 3.6. Analogously (c) results from ∀xβ
⊨ β. To prove (d), suppose that X ⊨ α, 
, and x∉{ free} X. Then ℳax ⊨ X
for any a ∈ A by Theorem 3.1, which just means 
. As regards (e), let X,
β ⊨ α. Observe that by contraposition and by (d),
whence X, ¬ ∀x ¬β ⊨ α. (e) captures deduction from an existence claim,
while (f) confirms an existence claim. (f) holds since α  ⊨ ∃xα according to
Corollary 3.6. Both (e) and (f) are permanently applied in mathematical
reasoning and will briefly be discussed in Example 1 on the next page. All above
properties have certain variants; for example, a variant of (d) is

(a) 
 collision-free),
(b) 
 (
 and 
 collision-free),
(c) 
 (anterior generalization),
(d) 
 (x∉{ free} X, posterior generalization),
(e) 
 (
, anterior particularization),
(f) 
 (
 collision-free, posterior particularization)
(g) 
.
This results from (d) with 
 for α and y for x, since 
.
From the above properties, complicated chains of deduction can, where
necessary, be justified step by step. But in practice this makes sense only in
particular circumstances, because formalized proofs are readable only at the
expense of a lot of time, just as with lengthy computer programs, even with well-
prepared documentation. What is most important is that a proof, when written
down, can be understood and reproduced. This is why mathematical deduction
tends to proceed informally, i.e., both claims and their proofs are formulated in a
mathematical “everyday” language with the aid of fragmentary and flexible
formalization. To what degree a proof is to be formalized depends on the
situation and need not be determined in advance. In this way the strict syntactic
structure of formal proofs is slackened, compensating for the imperfection of our
brains in regard to processing syntactic information.
Further, certain informal proof methods will often be described by a more or
less clear reference to so-called background knowledge, and not actually carried
out. This method has proven itself to be sufficiently reliable. As a matter of fact,
apart from specific cases it has not yet been bettered by any of the existing
automatic proof machines. Let us present a very simple example of an informal
proof in a language 
 for natural numbers that along with 0, 1, +, ⋅ contains the
symbol for divisibility, defined by 
. In addition, let  contain

a symbol f for some given function from  to 
. We need no closer information
on this function, but we shall write f i for f(i) in Example 1.
Example 1. We want to prove ∀n ∃x( ∀i ≤ n)f i x. That is, for every n, 
 have a common multiple. A careful proof proceeds by induction on n.
Here we focus solely on 
, the induction step.
X represents our prior knowledge about familiar properties of divisibility.
Informally we reason as follows: Suppose 
 and let x denote any
common multiple of 
. Then 
 is clearly a common multiple of f
sb0, …, f n + 1, hence 
. That’s all. To argue here formally like a
proof machine, let us start from the obvious 
.
Posterior particularization of x yields 
. From
this follows the desired 
 by anterior
particularization. Thus, formalizing a nearly trivial informal argument may need
a lot of writing and turns out to be nontrivial in some sense.
Some textbooks deal with a somewhat stricter consequence relation, which
we denote here by ⊨ {g}. The reason is that in mathematics one largely
considers derivations in theories. For 
 and 
 define X ⊨ {g} φ if 
, for all -structures 
. In contrast to ⊨, which may be called
the local consequence relation, ⊨ {g} can be considered as the global
consequence relation since it cares only about 
, not about a concrete valuation
w in 
 as does ⊨.
Let us collect a few properties of ⊨ {g}. Obviously, X ⊨ φ implies X ⊨ {g}
φ, but the converse does not hold in general. For example, 
,
but 
. By (d) from page 84, X ⊨ φ ⇒ X ⊨ φg holds in general
only if the free variables of φ do not occur free in X, while 
(hence φ ⊨ {g} φg ) holds unrestrictedly. A reduction of ⊨ {g} to ⊨ is provided
by the following equivalence, which easily follows from 
,
for each model 
:
(1)
. 

Because of Sg = S for sets of sentences S, we clearly obtain from (1)
(2)
S ⊨ {g} φ  ⇔   S ⊨ φ (S ⊆ ℒ0).  
In particular, ⊨ {g} φ ⇔ ⊨ φ. Thus, a distinction between ⊨ and ⊨ {g} is
apparent only when premises are involved that are not sentences. In this case the
relation ⊨ {g} must be treated with the utmost care. Neither the rule of case
distinction 
 nor the deduction theorem 
 is
unrestrictedly correct. For example 
, but it is false that 
. This means that the deduction theorem fails to hold for the
relation ⊨ {g}. It holds only under certain restrictions.
One of the reasons for our preference of ⊨ over ⊨ {g} is that ⊨ extends the
propositional consequence relation conservatively, so that features such as the
deduction theorem carry over unrestrictedly, while this is not the case for ⊨ {g}.
It should also be said that ⊨ {g} does not reflect the actual procedures of natural
deduction in which formulas with free variables are frequently used also in
deductions of sentences from sentences, for instance in Example 1.
We now make more precise the notion of a formalized theory in 
, where it
is useful to think of the examples in 2.3, such as group theory. Again, the
definitions by different authors may look somewhat differently.
Definition. An elementary theory or first-order theory in 
, also termed an
 -theory, is a set of sentences T ⊆ ℒ0 deductively closed inℒ0, i.e., T ⊨ α  ⇔  
α ∈ T, for all α ∈ ℒ0. If α ∈ T then we say that α is valid or true or holds in T, or
α is a theorem of T. The extralogical symbols of 
 are called the symbols of T. If
T ⊆ T′ then T is called a subtheory of T′, and T′ an extension of T. An -
structure 
 such that 
 is also termed a model of T, briefly a T-model. MdT
denotes the class of all models of T in this sense; MdT consist of 
-structures
only.
For instance, {α ∈ ℒ0 ∣ X ⊨ α} is a theory for any set 
, since ⊨ is
transitive. A theory T in 
 satisfies 
 for all 
, where 
is any formula. Important is also T ⊨ φ  ⇔   T ⊨ φg. These readily confirmed
facts should be taken in and remembered, since they are constantly used.
Different authors may use different definitions for a theory. For example, they
may not demand that theories contain sentences only, as we do. Conventions of
this type each have their advantages and disadvantages. Proofs regarding
theories are always adaptable enough to accommodate small modifications of the
definition. Using the definition given above we set the following Convention. In

talking of the theory S, where S is a set of sentences, we always mean the theory
determined by S, that is, 
. A set 
 is called an axiom system
for T whenever T = { α ∈ ℒ0 ∣ Xg ⊨ α}, i.e., we tacitly generalize all possibly
open formulas in X. We have always to think of free variables occurring in
axioms as being generalized.
Thus, axioms of a theory are always sentences. But we conform to standard
practice of writing long axioms as formulas. We will later consider extensive
axiom systems (in particular, for arithmetic and set theory) whose axioms are
partly written as open formulas just for economy.
There exists a smallest theory in , namely the set 
 of all generally valid
sentences in 
 , also called the “logical” theory. An axiom system for {
Taut} is the empty set of axioms. There is also a largest theory: the set 
 of all
sentences, the inconsistent theory, which possesses no models. All remaining
theories are called satisfiable or consistent. 6 Moreover, the intersection 
 of a nonempty family of theories T i is in turn a theory: if T ⊨ α ∈
ℒ0 then clearly T i ⊨ α and so α ∈ T i for each i ∈ I, hence α ∈ T as well. In this
book T and T′, with or without indices, exclusively denote theories.
For T ⊆ ℒ0 and α ∈ ℒ0 let T + α denote the smallest theory that extends T
and contains α. Similarly let T + S for S ⊆ ℒ0 be the smallest theory containing
T ∪S. If S is finite then 
 is called a finite extension of T.
Here ∧S denotes the conjunction of all sentences in S. A sentence α is termed
compatible or consistent with T if T + α is satisfiable, and refutable in T if T + ¬α
is satisfiable. Thus, the theory T F of fields is compatible with the sentence 
 . Equivalently, 1 + 1≠0 is refutable in T F , since the 2-element field
satisfies 
.
If both α and ¬α are compatible with T then the sentence α is termed
independent of T. The classic example is the independence of the parallel axiom
from the remaining axioms of Euclidean plane geometry, which define absolute
geometry. Much more difficult is the independence proof of the continuum
hypothesis from the axioms for set theory. These axioms are presented and
discussed in 3.4.
At this point we introduce another important concept; 
 are said to be
equivalent in or modulo T, 
, if 
 for all 
. Being an
intersection of congruences, 
 is itself a congruence and hence satisfies the
replacement theorem. This will henceforth be used without mention, as will the

obvious equivalence of 
, T ⊨ α ↔ β, and of T ⊨ (α ↔ β) g. A suggestive
writing of 
 would also be 
.
Example 2. Let T G be as on p. 50. Claim: 
. The only tricky
proof step is 
. Let x∘ x = x and choose some y with x∘ y = e.
The claim then follows from 
. A strict formal proof of
the latter uses anterior particularization.
Another important congruence is term equivalence. Call terms s, tequivalent
modulo (or in) T, in symbols 
 , if T ⊨ s = t, that is, 
 for all 
 and 
 . For instance, in 
 , 
 is
easily provable, so that 
. Another example: in the theory of
fields, each term is equivalent to a polynomial in several variables with integer
coefficients.
If all axioms of a theory T are  -sentences then T is called a universal or  -
theory. Examples are partial orders, orders, rings, lattices, and Boolean
algebras. For such a theory, Md T is closed with respect to substructures, which
means 
. This follows at once from Corollary 3.3. Conversely,
a theory closed with respect to substructures is necessarily a universal one, as
will turn out in 5.4. -theories are further classified. The most important
subclasses are equational, quasi-equational, and universal Horn theories, all of
which will be considered to some extent in later chapters. Besides -theories, the
-theories (those having 
-sentences as axioms) are of particular interest for
mathematics. More about all these theories will be said in 5.4.
Theories are frequently given by structures or classes of structures. The
elementary theory 
 and the theory 
 of a nonempty class 
 of
structures are defined respectively by
It is easily seen that 
 and 
 are theories in the precise sense
defined above. Instead of 
 one often writes K ⊨ α. In general, 
 is larger than K, as we shall see.
One easily confirms that the set of formulas breaks up modulo T (more
precisely, modulo 
 ) into equivalence classes; their totality is denoted by Bω
T. Based on these we can define in a natural manner operations ∧, ∨, ¬. For
instance, 
, where  denotes the equivalence class to which φ

belongs. One shows easily that Bω T forms a Boolean algebra with respect to ∧,
∨, ¬. For every n, the set BnT of all  in Bω T such that the free variables of φ
belong to 
 is a subalgebra of Bω T. Note that B sb0 T is
isomorphic to the Boolean algebra of all sentences modulo 
 , also called the
Tarski–Lindenbaum algebra of T. The significance of the Boolean algebras B n T
is revealed only in the somewhat higher reaches of model theory, and they are
therefore mentioned only incidentally.
Exercises
1. Suppose x∉{ free} X and c is not in X, α. Prove the equivalence of (i) X
⊨ α, (ii) X ⊨ ∀ xα, (iii) 
.
This holds then in particular if X is the axiom system of a theory or
itself a theory. Then x∉{ free} X is trivially satisfied.
 
2. Let S be a set of sentences, α and β formulas, x∉{ free} β, and let c be a
constant not occurring in S, α, β. Show that
 
3.
Verify for all α, β ∈ ℒ0 that β ∈ T + α  ⇔   α → β ∈ T.
 
4.
Let 
 be a theory, 
, and 
. Prove that T sb0
is also a theory (the so-called reduct theory in the language 
).
 
2.6 Explicit Definitions—Language Expansions
The deductive development of a theory, be it given by an axiom system or a
single structure or classes of those, nearly always goes hand in hand with
expansions of the language carried out step by step. For example, in developing
elementary number theory in the language 
 , the introduction of the
divisibility relation by means of the (explicit) definition 
 has
certainly advantages not only for purely technical reasons. This and similar
examples motivate the following Definition I. Let r be an n-ary relation symbol
not occurring in  . An explicit definition of r in  is to mean a formula of the
form

with 
 and distinct variables in , called the defining formula. For a
theory T, the extension 
 is then called a definitorial extension (or
expansion) of T by r, more precisely, by ηr.
T r is a theory in 
, the language resulting from 
 by adjoining the symbol
r. It will turn out that T r is a conservative extension of T, which, in the general
case, means a theory T′ ⊇ T in 
 such that 
 . Thus, T r contains
exactly the same 
-sentences as does T. In this sense, T r is a harmless extension
of T. Our claim constitutes part of Theorem 6.1. For 
 define the reduced formula 
 as follows: Starting from
the left, replace every prime formula 
 occurring in φ by 
 . Clearly, 
 , provided r does not appear in
φ.
Theorem 6.1 (Elimination theorem).
Let 
 be a definitorial extension of the
theory 
 by the explicit definition η r . Then for
all formulas 
 holds the equivalence
For 
 we get in particular T r ⊨ φ ⇔ T ⊨ φ ( since 
 ). Hence, T r is a conservative
extension of T, i.e., α ∈ T r ⇔ α ∈ T, for all α ∈ℒ0.
Proof. Each 
 is expandable to a model 
 with the same domain, setting 
. Since 
 for any 
,
we obtain 
 for
all 
 by the replacement theorem. Thus, ( ∗ )
follows from

Operation symbols and constants can be similarly introduced, though in this
case there are certain conditions to observe. For instance, in T G (see page 50)
the operation − 1 is defined by 
.
This definition is legitimate, since T G ⊨ ∀x ∃! y x∘y = e; Exercise 3. Only this
requirement (which by the way is a logical consequence of η) ensures that T G +
ηg is a conservative extension of T G. We therefore extend Definition I as
follows, keeping in mind that to the end of this section constant symbols are to
be counted among the operation symbols.
Definition II. An explicit definition of an n-ary operation symbol f not
occurring in 
 is a formula of the form
 (
 and y, x sb1, …, x n distinct).
ηf is called legitimate in 
 if 
, and 
 is then called a definitorial
extension by f, more precisely by ηf. In the case n = 0 we write c for f and speak
of an explicit definition of the constant symbolc. Written more suggestively y = c
↔ δ(y).
Some of the free variables of δ are often not explicitly named, and thus
downgraded to parameter variables. More on this will be said in the discussion
of the axioms for set theory in 3.4. The elimination theorem is proved in almost
exactly the same way as above, provided ηf is legitimate in T. The reduced
formula 
 is defined correspondingly. For a
constant c (n = 0 in Definition II), let 
, where 
 denotes the result of
replacing c in φ by z (∉{ var} φ). Now let n > 0. If f does not appear in φ, set φ rd
= φ. Otherwise, looking at the first occurrence of f in φ from the left, we
certainly may write 
 for
appropriate φsb0, 
, and y∉{ var} φ. Clearly, 
, with 

. If f still occurs in φsb1 then repeat this procedure, which ends in, say, m steps in
a formula φm that no longer contains f. Then put φ rd : = φm.
Frequently, operation symbols f are introduced in more or less strictly
formalized theories by definitions of the form
where of course f does not occur in the term 
. This procedure is
in fact subsumed by Definition II, because the former is nothing more than a
definitorial extension of T with the explicit definition
This definition is legitimate, since 
 is a tautology. It can readily be
shown that 
 is logically equivalent to 
. Hence, ( ∗ ) can indeed be
regarded as a kind of an informative abbreviation of a legitimate explicit
definition with the defining formula 
.
Remark 1. Instead of introducing new operation symbols, so-called iota-terms
from [HB] could be used. For any formula 
in a given language, let ιyφ be a term in which y appears as a variable bound by ι.
Whenever 
, then T is extended
by the axiom 
, so that ιyφ(
 , y) so to speak stands for the function term 
, which could have been introduced by an explicit definition. We
mention that a definitorial language expansion is not a necessity. In principle,
formulas of the expanded language can always be understood as abbreviations in
the original language. This is in some presentations the actual procedure, though
our imagination prefers additional notions over long sentences that would arise if
we were to stick to a minimal set of basic notions.
Definitions I and II can be unified in a more general declaration. Let T, T′ be
theories in the languages 
, 
, respectively.
Then T′ is called a definitorial extension (or expansion) of T whenever 
 for some list 
 of explicit definitions of new
symbols legitimate in T, given in terms of those of T (here legitimate refers to
operation symbols and constants only). Δ need not be finite, but in most cases it
is finite. A reduced formula 
 is stepwise constructed as

above, for every 
. In this way the somewhat long-
winded proof of the following theorem is reduced each time to the case of an
extension by a single symbol:
Theorem 6.2 (General elimination theorem).
Let T′ be a definitorial extension of T. Then α ∈ T′⇔ α rd ∈ T. In particular, α
∈ T′⇔ α ∈ T whenever 
 , i.e., T′ is a conservative
extension of T.
A relation or operation symbol ζ occurring in 
is termed explicitly definable in T if T contains an explicit definition of ζ whose
defining formula belongs to 
, the language of symbols
of T without ζ. For example, in the theory T G of groups the constant e is
explicitly defined by 
;
Example 2 page 91. Another example is presented in Exercise 3. In such a case
each model of T sb0 : = T ∩ ℒ sb0 can be expanded in only one way to a T-model.
If this special condition is fulfilled then ζ is said to be implicitly definable in T.
This could also be stated as follows: if T′ is distinct from T only in that the
symbol ζ is everywhere replaced by a new symbol ζ′, then either 
or 
,
depending on whether ζ, ζ′ are relation or operation symbols. It is highly
interesting that this kind of definability is already sufficient for the explicit
definability of ζ in T. But we will go without the proof and only quote the
following theorem.
Beth’s definability theorem. A relation or operation symbol implicitly
definable in a theory T is also explicitly definable in T.
Definitorial expansions of a language should be conscientiously
distinguished from expansions of languages that arise from the introduction of
so-called Skolem functions. These are useful for many purposes and are therefore
briefly described.
Skolem normal forms. According to Theorem 4.2, every formula α can be
converted into an equivalent PNF, 
, where α′ is open. Obviously then 
, where ∀ = ∃and 
. Because ⊨ α if and only if
¬α is unsatisfiable, the decision problem for general validity can first of all be
reduced to the satisfiability problem for formulas in PNF. Using Theorem 6.3
below, the latter—at the cost of introducing new operation symbols—is then

completely reduced to the satisfiability problem for 
-formulas.
Call formulas α and β satisfiably equivalent if both are satisfiable (not
necessarily in the same model), or both are unsatisfiable. We construct for every
formula, which w.l.o.g. is assumed to be given in prenex form α = Qsb1 x sb1⋯
Qk x kβ, a satisfiably equivalent 
-formula 
 with
additional operation symbols such that 
. The
construction of 
 will be completed after m steps, where m is
the number of 
-quantifiers among the 
. Take α = αsb0 and αi to be
already constructed. If αi is already an 
-formula let 
. Otherwise αi has the form ∀x sb1⋯ ∀x n
∃yβi for some n ≥ 0. With an n-ary operation symbol f (which is a constant in
case n = 0) not yet used let 
. Thus, after m steps an 
-formula 
 is obtained
such that 
; this
formula 
 is called a Skolem normal form (SNF) of α.
Example 1. If α is the formula 
 then 
 is just ∀x x < fx.For α = ∃x ∀y x ⋅ y = y we have 
.If α = ∀x ∀y ∃z(x < z ∧ y < z) then 
.
Theorem 6.3.
Let 
 be a Skolem normal form for the formula α. Then
(a) 
 , (b) α is satisfiably equivalent to 
Proof. (a): It suffices to show that αi + 1 ⊨ αi for each of the described
construction steps. 
implies 
, by (c) and (d) in 2.5. (b): If 
 is satisfiable then by (a) so too
is α. Conversely, suppose 

. For each 
 we choose some b ∈ A
such that 
 (which is
possible in view of the axiom of choice 
) and expand 
 to 
 by setting 
 for the new operation
symbol. Then evidently 
. Thus, we finally
obtain a model for 
 that expands the initial model.
Now, for each α, a tautologically equivalent ∃-formula 
 is gained as well (that is, 
). By the above
theorem, we first produce for β = ¬α a satisfiably equivalent SNF 
 and put 
. Then
indeed 
,
because
Example 2. For 
 we have
¬α ≡ β : = ∀x ∃y(ry ∧ ¬rx) and 
.
Thus, 
.
The last formula is a tautology. Indeed, if 
 then clearly 
. But the same holds if 
, for then never 
. Thus, 
 and hence also α is
a tautology, which is not at all obvious after a first glance at α. This shows how
useful Skolem normal forms can be for discovering tautologies.
Remark 2. There are many applications of Skolem normal forms, mainly in
model theory and in logic programming. For instance, Exercise 5 permits one to
reduce the satisfiability problem of an arbitrary first-order formula set to a set of 
-formulas (at the cost of adjoining new function symbols).
Moreover, a set X of 
-formulas is satisfiably equivalent to a set X′
of open formulas as will be shown in 4.1, and this problem can be reduced
completely to the satisfiability of a suitable set of propositional formulas, see
also Remark 1 in 4.1. The examples of applications of the propositional

compactness theorem in 1.5 give a certain feeling for how to proceed in this way.
Exercises
1. Suppose that T f results from T by adjoining an explicit definition η for f
and let 
 be constructed as explained
in the text. Show that T f is a conservative extension of T if and only if η
is a legitimate explicit definition.
2. Let S n↦n + 1 denote the successor function in 
. Show that 
 is a definitorial extension of 
; in other words,
0 and + are explicitly definable by S and ⋅ in 
.
3. Prove that 
is a legitimate explicit definition in T G (it suffices to prove 
). Show in addition that 
. Thus, T = G is a
definitorial and hence a conservative extension of T G. In this sense, the
theories T = G and T G are equivalent formulations of the theory of
groups.
4. As is well known, the natural < -relation of 
 is
explicitly definable in 
, for instance, by 
. Prove that the < -
relation of 
 is not explicitly definable in 
.
5. Construct to each 
 an SNF 
 such that X is satisfiably equivalent to 
 and 
, called a Skolemization of X. Since we do not suppose that X is countable,
the function symbols introduced in 
 must properly be
indexed.

1
2
3
4
5
References
Ta2. _________ , Introduction to Logic and to the Methodology of Deductive Sciences, Oxford 1941, 3⟨{
rd}⟩ ed. Oxford Univ. Press 1965 (first edition in Polish, 1936).
Ta1. A. TARSKI, Der Wahrheitsbegriff in den formalisierten Sprachen, Studia Philosophica 1 (1936), 261–
405 (first edition in Polish, 1933), also in [Ta4, 152-278].
HB. D. HILBERT, P. BERNAYS, Grundlagen der Mathematik, I, II, Berlin 1934, 1939, 2⟨{ nd}⟩ ed. Springer,
Vol. I 1968, Vol. II 1970.
Footnotes
Here r and f represent the general case and look different in a concrete situation. Relation symbols are
also called predicate symbols, in particular in the unary case, and operation symbols are sometimes called
function symbols. In special contexts, we also admit n = 0, regarding constants as 0-ary operations.
abbreviates ‘there is some 
 with 
and 
’. If 
is onto (and only this case will occur in our applications) then (S) is equivalent to the more suggestive
condition 
.
Since rkφ < rk ∀xφ, we may assume according to the recursive construction of σ that 
 is already defined for all global substitutions τ.
Since this equation is to mean 
, 
is not uniquely determined by φ. Hence, the phrase “Let 
”
implicitly includes along with a given φ also a tuple 
 given in advance. The notation 
 does not even state that φ contains free variables at all.
A suggestive way of writing “X ⊨ α, β implies X ⊨ α ∧ β,” a notation that was introduced already in
Exercise 3 in 1.3. A corresponding notation will also be used in stating the properties of ⊨ on the next
page.

6 Consistent mostly refers to a logic calculus, e.g., the calculus in 3.1. However, it will be shown in 3.2 that
consistency and satisfiability of a theory coincide, thus justifying the word’s ambiguous use.

(1)
Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-
1-4419-1221-3_3, © Springer Science+Business Media, LLC 2010
3. Complete logical Calculi
Wolfgang Rautenberg
1  
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
Wolfgang Rautenberg
Email: raut@math.fu-berlin.de
Abstract
Our first goal is to characterize the consequence relation in a first-order language
by means of a calculus similar to that of propositional logic. That this goal is
attainable at all was shown for the first time by Gdel in [Gö1]. The original
version of Gdel’s theorem refers to the axiomatization of tautologies only and
does not immediately imply the compactness theorem of first-order logic; but a
more general formulation of completeness in 3.2 does. The importance of the
compactness theorem for mathematical applications was first revealed in 1936
by A. Malcev, see [Ma].
The characterizability of logical consequence by means of a calculus (the
content of the completeness theorem) is a crucial result in mathematical logic
with far-reaching applications. In spite of its metalogical origin, the
completeness theorem is essentially a mathematical theorem. It satisfactorily
explains the phenomenon of the well-definedness of logical deductive methods
in mathematics. To seek any additional, possibly unknown methods or rules of
inference would be like looking for perpetual motion in physics. Of course, this
insight does not affect the development of new ideas in solving open questions.
We will say somewhat more regarding the metamathematical aspect of the
theorem and its applications, as well as the use of the model construction
connected with its proof in a partly descriptive manner, in 3.3, 3.4, and 3.5.
Our first goal is to characterize the consequence relation in a first-order language

by means of a calculus similar to that of propositional logic. That this goal is
attainable at all was shown for the first time by Gdel in [Gö1]. The original
version of Gdel’s theorem refers to the axiomatization of tautologies only and
does not immediately imply the compactness theorem of first-order logic; but a
more general formulation of completeness in 3.2 does. The importance of the
compactness theorem for mathematical applications was first revealed in 1936
by A. Malcev, see [Ma].
The characterizability of logical consequence by means of a calculus (the
content of the completeness theorem) is a crucial result in mathematical logic
with far-reaching applications. In spite of its metalogical origin, the
completeness theorem is essentially a mathematical theorem. It satisfactorily
explains the phenomenon of the well-definedness of logical deductive methods
in mathematics. To seek any additional, possibly unknown methods or rules of
inference would be like looking for perpetual motion in physics. Of course, this
insight does not affect the development of new ideas in solving open questions.
We will say somewhat more regarding the metamathematical aspect of the
theorem and its applications, as well as the use of the model construction
connected with its proof in a partly descriptive manner, in 3.3, 3.4, and 3.5.
Without beating around the bush, we deal from the outset with the case of an
arbitrary, not necessarily countable first-order language. Nonetheless, the proof
given, based on Henkin’s idea of a constant expansion [He], is kept relatively
short, mainly thanks to an astute choice of its logical basis. Although
mathematical theories are countable as a rule, a successful application of
methods of mathematical logic in algebra and analysis relies essentially on the
unrestricted version of the completeness theorem. Only with such generality
does the proof display the inherent unity that tends to distinguish the proofs of
magnificent mathematical theorems.
3.1 A Calculus of Natural Deduction
As in Chapter 2, let  be an arbitrary but fixed first-order language in the logical
signature 
. We define a calculus ⊢ by the system of deductive rules
enclosed in the box below. The calculus operates with sequents as in 1.4. It
supplements the basic rules given there with three predicate-logical rules. We
also use the same modes of speaking, for instance, ‘X ⊢ α’ is read as ‘X
derivable α’. Note that the initial rule (IR) is subject to a minor extension. Using
(MR), it could be pared down to 
 and 
, which are rules without premises
like (IR).

By (IR), X ⊢ t = t for arbitrary X and t, in particular ⊢ t = t. Here as
everywhere, ⊢ φ stands for ∅ ⊢ φ (read ‘φ is derivable’). The remaining
notation from 1.4 is also used here; thus, α ⊢ β abbreviates {α} ⊢ β, etc. Note
also that α ⊢ β ⊢ γ can have only the meaning α ⊢ β and β ⊢ γ.
⊢ is called a calculus of natural deduction because it models logical
inference in mathematics and other deductive sciences sufficiently well. 1 Our
aim is to show that ⊨ is completely characterized by ⊢. The calculus is
developed in the sequel only insofar as the completeness proof requires. While
undertaking further derivations can be instructive (see the examples and
exercises), this is not the principal point of formalizing proofs unless one is after
specific proof-theoretic goals. It should also be said that an acute study of
formalized proofs does not really promote our ability to draw correct
conclusions in everyday life.
All basic rules are sound in the sense of 1.4. The restrictions in the rules 
,
, and (=) ensure their soundness as shown by the properties (a), (g), and (b)
on page 79. Rule ( = ) could have been strengthened from the outset to allow α to
be any formula such that 
 are collision-free, but we get along with the
weak version. Also 
 could be strengthened by weakening its restriction that 
 are collision-free in various ways. As already stated in 2.3, we could in fact
avoid any kind of restriction by means of a more involved definition for
substitution. However, such measures would unnecessarily strengthen the
calculus. Weakly formulated logical calculi like the one given here often
alleviate certain induction procedures, for example in verifying these rules in
other logical calculi, as will be done in 3.6 for a certain Hilbert calculus.
Because ⊢ can be understood as an extension of the corresponding calculus
from 1.4, all the examples of provable rules given there carry over automatically,
the cut rule included. All further sound rules, such as the formal versions of
generalization and particularization in 2.5, are provable thanks to the
completeness of the calculus. This is also true of the rule 
),

which is sound by (d) in 2.5, though it does not result directly from (
).
However, we do not want to spend too much time on the proofs of other rules;
they are irrelevant for the completeness proof, which can then be used to justify
these rules retrospectively.
Just as in the propositional case the following proof method referring to the
base rules above will often be applied; it is legitimate because the proof of the
corresponding principle in 1.4 depends on neither the language nor the concrete
rules.
Principle of rule induction. Let  be a property of sequents (X,α) such that
(o)
 provided α ∈ X or α is of the form t = t,
 
(s)
 for (MR) , and similarly for (∧1)–(=). 
Then X ⊢ α implies 
 , for all X,α.
Since the basic rules are clearly sound, the soundness of the calculus, that is
to say 
, follows immediately by rule induction. Similarly one obtains the
following monotonicity property:
.
Here the derivability relation is indexed; note that every elementary language
defines its own derivability relation, and for the time being we are concerned
with the comparison of these relations in various languages. Only with the
completeness theorem will we see that the indices are superfluous, just as for the
consequence relation ⊨. To prove (mon) let 
 be the property ‘
’, for
which the conditions (o) and (s) of rule induction are easily verified. To confirm
at least one of the induction steps (i.e. the verification of (s) for each single rule),
let 
 and suppose 
. Then ( ∧ 1), applied in 
, yields 
as well.
As in propositional logic we have here the easily provable
Finiteness theorem. If X ⊢ α then X 0 ⊢ α for some finite X 0 ⊆ X.
The only difference to the proof from 1.4 is that a few more rules have to be
considered. Remember that L denotes the signature of , L 0 that of 
, etc. For
the moment we require a somewhat stronger version of the finiteness theorem,
namely
(fin) If 
 then there exist a finite signature L 0 ⊆ L and a finite

subset X 0 ⊆ X such that 
 .
Herein the claim 
, of course, includes 
. For the proof,
consider the property ‘there exist some finite X 0 ⊆ X and L 0 ⊆ L such that 
 ’. It suffices to confirm the conditions (o) and (s) of the principle of rule
induction. For α ∈ X ∪t = t we clearly have 
, where X 0 = { α} or X 0 =
∅. Thus, L 0 may be chosen to contain all the extralogical symbols occurring in
α or in t = t, and their number is surely finite. This confirms (o). The induction
step on (MR) is trivial. For ( ∧ 1) suppose 
 and 
 for some finite
X i ⊆ X and L i ⊆ L, i = 1, 2. Then (mon) gives 
, where X 0 = X 1 ∪X 2
and L 0 = L 1 ∪L 2. Applying ( ∧ 1) in ℒ 0, we obtain 
, which is
what we want. The induction steps for all remaining rules proceed similarly and
are even somewhat simpler. This confirms condition (s), which in turn proves
(fin).
In the foregoing proof, 
 contains at least the extralogical symbols of X 0
and α but perhaps also some others. Only with the completeness theorem can we
know that the symbols occurring in X 0, α in fact suffice. This insensitivity of
derivation with respect to language extensions can be derived purely proof-
theoretically, albeit with considerable effort, but purely combinatorially and
without recourse to the infinitistic means of semantics. A modest demonstration
of such methods is the constant elimination by Lemmas 2.1 and 2.2 from the
next section.
Now for some more examples of provable rules required later.
Example 1. (a) 
, (b) 
, (c) 
.
To show (a) let 
 and let α be the formula x = t′. Then the premise of
(a) is written 
. Rule ( = ) yields 
. Now, 
 equals t = t′,
since 
; hence X ⊢ t = t′. (b) is obtained immediately from (a), choosing
there t′ = s because X ⊢ s = s. And with this follows (c), for thanks to (b), the
premise of (c) now yields 
 and hence, by (a), the conclusion of
(c).
Example 2. In (a)–(d), n is as usual the arity of the symbols f and r. (a) and
(c) are provable for i = 1, …, n. In order to ease the writing, 
abbreviates 
, so that, for instance, rule (b) below has
actually n premises:

Proof of (a): Let X ⊢ t i = t and 
, where x is not to
occur in any of the t j . Since 
), it follows that 
 using (
= ). This is the conclusion of (a). (b) is then obtained by considering Example
1(c) and the n-fold iteration of (a), as can best be seen by first working through
the case n = 2. Rule (c) is just another application of (=) by taking the formula 
 for α, where again, x is supposed not to occur in any of the t j
. Applying (c) n times then yields (d).
The next example indicates that sometimes considerable effort is needed in
formally deriving what is nearly obvious from the semantic point of view. Of
course, difficulties in formal proofs strongly depend on the calculus in which
these proofs are carried out.
Example 3. (a) ⊢ ∃x t = x, for all x, t with 
, (b) ⊢ ∃x x = x.(a) holds
because 
 gives 
, for t≠t equals (t≠x) \frac{t} {x} ; here 
 is required. Clearly, ∀x t≠x ⊢ t = t as well. Thus, by ( ¬1),
Trivially, also 
. Therefore, by ( ¬2), ⊢ ∃x t =
x. The assumption x∉var} t is in fact essential in order to derive ∃x t = x; cf.
Example 1 in 2.3 page 63. (b) is verified similarly, starting with ∀x x≠x ⊢ x≠x, x
= x.
A set 
 is called inconsistent if X ⊢ α for all 
, and otherwise
consistent, exactly as in propositional logic. A satisfiable set X is evidently
consistent. By ( ¬1), the inconsistency of X is equivalent to X ⊢ α, ¬α for any α,
hence also to 
, since 
 and certainly 
) by
Example 3.
The relation ⊢ is completely characterized by some inconsistency condition,
as in 1.4. Indeed, the proofs of the two properties
from Lemma 1.4.2 remain correct for any meaningful definition of 
. The
properties C+ and C− will permanently be used in the sequel without our
explicitly referring to them.
As in propositional logic, 
 is called maximally consistent if X is consistent
but each proper extension of X in 
 is inconsistent. There are various
characterizations of this property, e.g. the one in Exercise 4, known already from

1.4. Examples of maximally consistent sets are the 
 for 
-
models 
, the typical ones as will turn out.
Exercises
1. Derive the rule 
 collision-free).
 
2. Prove 
 and 
 provided y∉var} α.
 
3. Using Exercise 2 and the cut rule prove 
.
 
4. Show that 
 is maximal consistent iff either φ ∈ X or ¬φ ∈ X for
each 
. This easily implies that a maximally consistent set X is
deductively closed, i.e. 
, for each 
.
 
3.2 The Completeness Proof
Let ℒ be a language and c a constant symbol. 
 is the result of adjoining c to 
. We have 
 if c occurs already in 
. Similarly 
 denotes the language
resulting from  by adjoining a set C of constants, a constant expansion of .
We shall also come across such expansions in Chapter 5. Let 
 (read “α z for
c”) denote the formula arising from α by replacing c with the variable z, and put 
. c then no longer occurs in 
. We actually require the
following assertion only for a single variable z, but as is often the case, we are
able too prove by induction only a stronger version unproblematically.
Lemma 2.1 (on constant elimination).
Suppose 
 . Then 
 for almost all variables z.
Proof by rule induction in 
. If α ∈ X then 
 is clear; if α is of the
form t = t, so too is 
. Thus, 
 in either case, even for all z. Only the
induction steps on ( ∀1), ( ∀2), and ( = ) are not immediately apparent. We
restrict ourselves to ( ∀1), because the induction steps for 
 and ( = ) proceed
analogously. Let 
 be collision-free, 
, and assume that 
for almost all z. In addition, we may suppose that 
 for almost all

z. A separate induction on α readily confirms 
 with 
 and 
. Clearly 
 are collision-free as well. By our assumption, X\frac{z}
{c} ⊢ ℒ ( ∀x α)\frac{z} {c} = ∀xα′. Rule 
 then clearly yields X\frac{z} {c}
⊢ ℒ α′\frac{t′} {x} = α\frac{t} {x}\frac{z} {c}, and this holds still for almost all
z which completes the proof of the induction step on 
.
This lemma leads to the following rule of “constant quantification,” the
semantic counterpart of which plays an essential role in Chapter 5:
Indeed, suppose that 
. Because of the finiteness theorem we may assume
that X is finite. By Lemma 2.1, where in the case at hand 
, some y not
occurring in X ∪{ α} can be found such that 
 (the latter holds
because c does not occur in α). Since 
, we thus obtain 
. Hence X
⊢ ∀xα by ( ∀2). This confirms ( ∀3). A likewise useful consequence of constant
elimination is
Lemma 2.2.
Let C be any set of constant symbols and 
 . Then 
 , for all
X ⊆ℒ and 
 . Thus, 
 is a conservative expansion of 
 .
Proof. (mon) states that 
. Suppose conversely that 
. To
prove 
 we may assume, thanks to (fin) and (MR), that C is finite. Since the
adjunction of finitely many constants can be undertaken stepwise, we may
suppose for the purpose of the proof that 
 for a single constant c not
occurring in . Lemma 2.1 then yields 
 for at least one variable z.
Now, 
 means the same as 
 because c occurs neither in X nor in α.
In the following, we represent the derivability relation in 
 and in every
constant expansion 
 of  with the same symbol ⊢. By Lemma 2.2 no
misunderstandings can arise from this notation. Since the consistency of X is
equivalent to 
, there is also no need to distinguish between the consistency
of 
 with respect to 
 or 
. This is highly significant for the proofs of the
next two lemmas.
The proof of the completeness theorem essentially proceeds with a model
construction from the syntactic material of a certain constant expansion of 
.
We first choose for each variable x and each 
 a constant 
 not occurring

in 
; more precisely, we choose exactly one such constant for each pair x, α.
Define (1) 
.
Here it is insignificant how many free variables α contains, and whether x
occurs at all in α. Note that 
. The formula on the right side
tells us that under the hypothesis ∃x ¬α the constant c represents a
counterexample to the validity of α, that is, an example for the validity of ¬α.
Note also that 
 whenever 
.
Lemma 2.3.
Let 
 , where α x is defined as in (1) , and let X ⊆ℒ
be consistent. Then 
 is consistent as well.
Proof. Assume that 
. There exist some n ≥ 0 and formulas 
 such that (a) 
. Since 
, there is
some minimal n with (b) 
. Let x : = x n , α : = α n ,
and 
. By (a), 
. Hence, X′ ⊢ α x , and so 
, by
( ∧ 2). But 
 yields X′ ⊢ ∀xα using ( ∀3), since c does not occur in X′ and
α. Thus, 
, whence 
, contradicting (b) and hence our
assumption. ___
Call 
 a Henkin set if X satisfies the following two conditions: 
(H1) and (H2) imply another useful property of a Henkin set X, namely
Indeed, 
) for x∉var} t by Example 3 in 3.1. Hence,
X⊬ ∀x t≠x in view of (H1). Thus, X⊬t≠c for some c by (H2), and so X ⊢ t = c
by (H1).
Lemma 2.4.
Let 
 be consistent. Then there exists a Henkin set 
 in a suitable
constant expansion 
 of  .

Proof. Put 
, X 0 : = X and assume that 
 and X n have been given. Let 
 result from 
 by adopting new constants 
 for all x ∈ Var, 
;
more precisely, 
, with the set C n of constants 
. Further, let 
. Here 
 is defined as in Lemma 2.3 so that 
. Using
Lemma 2.3 we have 
 for each n. Let 
; hence 
, where 
. Then 
, since X′, as the union
of a chain of consistent sets, is surely consistent (in 
). Let 
, x ∈ Var, and,
say, 
 with minimal n, and let α x be the formula defined as in (1) but with
respect to ℒn. Then ¬α x belongs to X n + 1. Hence ¬α x ∈ X′. Now let 
 be
the partial order of all consistent extensions of X′ in 
. Every chain K ⊆ H has
the upper bound ⋃K in H, because if all members of K are consistent then so is
⋃K. Also H≠∅, e.g. X′ ∈ H. By Zorn’s lemma, H therefore contains a maximal
element Y. In short, Y is a maximal consistent extension of X′. Since ¬α x ∈ X′ ⊆
Y it holds
(2) Y ⊢ ¬α x for all 
.
Further, Y is at the same time a Henkin set. Here is the proof: (H1) 
: Y ⊢
¬α implies Y ⊬ α due to the consistency of Y. ⇐ : If Y ⊬ α then surely α ∉ Y. As
a result 
, for Y is maximally consistent. Thus Y ⊢ ¬α by C−. You may
also use Exercise 4 in 3.1
(H2) 
: Clear by ( ∀1). ⇐ : Let 
 for all c in 
, so also Y ⊢ α\frac{c}
{x} for 
, where n is minimal with 
. Assume that 
. Then
Y ⊢ ¬ ∀xα by (H1). But 
 implies 
 using ( ∧
1). Now, since Y is consistent, Y ⊢ α x which contradicts (2). Thus, our
assumption was wrong and indeed Y ⊢ ∀xα.
Remark 1. In the original language , consistent sets are not generally
embeddable in Henkin sets. For instance, let the signature of 
 consist of the
constants c i , i ∈ I with any infinite set I. Then the consistent set X = v 0≠c i ∣i ∈
I represents a counterexample in ℒ =. In no consistent extension of X can be
derived v 0 = c i for some i ∈ I. In other words, (H3) is violated.
Lemma 2.5.
Every Henkin set 
 possesses a model.
Proof. The model constructed in the following is called a term model. Let t ≈ t′

whenever Y ⊢ t = t′. The relation ≈ is a congruence in the term algebra 
 of ℒ.
This means (repeating the definition on page 51) that
(a) ≈ is an equivalence relation,
 
(b)
, for operation symbols f. 
The claim (a) follows immediately from Y ⊢ t = t and Example 1 in 3.1; (b)
is just another way of formulating 2(b). Let 
. Here  denotes the
equivalence class of ≈ containing the term t, so that
(c)
. 
This set A is the domain of the sought model 
 for Y. The
factorization of 
 will ensure that = means identity in the model. Let C be the
set of constants in . By (H3) there is for each term t in 
 some c ∈ C such that
c ≈ t. Therefore even 
. Now put 
 and 
 for
variables and constants in ℒ. An operation symbol f occurring in 
 of arity n > 0
is interpreted by 
, defined by
This definition is sound because ≈ is a congruence in the term algebra 
.
Finally, define 
 for an n-ary relation symbol r by
This definition is also sound, since 
 implies 
 whenever 
. Here we use Example 2(d) in 3.1. We shall prove
of which (e) may be regarded as the goal of the constructions. (d) follows by
term induction. It is evident for prime terms, and the induction hypothesis 
 for i = 1, …, n leads with 
 to
(e) follows by induction on rkα. We begin with formulas of rank 0 (prime
formulas). Induction proceeds under consideration of rkα < rk ¬α, rkα, rkβ <
rk(α ∧ β), and 
, as in formula induction:

Because of Y ⊢ α for all α ∈ Y, (e) immediately implies 
.
⇔
 for all c ∈ C
(because 
)
 
⇔
 for all c ∈ C
(because 
)
 
⇔
 for all c ∈ C
(substitution theorem)
 
⇔
 for all c ∈ C
(induction hypothesis)
 
⇔
Y ⊢ ∀xα
(using (H2) ).
Just as for propositional logic, the equivalence of consistency and
satisfiability, and the completeness of ⊢, result from the above. Information
about the size of the model constructed in the next theorem will be given in
Theorem 4.1.
Theorem 2.6 (Model existence theorem).
Each consistent 
 (in particular, each consistent theory T in  ) has a
model.
Proof. Let Y ⊇ X be a Henkin expansion of X, i.e., a Henkin set in a suitable
constant expansion ℒC according to Lemma 2.4. By Lemma 2.5, Y and hence
also X has a model 
 in 
. Let 
 denote the 
-reduct of 
. In other
words, “forget” the interpretation of the constants not in 
. Then, by Theorem
2.3.1, 
 holds as well. ___
Theorem 2.7 (Completeness theorem).
Let 
 denote any first-order language. Then X ⊢ α ⇔ X ⊨ α, for all 
 and 

 .
Proof. The soundness of ⊢ states that X ⊢ α ⇒ X ⊨ α. The converse follows
indirectly. Let X ⊬ α, so that X, ¬α is consistent. Theorem 2.6 then provides a
model for X ∪ ¬α}, whence X⊭ α.
Thus, ⊨ and ⊢ can henceforth be freely interchanged. We will often confirm
X ⊢ α by proving X ⊨ α in a semi-formal manner as is common in mathematics.
In particular, for theories T, T ⊨ α is equivalent to T ⊢ α, for which in the
following we mostly write 
. Clearly, ⊢ T α means the same as α ∈ T for
sentences α. More generally, let X ⊢ T α stand for X ∪T ⊢ α and α ⊢ T β for
{α} ⊢ T β. We will also occasionally abbreviate α ⊢ T β & β ⊢ T γ to α ⊢ T β
⊢ T γ. In subsequent chapters, equivalences such as 
 and 
 will be used without further
mentioning and should be committed to memory. Several other useful
equivalences are listed in Exercise 4.
Remark 2. The methods in this section easily provide also completeness of a
logical calculus for identity-free (or = -free) languages in which = does not
appear, considered in the exercises and Chapter 4. Simply discard from the
calculus in 3.1 everything that refers to =. Most things run as before. The domain
of 
 is now the set 
 of all terms of 
 without a factorization of 
, so that 
. Note that (H3) is not to our disposal anymore so that the proof of
Lemma 2.5 must be modified. We will not go into details, since we need in 4.1
only a slight generalization of Exercise 1. In any case, consistency of a = -free
set X means the same, no matter whether X is regarded as belonging to a
language with or without =, because X has a model in either case. Moreover, if X
consists of -formulas only, we come along without a Henkin expansion in
constructing a model as will be shown in Theorem 4.1.1. The set of terms of the
original language is sufficient for model construction in this case.
Exercises
1. Let 
 be = -free, 
 the set of its ground terms, and 
 a
consistent set of -sentences. Construct a model 
 on the domain 
 by setting 
, 
 (hence 
 for all 
, shown by
induction on t), and 
 (
; X ⊇ U maximally
consistent, so that 
, for all 
, cf. e.g. Lemma 1.4.4).
 is called a Herbrand model; see also 4.1.
 

2. Let K≠∅ be a chain of theories in 
, i.e., T ⊆ T′ or T′ ⊆ T, for all T, T′
∈ K. Show that ⋃K is a theory that is consistent iff all T ∈ K are
consistent.
 
3. Suppose T is consistent and 
. Prove the equivalence of
(i) 
, (ii) ⊢ T ¬α for some conjunction α of formulas in Y.
 
4. Let x∉var} t and 
 collision-free. Verify the equivalence of
(i) 
, (ii) x = t ⊢ T α, (iii) ⊢ Tx = t → α, (iv) ⊢ T ∀x(x = t →
α), (v) ⊢ T ∃x(x = t ∧ α).
 
3.3 First Applications: Nonstandard Models
In this section we draw important conclusions from Theorem 2.7 and the model-
construction for proving it. Since the finiteness theorem holds for the provability
relation ⊢, Theorem 2.7 immediately yields
Theorem 3.1 (Finiteness theorem for the consequence relation).
X ⊨ α implies X 0 ⊨ α for some finite subset X 0 ⊆ X.
Let us consider a first application. The first-order theory of fields of
characteristic 0 is axiomatized by the set X containing the axioms for fields and
the formulas 
 (page 48). We claim that
(1) A sentence α valid in all fields of characteristic 0 is also valid in all
fields of sufficiently high prime characteristic p that depends on α.
 
Indeed, since X ⊨ α, for some finite subset X 0 ⊆ X we have X 0 ⊨ α. If p is
a prime number larger than all prime numbers q with ¬char q ∈ X 0, then α holds
in all fields of characteristic p, since these satisfy X 0. Thus (1) holds. From (1)
we obtain, for instance, the information that two given polynomials coprime
over all fields of characteristic 0 are also coprime over fields of sufficiently high
prime characteristic. The statement that given polynomials are coprime is readily
formalized in 
.
A noteworthy consequence of Theorem 3.1 is also the nonfinite
axiomatizability of many elementary theories. Before presenting examples, we
explain finite axiomatizability in a somewhat broader context.
A set Z of strings of a given alphabet A is called decidable if there is an

algorithm (a mechanical decision procedure) that after finitely many calculation
steps provides us with an answer to the question whether a string ξ of symbols of
A belongs to Z; otherwise Z is called undecidable. Thus it is certainly decidable
whether ξ is a formula. While this is all intuitively plausible, it nonetheless
requires more precision (undertaken in 6.2). A theory T is called recursively
axiomatizable, or just axiomatizable, if it possesses a decidable axiom system.
This is the case, for instance, if T is finitely axiomatizable, that is, if it has a
finite axiom system.
From (1) it follows straight away that the theory of fields of characteristic 0
is not finitely axiomatizable. For were F a finite set of axioms, their conjunction
α = ∧F would, by (1), also have a field of finite characteristic as a model. Here
is another instructive example. An abelian group  is called ndivisible if 
with 
, where ny is the n-fold sum 
, and  is called
divisible if 
 for all n ≥ 1. Thus, the theory of divisible abelian groups,
DAG, is axiomatized by the set X consisting of the axioms for abelian groups
plus all sentences 
. Also DAG is not finitely axiomatizable. This follows as
above from
(2) A sentence 
 valid in all divisible abelian groups is also valid
in at least one nondivisible abelian group.
 
To prove (2), let α ∈ DAG, or equivalently X ⊨ α. According to Theorem
3.1, X 0 ⊨ α for some finite X 0 ⊆ X. Let 
 be the cyclic group of order p,
where p is a prime > n for all n with 
. The mapping x↦nx from 
 to
itself is surjective for 0 < n < p; otherwise 
 would be a nontrivial
subgroup of 
 which cannot be. Hence, 
 for 0 < n < p. Thus, 
and so 
. On the other hand, 
 is not p-divisible because px = 0 for all 
. In exactly the same way, we can show that the theory of torsion-free
abelian groups is not finitely axiomatizable. In these groups is na≠0 whenever n,
a≠0.
In a similar manner, it is possible to prove for many theories that they are not
finitely axiomatizable. However, this often demands more involved methods. For
instance, consider the theory ACF of a.c. fields (see p. 48). It results from
adjoining to the (finitely axiomatizable) theory of fields the schema of all
sentences 
, where 
 denotes the term 

, called a monic polynomial of degree n
+ 1. Here let 
 denote distinct variables. Thus, in an a.c. field every
monic polynomial has a zero, and so does every polynomial of positive degree.
Nonfinite axiomatizability of ACF follows from the by no means trivial
existence proof of fields in which all polynomials up to a certain degree do
factorize but irreducible polynomials still exist.
As in propositional logic, the finiteness theorem for the consequence relation
leads immediately to the corresponding compactness result:
Theorem 3.2 (Compactness theorem).
Any set X of first-order formulas is satisfiable, provided every finite subset of
X is satisfiable.
Because of the finer structure of first-order languages, this theorem is
somewhat more amenable to certain applications than its propositional
counterpart. It can be proved in various ways, even quite independent of a
logical calculus; for instance, by means of ultraproducts, as will be carried out in
5.7. It can also be reduced to the propositional compactness theorem, see
Remark 1 in 4.1. For applications of Theorem 3.2 we will concentrate on the
construction of nonstandard models.
A theory 
 is called complete if it is consistent and has no consistent
proper extension in ℒ0. It is easily seen that the completeness of T is equivalent
to either ⊢ T α or ⊢ T ¬α but not both, for each 
. Hence, 
 is
complete for each 
-structure 
. Other equivalences of completeness are given
by Theorem 5.2.1. Note that completeness of a theory is not related to the
completeness theorem in 3.2.
We will frequently come across the theory 
 with 
. Here
 is the successor function. 
 is the standard structure for the
arithmetical language 
. The choice of signature is a matter of
convenience and has a long tradition. Of relations definable in 
, we name just
≤ and <, defined by x ≤ y ↔ ∃z z + x = y, and x < y ↔ x ≤ y ∧ x≠y. This will be
our standard definitions of the symbols ≤ and < in 
.
Certain axiomatic subtheories of the complete theory 
 are even more
frequently dealt with, in particular the so-called Peano arithmetic 
, a first-
order theory in ℒar that is important both for mathematical foundations as well
as for investigations in computer science; see e.g. [Kra]. The axioms of 
 are
as follows:

IS is called the induction schema and should not be mixed up with the
induction axiom IA discussed on page 39. In IS, φ runs over all formulas from 
, i.e., IS reads more precisely 
; see our
convention in 2.5. With IS one can prove 
 by induction on x: First
confirm 
 (induction initiation), and then 
, or
equivalently, 
 (induction step). The latter means the derivation of the
induction claim 
 from the induction hypothesis φ.
Example. Let φ be the formula 
. We prove 
. In
other words, each x≠0 has a predecessor, not something seen at once from the
axioms. Clearly, 
. Since 
, we get 
 (particularization). Since 
 as well,
we obtain 
. This confirms the
induction step 
. Hence, 
 by IS. The above is easily
supplemented by an inductive proof of 
.
Remark 1. Only a few arithmetical facts (for instance, ∀x 0 ≤ x) are
derivable in PA without IS. Already the derivation of such simple statements as 
 and 
 needs IS. The schema IS is extremely strong. In 7.2 it
will then become clear that 
 fully embraces elementary number theory and
practically the whole of discrete mathematics. More about 
 in the exercises;
these are exclusively devoted to 
, in order to give the reader familiarity with
this important theory as early as possible. Despite its strength, 
 is incomplete,
as will be shown in 6.5. It is not of any import that subtraction is only partially
defined in 
. A theory of integers, formulated similarly to 
, may be more
convenient for number theory, but is actually not stronger than 
; it is
interpretable in 
 in the sense of 6.6. We mention that 
 is not finitely
axiomatizable, shown for the first time in [Ry]. For this and other historical
remarks see, e.g., [HP].
We will now prove that not only 
 but also the complete theory 
 has
alongside the standard model 
 other models not isomorphic to 
, called
nonstandard models. In these models, exactly the same theorems hold as in 
.

The existence proof of a nonstandard model 
 of 
 is strikingly simple.
Let x ∈ Var and 
. Here and throughout the text we
use n to denote the term 
.
Therefore, 
, and generally 
. The term  (that is, 
) is mostly denoted by 0. Note that 
 is the formula 
. One may
replace x here by a constant symbol c, thus expanding the language. But both
approaches lead to the same result.
Every finite subset X 0 ⊆ X possesses a model. Indeed, there is evidently
some m such that 
, and X 1 certainly has a
model: one need only assign to x in 
 the number m. Thus, by Theorem 3.2, X
has a model 
 with the domain 
, where 
 denotes the interpretation
of x. We know that 
 satisfies all sentences valid in 
, including in particular
the sentences 
 and 
. Therefore, 
constitutes an embedding from 
 into 
 whose image can be thought of as
coinciding with 
. 2 In other words, it is legitimate to assume that 
 so
that 
.
Because 
, on the one hand 
 is elementarily equivalent to 
, and on
the other n < a for all n and any 
, since in 
 and hence in 
 we have 
. In short,  is a (proper) initial segment of 
, or 
 is an end
extension of 
. The elements of 
 are called nonstandard numbers.
Alongside c, other examples are c + c and 
 for 
. Clearly, c has both an
immediate successor and an immediate predecessor in the order, because 
. The figure gives a rough picture of a nonstandard model 
:
 has the same number-theoretic features as 
, at least all those that can be
formulated in 
. These include nearly all the interesting ones, as will turn out
to be the case in 7.1. For example, 
 holds in every
model of 
, that is, every nonstandard number is either even or odd. Clearly,

the model 
 contains gaps in the sense of 2.1. The most obvious example is 
.
Remark 2. Theorem 4.1 will show that 
 has also countable
nonstandard models. The order of such a model 
 is easy to make intuitive: it
arises from the half-open interval [0, 1) of rational numbers by replacing 0 with 
 and every other r ∈ [0, 1) by a specimen from . On the other hand, neither 
 nor 
 is effectively describable; see for instance [HP].
Replacing the induction schema IS in the axiom system for 
 by the so-
called induction axiom
results in a categorical axiom system that, up to isomorphism, has just a
single model (see e.g. [Ra5]). How is it possible that 
 is uniquely determined
up to isomorphism by a few axioms, but at the same time nonstandard models
exist for 
? The answer is simple: IA cannot be adequately formulated in 
. That is, IA is not an axiom or perhaps an axiom schema of the first-order
language of 
. It is a sentence of a second-order language, about which we shall
say more in 3.8. However, this intimated limitation regarding the possibilities of
formulation in first-order languages is merely an apparent one, as the
undertakings of the rest of the book will show, especially the considerations
about axiomatic set theory in 3.4.
In no nonstandard model 
 is the initial segment  definable, indeed not
even parameter definable, which means that there is no 
 and no 
 such that 
. Otherwise we would have 
. This yields 
 by IS, in contradiction to 
. The same reasoning shows that no proper initial segment 
without a largest element is definable in 
; such an A would clearly define a gap
in the order of 
. The situation can also be described as gaps in 
 are not
recognizable from within.
Introductory courses in real analysis tend to give the impression that a
meaningful study of the subject requires the axiom of continuity: Every
nonempty bounded set of real numbers has a supremum. On this basis, Cauchy
and Weierstrass reformed analysis, thus banishing from mathematics the
somewhat mysterious infinitesimal arguments of Leibniz, Newton, and Euler.
But mathematical logic has developed methods that, to a large extent, justify the
original arguments. This is undertaken in the framework of nonstandard

analysis, developed above all by A. Robinson around 1950. In the sequel, we
provide an indication of its basic idea.
The same construction as for 
 also provides a nonstandard model for the
theory of 
, where for each real number a, a name a was
added to the signature. Consider 
. Every finite subset
of X has a model on the domain . Thus, X is consistent, and as above, a model
of X represents a proper extension 
 of 
, a nonstandard model of analysis.
In each such model the same theorems hold as in 
. For instance, in 
 every
polynomial of positive degree can be decomposed into linear and quadratic
factors. In Chapter 5 it will be shown that the nonstandard models of 
 are
precisely the real closed extensions of 
. All these are elementarily equivalent
to 
.
For analysis, it is now decisive that the language can be enriched from the
very beginning, say by the adoption of the symbols exp, ln, sin, cos for the
exponential, logarithmic, and trigonometric functions, and further symbols for
further functions. We denote a thus expanded standard model once again by 
and a corresponding nonstandard model by 
. The mentioned real functions
available in 
 carry over to 
 and maintain all properties that can be
elementarily formulated. That means in fact almost all properties with interesting
applications, for example
as well as the addition theorems for the trigonometric functions and so on. All
these functions remain continuous and repeatedly differentiable. However, the
Bolzano–Weierstrass theorem and other topological properties cannot be
salvaged in full generality. They are replaced by the aforementioned
infinitesimal arguments.
In a nonstandard model 
 of 
 with 
 there exist not only infinitely
large numbers c (i.e., r < c for all 
), but also infinitely many small positive
numbers. Let c be infinite. Since 
 for all r > 0, 
 is smaller than
each positive real r, and yet positive. That is, 
 is fairly precisely what Leibniz
once named an infinitesimal. Taking a somewhat closer look reveals the
following picture: Every real number a is sitting in a nest of nonstandard
numbers 
 that are only infinitesimally distinct from a. In other words, 
 is an infinitesimal. Hence, quantities such as dx, dyexist in mathematical
reality, and may once again be considered as infinitesimals in the sense of their

inventor Leibniz. These quantities are exactly the elements of 
infinitesimally distinct from 0.
From the existence of nonstandard models for 
, it can be concluded that
the continuity axiom, just like IA, cannot be elementarily formulated. For by
adjoining this axiom to those for ordered fields, 
 is characterized, up to
isomorphism, as the only continuously ordered field; see e.g. [Ta4]. Hence, the
order of a nonstandard model 
 of 
 possesses gaps. Here, too, the gaps
are “not recognizable from within,” since every nonempty, bounded parameter-
definable subset of 
 has a supremum in 
. That is the case because in 
and thus also in 
, the following continuity schema holds, which ensures the
existence of a supremum for those sets; here 
 runs over all formulas
such that 
:
Analogous remarks can be made on complex numbers. There is an
algebraically closed field 
 in which familiar facts such as Euler’s
formula 
 continue to hold, in particular 
.
Exercises
1. Prove in 
 the associativity, commutativity, and distributivity of 
.
Before proving 
 derive 
 and 
 by
induction on y. The basic arithmetical laws provable in 
 are collected
in the axiom system 
 on page 235.
 
2. ≤ was defined in ℒar on page 34. Reflexivity and transitivity of ≤ easily
derive in 
. Prove in 
 the antisymmetry of ≤.
 
3. Prove 
 (or equivalently, 
). Use this to
prove 
 by induction on x.
 
4. Verify (a),(b), and (c) for arbitrary formulas α, β, γ ∈ ℒar such that 
 and 
. (a) 
, the
schema of <-induction,(b) 
, the
minimum schema,(c) 
, the schema
of collection.
 

3.4 ZFC and Skolem’s Paradox
Before turning to further consequences of the results from 3.2, we collect a few
basic facts about countable sets. The proofs are simple and can be found in any
textbook on basic set theory. A set M is called countable if there is a surjection 
 (i.e. 
 provided fn = a n ) or M = ∅, and otherwise
uncountable. Every subset of a countable set is itself countable. If f M → N is
surjective and M is countable then clearly so too is N. Sets M, N are termed
equipotent, briefly M ∼ N, if a bijection from M to N exists. If 
, then M is
said to be countably infinite. A countable set can only be countably infinite or
finite, which is to mean equipotent to 
 for some 
.
The best-known uncountable set is . It is equipotent to 
. The
uncountability of 
 is a particular case of an important theorem from Cantor:
The power set 
 of a set M has a higher cardinality than M, i.e., no injection
from M to 
 is surjective. The cardinality of sets will be explained to some
extent in 5.1. Here it suffices to know that two sets M, N are of the same
cardinality iff M ∼ N, and that there are countable and uncountable infinite sets.
If M, N are countable so too are M ∪N and M ×N, as is easy to see.
Moreover, as was shown already by Cantor, a countable union 
 of
countable sets M i is again countable. Cantor’s proof consists in writing down U
as an infinite matrix where the nth line enumerates of 
. Then
enumerate the matrix in the zigzag manner indicated by the figure on the right,
beginning with a 00. Accordingly, for countable M, in particular 
,
the set of all finite sequences of elements in M is again countable, because every 
 is countable. Hence, every first-order language with a countable signature
is itself countable, more precisely countably infinite. 3
By a countable theory we always mean a theory formalized in a countable
language. We now formulate a theorem significant for many reasons.

Theorem 4.1 (Lwenheim–Skolem).
A countable consistent theory T always has a countable model.
Proof. By Theorem 2.6, 
 has a model 
 with domain A, consisting of
the equivalence classes  for c ∈ C in the set of all terms of 
, where 
 is a set of new constants. By construction, C 0 is equipotent to 
 and thus countable. The same holds for every C n , and so C is also
countable. The map 
 from C to A is trivially surjective, so that 
 has a
countable (possibly finite) domain, and this was the very claim.
In 5.1 we will significantly generalize the theorem, but even in the above
formulation it leads to noteworthy consequences. For example, there are also
countable ordered fields as nonstandard models of the first-order theory 
 in which the usual theorems about real functions
retain their validity. Thus, one need not really overstep the countable to obtain a
rich theory of analysis.
Especially surprising is the existence, ensured by Theorem 4.1, of countable
models of formalized set theory. Although set theory can be regarded as the basis
for the whole of presently existing mathematics, it embraces only a few set-
building principles. The most important system of formalized set theory is 
,
created at the beginning of the twentieth century.
Remark 1. Z stands for E. Zermelo, F for A. Fraenkel, and C for AC, the
axiom of choice. ZF denotes the theory resulting from the removal of AC. ZFC
sets out from the principle that every element of a set is again a set, so that a
distinction between sets and systems of sets vanishes. Thus, ZFC speaks
exclusively about sets, unlike Russell’s type-theoretic system, in which, along
with sets, so-called urelements (objects that are members of sets but aren’t sets
themselves) are considered. Set theory without urelements is fully sufficient as a
foundation of mathematics and for nearly all practical purposes. Even from the
epistemological point of view there is no evidence that urelements occur in
reality: each object can be identified with the set of all properties that distinguish
it from other objects. Nonetheless, urelements are still in use as a technical tool
in certain set-theoretic investigations. We mention in passing that neither ZF nor
ZFC is finitely axiomatizable. This seems plausible if we look at the axioms
given below, but the proof is not quite easy.
To make clear that ZFC is a countable first-order theory and hence belongs to
the scope of applications of Theorem 4.1, we present in the following its axioms.
Each of the axioms will be briefly discussed. This will be at the same time an

excellent exercise in advanced formalization technique. The set-theoretic
language already denoted in 2.2 by ℒ ∈ is one of the most conceivably simple
languages and is certainly countable. Alongside = it contains only the
membership symbol ∈. This symbol should be distinguished from the somewhat
larger ∈ that is used throughout in our metatheory. The variables are now called
set variables. These will as a rule be denoted by lowercase letters as in other
first-order languages. In order to make the axioms and its consequences easily
legible, we employ the widely used abbreviations
Besides, we define the relation of inclusion by 
. Note
also that all free variables occurring in the axioms below (e.g., x, y in AE) have
to be thought of as being generalized according to our convention in 2.5. The
ZFC axioms are then the following:
Here φ runs over all ℒ ∈ -formulas with 
. AS is in fact a schema of
axioms. Let 
. From AS and AE, 
 is
derivable. Indeed, observe for 
 the obvious derivability of
This implies 
 and hence
the claim. Thus, 
 is a legitimate definition
in the sense of 2.6. 
 is called a set term and is just a suggestive writing
of a function term 
. This term still depends on the “parameter” vector . It
collects the free variables of  distinct from x, z. Thus, instead of introducing
each time a new operation symbol, one uses the more economical “curled
bracket notation.”
The empty set can explicitly be defined by 
. Indeed, thanks
to AS, 
 is provable. This formula is equivalent to ∃y
∀z z∉ y, since 
. Clearly, using AE, 
 is provable. This, together with ∃y ∀z z∉ y, yields ∃!
y ∀z z∉ y, which legitimates the explicit definition 
, as was
explained in detail in 2.6. The next axiom is
AU : 
 (axiom of union).

Here again, because of AE, ∃y can be replaced by ∃! y. As in 2.6, we may
therefore define an operator on the universe, 4denoted by x↦ ⋃x. We avoid the
word “function,” since functions are understood as special objects of the
universe. AU is equivalent to ∀x ∃y ∀z(( ∃u ∈ x)z ∈ u → z ∈ y), because ⋃x can
be separated from such a set y by means of AS. The following axiom could
analogously be weakened.
AP : 
 (power set axiom).
Let 
 denote the y that in view of AE is uniquely determined by x in AP.
What first can easily be proved is 
 as well as 
. Since 
, the set 
 contains
precisely two elements. This is decisive for defining a, b below.
The following axiom was added to those of Zermelo by Fraenkel.
AR : 
 (axiom of replacement).
Here 
 and 
. If ∀x ∃! yφ is provable, then we know
from 2.6 that an operator x↦Fx can be introduced. By AR, the image of a set u
under F is again a set which, as a rule, is denoted by 
. F may depend
on further parameters a1, …, an, so we had better write 
 for F. AR is very
strong; even AS is derivable from it, Exercise 4. An instructive example of an
application of AR, for which ∀x ∃! yφ is certainly provable, is provided by 
. The operator F = F a, b defined
by φ clearly satisfies F∅ = a and Fx = b if x≠∅. Accordingly, the image of the
two-element set 
 under F a, b contains the (not necessarily distinct)
members a, b. We then define
and call this set the pair set of a, b. We next put a ∪b : = ⋃a, b (while 
 already exists from AS). Further, let 
 and 
 for n ≥ 2. Now we can prove that 
, 
 The ordered pair of a, b is defined as (a, b) : = a, a, b. This
definition may look artificial but it implies the basic property 
. Only this is needed.
We now have at our disposal the tools necessary to develop elementary set
theory. Beginning with sets of ordered pairs it is possible to model relations and
functions and all concepts building upon them, even though the existence of
infinite sets is still unprovable. Mathematical requirements demand their

existence, though then the borders of our experience with finite sets are
transgressed. The easiest way to get infinite sets is using the set operator x↦Sx,
with 
.
AI : 
 (axiom of infinity).
Such a set u contains ∅, 
, 
 and is
therefore infinite in the naive sense. This holds in particular for the smallest set u
of this type, denoted by ω. In formalized set theory ω plays the role of the set of
natural numbers. ω contains 0 : = ∅, 
, 
Generally, 
. Thus, the 
 are represented in ZF by certain
variable-free terms, called ω-terms.
In everyday mathematics the following axiom is basically dispensable:
AF : 
 (axiom of foundation).
Put intuitively: Every set x≠∅ contains an ∈ -minimal element y. AF
precludes the possibility of “ ∈ -circularity” x 0 ∈ ⋯ ∈ x n ∈ x 0. In particular,
there is no set x with x ∈ x.
Remark 2. In axiomatic set theory, AF plays a highly important role. The
most important consequence of AF is the existence of the von Neumann
hierarchy 
. Here On denotes the class of all ordinal numbers. These
are generalizations of natural numbers, defined in each textbook on set theory. 
 is a set for each α ∈ On and defined by recursion: V 0 = ∅, 
, and 
 for limit ordinals λ. All this is more important for the foundations of
mathematics than for applications of set theory.
ZF is the theory with the above axioms. ZFC results from ZF by adjoining
the axiom of choice AC:
AC states that for every set u of disjoint nonempty sets x there is a set z, a choice
set, that picks up precisely one element from each x in u. One of the many
equivalences to AC is 
, for any index set I.
The above expositions clearly show that ZFC can be understood as a first-order
theory. In some sense, ZFC is even the purest such theory, because all
sophisticated proof methods that occur in mathematics, for instance transfinite
induction and recursion and every other type of induction and recursion, can be
made explicit and derived in the first-order language ℒ ∈ of ZFC without
particular difficulty.
Whereas mathematicians regularly transgress the framework of a theory,

even one that is unambiguously defined by first-order axioms, in that they make
use of combinatorial, number-or set-theoretic tools wherever it suits them, set
theory, as it stands now, imposes upon itself an upper limit. Within ZFC, all
sophisticated proof and definition techniques gain an elementary character, so to
speak. As a matter of fact, there are no pertinent arguments against the claim that
the whole of mathematics can be treated within the frame of ZFC as a single
first-order theory, a claim based on general mathematical experience that is
highly interesting for the philosophy of mathematics. However, one should not
make a religion out of this insight, because for mathematical practice it is of
limited significance only.
If ZFC is consistent—and no one really doubts this assumption although
there is no way of proving it—then by Theorem 4.1, ZFC must have a countable
model 
. The existence of such a model  is at first glance
paradoxical because the existence of uncountable sets is easily provable within
ZFC. An example is 
. On the other hand, because of 
, it must be
true (judged from the outside) that also 
 is countable. Thus, the notion
countable has a different meaning “inside and outside the world 
,” which
comes rather unexpectedly. This is the so-called Skolem’s paradox.
The explanation of Skolem’s paradox is that the countable model 
, to put it
figuratively, is “thinned out” and contains fewer sets and functions than
expected. Indeed, roughly put, it contains just enough to satisfy the axioms, yet
not, for instance, some bijection from 
 to 
, which, seen from the
outside, certainly exists. Therefore, the countable set 
 is uncountable
from the perspective of the world . In other words, countability is not an
absolute concept.
Moreover, the universe V of a ZFC-model is by definition a set, whereas ⊢
ZFC ¬ ∃v ∀z z ∈ v, i.e., there is no “universal set.” Thus, seen from within, V is
too big to be a set. ¬ ∃v ∀z z ∈ v is derived as follows: the hypothesis ∃v ∀z z ∈
v entails with AE and AS the existence of the “Russellian set” 
. That is, 
.
On the other hand, by Example 1 page 73, 
. Thus,
indeed ⊢ ZFC ¬ ∃v ∀z z ∈ v. Accordingly, even the notion of a set depends on
the model. There is no absolute definition of a set.
None of the above has anything to do with ZFC’s incompleteness.5

Mathematics has no problem with the fact that its basic theory is incomplete and
cannot be rendered complete, at least not in an axiomatic manner. More of a
problem is the lack of undisputed criteria for extending ZFC in a way coinciding
with truth or at least with our intuition.
Exercises
1. Let T be an elementary theory with arbitrarily large finite models. Prove
that T also has an infinite model.
 
2. Suppose 
 is an infinite well-ordered set (see 2.1). Show that
there is a not well-ordered set elementarily equivalent to 
. Thus, being
well-ordered is not a first-order property.
 
3. Prove that a consistent theory T coincides with the intersection of all its
complete extensions, i.e., T = ⋂T′ ⊇ T∣ T′{ complete}}.
 
4. Derive the axiom AS of separation from the replacement axiom AR.
 
5.
 is
one of several definitions of ‘a is finite’. Prove for each φ ∈ ℒ ∈ : 
.
 
3.5 Enumerability and Decidability
Of all the far-reaching consequences of the completeness theorem, perhaps the
most significant is the effective enumerability of all tautologies of a countable
first-order language. Once Gdel had proved this, the hope grew that the
decidability problem for tautologies might soon be resolved. Indeed, the wait
was not long, and a few years after Gdel’s result Church proved the problem to
be unsolvable for sufficiently expressive languages. This section is intended to
provide only a brief glimpse of enumeration and decision problems as they
appear in logic, computer science, and elsewhere. We consider them more
rigorously in Chapters 5 and 6.
The term effectively enumerable will be made more precise in 6.1 by the
notion of recursive enumerability. At this stage, our explanation of this notion
must be somewhat superficial, though like that for a decidable set it is highly
visualizable. Put roughly, a set M of natural numbers, say, or syntactic objects,
finite structures, or similar objects is called effectively (or recursively)

enumerable if there exists an algorithm that delivers the elements of M stepwise.
Thus, in the case of an infinite set M, the algorithm does not stop its execution
by itself.
The calculus of natural deduction enables first of all an effective enumeration
of all provable finite sequences of a first-order language with at most countably
many logical symbols, i.e., all pairs (X, α) such that X ⊢ α and X is finite, at least
in principle. First of all, we imagine all initial sequents as enumerated in an
ongoing, explicitly producible sequence 
 Then it is systematically
checked whether one of the sequent rules is applicable; the resulting sequents are
then enumerated in a second sequence and so on. Leaving aside problems
concerning the storage capacity of such a deduction machine, as well as the
difficulties involved in evaluating the flood of information that would pour from
such a device, it is simply a question of organization to create a program that
enumerates all provable finite sequents.
Moreover, it can be seen without difficulty that the tautologies of a countable
language  are effectively enumerable; one need only pick out from an
enumeration procedure of provable sequents (X, α) those such that X = ∅. In
short, the aforementioned deduction machine delivers stepwise a sequence 
 (without repetitions if so desired) that consists of exactly the
tautologies of 
. This would be somewhat easier with the calculus in 3.6.
However, we cannot in this way obtain a decision procedure as to whether any
given formula 
 is a tautology, for we do not know whether α ever appears
in the produced sequence. We will prove rigorously in 6.5 that in fact such an
algorithm does not exist, provided 
 contains at least a binary predicate or
operation symbol. Decision procedures exist only for 
 as will be shown in
5.2, and expansions of 
 containing only unary predicate and constant
symbols, and at most one unary operation symbol; see also [BGG].
The deduction machine can also be applied to enumerate the theorems of a
given axiomatizable theory T, in that parallel to the enumeration process for all
provable sequents of the language, a process is also set going that enumerates all
axioms of T. It must then continually be checked for the enumerated sequents
whether all their premises occur as already-enumerated assertions; if so, then the
conclusion of the sequent in question is provable in T. The preceding
considerations constitute an informal proof of the following theorem. A rigorous
proof free of merely intuitive arguments is provided by Theorem 6.2.4.
Theorem 5.1.
The theorems of an axiomatizable theory are effectively enumerable.

Almost all theories considered in mathematics are axiomatizable, including
formalized set theory ZFC and Peano arithmetic 
. While the axiom systems of
these two theories are infinite and cannot be replaced by finite ones, these sets of
axioms are evidently decidable.
Our experience hitherto shows us that all theorems of mathematics held to be
proved are also provable in ZFC. Hence, according to Theorem 5.1, all
mathematical theorems can in principle be stepwise generated by a computer.
This fact is theoretically highly important, even if it has little far-reaching
practical significance at present.
Recall the notion of a complete theory. Among the most important examples
is the theory of the real closed fields (Theorem 5.5.5). A noteworthy feature of
complete and axiomatizable theories is their decidability. We call a theory
decidable if the set of its theorems is a decidable set of formulas, and otherwise
undecidable. We shall prove the next theorem in an intuitive manner. A strict
proof, based on the rigorous definition of decidability based on the theory of
recursive functions in 6.1, will later be provided by Theorem 6.4.4 on page 247.
Theorem 5.2.
A complete axiomatizable theory T is decidable.
Proof. By Theorem 5.1 let 
 be an effective enumeration of all sentences
provable in T. A decision procedure consists simply in comparing for given 
 the sentences α and ¬α in the nth construction step of 
 with α n .
If α = α n then ⊢ T α; if α = ¬α n then 
. This process certainly terminates,
because due to the completeness of T, either α or ¬α will appear in the
enumeration sequence 
 of the theorems of T.
Conversely, a complete decidable theory is trivially axiomatizable (by T
itself). Thus, for complete theories, “decidable” and “axiomatizable” mean one
and the same thing. A consistent theory has a model and hence at least one
completion, i.e., a complete extension in the same language. The only
completion of a complete theory T is T itself. A remarkable generalization of
Theorem 5.2 is Exercise 3.
A (countable) decidable theory has always a decidable completion, see
Exercise 4. Hence, a theory all completions of which are undecidable is itself
undecidable. We will meet such theories in 6.5. On the other hand, if T has only
finitely many completions, T 0, …, T n say, all of which are decidable, then so is
T. Indeed, according to Exercise 3 in 3.4, α ∈ T iff α ∈ T i for all i ≤ n. 6 See also
Exercise 3 below.

In the early stages in the development of fast computing machines, high
hopes were held concerning the practical carrying out of mechanized decision
procedures. For various reasons, this optimism has since been muted, though
skillfully employed computers can be helpful not only in verifying proofs but
also in finding them. This area of applied logic is called automated theorem
proving (ATP). Convincing examples include computer-supported proofs of the
four-color theorem, the Robbins problem about a particular axiomatization of
Boolean algebras, Bieberbach’s conjecture in function theory, and the
nonexistence of a projective plane of order 10. ATP is used today both in
hardware and software verification, for instance in integrated circuit (chip)
design and verification. A quick source of information about ATP is the Internet.
Despite these applications, even a developed artificial-intelligence system
has presently no chance of simulating the heuristic approach in mathematics,
where a precise proof from certain hypotheses is frequently only the culmination
of a series of considerations flowing from the imagination. Creativity in
mathematics of today is still a domain of human beings, not of automata.
However, that is not to say that an automatic system may not be creative in a
new way, for it is not necessarily the case that the human procedural method,
influenced by all kinds of pictorial thoughts, is the sole means of gaining
mathematical knowledge.
Exercises
1. Let 
 (
) be a finite extension of T. Show that if T is
decidable so too is T′ (cf. Lemma 6.5.3).
 
2. Assume that T is consistent and has finitely many completions only.
Prove that each completion of T is a finite extension of T.
 
3. Show that an axiomatizable theory with finitely many completions is
decidable (observe Exercise 2, Exercise 3 in 3.4, and Theorem 5.2).
 
4. Using the Lindenbaum construction in 1.4, show that a decidable
countable theory T has a decidable completion, [TMR, p. 15].
 
5. Show that a consistent theory T that has finitely many completions has
also only finitely many extensions. More precisely, if T has n
completions then T has 2 n − 1 consistent extensions. Clearly, n = 1 if T
itself is complete.
 

3.6 Complete Hilbert Calculi
The sequent calculus of 3.1 models natural deduction sufficiently well. But it is
nonetheless advantageous to use a Hilbert calculus for some purposes, for
instance the arithmetization of formal proofs. Such calculi are based on logical
axioms and rules of inference such as modus ponens MP: α, α → β ⁄ β, also
called Hilbert-style rules. These rules can be understood as sequent rules without
premises. In a Hilbert calculus, deductions are drawn from a fixed set of
formulas X, e.g., the axioms of a theory, with the inclusion of the logical axioms.
The situation is basically the same as in 1.6. In the case X = ∅ one deduces from
the logical axioms alone, and only tautologies are derivable.
In the following we prove the completeness of a Hilbert calculus in the
logical symbols 
. It will be denoted here by 
. MP is its only rule of
inference. 
 refers to any first-order language  and is essentially an extension
of the corresponding propositional Hilbert calculus treated in 1.6. Once again,
implication, defined by α → β : = ¬(α ∧ ¬β), will play a useful part in presenting
the calculus.
The logical axiom systemΛ of our calculus is taken to consist of all formulas 
, where φ is a formula of the form Λ1–Λ10 below, and n ≥ 0. For
example, due to Λ9, x = x, 
 are logical axioms, even
though ∀y is meaningless in the last two formulas. One may also say that Λ is the
set of all formulas that can be derived from Λ1–Λ10 by means of the rule MQ: α ⁄
∀xα. Attention: MQ is not a rule of inference of the calculus ∣ ∼, nor is it
provable, although the set of tautologies is closed under MQ. We will later take a
closer look at MQ.
It is easy to recognize Λ1–Λ10 as tautologies. For Λ1–Λ4 this is clear by 1.6.
For Λ5–Λ8 the reasoning proceeds straightforwardly by accounting for Corollary
2.3.6 on page 71 and the logical equivalences in 2.4. For Λ9 the claim is trivial,
and Λ10 is equivalent to 
, and the latter is obviously the case.
Axiom Λ5 corresponds to the rule 
 of the calculus in 3.1, while Λ6 serves
to deal with superfluous prefixes. The role of Λ7 will become clear in the

completeness proof for 
, and Λ8 is part of bound renaming. Λ9 and Λ10
control the formal treatment of identity. If φ is a tautology, then for any prefix
block 
, so too is 
. Thus, Λ consists solely of tautologies. The same holds
for formulas derivable from Λ using MP, simply because ⊨ α, α → β implies ⊨
β.
Let 
 if there exists a proof Φ = (φ0,…,φn) of α from X, that is, α = φ n
, and for all k ≤ n either 
 or there exists some φ such that φ and φ → φ
k appear as members of Φ before φ k . This definition and its consequences are
the same as in 1.6. As is the case there and proved in the same way, it holds that 
. Moreover, Theorem 1.6.1 also carries over unaltered,
whose application will often be announced by the heading “proof by induction
on 
.” For instance, the soundness of 
 is proved by induction on 
, where soundness is as usual to mean 
, for all X and α.
In short, 
. The proof runs exactly as on page 37.
The completeness of 
 can now be relatively easily be traced back to that
of the rule calculus ⊢ of 3.1. Indeed, much of the work was already undertaken
in 1.6, and we can immediately formulate the completeness of the calculus 
.
Theorem 6.1 (Completeness theorem for 
).
 .
Proof. 
 has already been verified. 
 follows from the claim that 
satisfies all nine basic rules of ⊢. This implies 
, and since ⊢ =   ⊨ we
have also 
. For the rules ( ∧ 1) through ( ¬2) the claim holds according to
their proof for the Hilbert calculus in 1.6. Lemmas 1.6.2 through 1.6.5 carry over
word for word, because we have kept the four axioms on which the proofs are
based and have taken no new rules into account. ( ∀1) follows immediately from
Λ5 using MP, and (IR) is dealt with by Λ9. Only 
 and ( = ) provide us with a
little work, which, by the way, will clear up the role of axioms Λ6, Λ7, and Λ8.
( ∀2): Suppose x ∉ free} X. We first prove 
 by induction
on 
. Initial step: If α ∈ X then x is not free in α. Hence, X ∣ ∼ α → ∀xα
using Λ6, and MP yields X ∣ ∼ ∀xα. If α ∈ Λ then also ∀xα ∈ Λ, and likewise X ∣

∼ ∀xα. Induction step: Let X ∣ ∼ α, α → β and 
 according
to the induction hypothesis. This yields X ∣ ∼ ∀xα, ∀xα → ∀xβ by Axiom Λ7 and
MP and, by another application of MP, the induction claim X ∣ ∼ ∀xβ. Now, to
verify 
, let 
 and y ∉ free} X ∪var} α. By what we have just proved,
we get 
. This, MP, and 
 (Axiom Λ8) yield the
conclusion of 
, X ∣ ∼ ∀xα. Thus, ∣ ∼ indeed satisfies the rule ( ∀2).
( = ): Let α be a prime formula and 
. Further, let y be a
variable ≠x not in s and α. Then certainly 
, because
the latter is a logical axiom in view of Λ10. By the choice of y, rule 
 shows
that
Because of y∉var} α, s and 
, another application of 
 yields
Since 
 by assumption, two applications of MP then leads to
the desired conclusion 
.
A special case of the above completeness theorem is the following
Corollary 6.2.
For any 
 , the following properties are equivalent:
(i)
∣ ∼ α, that is, α is derivable fromΛby means of MP only, 
(ii) α is derivable fromΛ1–Λ10 by means of MP and MQ,  
(iii) ⊨ α, i.e., α is a tautology.
 
The equivalence of (i) and (iii) renders especially intuitive the possibility to
construct a “deduction machine” that effectively enumerates the set of all
tautologies of 
. Here, we are dealing with just one rule of inference, modus
ponens; hence we need the help of a machine to list the logical axioms, a
“deducer” to check whether MP is applicable, and, if so, to apply it, and an
output unit that emits the results and feeds them back into the deducer for further

processing. However, similar to the case of a sequent calculus, such a procedure
is not actually practicable; the distinction between significant and insignificant
derivations is too involved to be taken into account. Who would be interested to
find in the listing such a weird-looking tautology as for instance ∃x(rx → ∀y ry)?
Next we want to show that the global consequence relation 
 defined in 2.5
can also be completely characterized by a Hilbert calculus. It is necessary only to
adjoin the generalization rule MQ to the calculus ∣ ∼. The resulting Hilbert
calculus, denoted by 
, has two rules of inference, MP and MQ. Proofs in 
have to be correspondingly redefined.
Like every Hilbert calculus, 
 is transitive: 
.
This was verified in 1.6 for propositional Hilbert calculi, but the same argument
applies also to Hilbert calculi in first-order languages. With this remark, the
completeness of ⊢ {g} follows easily from that of ∣ ∼ :
Theorem 6.3 (Completeness theorem for 
 ).
 .
Proof. Certainly 
, since both MP and MQ are sound for ⊨ {g}.
Now let 
, so that 
 by (1) of 2.5. This yields 
 by Theorem
6.1, and so 
, since 
 by definition of 
. Clearly, 
 in
virtue of MQ; hence transitivity provides the desired 
.
We now are going to discuss a notion of equal interest for both logic and
computer science. 
 is called generally valid in the finite if 
 for all
finite structures 
. Examples of such sentences α not being tautologies can be
constructed in every first-order language  that contains at least a unary function
or a binary relation symbol. For instance, consider 
 from a language ℒ containing the function
symbol f. This sentence can be refuted only in an infinite -structure since an
injection in a finite -structure is surjective. It holds in all finite -structures 
,
but is not in 
. Thus, 
 is properly extended by the set of 
 -sentences
valid in the finite, 
.
 is for each 
 a theory T with the finite model property,
i.e., every 
 compatible with T has a finite T-model. More generally, the
theory 
 has for any class K of finite 
 -structures the finite model
property. Indeed, if T + α is consistent, i.e., ¬α∉T, then 
 for some 
 ;
hence 
 . Examples are the theories 
 and {FG} of all finite semigroups

and finite groups in 
, respectively. Both theories are undecidable. As regards 
, the proof is not particularly difficult; see 6.6. Unlike { Taut}, the set
{Tautfin} is not axiomatizable for most languages 
. This is the claim of
Theorem 6.4 (Trachtenbrot).
 is not (recursively) axiomatizable for any first-order language 
containing at least one binary operation or a binary relation symbol.
Proof. We restrict ourselves to the first case; for a binary relation symbol,
the same follows easily by means of interpretation (Theorem 6.6.3). If 
 were axiomatizable it would also be decidable
because of the finite model property; Exercise 2. The same is true also for 
 , and by Exercise 1 in 3.5, so too for FSG, because FSG is the
extension of 
 by a single sentence, the law of associativity. But as
already mentioned, FSG is undecidable. ___
The theorem is in fact a corollary of much stronger results that have been
established in the meantime. For the newer literature on decision problems of
this type consult [Id]. Unlike FG, the theory of finite abelian groups, as well as
of all abelian groups, is decidable, [Sz]. The former is a proper extension of the
latter; for instance (as stated in Exercise 4),
does not hold in all abelian groups, though it does in all finite ones.
As early as 1922 Behmann discovered by quantifier elimination that Taut
possesses the finite model property provided the signature contains only unary
predicate symbols; one can also prove this without difficulty by the Ehrenfeucht
game of 5.3. In this case, then, { Tautfin} = {Taut}, because α∉{Taut} implies
¬α is satisfiable and therefore has a finite model. Thus, α∉{Tautfin}. This
proves Tautfin} ⊆ {Taut} and hence {Tautfin} = {Taut}. With the Ehrenfeucht
game also a quite natural axiomatization of the theory FO of all finite ordered
sets is obtained. See Exercise 3 in 5.3.
Exercises
1. Show that MQ is unprovable in ∣ ∼ (that is, 
 does
not hold, in general).
 
2. Suppose (i) a theory T has the finite model property, (ii) the finite T-
models are effectively enumerable (more precisely, a system of
representatives thereof up to isomorphism). Show that (a) the sentences
 

α refutable in T are effectively enumerable, (b) if T is axiomatizable then
it is also decidable.
3. Let T be a finitely axiomatizable theory with the finite model property.
Show by working back to Exercise 2 that T is decidable.
 
4. Show that 
 holds in all finite
abelian groups. Moreover, provide an example of an infinite abelian
group for which the above proposition fails.
 
3.7 First-Order Fragments
Subsequent to Gödel’s completeness theorem it makes sense to investigate some
fragments of first-order languages aiming at a formal characterization of
deduction inside the fragment. In this section we present some results in this
regard; in the next section we shall do the same for some extensions. First-order
fragments are formalisms that come along without the full means of expression
in a first-order language, for instance by the omission of some or all logical
connectives, or restricted quantification. These formalisms are interesting for
various reasons, partly because of the growing interest in automatic information
processing with its more or less restricted user interface. The poorer a linguistic
fragment, the more modest the possibilities for the formulation of sound rules.
Therefore, the completeness problem for fragments is in general nontrivial.
A useful example dealt with more closely is the language of equations,
whose only formulas are equations of a fixed algebraic signature. We think of the
variables in the equations as being tacitly generalized and call these
generalizations identities, though we often speak somewhat sloppily of
equations. Theories with axiom systems of identities are called equational
theories and their model classes equational-defined classes or varieties.
Let  denote a set of identities defining an equational theory, γ a single
equation, and assume 
 . By Theorem 2.7 there is a formal proof for γ from 
 . But because of the special form of the equations, it can be expected that one
does not need the whole formalism to verify 
 . Indeed, Theorem 7.2 states
that the Birkhoff rules (B0)–(B4) below, taken from [Bi], suffice. This result is so
pleasing because when operating with (B0)–(B4), we remain completely inside
the language of equations. The rules define a Hilbert-style calculus denoted by
⊢B and look as follows: 

Here σ is a global substitution, though as explained in 2.2 it would suffice to
consider just simple σ. (B0) has no premise, which means that t = t is
derivable from any set of identities (or t = t is added as an axiom to Γ). These
rules are formally stated with respect to unquantified equations. However, we
think of all variables as being generalized in a formal derivation sequence. We
are forced to do this by the soundness requirement of (B4), because in general
only 
 . To verify 
 , we need only to show that
the property 
 is closed under (B0)–(B4), i.e., 
 (which is trivial), 
 , etc. We have already come across the rules of ⊢ B in 3.1,
stated there as Gentzen-style rules; they ensure that by 
 , a
congruence in the term algebra 
 is defined as in Lemma 2.5. (B4) states the
substitution invariance of ≈, which is to mean 
.
Let 
 denote the factor structure of 
 with respect to ≈ (no distinction is
made between the algebra 
 and its domain), and let  denote the congruence
class modulo 
 to which the term t belongs, so that
(1) 
.
Further, let 
 , say 
 , with arbitrary 
 . Any such
choice determines a global substitution 
 . Induction on t yields
(2) 
 (σ : = σw).
Lemma 7.1.
.
Proof. Let 
 , 
 , and σ = σ w . By (B4) then also 
 , so that 
 by (1). Thus, 
 using (2). Since w
was arbitrary, 
 . Now suppose the latter and let 
 be the so-called
canonical valuation 
 . Here we choose 
 (the identical substitution),
hence 
 by (2). 
 implies 
 , and in view of 
, we get 
 and so 
 by (1). ___
Theorem 7.2 (Birkhoff’s completeness theorem).

Let Γ be a set of identities and t 1 = t 2 an equation. Then 
.
Proof. The direction   ⇒   is the soundness of ⊢ B. Now let 
 . Then
certainly 
 according to Lemma 7.1, or equivalently 
 . Thus, 
 . Using Lemma 7.1 once again yields Γ ⊢ Bt 1 = t 2.
This proof is distinguished on the one hand by its simplicity and on the other
by its highly abstract character. It has manifold variations and is valid in a
corresponding sense, for example, for sentences of the form 
 with arbitrary
prime formulas  of any given first-order language. It is rather obvious how to
strengthen the Birkhoff rules to cover this more general case: Keep (B0), (B1),
and (B3) and replace the conclusions of (B3) and (B4) by arbitrary prime
formulas of the language.
There is also a special calculus for sentences of the form
(3) 
 (n ≥ 0, all γ i equations),
called quasi-identities. Theories whose axioms are of the form (3) are called
quasi-equational theories and their model classes quasi-varieties. The latter are
important both for algebra and logic. (B0) is retained and (B1)–(B3) are replaced
by the rules without premises (axioms)
Besides an adaptation of (B4), some rules are required for the formal handling of
the premises 
 in (3), for instance their permutability (for details see e.g.
[Se]). A highly important additional rule is here a variant of the cut rule, namely
the binary Hilbert-style rule
The most interesting case for automated information processing, where
Hilbert rules remaining inside the fragment still provide completeness, is that of
universal Horn theories. Here, roughly speaking, the equations γi in (3) may be
any prime formulas. Horn theories are treated in Chapter4. But for enabling a
real machine implementation, the calculus considered there, the resolution
calculus, is different from a Hilbert-or a Gentzen-style calculus.
Exercises
1. Show that a variety K is closed with respect to homomorphic images,
taking subalgebras, and forming arbitrary direct products of members of
K; in short, K has the properties H, S, and P.7
 

2. Develop a calculus for quasi-varieties as indicated in the text and prove
its completeness. This exercise is a comprehensive task; we recommend
to start with a study of [Se].
 
3.8 Extensions of First-Order Languages
Now we consider a few of the numerous possibilities for extending first-order
languages to increase the power of expression: We say that a language 
 of
the same signature as 
 is more expressive than ℒ if for at least one sentence 
, 
 is distinct from all Mdβ for 
. In 
, some of the properties of
first-order languages are lost. Indeed, the claim of the next theorem is that first-
order languages are optimal in regard to the richness of their applications.
Lindstrm’s Theorem (see [EFT] or [CK]). There is no language of a given
signature that is more expressive than the first-order language and for which
both the compactness theorem and the Lwenheim–Skolem theorem hold.
Many-sorted languages.  In describing geometric facts it is convenient to
use several variables, for points, lines, and, depending on dimension, also for
geometrical objects of higher dimension. For every argument of a predicate or
operation symbol of such a language, it is useful to fix its sort. For instance, the
incidence relation of plane geometry has arguments for points and lines. For
function symbols, the sort of their values must additionally be given. If 
 is of
sort k and 
 are variables of sort s (1 ≤ s ≤ k) then every relation
symbol r is assigned a sequence (s 1 , …, s n ); in a language without function
symbols, prime formulas beginning with r have the form 
 , where 
 denotes a variable of the sort s i.
Many-sorted languages represent only an inessential extension of the concept
hitherto expounded, provided the sorts are given equal rights. Instead of a
language  with k sorts of variables, we can consider a one-sorted language 
with additional unary predicate symbols P1, …, Pk and the adoption of certain
new axioms: ∃ xP i x for i = 1, …, k (no sort is empty, for otherwise it is
dispensable) and ¬ ∃ x(P i x ∧ P j x) for i≠ j (sort disjunction). For example,
plane geometry could also be described in a one-sorted language with the
additional predicates pt (to be a point) and li (to be a line). Apart from a few
differences in dealing with term insertion, many-sorted first-order languages

behave almost exactly like one-sorted languages.
Second-order languages.  Some frequently quoted axioms, e.g., the
induction axiom IA, may be looked upon as second-order sentences. The
simplest extension of a first-order language to one of higher order is the monadic
second-order language, a two-sorted language. Let us consider such a language 
 with variables 
 for individuals and variables 
 for sets of
these individuals, along with at least one binary relation symbol ∈ but without
function symbols. Prime formulas are x = y, X = Y, and x ∈ X. An -structure is
generally of the form (A, B, ∈ ), where 
 . The goal is that by
formulating additional axioms such as 
 (which
corresponds to the axiom of extensionality AE in 3.4), the relation symbol ∈
should be interpretable as the membership relation ∈ ; hence B should consist of
the subsets of A. This goal is not fully attainable, but nearly so. Axioms on A, B
can be found such that B is interpretable as a subset of 
 , with ∈ interpreted
as ∈. The same works by adding sort variables for members of 
 , 
,
etc. This “completeness of the theory of types” plays a basic role, for instance, in
the higher nonstandard analysis.
A more enveloping second-order language, 
 , is won by adopting
quantifiable variables for relations of each finite arity on the domains of
individuals. But 
 then fails to satisfy both the finiteness and the Lwenheim–
Skolem theorem (Theorem 4.1), even for 
 . The former fails because a
sentence α { fin} can be given in 
 such that ⊨ α{ fin} iff A is finite. For note
that A is finite iff every injective f  A → A is bijective. This can effortlessly be
formalized using a single universally quantified binary predicate variable
characterizing the graph of f.
The Lwenheim–Skolem theorem is also easily refutable for 
 ; one need
only write down in 
 the sentence ‘there exists a continuous order on A
without smallest or largest element’. This sentence has no countable model. For
if there were such a model, it would be isomorphic to the ordered set of rationals
according to a theorem of Cantor (Example 2 in 5.2) and therefore has gaps,
contradicting our assumption.
There is still a more serious problem as regards 
 : The ZFC-axioms,
seen as axioms of the underlying set theory, do not suffice to establish what
a tautology in 
 should actually be. For instance, the continuum
hypothesis CH (see page 174) can be easily formulated as an 
 -sentence,
α { CH} . But CH is independent of ZFC. Thus, if CH is true, α CH is an 
-
tautology, otherwise not. It does not look as though mathematical intuition

suffices to decide this question unambiguously.
New quantifiers.  A simple syntactic extension 
 of a first-order
language  is obtained by taking on a new quantifier denoted by { $ ∼$ $O$},
which formally is to be handled as the -quantifier. However, in a model 
, a new interpretation of { $ ∼$ $O$} is provided by means of the
satisfaction clause
(0) 
 there are infinitely many a ∈ A with 
.
With this interpretation, we write 
 instead of 
 , since yet
another interpretation of { $ ∼$ $O$} will be discussed. 
 is more
expressive than  , as seen by the fact, for example, that the finiteness theorem
for 
 no longer holds: Let X be the collection of all sentences 
 (there
exist at least n elements) plus 
 (there exist only finitely many
elements). Every finite subset of X has a model, but X itself does not. All the
same, 
 still satisfies the Lwenheim–Skolem theorem. This can be proved
straightforwardly with the methods of 5.1. Once again, because of the missing
finiteness theorem there cannot be a complete rule calculus for 
 .
Otherwise, just as in 3.1, one could prove the finiteness theorem after all.
However, there are several nontrivial, correct Hilbert-style rules for ℒ{ $∼$
$O$}00, for instance the four rules
In rule (Q1), which has no premises, clearly x≠ y, z. Intuitively, this rule tells us
that the pair set { y, z} is finite. (Q2) is bound renaming. (Q3) says that a set
containing an infinite subset is itself infinite. (Q4) is rendered intuitive with α =
α( x,y) as follows:
Suppose that 
 and let 
 .
Then 
 states ‘ 
 is infinite’, and 
 says ‘there
exist only finitely many b such that A b≠∅’. The conclusion ∃ y{ $ ∼$ $O$} xα
tells us therefore ‘ A b is infinite for at least one index b’. Hence, (Q4) expresses
altogether that the union of a finite system of finite sets is finite.
Now let us replace the satisfaction clause (0) by
(1)  
 there are uncountably many a ∈ A with 
.
Also with this interpretation, (Q1)–(Q4) are sound for 
 (
with the interpretation (1) ). Rule (Q4) now evidently expresses that a countable
union of countable sets is again countable. Moreover, the logical calculus 

resulting from the basic rules of 3.1 by adjoining (Q1)–(Q4) is, surprisingly,
complete for these semantics when restricted to countable sets X. Thus, 
 , for any countable 
, [CK]. This implies the
following compactness theorem for 
 : If every finite subset of a
countable set 
 has a model then so too does X. For uncountable sets
of formulas this is false in general; Exercise 1.
The above is a fairly incomplete listing of languages with modified
quantifiers. There are also several other extensions of first-order languages, for
instance languages with infinite formulas (containing infinitely long
conjunctions and disjunctions). In this respect we refer to the literature on model
theory.
Programming languages.  All languages hitherto discussed are of static
character inasmuch as there are spatially and temporally independent truth
values for given valuations w in a structure 
 . But one can also connect a first-
order language  in various ways with a programming language having
dynamic character and aiming at the description of certain types of information
processing. The choice of  depends on what the programming language is
aiming at.  can as a rule be reconstructed from the description of the
programming language’s syntax.
The theory of programming languages, both syntax and semantics, has its
roots in mathematical logic. Nonetheless, it has assumed an independent status
and belongs rather to computer science than to logic. Hence our considerations
will be rather brief.
We describe here a simple example of such a language, 
 , where  is a
fixed first-order language. Only the open formulas of  will be used in 
 , not
the quantifiers. The elements of 
 are certain strings, called programs, denoted
by 
, and defined below.
The dynamic character of 
 arises by modifying traditional semantics as
follows: A program 
 starts with a valuation 
 where A is the domain
of a given  -structure 
 . The program 
 alters stepwise the values of the
variables as a run of the program proceeds in time. If 
 terminates upon feeding
in w, i.e., the calculation ends after finitely many steps of calculation, then the
result is a new valuation 
. Otherwise we take 
 to be undefined. The
precise description of this in general only partially defined operation 
 is
called the procedural semantics of 
. A closer description will be given below.
It is possible to meaningfully consider issues of completeness, say, for
procedural semantics as well. For instance, if  speaks about natural numbers

one may ask what conditions have to be posed on 
 such that each computable
function is programmable in 
.
The syntax of 
 is specified as follows: The logical signature of  is
extended by the symbols WHILE, DO, END , 
 , and  (the semicolon serves
only as a separator for concatenated programs and could be omitted if programs
are arranged 2-dimensionally). Programs on 
 are defined inductively as
strings of symbols in the following manner:
For any x ∈ { Var} and term 
 , the string 
 is a program.
If α is an open formula in 
 and 
 are programs, so too are the strings 
 and 
.
No other strings are programs in this context. 
 is to mean that first 
and then 
 are executed. Let 
 denote the n-times repeated execution of 
 ,
more precisely, 
 is the empty program ( 
 ) and 
.
The procedural semantics for the programming language 
 is made precise
by the following stipulations:
(a) 
 (i.e., w alters at most the value of the variable x).
(b) If 
 and 
 are defined, so too is 
 , and 
.
(c) For 
 let 
 with k specified below.
According to our intuition regarding the “WHILE loop,” k is the smallest
number such that 
 for all i < k and 
 , provided such a k
exists and all 
 for i ≤ k are well defined. Otherwise 
 is considered to be
undefined. If k = 0, that is, 
 , then 
 , which amounts to saying
that 
 is not executed at all, in accordance with the meaning of WHILE in
standard programming languages.
Example. Let 
 and consider 
 , where 
denotes the successor function and 
 the predecessor function, defined by 
 (so that 
). Let 
 be the program
If x and y initially have the values x w = m and y w = n, the program ends
with 
 . In other words, 
 terminates for every input m, n for x, y

and computes the output m + n in the variable z, while x, y keep their initial
values.
In 
 , the self-explanatory program schema 
 is
definable by the following composed program:
where x is a variable not appearing in 
, 
, and α.
Exercises
1. Show (a) both 
 and 
 violate the Lwenheim–Skolem
theorem, (b) the finiteness theorem is false for uncountable sets of
formulas in 
, in general (although it holds for countable sets of
formulas).
 
2. Express the continuum hypothesis as a (possibly false) theorem of ℒII.  
3. Verify the correctness of the definition of 
 given
at the end of the above text.
 
4. Define the 
 loop in the programming language 
 . In
this loop, 
 is executed before the test α is started. That is, 
 is
executed at least once.
 
References
Ma.
A. I. MAL’ CEV, The Metamathematics of Algebraic Systems, North-Holland 1971.
Me.
E. MENDELSON, Introduction to Mathematical Logic, Princeton 1964, 4⟨{ th}⟩ ed. Chapman & Hall
1997.
Ge.
G. GENTZEN, The Collected Papers of Gerhard Gentzen (editor M. E. SZABO), North-Holland 1969.
Po.
W. POHLERS, Proof Theory, An Introduction, Lecture Notes in Mathematics 1407, Springer 1989.
Kra.
J. KRAJÍČEK, Bounded Arithmetic, Propositional Logic, and Complexity Theory, Cambridge
Univ. Press 1995.
Ry.
C. RYLL-NARDZEWKI, The role of the axiom of induction in elemenary arithmetic, Fund. Math. 39
(1952), 239–263.
HP.
P. HÁJEK, P. PUDLÁK, Metamathematics of First-Order Arithmetic, Springer 1993.
BGG.

1
2
3
4
E. BÖRGER, E. GRÄDEL, Y. GUREVICH, The Classical Decision Problem, Springer 1997.
Id.
P. IDZIAK, A characterization of finitely decidable congruence modular varieties,
Trans. Amer. Math. Soc. 349 (1997), 903–934.
Sz.
W. SZMIELEW, Elementary properties of abelian groups, Fund. Math. 41 (1954), 203–271.
Bi.
G. BIRKHOFF, On the structure of abstract algebras, Proceedings of the Cambridge Philosophical
Society 31 (1935), 433–454.
Se.
A. SELMAN, Completeness of calculi for axiomatically defined classes of algebras, Algebra
Universalis 2 (1972), 20–32.
Mo.
D. MONK, Mathematical Logic, Springer 1976.
EFT.
H.-D. EBBINGHAUS, J. FLUM, W. THOMAS, Mathematical Logic, New York 1984, 2⟨{ nd}⟩ ed.
Springer 1994.
CK.
C. C. CHANG, H. J. KEISLER, Model Theory, Amsterdam 1973, 3⟨{ rd}⟩ ed. North-Holland 1990.
Gö1.
K. GÖDEL, Die Vollständigkeit der Axiome des logischen Funktionenkalküls, Monatshefte
f. Math. u. Physik 37 (1930), 349–360, also in [Gö3, Vol. I, 102–123], [Hei, 582–591].
Ta4.
_________ , Logic, Semantics, Metamathematics (editor J. CORCORAN), Oxford 1956, 2⟨{ nd}⟩ ed.
Hackett 1983.
TMR. A. TARSKI, A. MOSTOWSKI, R. M. ROBINSON, Undecidable Theories, North-Holland 1953.
Ra5.
_________ , Messen und Zhlen, Eine einfache Konstruktion der reellen Zahlen, Heldermann 2007.
Footnotes
We deal here with a version of the calculus NK from [Ge] adapted to our purpose; more involved
descriptions of this and related sequent calculi are given in various textbooks on proof theory; see e.g.
[Po].
Whenever 
 is embeddable into 
 there is a structure 
 isomorphic to 
 such that 
. The
domain B′ arises from B by interchanging the images of the elements of 
 with their originals.
Here we use the axiom of choice, since for every M i some enumeration is chosen. It can be shown that
without the axiom of choice the proof cannot be carried out.
A frequently used synonym for the domain of a ZFC-model, mostly denoted by V, and ‘for all sets a’ is

5
6
7
then often expressed as ‘for all a ∈ V ’.
In 6.6 the incompleteness of ZFC and all its axiomatic extensions will be proved.
The elementary absolute (plane) geometry T has precisely two completions, Euclidean and non-Euclidean
(or hyperbolic) geometry. Both are axiomatizable, hence decidable. Completeness follows in either case
from the completeness of the elementary theory of real numbers, Theorem 5.5.5. Thus, absolute geometry
is decidable as well. Further applications can be found in 5.2.
Conversely, if a class K has these three properties then K is a variety. This is Birkhoff’s HSP theorem, a
basic theorem of universal algebra; see e.g. [Mo].

(1)
Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-
1-4419-1221-3_4, © Springer Science+Business Media, LLC 2010
4. Foundations of Logic Programming
Wolfgang Rautenberg
1  
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
Wolfgang Rautenberg
Email: raut@math.fu-berlin.de
Abstract
Logic programming aims not so much at solving numerical problems in science
and technology, as at treating information processing in general, in particular at
the creation of expert systems of artificial intelligence. A distinction has to be
made between logic programming as theoretical subject matter and the widely
used programming language for practical tasks of this kind, PROLOG. In regard
to the latter, we confine ourselves to a presentation of a somewhat simplified
version, which nonetheless preserves the typical features.
The notions dealt with in 4.1 are of fairly general nature. Their origin lies in
certain theoretical questions posed by mathematical logic, and they took shape
before the invention of the computer. For certain sets of formulas, in particular
for sets of universal Horn formulas, which are very important for logic
programming, term models are obtained canonically. The newcomer need not
understand all details of 4.1 at once, but should learn about Horn formulas in 4.2
and after a glance at the theorems may then continue with 4.3.
Logic programming aims not so much at solving numerical problems in science
and technology, as at treating information processing in general, in particular at
the creation of expert systems of artificial intelligence. A distinction has to be
made between logic programming as theoretical subject matter and the widely
used programming language for practical tasks of this kind, PROLOG. In regard
to the latter, we confine ourselves to a presentation of a somewhat simplified

version, which nonetheless preserves the typical features.
The notions dealt with in 4.1 are of fairly general nature. Their origin lies in
certain theoretical questions posed by mathematical logic, and they took shape
before the invention of the computer. For certain sets of formulas, in particular
for sets of universal Horn formulas, which are very important for logic
programming, term models are obtained canonically. The newcomer need not
understand all details of 4.1 at once, but should learn about Horn formulas in 4.2
and after a glance at the theorems may then continue with 4.3.
The resolution method and its combination with unification applied in
PROLOG were directly inspired by mechanical information processing. This
method is also of significance for tasks of automated theorem proving, which
extends beyond logic programming. We treat resolution first in the framework of
propositional logic in 4.3. Its highlight, the resolution theorem, is proved
constructively, without recourse to the propositional compactness theorem. In
4.5, unification is dealt with, and 4.6 presents the combination of resolution with
unification and its application to logic programming. An introduction to this area
is also offered by [Ll].
4.1 Term Models and Herbrand’s Theorem
In the proof of Lemma 3.2.5 as well as in Lemma 3.7.1 we have come across
models whose domains are equivalence classes of terms of a first-order language
. In general, a term model is to mean an -model 
 whose domain F is the set
of congruence classes 
 of a congruence ≈ (= ≈ ℱ ) on the algebra 
.
If ≈ is the identity in 
, one identifies F with 
 so that then 
. Function
symbols and constants are always interpreted canonically: 
 and 
, while no particular condition is imposed on
realizing the relation symbols in 
.
Further, let 
 (x ∈ Var). This is called the canonical valuation. In the
terminology of 2.3, 
, where 
 denotes the -structure
belonging to the model 
 with the domain 
. We claim that
independent of a specification of 
 and of the 
,
(1) 
 for all 
,
(2) 
 for all 
 (α open).
(1) is verified by an easy term induction (cf. the proof of (d) page 101). (2)

follows from left to right by Corollary 2.3.6. The converse runs as follows: 
 implies 
 for all 
 in view of (1)
and of Theorem 2.3.5. But this means that 
, because the  for 
exhaust the domain of 
.
Interesting for both theoretical logic and automated theorem proving
including logic programming, is the question, for which consistent 
 can a
term model be constructed in 
. An answer to this question is given by
Theorems 1.1 and 2.1 below. First, we associate with each given 
 a special
term model as follows.
Definition. The term model 
 (associated with a set 
) is the
term model for which 
 and 
 are defined by
It is easily verified that 
 is a congruence in 
 and that the definition of 
 does not depend on the representatives. If X is the axiom system of some
theory T, then we write also 
 for 
, and s ≈ T t for 
. By (1), 
. Similarly 
. In general, 
 is
not a model for X. Our definition merely implies
(3) 
 (π prime).
Here is a simple example in which the associated term model 
 is a model
for T. It will turn out to be a special case of Theorem 2.1.
Example 1. Let T be the theory of semigroups and  the algebra belonging
to the term model 
. Every term t is equivalent in T to a term in left
association, denoted by x 1⋯x n (the operation symbol is not written and 
 is an enumeration of the variables of t in order of appearance in t from
left to right, possibly with repetitions). Thus, t ≈ T x 1⋯x n . For instance, v 0((v 1
v 0)v 1) ≈ T v 0 v 1 v 0 v 1. It is easy to see that 
 with 
is an isomorphism, where  is the semigroup of strings over the alphabet Var,
and (x 1, …, x n ) denotes here the string with the letters 
. Thus, with 
also  is a semigroup, hence 
. This amounts to the same as saying 
.
As already announced earlier, we slightly extend the concept of a model. Let 
 and 
 be defined as in 2.2. Pairs 
 with 
 are called 
-models. Here w need not be defined for 
 One may also say that an
allocation to these variables has deliberately been “forgotten.” In the case k = 0

Choose is w = ∅, so that an 
-model coincides with an -structure. Put 
. To ensure that the set 
 of ground terms is
nonempty, we tacitly assume in this chapter that  contains at least one constant
when considering 
. Clearly, 
 is a subalgebra of 
, since 
.
The concept of a term model can equally be related to 
: Let ≈ be a
congruence in 
 and 
 the factor structure 
 whose domain is 
 with 
, together with some interpretation of the relation
symbols in 
. We extend 
 canonically to an 
-model 
 by the (partial)
valuation 
 for 
, which is empty for k = 0 so that 
 and 
 can be
identified. For each k, the following conditions are verified similarly as with (1),
(2), (3). The 
-model 
 in (3 k ) is defined analogously to 
 on the
domain 
 with 
 for 
; in particular, 
 arises by
factorizing 
.
(1 k ) 
 for all 
,
(2 k ) 
 for all 
 (α open),
 (π a prime formula from 
).
Let 
 a universal formula ( -formula). Then 
 is called an instance
of φ. And if 
 then 
 is called a 
 -instance, for k = 0 also called a
ground instance of φ. If U is a set of universal formulas, let 
 denote the set
of ground instances of all φ ∈ U. Note that 
 for U≠∅, provided 
contains constants.
Theorem 1.1.
Let 
 be a set of ∀-formulas and 
 the set of all instances of the formulas
in U. Then the following are equivalent: (i) U is consistent, (ii) 
 is consistent,
(iii) U has a term model in .
The same holds if 
 (in particular for sets 
 of ∀-sentences),
where 
 now denotes the set of all 
 -instances of the formulas in U.
Proof. (i)
(ii): Clear since 
. (ii)
(iii): Choose some maximally

consistent 
. Then 
 for prime formulas π, by (3). Induction
on ∧, ¬easily yields 
, for all open α. Since 
 we obtain 
. But this yields ℱ X ⊨ U for the term model ℱ = ℱ X according to (2).
(iii)
(i): Trivial. For the case 
 the proof runs similarly using (3 k ), (2 k ),
and 
. __
By Theorem 1.1, a consistent set U of universal sentences has a term model 
. For our purposes, the important case is that U is = -free. Then U has a model
 on the set 
 of all ground terms, since 
 (that replaces 
 in the proof of
Theorem 1.1 for k = 0) is then constructed without a factorization of 
. Such a
model 
 is called a Herbrand model (cf. also Exercise 1 in 3.2). Its domain 
 is called the Herbrand universe of . In general, U has many Herbrand
models on the same domain 
 with the same canonical interpretation of
constants and functions: 
 and 
 for all 
. Only the
relations may vary. If U is a universal Horn theory (to be explained in 4.2), then
U has a distinguished Herbrand model, the minimal Herbrand model. It will be
defined on page 18.
Example 2. Let 
 consist of the = -free universal sentences (a) 
, (b) 
.Here the Herbrand universe 
consists of all ground terms 
. Obviously, 
. Since 
 for each 
 (canonical interpretation), 
 itself is then a Herbrand
model. There are sever other Herbrand models for U, since < may be interpreted
in various ways as will be seen in Example 3 in 4.2.
Remark 1. With Theorem 1.1 the problem of satisfiability for 
 can
basically be reduced to a propositional satisfiability problem. By Exercise 5 in
2.6, X is—after adding new operation symbols—satisfiably equivalent to a set U
of ∀-formulas which, by Theorem 1.1, is in turn satisfiably equivalent to the set
of open formulas 
. Now replace the prime formulas π occurring in the
formulas of 
 with propositional variables 
, distinct variables for distinct
prime formulas, as in 1.5. In this way one obtains a satisfiably equivalent set of
propositional formulas. This works immediately on = -free sets of ∀-formulas.
By dealing with the congruence conditions for = (page 14), this method can be
generalized for sets of arbitrary ∀-formulas but is slightly more involved.
Although we will focus on a certain variant of the next theorem, its basic

concern (the construction of explicit solutions of existential assertions) is the
same in logic programming and related areas.
Theorem 1.2 (Herbrand’s theorem).
Let 
 be a set of universal formulas, 
 , α open, and let 
 be the set
of all instances of members from U. Then the following properties are
equivalent:
The same holds if  is replaced here by 
 , 
 by 
 , and 
 by 
 , for
each k ≥ 0, where 
 is now the set of all 
 -instances.
Proof: Because 
, certainly (iii) ⇒ (ii) ⇒ (i). It remains to be shown that (i)
⇒ (iii): According to (i), 
 is inconsistent; hence also 
 by Theorem 1.1. Replacing here the prime formulas with
propositional variables as indicated in Remark 1 above, (iii) follows already
propositionally according to Exercise 1 in 1.4 (with 
). The proof
for 
, 
, and 
 runs analogously.
Remark 2. Herbrand’s theorem was originally a proof-theoretic statement. It
has several versions. The theorem’s assumption that α is open is essential, as can
be seen from the example ⊢ ∃xα with α : = ∀y(ry → rx) and U = ∅. Indeed, ⊢
∃xα holds, for ∃xα is a tautology, Example 2 in 2.6. But there are no terms t 0, …,
t m (variables in this case) such that 
. The last formula can be falsified
in a model with n + 2 elements in its domain as is readily seen.
Exercises
1. Verify the conditions (1 k ), (2 k ), (3 k ) in detail.  
2. Prove Herbrands theorem also for 
 instead of . 
4.2 Horn Formulas
We will define Horn formulas (after [Hor]) for a given language  recursively.
The following definition covers also the propositional case; simply omit
everything that refers to quantification.

(.)
Definition. (a) Literals (i.e. prim formulas and their negations) are basic
Horn formulas. If α is prime and β a basic Horn formula, then α → β is a basic
Horn formula. (b) Basic Horn formulas are Horn formulas. If α, β are Horn
formulas then so too are (α ∧ β), ∀xα, and ∃xα.
For instance, ∀y(ry → rx) and ∀x(y ∈ x → x∉ y) are Horn formulas.
According to our definition, α1 →⋯ → α n → β (n ≥ 0) is the general form of a
basic Horn formula, where α1, …, α n are prime and β is a literal. Note that in the
propositional case, the α i are propositional variables and β is a propositional
literal. We also call a formula a (basic) Horn formula if it is equivalent to an
original (basic) Horn formula. Thus, since
and by writing α0 for β in case β is prime, and β = ¬α0 if β is negated, basic Horn
formulas are up to logical equivalence of the type
for prime formulas 
. I and II are disjunctions of literals of which at
most one is a prime formula. Basic Horn formulas are often defined in this way.
But our definition above has pleasant advantages in inductive proofs as we shall
see, for instance, in the proof of Theorem 2.1. Basic Horn formulas of type I are
called positive and those of type II negative.
A propositional Horn formula, i.e., a conjunction of propositional basic Horn
formulas, can always be conceived of as a CNF whose disjunctions contain at
most one nonnegated element. It is possible to think of an open Horn formula of 
 as resulting from replacing the propositional variables of some suitable
propositional Horn formula by prime formulas of 
.
Each Horn formula is equivalent to a prenex Horn formula. If its prefix
contains only ∀-quantifiers, then the formula is called a universal Horn formula.
If the kernel of a Horn formula φ in prenex form is a conjunction of positive
basic Horn formulas, φ is termed a positive Horn formula. Horn formulas
without free variables are called Horn sentences. The universal Horn sentences
in the following example are all positive.
Example 1. Identities and quasi-identities are universal Horn sentences, as
are transitivity 
, reflexivity, and irreflexivity, but not
connexity 
. Also the congruence conditions for = are Horn
sentences. Therein 
 is to mean 
:
∀x ∃y x∘y = e is a Horn sentence, while 
 is not, and is

even not equivalent to a Horn sentence in the theory T F of fields. Otherwise
MdT F would be closed under direct products; see Exercise 1. This is not the
case: 
 is a ring that has zero-divisors, for example (1, 0) ⋅(0, 1) = 0. Thus,
ℚ ×ℚ cannot be a field.
A Horn theory is to mean a theory T with an axiom system of Horn
sentences. If these axioms are universal Horn sentences, then T is called a
universal Horn theory. Examples are the theories of groups in various languages,
of rings, and all equational and quasi-equational theories.
Theorem 2.1.
Let U be a consistent set of universal Horn formulas in a language 
 with term
set 
 . Then 
 is a model for U. In the case 
 , 
 is a
model for U as well.
Proof. 
 follows from 
, for Horn formulas α. ( ∗ ) is
proved inductively on the construction of α in the definition of Horn formulas.
For prime formulas π, ( ∗ ) is clear, for then (3) reads as 
: 
.
Now suppose that 
. Then 
, for U is consistent. Hence 
 by 
, and so 
. This confirms ( ∗ ) for all literals. Now let α be prime, β a
basic Horn formula, 
, and assume 
. Then 
; hence 
,
and so 
 by the induction hypothesis. This proves 
. Induction on
∧ is clear. Finally, let 
 for some open Horn formula α, and 
. Since
then certainly 
, we get 
 by the induction hypothesis.  was
arbitrary and hence 
 by (2) from 4.1. This proves ( ∗ ). The case 
is treated analogously. Consider (2 k ), (3 k ) and take 
 for 
. __
Incidentally, U’s consistency in the theorem is always secured if U consists
of positive Horn formulas; Exercise 2. The most interesting case in Theorem 2.1
is that U is the axiom system of a universal Horn theory. Then 
, and since
 for each k, also 
.
Example 2. Let T be the simple universal Horn theory from Example 1 in
4.1. That 
 for the algebra  underlying 
 was shown there by proving

that  is isomorphic to the word-semigroup on the alphabet Var, while Theorem
2.1 yields 
 directly. It is easily seen that  is generated by 
  is
called the free semigroup with the free generators 
 Also 
 is a
universal Horn theory so that the free group generated from 
 is defined
as well.
Remark 1. A universal Horn theory T is said to be nontrivial if ⊬T ∀xy x =
y. The generators 
 of  are then distinct and 
 is called the free model
of T with the free generators 
. The word “free” comes from the fact that if 
 is any T-model, then the mapping 
 (
) generates a
homomorphism 
; moreover, we can make “free use” of the values 
 of
the free generators , keeping 
 fixed and choosing suitable valuations in 
. h is given by 
. Note that h is well defined, for 
, and it is a matter of
routine to check the homomorphism conditions p. 50. E.g., if f is n-ary then 
. Similarly, 
 is the
free model of T with the free generators 
. We will not make use of
these remarks, made for the more advanced reader.
Let 
 be as in Theorem 2.1 but 
-free, and T be axiomatized by U. Let
 be well defined, i.e. 
 contains constant symbols. Then 
 is a Herbrand
model for T, called the free or minimal Herbrand model for T, and henceforth
denoted by 
 or 
. The domain of 
 is the set of ground terms. A not too
simple example of describing the minimal Herbrand model for a set U of = -free
universal Horn sentences is Example 3. Let U and 
 be as in Example 2 in 4.1.
Both (a) and (b) are universal Horn sentences. We determine the minimal
Herbrand model 
 (whose domain consists of the terms n) by proving 
,
with the isomorphism 
. Since 
 by the definition
page 5, it suffices to prove ( ∗ ) : 
. The direction ⇒ is shown
by induction on k, beginning with k = Sm. The initial step is clear since 
 by (a). Let m < Sk. By the induction hypothesis we then get 
, or m = k. In both cases, 
 by (a) and (b). The direction ⇐
in ( ∗ ) is obvious because 
 is a model of U.
Remark 2. The set U in Example 2 in 4.1 has many models on the Herbrand

universe . One may interpret < by any transitive relation on 
 that extends 
,
e.g., by 
. This interpretation will be excluded by adding ∀x x ≮ x to U, but the
minimal Herbrand model remains the same if enlarging U this way.
Most useful for logic programming is the following variant of Herbrand’s
theorem. The main difference is that for sets U of universal Horn formulas we
get a single solution 
 whenever 
. The theorem does also hold with the
same proof if k is dropped throughout.
Theorem 2.2.
Let 
 (
 ) be a consistent set of universal Horn formulas, γ = γ 0 ∧⋯
∧γ m , where all γ i are prime, and 
 . Then the following are equivalent:
In particular, for a consistent universal Horn theory T of any =-free
language with constants, 
 is always equivalent to 
 .
Proof. (i) ⇒ (ii): Let 
. Then 
 for some , because 
 for all  implies 
 by (2 k ), contradicting (i). Thus, for all
i ≤ m, 
. Therefore 
 by (3 k ), and so 
. (ii) ⇒ (iii): Trivial.
(iii) ⇒ (i): Theorem 2.1 states that 
. Hence (iii) implies 
. The
particular case follows from (i)
(iii) when we choose k = 0 and observe that 
 by definition.
Exercises
1. Show that 
 for a Horn theory T is closed under direct products (i.e. 
), and if T is a universal Horn theory, then
also under substructures (
).
 
2. Prove that a set of positive Horn formulas is always consistent.
 
3. Prove 
, where U consists of the universal Horn sentences  

∀x x ≤ x, ∀x x ≤ Sx, ∀x ∀y ∀z(x ≤ y ∧ y ≤ z → x ≤ z).
4.3 Propositional Resolution
We recall the problem of quickly deciding the satisfiability of propositional
formulas. This problem is of eminent practical importance, for many non-
numerical (sometimes called “logical”) problems can be reduced to this. The
truth table method, practical for formulas with few variables, grows in terms of
calculation effort exponentially with the number of variables; even the most
powerful computers of the forseeable future will not be able to carry out the
table method for propositional formulas with just 100 variables. As a matter of
fact, no essentially better procedure is known, unless one is dealing with
formulas of a particular shape, for instance with certain normal forms. The
general case represents an unsolved problem of theoretic computer science, not
discussed here, the so-called P = NP problem; see for instance [GJ] or look for
progress on the Internet.
For conjunctive normal forms, the optimal procedure for contemporary
computers is the resolution procedure introduced in the following. For the sake
of a sparing presentation one switches from a disjunction λ1  ∨  ⋯  ∨  λ n of
literals λ i to the set 
. In so doing, the order of the disjuncts and their
possible repetition, inessential factors for the question of satisfiability, are
eliminated. For instance, λ1  ∨  λ2  ∨  λ1 is equally represented by {λ1, λ2}.
A finite, possibly empty set of literals is called a (propositional) clause. By a
clause in 
 is meant a clause K with 
. In the
following, K, H, G, L, P, N denote clauses and 
 sets of clauses. 
 corresponds to the formula λ1  ∨  ⋯  ∨  λ n . The empty clause
(i.e., n = 0) is denoted by 
. It corresponds to the empty disjunction (which is 
, see the footnote page 13).
 (
) is called a positive clause if m >
0, for m = 1 also a definite, and for m = 0 a negative clause. These conventions
will also be adopted when the q i and r i later denote prime formulas of a first-
order language.
Write w ⊨ K (a propositional valuation wsatisfies the clauseK) if K contains
some λ with w ⊨ λ. K is termed satisfiable if there is some w with w ⊨ K. Note
that the empty clause 
, as the definition’s wording suggests, is not satisfiable.
w is a model for a set 
 of clauses if w ⊨ K for all 
. If 
 has a model then

 is called satisfiable. In contrast to the empty clause 
, the empty set of
clauses is clearly satisfied by every valuation, again by the definition’s wording.
w satisfies a CNF α iff w satisfies all its conjuncts, and hence all of the
clauses corresponding to these conjuncts. Since every propositional formula can
be transformed into a CNF, α is satisfiably equivalent to a corresponding finite
set of clauses. For instance, the CNF (p  ∨  q) ∧ ( ¬p  ∨  q  ∨  r) ∧ (q  ∨   ¬r) ∧
( ¬q  ∨  s) ∧ ¬s is satisfiably equivalent to the corresponding set of clauses p, q,
¬p, q, r, q, ¬r, ¬q, s, ¬s. It will turn out later that this set is not satisfiable. We
write 
 if every model of 
 also satisfies the clause H. A set of clauses 
 is
accordingly unsatisfiable if and only if 
.
For λ∉K we frequently denote the clause K ∪{ λ} by K, λ. Moreover, let 
 for λ = p, 
 for λ = ¬p (hence 
 in any case), and set 
. The resolution calculus operates with sets of clauses and
individual clauses, and has a single rule working with these objects, the so-called
resolution rule
RR may be read as follows: If the clauses K, λ and 
 are derivable, then also
the clause K ∪L, called a resolvent of the clauses K, λ and 
.
A clause H is said to be derivable from a set of clauses 
, in symbols 
,
if H can be obtained from 
 by the stepwise application of RR; equivalently, if
H belongs to the resolution closure 
 of 
, which is the smallest set of
clauses 
 closed with respect to applications of RR. The definition of the
resolution closure corresponds completely to the definition of an MP-closed set
of formulas in 1.6.
Example 1. Let 
. Application of RR leads to the
resolvents p, ¬p and q, ¬q, from which we see that a pair of clauses has several
resolvents, in general. Every subsequent application of RR yields already
available clauses, so that 
 contains only the clauses p, ¬q, q, ¬p, p, ¬p, and
q, ¬q.
Applying RR to p, ¬p yields the empty clause 
. Hence 
, with the
unsatifiable set of clauses 
. By the resolution theorem below, the
derivability of the empty clause from a set of clauses 
 is characteristic of the
nonsatisfiability of 
. To test this one needs only to check whether 
, i.e.,
. This question is effectively decidable for finite sets 
 because then 
 is finite as well. Indeed, a resolvent that results from applying RR to

clauses in 
 contains at most these very same variables. Further, there are
only finitely many clauses in 
, exactly 22n . But that is still an
exponential increase as n increases. Aside from this, the mechanical
implementation of the resolution calculus mostly involves potentially infinite
sets of clauses. We consider this problem more closely at the end of 4.6.
The derivation of a clause H from a set of clauses 
, especially the
derivation of the empty clause, can best be graphically represented by a
resolution tree as in Example 2. This is a tree that branches “upward” with an
endpoint H without edge exits, called the root of the tree. Points without entering
edges are called leaves. A point that is not a leaf has two entrances, and the
points leading to them are called their predecessors. The points of a resolution
tree bear sets of clauses in the sense that a point not being a leaf is a resolvent of
the two clauses above it.
Example 2. The following figure shows one of the many resolution trees for
the already-mentioned collection of clauses
The leaves of this tree are all occupied by clauses in 
. It should be clear
that an arbitrary clause H belongs to the resolution closure of a set of clauses 
just when there exists a resolution tree with leaves in 
 and root H. A resolution
tree with leaves in 
 and the root 
 as shown in the figure on the left for 
 is called a resolution for 
, or more precisely, a successful resolution for
. Thus, because of 
, the set of clauses 
 is unsatisfiable, and hence
so is the conjunctive normal form that corresponds to the set 
, namely the
formula

Remark 1. If a resolution tree ends with a point 
, either to which RR
cannot be applied or where upon application the points are simply reproduced,
then one talks of an unsuccessful resolution. In this case, most interpreters of the
resolution calculus will “backtrack,” which means the program searches
backward along the tree for the first point where one of several resolution
alternatives was chosen, and picks up another alternative. Some kind of selection
strategy must in any case be implemented, since just as with any logical
calculus, the resolution calculus is nondeterministic, that is, no natural
preferences exist regarding the order of the derivations leading to a successful
resolution, even if the existence of such a resolution is known for other reasons.
We remark that despite the derivability of the empty clause, for infinite
unsatisfiable sets of clauses 
 there also may exist infinite resolution trees with
nonrepeating points, where □ never appears. Such a tree has no root. For
example, the set of clauses
is not satisfiable. Here we obtain the infinite resolution tree in the figure on
the right, occupied by leaves from 
, which has no root and does not reflect that
□ is derivable by just a single application of RR to the first two clauses of 
. In
the diagram, the resolution calculus is running on 
 with a rather stupid strategy.
This and similar examples indicate that the resolution calculus is incapable in
general of deciding the satisfiability of infinite sets 
 of clauses. Indeed, this
will be confirmed in 4.6. Nonetheless, by Theorem 3.2 below there does exist—
if 
 is in actual fact unsatisfiable—a successful resolution for 
 that can in
principle be found in finitely many steps.
We commence the more detailed study of the resolution calculus with
Lemma 3.1 (Soundness lemma).

Let 
 be a set of clauses and K a single clause. Then 
 .
Proof. As in the case of a Hilbert calculus, it suffices to confirm the soundness
of the rule RR, that is, to prove that a model for K, λ and 
 is also one for K
∪L. Thus let w ⊨ K, λ and 
. Case 1: w⊭ λ. Then there must be a literal
λ′ ∈ K with w ⊨ λ′. Hence w ⊨ K and therefore w ⊨ K ∪L. Case 2: w ⊨ λ.
Then 
. Similar to the above we get w ⊨ L. Hence w ⊨ K ∪L as well.
For the case 
 the lemma shows 
, that is, the unsatisfiability of 
. The converse of Lemma 3.1 is in general not valid; for instance {p ⊨ p, q,
but 
. It does hold, though, for H = □. This follows from Theorem
3.2 below, often stated as “ 
 is unsatisfiable iff 
.” In its proof we
recursively construct a global valuation w from partial valuations, defined only
for 
.
Theorem 3.2 (Resolution theorem).
 is satisfiable iff 
 .
Proof. Clearly 
 if 
 is satisfiable, so 
 by Lemma 3.1. Now let 
, or equivalently, 
 with 
. Let 
 denote the set of all
literals in 
, and 
 be the set of all 
 with 
 such that p n
or ¬p n or both belong to K. Note that 
, 
, and
. We will construct a model for 
 (hence for 
) stepwise. v n : =
wp n will be defined recursively on n (more precisely, by naive course-of-value
recursion, discussed e.g. in 6.1) such that w n : = (v 1, …, v n ) has the property (
∗ ) : 
.
We agree to say that the “empty valuation” satisfies 
, hence ( ∗ )
holds trivially for n = 0. Let 
 be defined so that ( ∗ ) is true. We will
define v n + 1 = wp n + 1 such that (+) : 
 is satisfied. This clearly
implies 
, hence 
 for all n, so that 
 is a
model for the whole of 
. In order to verify (+) we need to consider only those 
 containing no 
 with w n ⊨ λ and not both p n + 1 and ¬p n + 1.

These K will be called sensitive clauses during this proof, since every other 
 either contains some 
 with w n ⊨ λ or else both p n + 1 and ¬p
n + 1, and hence is satisfied by any expansion of w n to w n + 1. 1 We may assume
that there is a sensitive 
 (otherwise put v n + 1 = 0) and prove the
following claim: Either p n + 1 ∈ K for all sensitive K—then put v n + 1 = 1—or
else ¬p n + 1 ∈ K for all sensitive K, in which case put v n + 1 = 0, so that (+) holds
in either case. To prove this claim assume that there are sensitive 
with p n + 1 ∈ K and ¬p n + 1 ∈ H, hence ¬p n + 1∉K and p n + 1∉H. Applying RR
to H, K, we then obtain either □ (a contradiction to 
), or else a clause from
 for some i ≤ n whose literals are not satisfied by w n , a contradiction to ( ∗
), i.e. to 
 for all i ≤ n. This confirms the claim and completes the proof.
Remark 2. The foregoing proof is constructive, that is, if 
 and the 
 in the proof above are computable, then a valuation satisfying 
 is
computable as well. Moreover, we incidentally proved the propositional
compactness theorem for countable sets of formulas X once again. Here is the
argument: Every formula is equivalent to some KNF, and hence X is satisfiably
equivalent to a set of clauses 
. So if X is not satisfiable, the same is true of 
. Consequently, 
 by Theorem 3.2. Therefore 
 for some
finite subset 
, for there must be some successful resolution tree whose
leaves are collected in 
. Having this, it is obvious that just a finite subset of X
is not satisfiable, namely the one that corresponds to the set of clauses 
.
4.4 Horn Resolution
A clause belonging to a propositional basic Horn formula is called a
(propositional) Horn clause. It is called positive or negative if the corresponding
Horn formula is positive or negative. Positive Horn clauses are of the form 
 with n ≥ 0, negative ones of the form 
. The empty
clause (k = 0) is counted among the negative ones. It is important in practice that
the resolution calculus can be formulated more specifically for Horn clauses.
The empty clause, if it can be obtained from a set of Horn clauses at all, can also
be obtained using a restricted resolution rule, which is applied only to pairs of
Horn clauses in which one premise is positive (the left one in HR below) and the

other one is negative. This is the rule of Horn resolution
The calculus operating with Horn clauses and rule HR is denoted by 
. A
positive Horn clause is clearly definite. Hence, the resolvent of an application of
HR is unique and always negative. An H-resolution tree is therefore of the
simple form illustrated by the figure on the right. Therein 
 denote
positive and 
 negative Horn clauses. Such a tree is also called an H-
resolution for 
—where 
 here and elsewhere is taken to mean a set of
positive Horn clauses and N is a negative clause ≠ □ —if it satisfies (1) 
for all i ≤ ℓ, and (2) 
. It is evidently possible to regard an H-
resolution for 
 simply as a sequence 
 with the properties (0) 
 for all i ≤ ℓ, (1), and (2). Here 
 denotes the uniquely
determined resolvent resulting from applying HR to the positive clause P and the
negative clause N.
Before proving the completeness of 
 we require a little preparation. Let 
 be a set of positive Horn clauses. In order to gain an overview of all models w
of 
, consider the natural correspondence
between valuations w and subsets of 
. Put 
. Clearly, 
 is always satisfied by the “maximal” valuation w with 
, for w
satisfies every positive clause and 
 contains only such clauses. It is obvious
that 
 iff V = V w satisfies the following two conditions:

(a) p ∈ V provided 
,
(b) 
, provided 
 (n > 0).
Of all subsets 
 satisfying (a) and (b) there is clearly a smallest one,
namely 
. Let 
 be the 
-model corresponding to 
 and
call it the minimal 
 -model. We may define 
 for the minimal model 
 of 
 also as follows: Put 
 and
Then 
. Indeed, 
 for all k and all 
. Hence, 
. Also 
, since 
 with 
.
The minimal m with p ∈ V m is termed the 
 -rank of p, denoted by 
.
Those p with 
 are of 
-rank 0. The variables arising from these by
applying (b) have 
-rank 1 if not already in V 0, and so on.
Lemma 4.1.
Let 
 be a set of positive Horn clauses and 
 . Then 
 ,
where N ={ ¬q 0 ,…,¬q k }.
Proof. For 
 set 
. Let 
 be the number of i ≤ n such that 
. The claim is
proved inductively on 
 and 
. First suppose ρ =
0, i.e., 
. Then there is certainly an H-resolution for 
,
namely the tree 
. Now take ρ > 0 and w.l.o.g. 
.
Then there are 
 such that 
 and 
. Thus, 
 is either < ρ, or it is = ρ, in
which case 
. By the induction hypothesis, in both cases 
 for 
. Hence, an H-resolution 
 for 
 exists. But then 
, with P 0 : = P and N 0 : = N, is just an H-
resolution for 
. __
Theorem 4.2 (the H-Resolution theorem).

A set 
 of Horn clauses is satisfiable iff 
 .
Proof. The condition 
 is certainly necessary if 
 is satisfiable. For the
converse assume that 
 is unsatisfiable, 
, all 
 are positive and
all 
 negative. Since 
 but 
 there is some 
 such that 
. Consequently, 
 and
therefore 
. By Lemma 4.1 we then obtain 
, and a
fortiori 
. __
Corollary 4.3.
Let 
 be a set of Horn clauses, all 
 positive and all 
negative. Then the following conditions are equivalent:
(i) 
 is unsatisfiable, (ii) 
 is unsatisfiable for some 
 .
Proof. (i) implies 
 by Theorem 4.2. Hence, there is some 
 and
some H-Resolution for 
, whence 
 is unsatisfiable. (ii)
(i) is trivial
because 
 is a subset of 
. __
Thus, the investigation of sets of Horn clauses as regards satisfiability can
completely be reduced to the case of just a single negative clause.
The hitherto illustrated techniques can without further ado be carried over to
quantifier-free formulas of a first-order language , in that one thinks of the
propositional variables to be replaced by prime formulas of . Clauses are then
finite sets of literals in . By Remark 1 in 4.1 a set of -formulas is satisfiably
equivalent to a set of open formulas, which w.l.o.g. are given in conjunctive
normal form. Splitting these into their conjuncts provides a satisfiably equivalent
set of disjunctions of literals. Converting these disjunctions into clauses, one
obtains a set of clauses for which, by the remark just cited, a consistency
condition can be stated propositionally. Now, because predicate-logical proofs
are always reducible to the demonstration of certain inconsistencies by virtue of
the equivalence of X ⊢ α with the inconsistency of X, ¬α, these proofs can
basically also be carried out by resolution.
To sum up, resolution by Theorem 3.2 and 4.2 is not at all restricted to
propositional logic but includes application to sets of literals of first-order
languages. Theorem 7.3, the predicate logic version of Theorem 3.2, will
essentially be reduced to the latter. Moreover, questions concerning resolution in
first-order languages can basically be treated propositionally, as indicated by the
exercises below.

Before elaborating on this, we consider an additional aid to automated proof
procedures, namely unification. This will later be combined with resolution, and
it is this combination that makes automated proof procedures fast enough for
modern computers equipped with efficient interpreters of PROLOG.
Exercises
1. Prove that the satisfiable set of clauses 
 does not
have a smallest model. The 2nd clause in 
 is not a Horn clause. Thus,
in general only Horn clauses have a smallest model.
 
2. Let p m, n, k for 
 be propositional variables, S the successor
function, and 
 the set of all clauses belonging to the Horn formulas
2
Let the standard model w s be defined by 
.
Show that the minimal model 
 coincides with w s .
 
3. Let 
 be the set of Horn clauses of Exercise 2. Prove that
(a) 
, (b) 
.
(a) and (b) together are equivalent to the the single condition
(c) 
.
 
4.5 Unification
A decisive aid in logic programming is unification. This notion is meaningful for
any set of formulas, but we confine ourself to ¬-free clauses K≠ □ of an identity-
free language. K contains only unnegated prime formulas, each starting with a
relation symbol. Such a clause K is called unifiable if a substitution σ exists, a
so-called unifier of K, such that 
 contains exactly one element;
in other words, 
 is a singleton. Here σ can most easily be understood as a
simultaneous substitution, that is, σ is globally defined and 
 for almost all
variables x. Simultaneous substitutions form a semigroup with respect to
composition, with the neutral element ι, a fact we will heavily make use of.
Example 1. Consider K = rxfxz, rfyzu, r and f binary. Here 
 is a
unifier: 
, as is readily confirmed. Clearly, ω as a composition

of simple substitutions can be understood as a simultaneous substitution, see
page 60.
Obviously, a clause containing prime formulas that start with distinct relation
symbols is not unifiable. A further obstacle to unification is highlighted by
Example 2. Let 
 (r, f unary). Assume 
. This clearly
implies 
 and hence 
. This is impossible, for 
 and 
 are
clearly of different lengths. Hence, K is not unifiable.
If σ is a unifier then so too is στ for any substitution τ. Call ω a generic or a
most general unifier of K if any other unifier τ of K has a representation τ = ωσ
for some substitution σ. By Theorem 5.1 below, each unifiable clause has a
generic unifier. For instance, it will turn out below that ω in Example 1 is
generic.
A renaming of variables, a renaming for short, is for the sake of simplicity a
substitution ρ such that ρ2 = ι. This definition could be rendered more generally,
but it suffices for our purposes. ρ is necessarily bijective and maps variables to
variables. Let 
 for 
 and 
 otherwise. Then clearly 
, that is, ρ swaps the variables x i and y i . In this case we shall write 
.
If ω is a generic unifier of K then so too is ω′ = ωρ, for any renaming ρ.
Indeed, for any given unifier τ of K there is some σ such that τ = ωσ. For σ′ : =
ρσ then τ = ωρ2σ = (ωρ)(ρσ) = ω′σ′. Choosing in Example 1 for instance 
, we obtain the generic unifier ω′ = ωρ for K, with 
.
We now consider a procedure in the form of a flow diagram, the unification
algorithm, denoted by . It checks each nonempty clause K of prime formulas of
an identity-free language for unifiability, and in the positive case it produces a
generic unifier.  uses a variable σ for substitutions with initial value ι, and a
variable L for clauses with initial value K. Later on, L contains 
 for the actual
value of σ that depends on the actual state of the procedure. Here the diagram of 
:

The first distinction letters of two strings are the first symbols from the left
that distinguish the strings. The first letter of α ∈ L is a relation symbol. By
Exercise 1 in 2.2, any further symbol ζ in α determines uniquely at each position
of its occurrence a subterm of α whose initial symbol is ζ. The diagram has just
one (thick-lined) loop that starts and ends in the test ‘Is L a singleton?’. The loop
runs through the operation 
, 
, which assigns a new value to σ and
then the new value 
 to L. This reduces the number of variables in L, since x∉
var} L. Hence,  necessarily leaves the loop after finitely many steps, and must
stop and halt in one of the two OUTPUT boxes of . But we do not yet know
whether  always ends up in the “right” box, i.e., whether  answers correctly.
The final value σ is printed in the lower OUTPUT box.
Example 3. Let  be executed on K from Example 1. The first distinction
letters of the two members α1, α2 ∈ K are ζ1 = x and ζ2 = f at the second position.
The subterm beginning with ζ2 in α2 is t = fyz. Hence, after the first run through
the loop with 
 we get 
. Here the first
distinction letters are f and u at position 5. The term beginning with f at this
position is t = ffyzz. Since u∉ var} ffyzz, the loop is run through once again and
we obtain 
. This is a unifier, and  comes to a halt with
OUTPUT ‘K is unifiable with the generic unifier 
’.
We recommend a thorough study of this example. That the σ at the end of the
example is indeed generic follows from
Theorem 5.1.
The unification algorithm  is sound, i.e., upon input of a negation-free

clause K it always answers correctly. 3  unifies with a generic unifier.
Proof. This is obvious if two elements of K are already distinguished by the
first letter. Assume therefore that all α ∈ K begin with the same letter. If  stops
with the output ‘K is unifiable …’, K is in fact unifiable, since it must have been
previously verified that 
 is a singleton. Conversely, we claim that  also
halts with the correct output, provided Kis unifiable. The latter will be our
assumption till the end of the proof, along with the choice of an arbitrary but
fixed unifier τ of K.
It has to be confirmed that both tests on the right side of the diagram do not
end with the upper OUTPUT, i.e., the test questions are answered Yes. For the
upper test this is clear, since substitutions preserve the symbols ζ1, ζ2 so that
unification would be impossible; this contradicts our assumption. For the lower
test (‘Is x∉ var} t?’) the correctness of the answer will be verified below. Let i (
) denote the moment after the ith run through the loop has been
finished. i = 0 before the first run. Put σ0 : = ι and let σ i for i > 0 be the value of
σ after the ith run through the loop. Below we shall prove that ( ∗ ) there exists a
substitution τ i with 
.
Assume that x ∈ var} t in the (i + 1)th run, with 
 chosen as in the
diagram. Choose τ i according to ( ∗ ). Since σ i τ i = τ is a unifier, 
.
Hence 
: 
 (the terms 
 and 
 start at the same position and must be
identical; see Exercise 1 in 2.2). But then x ∈ var} t is impossible, since
otherwise 
 and 
 were of different length as in Example 2. This confirms the
correctness of the diagram in the unifiable case as claimed above. ( ∗ ) says in
particular that σ m τ m = τ. Since τ was arbitrary, and the σ i do not depend on the
choice of τ, it follows that σ : = σ m is indeed a generic unifier of K.
It remains to prove ( ∗ ) by induction on i ≤ m. This is trivial for i = 0:
choose simply τ0 = τ, so that σ0τ0 = ιτ = τ. Suppose ( ∗ ) holds for i < m. As was
shown, 
holds while running through the test ‘x∉ var} t?’ We set τ i + 1 : = 
τ i and claim that  τ i + 1 = τ i . Indeed, for y≠x we obtain 
, but in view of 
 we have also

(.)
 and 
 yield the induction claim
This completes the proof.
Exercises
1. Let α, β be prime formulas without shared variables. Show that the
properties (i) and (ii) are equivalent:
(i) {α, β} is unifiable, (ii) there are substitutions σ, τ with 
.
 
2. Show: 
 is idempotent (which is to mean σ2 = σ) if and only if x i ∉
var} t j , for all i, j with 1 ≤ i, j ≤ n.
 
3. A renaming ρ is termed a separator of a pair of clauses K 0, K 1 if 
. Show that if K 0 ∪K 1 is unifiable then so is 
, but not conversely, in general.
 
4. Assume ζ1, ζ2∉ Var for the first distinction letters ζ1, ζ2 of the clauses K
1≠K 2. Show rigorously that 
 is not unifiable.
 
4.6 Logic Programming
A rather general starting point in dealing with systems of artificial intelligence
consists in using computers to draw consequences φ from certain data and facts
given in the form of a set of formulas X, that is, proving X ⊢ φ mechanically.
That this is possible in theory was the subject of 3.5. In practice, however, such a
project is in general realizable only under certain limitations regarding the
pattern of the formulas in X, φ. These limitations refer to any first-order
language  adapted to the needs of a particular investigation. For logic
programming the following restrictions on the set X and the formula φ are
characteristic:
 is identity-free and contains at least one constant symbol,
each α ∈ X is a positive universal Horn sentence,

φ is a sentence of the form 
 with prime formulas γ i .
Note that ¬φ is equivalent to 
 and hence a negative
universal Horn sentence. Because ∀-quantifiers can be distributed among
conjunctions, we may assume that each α ∈ X is of the form
A finite set of sentences of this type is called a logic program and will
henceforth be denoted by 
. The availability of a constant symbol just ensures
the existence of a Herbrand model for 
. In the programming language
PROLOG, ( ∗ ) is formally written without quantifiers as follows and called a
program clause: 
 (or just β : − in case m = 0).
: − symbolizes converse implication mentioned in 1.1. For m = 0 such
program clauses are called facts, and for m > 0 rules. In the sequel we make no
distinction between a logic program 
 as a set of formulas and its transcript in
PROLOG. The sentence 
 in the last bulleted item above is
also called a query to 
. In PROLOG, ¬φ is mostly denoted by : − γ0, …, γ k . 4 
 may be empty. This notation comes from the logical equivalence of the
kernel of 
 to the converse implication 
, omitting the writing of 
.
Using rules one not only proceeds from given facts to new facts but may also
arrive at answers to queries. The restriction as regards the abstinence from = is
not really essential. This will become clear in Examples 1 and 4 and in the
considerations of this section. Whenever required, 
 can be treated as an
additional binary relation symbol by adjoining the Horn sentences from Example
1 in 4.2.
Program clauses and negated queries can equally well be written as Horn
clauses: β : − β0, …, β m as 
, and : − γ0, …, γ k as For a logic
program 
, let 
 denote the corresponding set of positive Horn clauses. 
 and 
 can almost always be identified. To justify this semantically, let 
 for a
given -structure 
 and 
 simply mean 
, which is
clearly equivalent to 
. Note that 
, since □ corresponds to the
formula 
. For 
-models 
, let 
 have its ordinary meaning 
.
If 
 for all 
, then 
 is called a model for a given set 
 of clauses,
and 
 is called satisfiable or consistent if such an 
 exists. This is clearly
equivalent to the consistency of the set of sentences corresponding to 
. Further,

let 
 if every model for 
 also satisfies H. Evidently 
 for 
 and
arbitrary substitutions σ, since 
. The clause 
 is also termed
an instance of K, in particular a ground instance whenever 
 contains no
variables.
A logic program 
, considered as a set of positive Horn formulas, is always
consistent. All facts and rules of 
 are valid in the minimal Herbrand model 
.
This model should be thought of as the model of a domain of objects about
which one wishes to express properties by means of 
. A logic program 
 is
always written such that a real situation is modeled as precisely as possible by
the minimal Herbrand model 
.
Suppose that 
, where γ is a conjunction of prime formulas as at the
beginning, i.e., 
 is a query. Then a central goal of logic programming is to
gain “solutions” of 
 in 
, which by Theorem 2.2 always exist. Here 
 is called a solution of 
 whenever 
. One also speaks of the
solution 
, or an answer to the query : − γ.
Logic programming follows the strategy of proving 
 for a query φ by
establishing the inconsistency of 
. To verify this we know from Theorem
1.1 that an inconsistency proof of 
 suffices. The resolution theorem
shows that for this proof in turn, it suffices to derive the empty clause from the
set of clauses 
 corresponding to 
. Here 
 generally
denotes the set of all ground instances of members of a set 
 of clauses, and N =
¬γ1, …, ¬γ n } is the negative clause corresponding to the query φ, the so-called
goal clause.
As a matter of fact, we proceed somewhat more artfully and work not only
with ground instances but also with arbitrary instances. Nor does the search for
resolutions take place coincidentally or arbitrarily, but rather with the most
sparing use of substitutions possible for the purpose of unification. Before the
general formulation of Theorem 6.2, we exhibit this method of “unified
resolution” by means of two easy examples. In the first of these, 
 denotes
the ternary relation graph + in 
.
Example 1. Consider the following program 
 in 
:
∀x{ sum} x0x ; ∀x ∀y ∀z({ sum} xyz →{ sum} xSySz).
In PROLOG one may write this program somewhat more briefly as
{ sum} x0x : − ; { sum} xSySz : − { sum} xyz.
The first program clause is a “fact,” the second one is a “rule.” The set of

Horn clauses that belongs to 
 is
 describes { sum} = graph + in  together with 0, S; more precisely,
that is, 
 
. This is deduced in
Example 3 on page 60, but more directly from Exercise 2 in 4.3. By replacing
therein p m, n, k with 
, the formulas of this exercise correspond precisely
to the ground instances of 
.
Examples of queries to 
 are 
 and 
. Another example
is 
 (here the ∃-prefix is empty). For each of these three queries φ,
clearly 
 holds. Hence, 
 by the last part of Theorem 2.2. But how can
this be confirmed by a computer?
As an illustration, let 
. Since 
 we know that 
 is a solution of
We will show that 
, where x occurs free in the last formula, is
the general solution of ( ∗ ). The inconsistency proof of 
 results by
deriving □ from suitable instances of 
 that will be constructed by certain
substitutions. 
 is the goal clause corresponding to φ. The
resolution rule is not directly applicable to 
. But with 
 it is
applicable to 
, with the Horn clause 
.
Indeed, one easily confirms that 
 and 
. The resolvent of the pair of Horn clauses 
 is N 1 : =
¬{ sum} u0z. This can be stated as follows: Resolution is becoming possible
thanks to the unifiability of the clause 
, where { sum}
xSySz belongs to P and 
 to N. But we have still to continue to try to get
the empty clause. Let 
. Then P 1, N 1 can be brought to
resolution by unification with 
. For notice that 
 and 
. Now simply apply RR to this pair of clausesto obtain □. The

diagram on the left makes this kind of a description more intuitive. Note that the
set braces of the clauses have been omitted in the diagram. This resolution can
certainly be produced by a computer; what the computer has to do is just to look
for appropriate unifiers for pairs of clauses. In this way, ( ∗ ) is proved by
Theorem 6.2(a) below. At the same time, by Theorem 6.2(b), applied to the
resolution represented by the above diagram, we obtain a solution of ( ∗ ),
namely 
. This solution is an example of a most general
solution of ( ∗ ), because by substitution we obtain from 
 all individual
solutions, namely the sentences 
.
Example 2. The logic program 
, written 
 ; { mt} x : − { hu} x in PROLOG, formalizes the two premises of the
old classical Aristotelian syllogism All humans are mortal; Socrates is a human.
Hence, Socrates is mortal.
Here 
 is just the single-point model {{ Socr}}, since { Socr} is the only
constant and no functions occur. The figure on the right shows a resolution of the
query : − { mt} x (in words, “Is there a mortal x in the Herbrand model 
?”),
with the solution x : = { Socr}. The familiar logic argument runs as follows: ∀x({
hu} x →{ mt} x) implies { hu} { Socr} →{ mt} { Socr} by specification of x.
Thus, since { hu} { Socr}, MP yields { mt} { Socr}. We learn from this example
among other things that proofs using MP can also be gained by resolution.
Of course, the above examples are far too simple to display the efficiency of

logic programming in practice. Here we are interested only in illustrating the
methods, which are essentially a combination of resolution and unification.
Clearly, these methods concern basically the implementation of PROLOG and
may be less interesting to the programmer who cares in the first line about
successful programming, whereas the logician cares about the theory behind
logical programming.
Following these preliminary considerations we will now generalize our
examples and start with the following definition of the rules UR and UHR of
unified resolution and unified Horn resolution, respectively. Therein, K 0, K 1
denote clauses and ω a substitution.
Definition Let 
 be the set of all clauses K such that there are
clauses H 0, H 1 and negation-free clauses G 0, G 1≠ □ such that after a possible
swapping of the indices 0, 1,
K 0 = H 0 ∪G 0 and 
 (
),
ω is a (w.l.o.g. generic) unifier of G 0 ∪G 1 and 
.
K is called a U-resolvent of K 0, K 1 or an application of the rule UR to K 0
,K 1 if 
 for some ω and some separator ρ of K 0, K 1. The
restriction of UR to Horn clauses K 0, K 1 (K 0 positive, K 1 negative) is denoted
by UHR and 
 by 
. The resolvent K is then termed a 
-resolvent of K 0, K 1.
This definition becomes more lucid by some additional explanations.
According to (b), 
 for some prime formula π. Hence K results
from applying standard resolution on suitable premises. Applying UR or UHR to
K 0, K 1 always includes a choice of ω and ρ (ρ may enlarge the set of resolvents,
see Exercise 3 in 4.5). In the examples we used UHR. In the first resolution step
of Example 1 is 
 (with ρ = ι). The splitting of K 0 and K
1 according (a) above reads H 0 = ¬{ sum} xyz, 
, and H 1 = ∅, 
. UHR was used again in the second resolution step, as well as in
Example 2, strictly following the instruction of the above definition.
We write 
 if H is derivable from the set of clauses 
 using UR.
Accordingly, let 
 be defined for sets of Horn clauses 
, where only
UHR is used. As in 4.3, derivations in 
 or 
 can be visualized by
means of trees. A (successful) U-Resolution for 
 is just a U-resolution tree with

leaves in 
 and the root □, where the applied substitutions are tied to the
resolution nodes as in the diagram below.
A UH-resolution is defined similarly; it may as well be regarded as a
sequence 
 with 
 for i < ℓ and 
. If 
 is a set of positive clauses and N a negative clause, and
if further 
 holds for all i ≤ ℓ and N 0 = N, one speaks of a 
 -resolution
for 
. In general, 
 consists of the clauses of some logicprogram and N is
any given goal clause. In place of 
-resolution one may also speak of SLD-
resolution ( Linear resolution with Selection function for Definite clauses). This
name has nothing to do with some special strategy for searching a successful
resolution, implemented in PROLOG. For details on this matter see for instance
[Ll]. The diagram illustrates a 
-resolution 
 for 
. It
generalizes the diagrams in Examples 1 and 2, which represent particularly
simple examples of 
-resolutions.
First of all we prove the soundness of the calculus 
 in Lemma 6.1. This
clearly covers also the calculus 
 of unified Horn resolution, where one has
to do with special clauses only.
Lemma 6.1 (Soundness lemma).
 implies 
.
Proof. It suffices to show that K 0, K 1 ⊨ H if H is a U-resolvent of K 0, K 1. Let 
, 
, 
, 
, 
,

and 
. Then 
 as well. Further, let 
, with 
. If 
 then evidently 
. Otherwise 
, hence 
. So 
 in any case.
This states that 
, because w was arbitrary.
With respect to the calculus 
 this lemma serves the proof of (a) in
Theorem 6.2 (Main theorem of logic programming).
Let 
 be a logic program, 
 a query, γ = γ 0 ∧⋯∧γ k , and N ={ ¬γ 0 ,…,¬γ
k}. Then the following hold:
 iff 
 ( Adequacy ).
If 
 is a 
 -resolution for 
 and 
 , then 
 (Solution soundness) .
Suppose that 
 
 . Then there is some { UH}-resolution 
 and some σ such that 
 for i = 1,…,n, where 
 Solution completeness).
The proof of this theorem is undertaken in 4.7. It has a highly technical character
and uses a substantial amount of substitutions. Here are just a few comments. In
view of 
 it is clear that
( ∗ )   
is equivalent to the inconsistency of 
, hence also to that of the
corresponding set of Horn clauses 
. Theorem 6.2(a) states that this is
equivalent to 
, which is not obvious. (b) tells us how to achieve a
solution of ( ∗ ) by a successful resolution. Since 
 in (b) may still contain free
variables (like 
 for ω = ω1ω2 in Example 1) and since 
 for any τ, we often obtain whole families of solutions of ( ∗ )
in the Herbrand model 
 by substituting ground terms. By (c), all solutions in 
 are gained in this way, though not always with a single generic resolution as
in Example 1. However, the theorem makes no claim as to whether and under
what circumstances ( ∗ ) is solvable.
Logic programming is also expedient for purely theoretical purposes. For

instance, it can be used to make the notion of computable functions on 
entirely precise. The definition below provides just one of several similarly
styled, intuitively illuminating possibilities. We will construct an undecidable
problem in Theorem 6.3 below that explains the principal difficulties
surrounding a general answer to the question 
. Because in 6.1
computable functions are equated with recursive functions, we keep things fairly
brief here.
Definition. 
 is called computable if there is a logic program 
 in a
language that, in addition to 0 and S, contains only relation symbols, including
an (n + 1)-ary relation symbol denoted by 
 (to mean graphf), such that for all 
 and m the following is satisfied:
(1) 
. 5
A function 
 satisfying (1) is certainly computable in the intuitive
sense: a deduction machine is set to list all formulas provable from 
, and one
simply has to wait until a sentence 
 appears. Then the value 
 is
computed. By Theorem 6.2(a), the left-hand side of (1) is for 
 equivalent
to 
. Therefore, f is basically also computable with the Horn
resolution calculus.
The domain of the Herbrand model 
 is 
, and by Theorem 2.2,
so that (1) holds when just the following claim has been proved:
(2) 
 for all 
.
Example 3. 
 in Example 1 computes +, more precisely graph +, since 
 was shown there. So (2) holds and hence also (1).
A logic program 
 for computing 
 arises from 
 by expanding
the language to 
 and adding to 
 the program clauses 
 and 
. Here we see that besides the graph
of the target function some additional relations may be involved in the function’s
computation.
Example 4. The program 
 in 
, containing only rS xSx : −,

computes the graph 
 of the successor function. Clearly, 
, since 
 ( □ is a resolvent of 
 and 
, where σ
equals 
). Let m≠Sn. Then 
. Hence, 
.
This confirms (1).
It is not difficult to recognize that each recursive function f can be computed
by a logic program 
 in the above sense in a language that in addition to some
relation symbols contains only the operation symbols 0, S. Exercises 1, 2, and 3
provide the main steps in the proof, which proceeds by induction on the
generating operations Oc, Op, and Oμ of recursive functions from 6.1. Example
4 confirms the induction initiation for the initial primitive recursive function S.
The interested reader should study 6.1 to some extent in order to understand
what is going on.
Thus, the concept of logic programming is very comprehensive. On the other
hand, this has the consequence that the question 
 is, in general, not
effectively decidable. Indeed, this undecidability is the assertion of our next
theorem.
Theorem 6.3.
A logic program 
 exists whose signature contains at least a binary relation
symbol r, but no operation symbols other than 0,S , such that no algorithm
answers the question 
 for each k.
Proof. Let 
 be recursive, but 
 not recursive.
Such a function f exists; see Exercise 5 in 6.5. Then we get for 
 from the
definition page 59,
Thus, if the question 
 were decidable then so too would be the
question m ∈ ran f, and this is a contradiction to the choice of f. __
Exercises
1. Let 
 and 
 be computable by means of the logic
 

programs 
 and 
, and let 
 arise from g, h by primitive
recursion. This is to mean that 
 and 
for all 
.
Provide a logic program for computing (the graph of) f.
2. Let 
 and 
 be logic programs for computing 
 and 
 (i = 1, …, m). Further, let the function f be defined by 
 for all 
. Give a logic program for computing f.
 
3. Let 
 and 
 be logic program for computing g. Further
assume that to each 
 there is some 
 with 
 and let 
for 
 be the smallest m such that 
. Give a logic program
for computing 
.
These exercises and the examples show that the LP-computable
functions coincide with the general recursive functions.
 
4.7 A Proof of the Main Theorem
While we actually require the following lemmas and Theorem 7.3 below only
for the unified Horn resolution, the proofs are carried out here for the more
general U-resolution. These proofs are not essentially more difficult. As a matter
of fact, the only difficulty in the proofs is the handling of substitutions. The
calculi 
 and 
 from 4.3 now operate with variable-free clauses of a fixed
identity-free first-order language with at least one constant. The presence of
constants assures that variable-free clauses are available. ρ, σ, τ denote
simultaneous substitutions throughout.
Lemma 7.1.
Let K 0 ,K 1 be clauses with separator ρ and let 
 , 
 be variable-free.
Suppose that K is a resolvent of 
 . Then there are substitutions ω,τ and
some 
 such that 
 , i.e., K is a ground instance of some U-
resolvent of K 0 ,K 1 . Further, for a given finite set V of variables, ω,τ can be
selected in such a way that 
 for all x ∈ V. The same holds for Horn
resolution.

Proof. Suppose w.l.o.g. that 
 and 
 for some prime
formula π, and K = L 0 ∪L 1. Put 
, 
, and 
, i = 0, 1. Then K 0 = H 0
∪G 0, 
, 
, 
. Let ρ be a separator of K 0, K 1 and
define σ by 
 in case 
, and 
 else, so that e.g. 
.
Note that also ρσ = ρρσ0 = σ0, hence 
. Therefore 
, that is, σ unifies 
. Let ω be a
generic unifier of this clause, so that σ = ωτ for suitable τ. Then 
 by definition of the rule UR, and 
, 
 yield the desired
The second part of our lemma is easily confirmed. Since V is finite, ρ can
clearly be chosen such that 
, hence 
 for all x ∈ V. By
definition of σ and in view of ωτ = σ it then obviously follows that 
whenever x ∈ V. __
The next lemma claims that if the □ is derivable from 
 with resolution
only (i.e., without unification and separation), then □ is also directly derivable
from 
 in the calculi 
 and 
, respectively.
Lemma 7.2 (Lifting lemma).
Suppose that 
 for some set of clauses 
 . Then 
 . And if 
consists of Horn clauses only, then also 
 .
Proof. We shall verify the more general claim
( ∗ )  If 
 then 
 and 
 for some H and τ.
For K = □, ( ∗ ) is our claim since 
. ( ∗ ) follows straightforwardly by
induction on 
. It is trivial for 
, by definition of 
. For
the inductive step let 
, with 
 for suitable σ0, σ1
according to the induction hypotheses, and let K be a resolvent of 
. That
then 
 and 
 for suitable H, ω, τ is exactly the first claim
of Lemma 7.1. This proves ( ∗ ). The case for Horn clauses is completely

similar.
Theorem 7.3 (U-Resolution theorem).
A set of clauses 
 is inconsistent iff 
 ; a set of Horn clauses 
 is
inconsistent iff 
 .
Proof. If 
 then 
 by Lemma 6.1; hence 
 is inconsistent. Suppose
now the latter, so that the set U of ∀-sentences corresponding to 
 is
inconsistent as well. Then 
 is inconsistent according to Theorem 1.1; hence
 as well. Thus, 
 by Theorem 3.2 and so 
 by the Lifting
lemma. For sets of Horn clauses the proof runs analogously, using Theorem 4.2
instead of Theorem 3.2.
Proof of Theorem 6.2. (a): 
 is equivalent to the inconsistency of 
 or of 
. But the inconsistency of 
 is, by the U-Resolution
theorem, precisely the same as saying 
.
(b): Proof by induction on the length ℓ of a (successful) 
-resolution 
 for 
. Let ℓ = 0, so that 
 for suitable ρ, ω ( =
ω0), and 
. Then ω unifies 
, i.e., 
 for some prime
formula π and all i ≤ k. Hence trivially 
 for each i ≤ k, and so 
 as claimed in Theorem 6.2(b) for the case ℓ = 0. Now let ℓ
> 0. Then 
 is a 
-resolution for 
 as well. By the induction
hypothesis,
(1) 
It suffices to verify 
 for all i ≤ k with 
 in agreement with
the notation in Theorem 6.2. To this end we distinguish two cases for given i: if 
 then 
 by (1), hence 
. Now suppose 
.
Then 
 disappears in the resolution step from 
 to N 1. So P 0 takes
the form P 0 = ¬β1, …, ¬β m , β}, where 
 and 
 for j = 1, …,
m. Thus (1) evidently yields 
 and therefore 
. At the
same time, it holds 
 because of 
(the latter holds since P 0 = ¬β1, …, ¬β m , β}). Using MP we then obtain 
. From 
, applying ω1⋯ω ℓ on both sides, we obtain 
.

Hence 
 also in the second case. Thus, 
 for all i ≤ n, P 0 = ¬β1, …,
¬β m , β}). Using MP we then
(c): Let 
 with 
, and 
. Then 
 is inconsistent, and
so too is 
 by Theorem 1.1 (consider 
). According to the
H-resolution theorem 4.2, there is some H-resolution 
 for 
with 
. Here let, say, 
 for appropriate clauses 
 and σ i .
From this facts we will derive
(2) for finite V ⊆{ Var} there exist ρ i ,N i ,ω i ,τ such that 
 is a
 -resolution for 
 . Moreover, 
 for ω := ω 0 ⋯ω ℓ and
all x ∈ V.
 
This completes our reasoning, because (2) yields (for V = x 1, …, x n }) 
 for i = 1, …, n, whence (c). For the inductive proof of (2) look at
the first resolution step Q 1 = HR(P 0 ′, Q 0) in B, with 
, 
, σ1 : =
τ. By Lemma 7.1 with K 0 : = P 0, K 1 : = N 0 : = N, we choose ω0, ρ0, τ0, H such
that 
 and 
, as well as 
 for all x ∈ V. If ℓ = 0,
that is, if Q 1 = □, then also H = □ and (2) is proved with τ = τ0. Now suppose ℓ >
0. For the H-resolution 
 for 
 and for 
there exist by the induction hypothesis ρ i , N i , ω i for i = 1, …, ℓ and some τ,
such that 
 is a 
-resolution for 
 and simultaneously 
 (instead of Q 0 = N σ we have now to consider 
). Because of 
 and 
 for x ∈ V we get
(3) 
, for all x ∈ V.
 is certainly a 
-resolution. Moreover, by virtue of (3), and
by choosing V = x 1, …, x n }, it holds 
 for i = 1, …, n. This proves
(2), hence (c), and completes the proof of Theorem 6.2.
References

1
2
3
4
5
Ll.
J. W. LLOYD, Foundations of Logic Programming, Berlin 1984, 2⟨{ nd}⟩ ed. Springer 1987.
Hor. A. HORN, On sentences which are true of direct unions of algebras, J. Symb. Logic 16 (1951), 14–21.
GJ.
M. GAREY, D. JOHNSON, Computers and Intractability, A Guide to the Theory of NP-Completeness,
Freeman 1979.
Footnotes
The newcomer should write down all eight candidates for the subset 
 of 
. Only p 1} and ¬p 1} are sensitive to v 1 = wp 1.
In 4.6 these formulas will be interpreted as the ground instances of a logic program for computing the
sum of two natural numbers.
The proof will be a paradigm for a so-called correctness proof of an algorithm. Such a proof is often
fairly lengthy and has almost always to be carried out inductively on the number of runs through a loop
occurring in the algorithm.
Sometimes also ? − γ0, …, γ k . Like many programming languages, PROLOG also has numerous
“dialects.” We shall therefore not consistently stick to a particular syntax. We also disregard many details,
for instance that variables always begin with capital letters and that PROLOG recognizes certain
unchanging predicates like read, 
, to provide a convenient user interface.
By grounding computability in different terms one could f provisionally call LP-computable. Our
definition is related to the Herbrand–Gödel definition of computable functions but we will not step into
further details in this respect.

(1)
Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-
1-4419-1221-3_5, © Springer Science+Business Media, LLC 2010
5. Elements of Model Theory
Wolfgang Rautenberg
1  
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
Wolfgang Rautenberg
Email: raut@math.fu-berlin.de
Abstract
Model theory is a main branch of applied mathematical logic. Here the
techniques developed in mathematical logic are combined with construction
methods of other areas (such as algebra and analysis) to their mutual benefit. The
following demonstrations can provide only a first glimpse in this respect, a
deeper understanding being gained, for instance, from [CK] or [Ho]. For further-
ranging topics, such as saturated models, stability theory, and the model theory
of languages other than first-order, we refer to the special literature, [Bue],
[Mar], [Pz], [Rot], [Sa], [She].
The theorems of Lwenheim and Skolem were first formulated in the
generality given in 5.1 by Tarski. These and the compactness theorem form the
basis of model theory, a now wide-ranging discipline that arose around 1950.
Key concepts of model theory are elementary equivalence and elementary
extension. These not only are interesting in themselves but also have multiple
applications to model constructions in set theory, nonstandard analysis, algebra,
geometry and elsewhere.
Model theory is a main branch of applied mathematical logic. Here the
techniques developed in mathematical logic are combined with construction
methods of other areas (such as algebra and analysis) to their mutual benefit. The
following demonstrations can provide only a first glimpse in this respect, a
deeper understanding being gained, for instance, from [CK] or [Ho]. For further-

ranging topics, such as saturated models, stability theory, and the model theory
of languages other than first-order, we refer to the special literature, [Bue],
[Mar], [Pz], [Rot], [Sa], [She].
The theorems of Lwenheim and Skolem were first formulated in the
generality given in 5.1 by Tarski. These and the compactness theorem form the
basis of model theory, a now wide-ranging discipline that arose around 1950.
Key concepts of model theory are elementary equivalence and elementary
extension. These not only are interesting in themselves but also have multiple
applications to model constructions in set theory, nonstandard analysis, algebra,
geometry and elsewhere.
Complete axiomatizable theories are decidable; see 3.5. The question of
decidability and completeness of mathematical theories and the development of
well-honed methods that solve these questions have always been a driving force
for the further development of mathematical logic. Of the numerous methods,
we introduce here the most important: Vaught’s test, Ehrenfeucht’s game,
Robinson’s method of model completeness, and quantifier elimination. For more
involved cases, such as the theories of algebraically closed and real closed fields,
model-theoretic criteria are developed and applied. For a complete
understanding of the material in 5.5 the reader should to some extent be familiar
with some basic algebraic constructions, mainly concerning the theory of fields.
5.1 Elementary Extensions
In 3.3 nonstandard models were obtained using a method that we now
generalize. For given  and a set A let 
 denote the language resulting from 
by adjoining new constant symbols  for all a ∈ A. The symbol  should depend
only on a, not on A, so that 
 whenever A ⊆ B. To simplify notation we
shall write from Theorem 1.3 onward just a rather than ; there will be no risk of
misunderstanding.
Let  be an -structure and A ⊆ B (the domain of ). Then the 
-
expansion in which  is interpreted by a ∈ A will be denoted by 
. According
to Exercise 3 in 2.3 we have for arbitrary 
 and arbitrary 
,
(1) 
 (
).
It is important to notice that every sentence from 
 is of the form 
 for
suitable 
 and 
. Instead of 
 (which is equivalent to 
) we later will write just 
 or even 
, as in Theorem 1.3.

Thus,  may also denote a constant expansion of 
 if it is not the distinction
that is to be emphasized. This notation is somewhat sloppy but points up the
ideas behind the constructions.
Note that for an 
-structure 
, the 
-expansion 
 receives a new
constant symbol for everya ∈ A, even if some elements of 
 already possess
names in . The set of all variable-free literals 
 such that 
 is
called the diagram of 
, denoted by 
. For instance, 
 contains for all 
 the literals 
, 
, 
, or 
, depending on whether indeed a
= b, a≠b, a < b, or 
 for the reals a, b. Diagrams are important for various
constructions in model theory.
The notion of an embedding 
 as defined in 2.1 (that is, the image of 
 under ı is an isomorphic copy of 
) embraces the notion of a substructure.
Indeed, 
 iff 
, i.e. if  is the identical or trivial embedding of 
 into 
.
Let 
. In this chapter, the embeddability of an 
-structure 
 into a
given 
-structure 
 often means the embeddability of 
 into the 
-reduct 
of , and we shall write 
 also in such a situation. In this sense the group 
, for example, is embeddable into the field 
.
Theorem 1.1.
Let 
 , 
 be an 
 -structure, and  an 
 -structure. Then 
 iff 
 is an embedding of 
 in 
Proof. 
: Let 
 and a, b ∈ A, a≠b. Then 
. Hence 
, or
equivalently, 
. Thus ı is injective. For a relation symbol r from 
 and 
 we have in view of 
,
Similarly 
 is obtained, for note that whenever 
 and b ∈
A then 
. Thus, ı is indeed an
embedding. ⇐ : For variable-free terms t in 
 one easily verifies 
,
where here and elsewhere 
 means more precisely 
. Since ı is injective, it
follows for variable-free equations t 1 = t 2 in 
,

In the same way we get 
. Sentences of the form 
and their negations are dealt with analogously. Thus, 
.
Corollary 1.2.
Let 
 be  -structures and 
 an 
 -expansion of  . Then 
 iff 
 is
embeddable into  . Moreover, if A ⊆ B then 
.
Indeed, by the theorem with 
, the mapping 
 realizes the
embedding, and also the converse of the first claim is obvious. ı is the identical
mapping in case A ⊆ B, which verifies the “Moreover” part with 
.
Frequent use will be made of this corollary, without mentioning it explicitly.
Taking a prime model for a theory T to mean a model embeddable into every T-
model, the corollary states that 
 is a prime model for 
, understood as a
theory. We are using the concept of a prime model only in this sense. It must be
distinguished from the concept of an elementary prime model for T as defined in
Exercise 2.
Probably the most important concept in model theory, for which a first
example appears on the next page, is given by the following Definition. Let 
be a first-order language and let 
 be -structures. 
 is called an elementary
substructure of 
, and  an elementary extension of 
, in symbols 
, if A
⊆ B and
(2) 
, for all 
 and 
.
Clearly, 
. Terming 
 the
elementary diagram of 
, 
 is obviously equivalent to A ⊆ B and 
. Indeed, (2) already holds given only 
, for all 
 and 
.
(2) is equivalent to 
, by (1). And since every 
is of the form 
 for appropriate 
, 
, and n ≥ 0, the property 
 is also characterized by A ⊆ B and 
 (elementary equivalence in 
).
In general, 
 means much more than 
 and 
. For instance, let

(.)
 and 
. Then certainly 
, and since 
, we have
also 
. But 
 is false. For example, 
 is true in 
, but
obviously not in 
. The following theorem will prove to be very useful for,
among other things, the provision of nontrivial examples for 
:
Theorem 1.3 (Tarski’s criterion).
For arbitrary  -structures 
 with 
 the following conditions are
equivalent:
(i)
 ,
 
(ii) For all 
 and 
 holds the implication 
 for some a ∈ A.
 
Proof. (i) ⇒ (ii): Let 
 and 
, so that also 
. Then 
 for some a ∈ A. But 
; hence 
. (ii) ⇒ (i): Since 
, (2) certainly holds for prime formulas. The induction steps for ∧, ¬are obvious.
Only the quantifier step needs a closer look:
We prove the direction ⇒ in the last equivalence indirectly: Assume that 
. Then 
. Hence 
 for some a ∈ A according
to (ii). Thus, 
 cannot hold for all a ∈ A.
Interesting examples for 
 are provided in a surprisingly simple way by
the following theorem, which, unfortunately, is applicable only if 
 has “many
automorphisms,” as is the case in the example below, and in geometry, for
instance.
Theorem 1.4.
Let 
 . Suppose that for all n, all 
 , and all b ∈ B there is an
automorphism 
 such that 
 , and 
 . Then 
 .
Proof. It suffices to verify (ii) in Theorem 1.3. Let 
, or equivalently 

 for some b ∈ B. Then 
 according to Theorem 2.3.4, and
since 
, we obtain 
 with a : = ıb ∈ A. This proves (ii).
Example. It is readily shown that for given 
 and 
 there
exists an automorphism of 
 that maps b to a rational number and leaves 
 fixed (Exercise 3). Thus, 
. In particular 
.
Here is a look at some less simple examples of elementary extensions,
considered more closely in 5.5. Let 
 denote the field of algebraic
numbers and  the field of complex numbers. The domain  consists of all
complex numbers that are zeros of (monic) polynomials with rational
coefficients. Then 
. Similarly, 
 where 
 denotes the field of all
real algebraic numbers and 
 is the field of all reals. The claim 
 follows
from the model completeness of the theory 
 proved on page 198. Similarly 
 will be shown.
Before continuing we will acquaint ourselves somewhat with transfinite
cardinal numbers. It is possible to assign a set-theoretic object denoted by | M |
not only to finite sets but to arbitrary sets M such that (3) 
 (
∼ means equipotency; see page 111).
| M | is called the cardinal number or cardinality of M. This is just the
number of elements in M for a finite M; for an infinite set M, | M | is called a
transfinite cardinal number, or briefly a transfinite cardinal.
At this stage it is unimportant just how | M | is defined in detail. The
interested reader will find some definition in every textbook on set theory.
Significant are (4) and (5), taken as granted, from which (6) and (7)
straightforwardly follow.
(4) The cardinal numbers are well-ordered according to size, i.e., each
nonempty collection of them possesses a smallest element. Here let | N |
≤ | M | if there is an injection from N to M. The smallest transfinite
cardinality is | ℕ |, i.e., | ℕ | ≤ | M | for all infinite sets M.
 
(5)
 for arbitrary sets M and N of which
at least one is infinite.
 
Remark. With this definition of ≤ it follows that | M | ≤ | N | & | N | ≤ | M |
implies | M | = | N | (without 
). This is called the Cantor–Bernstein theorem.
Actually, the first proof of this theorem without AC (even more elegant than

Bernstein’s) is due to Dedekind, who left it unpublished in his diary from 1887.
This theorem holds under surprisingly weak assumptions; see [De].
We first derive from (4) and (5) that 
 has the same
cardinality as M for infinite M, where M ∗ denotes the set of all nonempty finite
sequences of elements of M. In short,
(6)
 (M infinite). 
Indeed, | M 1 | = | M |, and the hypothesis | M n | = | M | obviously yields 
 by (5). Thus | M n | = | M | for all n. Therefore 
. One similarly obtains from (4), (5) for every
transfinite cardinal κ the property
(7) If 
 are sets and | A n | ≤ κ for all 
 then 
. 
The smallest transfinite cardinal number (i.e., 
) is that of the countably
infinite sets, denoted by 
. The next one is ℵ1. Then follows ℵ 2, ℵ 3, 
There is a smallest cardinal larger than all ℵ n , denoted by ℵ ω, etc. The Cantor–
Bernstein theorem shows that the power set 
 and the set 
 have the same
cardinality, denoted by 
. Certainly 
, hence 
. Cantor’s
continuum hypothesis (CH) states that 
.
CH is independent in ZFC; see e.g. [Ku]. While there are axioms extending
beyond ZFC that decide CH one way or another, none of these is sufficiently
plausible to be regarded as “true.” In the last decades some evidence has been
collected that suggests that 
, but this seemingly does not yet convince
the majority of mathematicians.
The cardinality of a structure 
 is always that of its domain, that is, 
. Theorem 1.5 below, essentially due to Tarski and therefore sometimes
called the Lwenheim–Skolem–Tarski theorem, generalizes Theorem 3.4.1 (page
112) essentially. The additive “downward” prevents a mix-up of these theorems.
For 
, Theorem 1.5 ensures the existence of some 
 (in particular 
) such that 
.
Theorem 1.5 (Lwenheim–Skolem theorem downward).

Suppose that  is an 
 -structure such that 
 and let A 0 ⊆ B be
arbitrary. Then  has an elementary substructure 
 of cardinality 
 such that A 0 ⊆ A.
Proof. We construct a sequence A 0 ⊆ A 1 ⊆ ⋯ ⊆ B as follows. Let A k be
given. For every 
 and 
 such that 
 we select some
b ∈ B with 
 and adjoin b to A k , thus getting A k + 1. In particular, if α
is 
 then certainly 
. Since 
, there is no
alternative selection; hence 
. Thus, 
 is closed under the
operations of 
, and therefore defines a substructure 
. We shall prove 
 by Tarski’s criterion. Let 
 for 
 and let 
.
Then 
 for some k. Therefore, there is some a ∈ A k + 1 (hence a ∈ A)
such that 
. This proves (ii) in Theorem 1.3 and so 
. It remains to
show that 
. There are at most κ formulas and κ finite
sequences of elements in A 0. Thus, by definition of A 1, at most κ new elements
are adjoined to A 0. Hence | A 1 | ≤ κ. Similarly, | A n | ≤ κ is verified for each n >
0. By (7) we thus get 
.
Combined with the compactness theorem, the above theorem yields
Theorem 1.6 (Löwenheim–Skolem theorem upward).
Let  be any infinite 
 -structure and 
 . Then there exists an 
 with 
 .
Proof. Choose some D ⊇ C with | D | = κ. From (6) it follows that 
,
because the alphabet of 
 has cardinality κ. Since 
, by the
compactness theorem, 
 has a model 
. Since 
 (d ∈ D) is injective, we may assume 
 for all d ∈ D, i.e., D ⊆ B.
By Theorem 1.5 with 
 for  and D for A 0, there is some 
 with D ⊆ A
and 
. Hence | A | = κ. From C ⊆ D and 
 it follows that 
. Since C ⊆ D ⊆ A in addition, the 
-

reduct of 
 is an elementary extension of the given structure .
These theorems show in particular that a countable theory T with at least one
infinite model also has models in every infinite cardinality. Further, ⊢ T α
already holds when merely 
 for all T-models 
 of a single infinite cardinal
number κ, as long as T has only infinite models, because under this assumption
every T-model is elementarily equivalent to a T-model of cardinality κ.
Exercises
1. Let 
 and 
, where A ⊆ B. Prove that 
.
 
2. An embedding 
 is termed elementary if 
, where 
denotes the image of 
 under ı. Show similarly to Theorem 1.1 that an 
-structure 
 is a model of 
 iff 
 is elementarily embeddable
into 
.
 
3. Let 
 and 
. Show that there is an automorphism of 
 that maps b to a rational number and leaves all a i fixed.
 
4. Let 
. Construct a structure  in which 
 are both elementarily
embeddable.
 
5. Let 
 be an -structure generated from G ⊆ A and 
 the set of ground
terms in 
. Prove that (a) for every a ∈ A there is some 
 such
that 
, (b) if 
 and 
 then 
. Here 
.
 
5.2 Complete and κ-Categorical Theories
According to the definition on page 105, a theory 
 is complete if it is
consistent and each extended theory 
 in 
 is inconsistent. A complete
theory need not be maximally consistent in the whole of 
. For instance, in
general neither ⊢ Tx = y nor ⊢ Tx≠y, even if T is complete. Some equivalent
formulations of completeness, whose usefulness depends on the situation at
hand, are presented by the following
Theorem 2.1.
For a consistent theory T the following conditions are equivalent : 1

(.)
Proof. (i)
(ii): Since 
 for each model 
, it must be that 
. (ii)  ⇒  (iii): For 
 we have by (ii) 
, and
therefore 
. (iii)  ⇒  (iv): Let 
, 
, and 
, say. Then 
for all 
 by (iii), hence ⊢ T α. (v) is a special case of (iv) because 
, for arbitrary 
. (v)  ⇒  (i): Let T′ ⊃ T and α ∈ T′ ∖ T. Then ⊢
T ¬α by (v); hence also ⊢ T′ ¬α. But then T′ is inconsistent. Hence, by the above
definition, T is complete.
We now present various methods by which conjectured completeness can be
confirmed. The completeness question is important for many reasons. For
example, according to Theorem 3.5.2, a complete axiomatizable theory is
decidable whatever the means of proving completeness might have been.
An elementary theory with at least one infinite model, even if it is complete,
has many different infinite models. For instance, according to Theorem 1.6, the
theory possesses models of arbitrarily high cardinality. However, sometimes it
happens that all of its models of a given finite or infinite cardinal number κ are
isomorphic. The following definition bears this circumstance in mind.
Definition. A theory T is κ-categorical if there exists up to isomorphism
precisely one T-model of cardinality κ.
Example 1. The theory Taut= of tautological sentences in 
 is κ-
categorical for every cardinal κ. Indeed, here models 
 of cardinality κ are
naked sets and these are trivially isomorphic under any bijection from A onto B.
The theory DO of densely ordered sets results from the theory of ordered sets
(formalized in 2.3; see also 2.1) by adjoining the axioms
It is obvious that a densely ordered set is infinite. DO can be extended by the
axioms 
 and 
 to the theory DO11 of densely
ordered sets with edge elements. Replacing R by ¬R results in the theory DO10
of densely ordered sets with left but without right edge elements. Accordingly
DO01 denotes the theory with right but without left, and DO00 that of dense
orders without edge elements. The paradigm of a model for DO00 is 
.
Another model is 
.

Example 2. DO00 is ℵ0-categorical (Exercise 1 treats the other DO ij ). The
following proof is due to Cantor. A function f with { dom} f ⊆ M and { ran} f ⊆
N is said to be a partial function from M to N. Now let 
 and 
 be countable DO00-models. Define f 0 by f 0 a 0 = b 0 so that 
, 
 (step 0). Assume that in the nth step a partial
function f n fromA to B with finite domain was constructed with a < a′ ⇔ f n a < f
n a′, for all a, a′ ∈ { dom} f n (a so-called partial isomorphism), and that 
 and 
. These conditions are trivially
satisfied for f 0. Let m be minimal with 
. Choose 
such that 
 is also a partial isomorphism. This is possible thanks
to the denseness of B. Now let m be minimal with 
. Choose a
suitable a ∈ A ∖ { dom} g n such that 
 is a partial
isomorphism too. This “to and fro” construction clearly provides for both 
 and 
. Claim: 
 is an isomorphism from
A onto B. Indeed, f is a function. Moreover, { dom} f = A and { ran} f = B. The
isomorphism condition x < y ⇔ fx < fy is clear, since any x, y ∈ A belong already
to { dom} f n for suitable n.
Example 3. The successor theoryT{ suc} in 
 has the axioms
The last axiom says there are no “circles.”T{ suc} is not ℵ0-categorical, but
it is ℵ1-categorical. Indeed, each model 
 with 
 consists up to
isomorphism of the (countable) standard model 
 and ℵ1 many “threads”
of isomorphism type 
, where 
. For if there were only countably
many such threads then the entire model would be countable. Hence any two T{
suc}-models of cardinality ℵ1 are isomorphic.
Example 4. The theory 
 of a.c. fields of given characteristic p (page
105) is ℵ1-categorical. We sketch here a proof very briefly because 
 is
analyzed in 5.5 in a different way. The claim follows from the facts that each
field is embeddable into an a.c. field (cf. Example 1 in 5.5) and that a
transcendental extension 
 of a field 
 (that is, every a ∈ K′ ∖ K is
transcendental over K) has a transcendence basisB. This is a maximal system of

algebraically independent elements in K′ ∖ K. The isomorphism type of 
 is
completely determined by the cardinality of B.
It is fairly plausible that in Examples 3 and 4 κ-categoricity holds for every
cardinal κ > ℵ0. This observation is no coincidence. It is explained by the
following theorem.
Morley’s theorem. If a countable theory T is κ-categorical for some κ > ℵ0
then it is κ-categorial for all κ > ℵ0.
The proof makes use of extensive methods and must be passed over here. On
the other hand, the proof of the following theorem requires but little effort.
Theorem 2.2 (Vaught’s test).
A countable consistent theory T without finite models is complete provided T is
κ-categorical for some κ.
Proof. Note first that κ ≥ ℵ0 because T possesses no finite models. Assume that
T is incomplete. Choose some 
 with 
 and 
. Then T, α and T,
¬α are consistent. These sets have countable infinite models by Theorem 1.5,
and according to Theorem 1.6 there are also models 
 and 
 of cardinal κ.
Since 
, by hypothesis 
; hence 
, which contradicts 
 and
.
Example 5. (a) DO00 has only infinite models and is ℵ0-categorical by
Example 2. Hence DO00 is complete by Vaught’s test. This fact confirms 
 once again. In fact, each DO ij is complete (Exercise 1). Thus, 
 for 
 iff 
 have “the same edge configuration,” which tells us
that the DO ij are the only completions of DO. Since DO is axiomatizable, it
follows by Exercise 3 in 3.5 that DO is decidable. The same applies to each of its
finite extensions DO ij .
(b) The successor theory T{ suc} is ℵ1-categorical (Example 3) and has only
infinite models. Hence it is complete by Vaught’s test and as an axiomatizable
theory thus decidable (Theorem 3.5.2).
(c) 
 is ℵ1-categorical by Example 4. Each a.c. field 
 is infinite. For
assume the converse, 
, say. Then the polynomial 
would have no root, a contradiction. Thus, by Vaught’s test, 
 is complete
and decidable (since it is axiomatizable). This result will be derived by quite
different methods in 5.5.
The model classes of first-order sentences are called elementary classes.

These clearly include the model classes of finitely axiomatizable theories. For
each such theory T, 
 is an intersection of elementary classes,
also termed a Δ-elementary class. Thus, the class of all fields is elementary, and
that of all a.c. fields is Δ-elementary. On the other hand, the class of all finite
fields is not Δ-elementary because its theory evidently has infinite models. An
algebraic characterization of elementary and Δ-elementary classes will be
provided in 5.7.
The model classes of complete first-order theories are called elementary
types. MdT is the union of the elementary types belonging to the completions of
a theory T. For instance, DO has just the four completions DO ij determined by
the edge configuration, that is, by those of the sentences 
, valid in the
respective completion. For this case, the next theorem provides more
information on T, in particular on ≡ T.
Let 
 be nonempty and T a theory. Take 
 to denote the set (still
dependent on T) of all formulas equivalent in T to Boolean combinations of
formulas in X. Clearly, 
 since 
 for α ∈ X. Therefore, 
,
because 
 whenever α ∈ T. Call X ⊆ ℒ0 a Boolean basis for 
 in T if
every 
 belongs to 
, i.e., every sentence in 
 is a Boolean combination
of sentences from X. Example 6(b) below indicates how useful a Boolean base
for decision problems can be. 
 is to mean 
, for all α ∈ X.
Theorem 2.3 (Basis theorem for sentences).
Let T be a theory and 
 a set of sentences with 
 , for all 
. 2 Then X is a Boolean basis for ℒ0 in T.
Proof. Let 
 and 
. We claim ( ∗ ) : 
.
Otherwise let 
. Then 
; indeed, for any 
 we have 
 and hence 
. Therefore 
 for some 
, because 
 is closed under conjunctions. This yields 
, i.e., 
. Thus 
, in contradiction to 
. So ( ∗ ) holds. Hence there are 
 such that 
. We know that α ⊢ T β i and so that α
⊢ T β as well. This and β ⊢ T α confirms 
, and since 
, also 
.
Example 6. (a) Let T = DO and 
. Then 
, for all 

. Indeed, 
 states that 
 possess the same edge configuration.
But then 
, because the DO ij are all complete; see Example 5(a). Therefore,
 and  form a Boolean basis for 
 in DO. This theory has four completions,
and so by Exercise 5 in 3.5, exactly 15 (
) consistent extensions.
(b) Let 
 and 
. Again, 
, for all 
, because by Example 5(c), 
 is complete for each p (including p =
0). Hence, by Theorem 2.3, the { char} p constitute a Boolean basis for
sentences modulo 
. This implies the decidability of 
: let 
 be
given; just wait in an enumeration process of the theorems of 
 until a
sentence of the form α ↔ β appears, where β is a Boolean combination of the {
char} p. Such a sentence definitely appears. Then test whether 
, for
example by converting β into a CNF.
Corollary 2.4.
Let 
 be a theory with arbitrarily large finite models, such that all finite T-
models with the same number of elements and all infinite T-models are
elementarily equivalent. Then (a) the sentences ∃n form a Boolean basis for 
in T,
(b) T is decidable provided T is finitely axiomatizable.
Proof. Let 
. Then by hypothesis, 
, for all 
. Thus, (a) follows by Theorem 2.3. By (a) and Exercise 4 in 2.3 each α
∈ ℒ0 is equivalent in T to 
 with k 0 < ⋯ < k n or to 
 for
some k. Hence a sentence α that has a T-model has also a finite T-model by the
first assumption on T, i.e., T has the finite model property. Thus, (b) holds by
Exercise 3 in 3.6.
An easy example of application is the theory Taut= of tautologies in 
. The
formulas constructed from the Boolean base 
 in the proof also permit
a simple description of the elementary classes of 
. These are finite unions of
classes determined by sentences ∃ k and ∃= m . Another example is the theory FO
of finite ordered sets. We prove in the next section that FO satisfies the
assumptions of Corollary 2.4. Hence, the elementary classes of FO have the
same simple description.
These examples illustrate the following: If we know the elementary types of

a theory T—these correspond to the completions of T—then we also know their
elementary classes. As a rule, the type classification, that is, finding an
appropriate set X satisfying the hypothesis of Theorem 2.3, is successful only in
particular cases. The required work tends to be extensive. We mention in this
regard the theories of abelian groups, of Boolean algebras, and of other locally
finite varieties; see for instance [MV]. The above examples are just the simplest
ones.
Easy to deal with is the case of an incomplete theory T that has finitely many
completions. Example 6(a) is just a special case. According to Exercise 5 in 3.5,
T then has finitely many extensions. Moreover, all these are finite extensions.
Indeed, if 
 is a nonfinite extension then w.l.o.g. 
,
which obviously implies that T has infinitely many completions, contradicting
our hypothesis. Thus, we may assume that T 1, …, T m are the completions of T
and that 
 for some 
. Then 
 is a Boolean basis for 
 in T. Exercise 4 provides a canonical axiomatization of all consistent
extensions of T.
Exercises
1. Prove that also DO10, DO11, and DO01 are ℵ0-categorical and hence
complete. In addition, verify that these theories and DO00 are the only
completions of DO.
 
2. Prove that T{ suc} (page 178) is also completely axiomatized by the first
two given axioms plus IS: 
; here φ runs over
all formulas of 
 (the “induction schema” for 
).
 
3. Show that the theory T of torsion-free divisible abelian groups is ℵ1-
categorical and complete (hence decidable). This shows, e.g., that the
groups 
 and 
 are elementarily equivalent.
 
4. Let T be incomplete and let 
 be all the completions of
T. Prove that 
 are all consistent extensions of T. Here n ≤ m
and 
. (Note that 
.)
 
5. Show that an ℵ0-categorical theory T with no finite models has an
elementary prime model. Example: 
 is an elementary prime model 

for DO00.
5.3 The Ehrenfeucht Game
Unfortunately, Vaught’s criterion has only limited applications because many
complete theories are not categorical in any transfinite cardinality. Let SO denote
the theory of discretely ordered sets, i.e., of all (M, < ) such that every a ∈ M has
an immediate successor provided a is not the right edge element, and likewise an
immediate predecessor provided a is not a left edge element. “SO” is intended to
recall “step order,” because the word “discrete” in connection with orders often
has the stronger sense “each cut is a jump.” SO ij (i. j ∈ { 0, 1}) is defined
analogously to DO ij (see page 177). For instance, SO10 is the theory of
discretely ordered sets with left and without right edge element. Clearly, 
 is
a prime model for SO10. The models of SO10 arise from arbitrary orders (M, < )
with a left edge element by replacing the latter by 
 and every other element
of M by a specimen of 
. From this it follows that SO10 cannot be κ-
categorical for any κ ≥ ℵ0. Yet this theory is complete, as will be shown, and the
same applies to SO00 and SO01. Only SO11 is incomplete and is the only one of
the four theories that has finite models. It coincides with the elementary theory
of all finite ordered sets; Exercise 3.
We prove the completeness of SO10 game-theoretically using a two-person
game with players I and II, Ehrenfeucht’s game 
, which is played in k
rounds, k ≥ 0. Here 
 are given -structures and 
 is a relational language,
i.e., 
 does not contain constants or operation symbols. With regard to our goal
this presents no real loss of generality because each structure can be converted
into a relational one by replacing its operations by the corresponding graphs.
Another advantage of relational structures used in the sequel is that there is a
bijective correspondence between subsets and substructures.
We now describe the game 
. Player I chooses in each of the k rounds
one of the two structures 
 and . If this is 
, he selects some a ∈ A. Then
player II has to answer with some element b ∈ B. If player I chooses 
 and
some b from B then player II must answer with some element a ∈ A. This is the
entire game. Clearly, it has still to be explained who wins. After k rounds,
elements 
 and 
 have been selected, where a i , b i
denote the elements selected in round i. Player II wins if the mapping a i ↦b i (

) is a partial isomorphism from 
 to 
; in other words, if the
substructure of 
 with the domain 
 is isomorphic to the substructure
of 
 with the domain 
. Otherwise, player I is the winner.
We write 
 if player II has a winning strategy in the game 
, that
is, in every round player II can answer any move from player I such that at the
end player II is the winner. For the “zero-round game” let 
 by definition.
Example. Let 
 be a proper initial segment of 
. We show
that 
 for arbitrary k > 0. Player II plays as follows: If player I chooses
some b 1 in B in the first round then player II answers with 
 if 
, otherwise with a 1 = d(0, b 1). 3 The procedure is similar if
player I begins with 
. If player I now selects some b 2 ∈ B such that 
, then player II answers with 
 depending
on whether b 2 > b 1 or b 2 < b 1, and otherwise
with the element of the same distance from 0 or a 1 as that of b 2 from 0 or b
1 in B. Similarly in the third round, etc. The figure shows the course of a 3-round
game played in the described way. Player I has chosen from  only for
simplicity. With this strategy player II wins every game, as can be shown by
induction on k. The reader should play a few rounds before proving this
rigorously.
In contrast to the example, for 
 and 
 player II’s chances
have already dropped in 
 if player I selects 0 ∈ A in the first round.
Player II will lose already in the second round. This has to do with the fact that
the existence of an edge element is expressible by a sentence of quantifier rank
2. We write 
 for -structures 
 if 
, for all 
 with 
. It is always the case that 
 for all 
, because in relational
languages there are no sentences of quantifier rank 0. Below we will prove the
following remarkable

Theorem 3.1.
 implies 
 . Hence, 
 provided 
 for all k.
For finite signatures a somewhat weaker version of the converse of the
theorem is valid as well, though we do not discuss this here. Before proving
Theorem 3.1 we demonstrate its applicability. The theorem and the above
example yield 
 for all k and hence 
 for every 
,
because 
 is a prime model for SO10 and hence can always be regarded as
an initial segment of 
. Therefore SO10 is complete. For reasons of symmetry
the same holds for SO01, and likewise for SO00. On the other hand, SO11 has the
finite model property according to Exercise 3 and coincides with the theory FO
of all finite ordered sets.
For the proof of Theorem 3.1 we first consider a minor generalization of 
, the game 
 with prior moves 
. In the first
round player I selects some a n + 1 ∈ A or b n + 1 ∈ B and player II answers with b
n + 1 or a n + 1, etc. The game protocol consists of sequences 
 and 
 at the end. Player II has won if a i ↦b i (
) is a partial
isomorphism. Clearly, for n = 0 we obtain precisely the original game 
.
This adjustment brings about an inductive characterization of a winning
strategy for player II independent of more general concepts as follows:
Definition. Player II has a winning strategy in 
 provided a i ↦b i for 
 is a partial isomorphism. Player II has a winning strategy in 
 if for every a ∈ A there is some b ∈ B, and for every b ∈ B some
a ∈ A, such that player II has a winning strategy in 
. Here 
 denotes the operation of appending the element c to the sequence .
We shall write 
 if player II has a winning strategy in 
. In particular, 
 (this represents the choice 
) is now
precisely defined.
Lemma 3.2.
Let 
 , where 
 and 
 . Then (∗) : 
 ,
for all 
 such that 
 .

Proof by induction on k. Let k = 0. Since a i ↦b i (
) is a partial
isomorphism, ( ∗ ) is valid for prime formulas, and since the induction steps in
the proof of ( ∗ ) for ¬, ∧ are obvious, it is valid also for all formulas φ with qrφ
= 0. Now let 
. The only interesting case is 
 such that 
, because it is easily seen that every other formula of quantifier rank k
+ 1 is a Boolean combination of such formulas and of formulas of quantifier
rank ≤ k. Induction over ¬and ∧ in proving ( ∗ ) is harmless. Let 
and b ∈ B. Then Player II chooses some a ∈ A with 
, so
that according to the induction hypothesis, 
. Clearly, the
latter is supposed to hold for sequences 
 of elements of arbitrary length.
Because of 
, also 
. Since b was arbitrary, we obtain 
. For reasons of symmetry, 
 holds as
well.
Theorem 3.1 is just the application of this lemma for the case n = 0 and has
therefore been proved. The method illustrated is wide-ranging and has many
generalizations.
Exercises
1. Let 
 be two infinite discretely ordered sets with the same edge
configuration. Prove that 
 for all k. Hence 
 are elementarily
equivalent.
 
2. Let 
, k > 0, and | A |, | B | ≥ 2 k − 1. Prove that 
, so that 
 according to Theorem 3.1.
 
3. Infer from Exercise 2 that SO11 has the finite model property and
coincides with the elementary theory FO of all finite ordered sets.
 
4. Show that L, R, ∃1, 
 constitute a Boolean basis modulo SO and use
this to prove the decidability of SO. 4
 

5.4 Embedding and Characterization Theorems
Many of the foregoing theories, for instance those of orders, of groups in 
,
and of rings, are universal or ∀-theories, considered already on page 83. We also
know that for every theory T of this kind 
 implies 
; in short, T is
S-invariant. DO obviously does not have this property, and so there cannot exist
an axiom system of ∀-sentences for it. According to Theorem 4.3 the ∀-theories
are completely characterized by the property of { S}-invariance. This fact
presents a particularly simple example of the model-theoretic characterization of
certain syntactic forms of axiom systems.
 is called the universal part of a theory T.
Note the distinction between the set T ∀ and the ∀-theory T ∀, which of course
contains more than just ∀-sentences. For 
 put 
. If 
 is an 
-structure and 
 an -structure then 
 or ‘
 is a substructure of 
 ’ will
often mean in this section that 
 is a substructure of the 
-reduct of 
. The
phrase ‘
 is embeddable into 
 ’ introduced in 5.1 is to be understood
correspondingly. Examples will be found below. First we state the following
Lemma 4.1.
Every T 0 ∀ -model 
 is embeddable into some T-model.
Proof. It is enough to prove ( ∗ ) : 
 is consistent, because 
 is
embeddable into each 
 by Theorem 1.1. Assume that ( ∗ ) is false.
Then there is a conjunction 
 of sentences in 
 such that 
, or
equivalently, ⊢ T ¬ϰ(\vec{a}). Here let  embrace all the constants of 
 that
appear in the members of ϰ but not in T. By the rule ( ∀3) of constant
quantification from 3.2, 
. Hence 
 and thus 
, a contradiction to 
.
Lemma 4.2.
 consists of just the substructures of all T-models.
Proof. Every substructure of a T-model is of course a T ∀-model. Furthermore,
each 
 is (by Lemma 4.1 for 
) embeddable into some 
, and
this is surely equivalent to 
 and 
 for some 
, because MdT is

always closed under isomorphic images.
Example. (a) Let 
 be the theory of abelian groups in 
. A substructure
of 
 is obviously a commutative regular semigroup. Conversely, it is not
hard to prove that every such semigroup is embeddable into an abelian group.
Therefore, the theory 
 coincides with the theory of the commutative regular
semigroups. Warning: noncommutative regular semigroups need not be
embeddable into groups.
(b) Substructures of fields in 
 are integral domains. Conversely,
according to a basic algebraic construction, every integral domain (not every
ring) is embeddable into a field, its quotient field. It is constructed similarly to
the field 
 from the integral domain . Hence, by Lemma 4.2, the theory T J of
integral domains, axiomatized by the axioms for commutative rings with 1 and
without zero-divisors, has the same universal part as the theory T F of fields.
Also, 
 has the same universal part, because every field is embeddable into
some algebraically closed field, its algebraic closure; see [Wae] and Example 1
in 5.5.
Theorem 4.3.
T is a universal theory iff T is S-invariant.
Proof. This follows immediately from Lemma 4.2, since for an S-invariant
theory T, clearly MdT = MdT ∀. In other words, T is axiomatized by its universal
part T ∀.
This theorem is reminiscent of the HSP theorem cited on page 129.
However, the latter concerns identities only. It has a proof that is akin to the
proof of the following remarkable theorem, which presents an elegant model-
theoretic characterization of universal Horn theories introduced in 4.2. Call a
theory T SP-invariant if MdT is closed under direct products and substructures.
Always remember that a statement like 
 with 
 is to mean either 
 or 
.
Theorem 4.4.
T is a universal Horn theory iff T is SP-invariant.
Proof. ⇒ : Exercise 1 in 4.2. ⇐ : Trivial if ⊢ T ∀xy x = y, for then T is
axiomatized by ∀xy x = y. Otherwise let U be the set of all universal Horn
sentences of T. We prove MdT = MdU. Only MdU ⊆ MdT is not obvious. Let 

. To verify 
 it suffices to show ( ∗ ) : 
, since for 
 w.l.o.g. 
, so 
 thanks to S-invariance. Let 
, so that 
 for some I≠∅, all π i
prime. We first show 
: 
 for all i ∈ I. Indeed, otherwise 
 for some conjunction 
 of sentences in P, with the tuple  of
constants not in T. Therefore 
. Hence α ∈ U, for α is a
universal Horn sentence in the language of T, whence 
. But this contradicts
 and confirms 
. Choose 
. Then 
 (note that 
 is impossible since
). This verifies ( ∗ ).
The following application of Lemma 4.1 aims in a somewhat different
direction.
Theorem 4.5.
Let 
 and let 
 be an 
 -structure. For 
 the following are
equivalent:
(i)
 is embeddable into some T-model,
 
(ii) every finitely generated 
 is embeddable into a T-model, 
(iii)
 .
 
Proof. (i) ⇒ (ii): Trivial. (ii) ⇒ (iii): Let 
 with 
 open, w.l.o.g. 
. Let 
 for 
 be the substructure in 
generated from 
. By (ii), 
 for some model 
. Since 
,
it holds that 
; therefore 
, so that 
 by Theorem 2.3.2.
Since 
 was chosen arbitrarily, 
, and since 
 was arbitrarily
taken from 
, it follows that 
. (iii) ⇒ (i): This is exactly the claim of

Lemma 4.1.
Examples of applications. (a) Let T be the theory of ordered abelian groups
in 
. Such a group is clearly torsion-free, which is expressed by a
schema of ∀-sentences in 
. Conversely, Theorem 4.5 implies that
a torsion-free abelian group (the 
 in the theorem) is orderable, or what amounts
to the same thing, is embeddable into an ordered abelian group. One needs to
show only that every finitely generated torsion-free abelian group G is orderable.
By a well-known result from group theory, 
 for some n > 0. But 
 can
be ordered lexicographically, as is easily seen by induction on n. For nonabelian
groups, the conditions corresponding to torsion-freeness are somewhat more
involved.
(b) Without needing algebraic methods we know that there exists a set of
universal sentences in 
, whose adoption to the theory of fields
characterizes the orderable fields. Sufficient for this, by Theorem 4.5, is the set
of all ∀-sentences in 
 provable from the axioms for ordered fields.
Indeed, even the schema of sentences ‘ − 1 is not a sum of squares’ is enough to
characterize the orderable fields (see [Wae]).
Not just ∀-theories but also ∀-formulas can be characterized model-
theoretically. Call α(\vec{x}) S-persistent or simply persistent in T provided all 
 have the property
According to the next theorem this property is characteristic for the ∀-
formulas up to equivalence in T.
Theorem 4.6.
If 
 is persistent in T then α is equivalent to some ∀-formula α′ in T,
which can be chosen such that 
 .
Proof. Let Y be the set of all formulas 
 with 
, where β is
open and  and  are of length n ≥ 0 and m ≥ 0, respectively. We shall prove (a):
. This would complete the proof because then, thanks to 
, there is a conjunction 
 of formulas from Y with 
. Since also α ⊢ Tϰ, we have α ≡ T ϰ. Moreover, α′ : = ϰ ∈ Y, since Y is
closed under conjunction according to Exercise 3 in 2.4. This proves the claim.

For proving (a) we assume (b) 
 with 
. We need to show that 
. This follows from (c): 
 is consistent, for if 
,
then w.l.o.g. 
; and also 
 since α is persistent. If (c) were false then 
 for some conjunction 
 of sentences from 
 with the m-
tuple \vec{b} of constants of ϰ from 
. Thus 
.
Since the 
 do not appear in T, we get 
.
Therefore, and by (b), 
, or equivalently 
, in
contradiction to 
.
Remark. Let T be countable and all T-models infinite. Then α is already
equivalent in T to an ∀-formula, provided α is κ-persistent; this means that (sp)
holds for all T-models 
 of some fixed cardinal κ ≥ ℵ0. For in this case each
T-model is elementarily equivalent to a model of cardinality κ by the
Löwenheim–Skolem theorems. Hence, it suffices to verify (a) in the above proof
by considering only models 
 of cardinality κ.
Sentences of the form 
 with kernel α are called ∀∃-sentences. Many
theories, for instance of real or of algebraically closed fields and of divisible
groups, are ∀∃-theories, i.e., they possess axiom systems of ∀ ∃-sentences. We
shall characterize the ∀ ∃-theories semantically.
A chain K of structures is a set K of 
-structures such that 
 or 
for all 
. Chains are very often given as sequences 
.
No matter how K is given, a structure 
 can be defined in a natural way:
Let 
 be its domain. Further, let 
 for 
,
where 
 is chosen such that 
. Such an 
 exists: Let 
 simply
be the maximum of the chain members containing a 1, …, a n , respectively. The
definition of 
 is independent of the choice of 
. Indeed, let 
 and 
. Since 
 or 
, it holds that 
 in either case.
Finally, for function symbols f, let 
, where 
 is chosen such
that 
. Here too the choice of 
 is irrelevant.  was just defined in
such a way that each 
 is a substructure of .
Example 1. Let 
 be the additive group of n-place decimal numbers (with

at most n decimals after the decimal point). Since 
, the 
 form a
chain. Here 
 is just the additive group of finite decimal numbers.
The corresponding holds if the 
 are understood as ordered sets. Because then 
, while 
 for all n, MdSO is not closed under union of chains.
Therefore SO is not an ∀ ∃-theory (in contrast to DO), as follows from a simple
observation in the next paragraph.
It is easy to see that an ∀ ∃-sentence 
 valid in all
members 
 of a chain K of structures is also valid in 
. For let 
.
Then clearly 
 for some 
, hence 
. Since 
, it
follows that 
 according to Corollary 2.3.3. Now,  was arbitrarily be
chosen, hence indeed 
. Thus, if T is an ∀ ∃-theory then MdT is
always closed under union of chains, or as is said, T is inductive.
This property is characteristic of ∀ ∃-theories, Theorem 4.9. However, the
proof is no longer simple. It requires the notion of an elementary chain. This is a
set K of 
-structures such that 
 or 
, for all 
. Clearly, K is
then also a chain with respect to ⊆.
Lemma 4.7 (Tarski’s chain lemma).
Let K be an elementary chain and put 
 . Then 
 for every 
 .
Proof. We have to show that 
, with 
. This follows
by induction on 
 and is clear for prime formulas. The induction steps
over ∧, ¬are also straightforward. Let 
 and a 0 ∈ C arbitrary. There
is certainly some 
 such that 
 and 
. Thus, 
and hence 
. By the induction hypothesis (which is supposed to hold
for any chain member) so too 
. Since a 0 ∈ C was arbitrary, 
. The converse 
 follows similarly.
We require yet another useful concept, found in many of the examples in 5.5.
Let 
. Then 
 is termed existentially closed in 
, in symbols 
,
provided
( ∗ ) 
,

where 
 runs through all conjunctions of literals from 
. ( ∗ )
then holds automatically for all open 
. One sees this straight away by
converting φ into a disjunctive normal form and distributing 
 over the
disjuncts. Clearly 
. Moreover, 
 satisfies a readily
proved chain lemma as well: If K is a chain of structures such that 
 or 
 for all 
, then 
 for every 
. This is an easy
exercise.
The next lemma presents various characterizations of 
. Let 
denote the universal diagram of 
, which is the set of all ∀-sentences of 
valid in 
. Clearly 
. In (iii) the indexing of 
 with A is omitted to
ease legibility.
Lemma 4.8.
Let 
 be 
 -structures and 
 . Then are equivalent
(i) 
 , (ii) there is an 
 such that 
 , (iii) 
 .
Proof. (i) ⇒ (ii): Let 
. We obtain some 
 such that 
 as a
model of 
 (more precisely, as the 
-reduct of such a model), so that
it remains only to show the consistency. Suppose the opposite, so that 
 for some conjunction 
 of members from 
 with the n-tuple 
 of all constants of 
 in ϰ. Since 
 do not occur in 
, we get 
. Thus 
. On the other hand, 
; hence 
. With (i) and 
 also 
, in contradiction to 
. (ii) ⇒ (iii): Since 
, we have 
. Because of 
, evidently 
. (iii) ⇒ (i): By (iii), 
, for all ∀-
sentences α of 
. The latter is equivalent to 
, for all ∃-sentences 
, and hence to property (i).
Theorem 4.9.
A theory T is an ∀∃-theory if and only if T is inductive.
Proof. As already shown, an ∀ ∃-theory T is inductive. Conversely let T be
inductive. We show that 
, where 
 denotes the set of all ∀ ∃-

theorems provable in T. The nontrivial part is the verification of 
.
So let 
. Claim: 
 is consistent. Otherwise 
 for some
conjunction 
 of sentences of 
 with the tuple \vec{a} of constants in
A appearing in ϰ but not in T. Hence 
. Now, 
 is equivalent to an
∀-formula, and so 
 to an ∃-formula. Thus, 
 belongs up to
equivalence to 
. Therefore 
, which contradicts 
. This
proves the claim.
Now let 
 and w.l.o.g. 
. Then also 
 in view of
Lemma 4.8. By the same lemma there exists an 
 with 
, so
that 
 as well. We now repeat this construction with 
 in place of 
and obtain structures 
 such that 
, 
, and 
.
Continuing this construction produces a sequence 
 of
structures with the inclusion relation illustrated in the following figure:
Let 
. Clearly, also 
, and since by construction 
, we get 
 from the chain lemma. At the same time we
also have 
, and since by construction 
 for all i, it holds
that 
, for T is inductive. But then too 
 because 
. This is what
we had to prove.
A decent application of the theorem is that SO10 cannot be axiomatized by ∀
∃-axioms, for SO10 is not inductive according to Example 1. SO10 is an ∀ ∃ ∀-
theory, and we see now that at least one ∀ ∃ ∀-axiom is needed in its
axiomatization.
The “sandwich” construction in the proof of Theorem 4.9 can still be
generalized. We will not elaborate on this but rather add some words about so-
called model compatibility. Let T0 + T1 be the smallest theory containing T0 and
T1. From the consistency of T0 and T1 we cannot infer that T0, T1 are
compatible, i.e., T0 + T1 is consistent, even if T0 and T1 are model compatible in
the following sense: every T0-model is embeddable into some T1-model and

vice versa. This property is equivalent to 
 by Theorem 4.5, hence it is
an equivalence relation. Thus, the class of consistent 
-theories splits into
disjoint classes of pairwise model compatible theories. That model compatible
theories need not be compatible in the ordinary sense is shown by the following
Example 2. DO and SO are model compatible (Exercise 2) but DO + SO is
clearly inconsistent. Since DO is inductive, we get another argument that SO is
not inductive: if it were inductive, DO + SO would be consistent according to
Exercise 3.
Exercises
1. Let X be a set of positive sentences, i.e., the α ∈ X are constructed from
prime formulas by means of ∧,   ∨  , ∀, ∃only. Prove that if 
 then
also 
, whenever 
 is a homomorphic image of 
, that is, MdX is
closed under homomorphic images. Once again the converse holds
(Lyndon’s theorem; see [CK]).
 
2. Show that the theories DO and SO are model compatible.
 
3. Suppose T0 and T1 are model compatible and inductive. Show that T0 +
T1 is an inductive theory that, in addition is model compatible with T0
and T1.
 
4. For inductive T show that of all inductive extensions model compatible
with T there exists a largest one, the inductive completion of T. For
instance, this is 
 for the theory T F of fields.
 
5.5 Model Completeness
A theory T is called model complete if for every model 
 the theory 
is complete in 
. This notion was introduced in [Ro1]. For 
 where 
 (hence 
), the completeness of 
 obviously means the
same as 
, or equivalently, 
. In short, a model complete theory T
has the property
Conversely, if ( ∗ ) is satisfied then 
 is also complete. Indeed, let 

 so that w.l.o.g. 
 and hence 
. But then all these  are
elementarily equivalent in 
 to 
 and therefore to each other, which tells us
that 
 is complete. ( ∗ ) is therefore an equivalent definition of model
completeness, and this definition, which is easy to remember, will be preferred
in the sequel.
It is clear that if 
 is model complete then so too is every theory that
extends it in 
. Furthermore, T is then inductive. Indeed, a chain K of T-models
is always elementary, by ( ∗ ). By Lemma 4.7, we obtain that 
 for any 
, and therefore ⋃K ⊨ T thanks to 
, which confirms the claim.
Hence, by Theorem 4.9, only an ∀ ∃-theory can be model complete.
An example of an ∀ ∃-theory that is not model complete is DO. Let 
 be 
 for 
. Then 
 but (ℚ 1, < ) ⋠ (ℚ 0, < ) so that
( ∗ ) does not hold. These two models also show that the complete theory DO10
is not model complete. Another example of a complete but not model complete
theory is SO10, since as was noticed on page 193, SO10 is not an ∀ ∃-theory.
Conversely, a model complete theory need not be complete: A prominent
example is 
, which will be treated in Theorem 5.4. Nonetheless, with the
following theorem the completeness of a theory can often be obtained more
easily than with other methods.
Theorem 5.1.
If a theory T is model complete and has a prime model then T is complete.
Proof. Suppose that 
 and let 
 be a prime model. Then 
 up to
isomorphism, and so 
 by ( ∗ ), in particular 
. Hence, all T-models
are elementarily equivalent to each other so that T is in fact complete.
The following theorem states additional characterizations of model
completeness, of which (ii) is as a rule more easily verifiable than the definition.
The implication (ii) ⇒ (i) carries the name Robinson’s test for model
completeness.
Theorem 5.2.
For any theory T the following items are equivalent:
(i)
T is model complete,
 

(ii)
 , for all 
 ,
 
(iii) each ∃-formula α is equivalent in T to an ∀-formula β such that { free}
β ⊆{ free} α,
 
(iv) each formula α is equivalent in T to an ∀-formula β such that { free} β
⊆{ free} α.
 
Proof. (i) ⇒ (ii): evident, since 
. (ii) ⇒ (iii):
According to Theorem 4.6 it is enough to verify that every ∃-formula 
 is persistent in T. Let 
, 
, 
, and 
. Then
, because 
 thanks to (ii). (iii) ⇒ (iv): induction on α. (iii) is used
only in the ¬-step: Let α ≡ β, β some ∀-formula (induction hypothesis). Then ¬β
≡ γ for some ∀-formula γ, hence ¬α ≡ γ. (iv) ⇒ (i): let 
, 
, and 
 with 
. Then 
, since by (iv), α(\vec{x}) ≡ T β for some ∀-
formula β. This shows that 
, hence (i).
Remark 1. If T is countable and has infinite models only, then it is possible
to restrict the criterion (ii) to models 
 of any chosen infinite cardinal number
κ. Then we can prove that an ∃-formula is κ-persistent as defined in the remark
on page 190, which by the same remark suffices to prove the claim of Theorem
5.2 and hence (iii). Once we have obtained (iii) we have also (i). This remark is
significant for Lindström’s criterion, Theorem 5.7.
A relatively simple example of a model complete theory is 
, the theory
of (nontrivial) 
-vector spaces 
, where 0 denotes the zero vector
and each 
 is taken to be a unary operation on the set of vectors V. 
formulates the familiar vector axioms, where, for example, the axiom 
 is reproduced as a schema of sentences, namely 
 for all 
. Let 
 with 
. We claim that 
. By Theorem 5.2(iii), 
 is then model complete. For the claim let 
, with a conjunction α of literals in x 1, …, x n and constants a 1, …, a m ,
b 1, …, b k ∈ V. Then α is essentially a system of the form

Indeed, the only prime formulas are term equations, and every term in x1, …,
xn is equivalent in 
 to some term of the form 
. Without
going into detail, it is plausible by the properties of linear systems that the
system (s) has already a solution in , provided it is solvable at all; see for
instance [Zi].
For the rest of this section we assume some knowledge of classical algebra,
where closure constructions are frequently undertaken. For instance, a torsion-
free abelian group has a divisible closure, a field 
 has an algebraic closure (a
minimal a.c. extension of 
), and an ordered field has a real closure; see
Example 2 below. Generally speaking, we start from a theory T and 
. By
a closure of 
 in T we mean a T-model 
 such that 
, for
every 
. More precisely, if 
 then there is an embedding of 
 into 
leaving A pointwise fixed. In this case we say that T permits a closure operation.
Supposing this, let 
, 
, and 
. Then there is a smallest
submodel of 
 containing A ∪{ b}, the T ∀-model generated in  by A ∪{ b},
denoted by 
. Its closure in T is denoted by 
. In view of 
, it is
called an immediate extension of 
 in T.
Example 1. Let 
. A T ∀-model 
 is here an integral domain. T
permits a closure operation: 
 is the so-called algebraic closure of the quotient
field of 
. That there exists an a.c. field 
 that in addition is embeddable
into every a.c. field 
, is Steinitz’s theorem regarding a.c. fields, [Wae, p.
201]. Let now 
 with 
 and 
. Then b is transcendental over 
, because 
 is a.c. Thus, 
, for all a 0, …, a n ∈ A with a
n ≠0. For this reason 
 is isomorphic to the ring 
 of polynomials ∑ i ≤ n a i
x i with the “unknown” x (the image of b). Hence, 
 provided 
, with 
 and 
, 
. The isomorphism of 
 extends in a natural way to their quotient fields (represented by the
field of rational functions over 
) and hence to their closures 
 and 
.
Thus, a T-model has up to isomorphism only one immediate extension in T. Not
so in the next, more involved, example.

Example 2. A real closed field is an ordered field 
 (such as 
) in which
every polynomial of odd degree has a zero and every a ≥ 0 is a square in A.
These properties will turn out to be equivalent to the continuity scheme CS (p.
98). Let RCF denote the theory of these fields. Although ≤ is definable in RCF
by 
, order should here be a basic relation. Let T : =
RCF. A T ∀-model 
 is an ordered integral domain that determines the order of
its quotient field 
. According to Artin’s theorem for real closed fields [Wae, p.
244], some 
 can be constructed, called the real closure of 
 or of its
quotient field 
 in T.Let 
, 
, and 
. Then b is
transcendental over 
, because no algebraic extension of 
 is orderable—this is
another characterization of real closed fields. Here 
 is isomorphic to the
ordered ring 
 of polynomials over 
 and determines the isomorphism type
of its quotient field 
 and of its real closure 
. Actually, 
 is
determined by its restriction to 
, or by the partition 
. To see this, note that it is provable in
RCF that a polynomial p(x) with the zeros a 1, …, a n ∈ A decomposes in 
 as 
 with c ∈ A, n ≥ 0, and q(x) a product of
irreducible polynomials of degree 2 or q(x) = 1. In 
 (and 
) one has q(b) >
0. Indeed, each irreducible factor 
 of q(b) is > 0 since 
 (d, e ∈ A). Thus we know whether p(b) > 0 if
we know the signs of b − a i for all zeros a i of p(x) in A. This suffices to fix the
order in 
 as is easily seen, and hence in 
 by Artin’s theorem.
For inductive theories T that permit a closure operation, Robinson’s test for
model completeness can still be simplified as follows:
Lemma 5.3.
Let T be inductive, and suppose T permits a closure operation. Assume
further that 
 for all 
 in the case that 
 is an immediate
extension of 
 in T. Then T is model complete.
Proof. Let 
. By Theorem 5.2(ii) it suffices to show that 
. Let H be the set of all 
 such that 
. Trivially 
.

Since T is inductive, a chain K ⊆ H satisfies ⋃K ⊨ T. One easily verifies 
 as well, so that 
. By Zorn’s lemma there is a maximal
element 
. Claim: 
. Assume 
. Then there is an immediate
extension 
 of 
 such that 
. Since 
, and by
hypothesis 
, we get 
. This, however, contradicts the
maximality of 
 in H. Therefore, it must be the case that 
.
Consequently, 
.
Theorem 5.4.
 is model complete and thus so too 
 , the theory of a.c. fields of given characteristic p (
= 0 or a prime). Moreover, 
 is complete.
Proof. Let 
, 
, and 
. By
Lemma 5.3 it suffices to show that 
. Here 
is an immediate extension of 
 in 
. Let 
, β
quantifier-free, and 
. We shall prove 
 and for this we consider
With b for x one sees that 
 (for b is
trancendental over 
). Let 
,
with c for x. Since 
, w.l.o.g. 
. By Example 1 
, and so 
. 
 implies 
, for α is an ∃-sentence. Since 
 has been chosen arbitrarily we obtain 
, and from this by the finiteness theorem evidently

for some k and monic polynomials 
. Particularization and
the deduction theorem show that 
. Every a.c. field is infinite (Example 5(c) in 5.2), and a polynomial has only
finitely many zeros in a field. Thus, 
. Hence, 
 and so 
. This proves 
 and in view
of Lemma 5.3 the first part of the theorem. The algebraic closure of the prime
field of characteristic p is obviously a prime model for 
. Therefore, by Theorem 5.1, 
 is complete.
The following significant theorem is won similarly. It was originally proved
in [Ta3] by means of quantifier elimination. Incidentally, the claim of
completeness is not obtainable by means of Vaught’s criterion, in contrast to the
case of 
.
Theorem 5.5.
The theory RCF of real closed fields is model complete and complete. It is thus
identical to the theory of the ordered field of real numbers, and as a complete
axiomatizable theory it is also decidable.
Proof. Let 
. It once again suffices to
show that 
 for an immediate
extension 
 of 
 in RCF. Let 
, 
, with 
. Then U ∪V = A. Now
let 
, β
quantifier-free, 
. The model 
 with b for x then clearly satisfies the set of formulas
Suppose 
, interpreting x as c. We may assume 
 because 
. Since c∉U ∪V = A, c is transcendental over 
 (see Example 2). Hence, the quotient field 

 of 
 is isomorphic to the field of
rational functions over 
 with the unknown x. The order of 
 is fixed by the partition A = U ∪V coming from 
. Thus, 
. The
isomorphism 
 extends to one
between the real closures 
 and 
. As in Theorem 5.4 we thus obtain 
, and so for some a 1, …, a k , b 1, …, b l ∈ A (where k, l ≥
0 but k + l > 0),
Now, an ordered field is densely ordered without edge elements, hence is
infinite. Therefore, 
which results in 
. Thus, 
, and 
 is
proved. To verify completeness observe that RCF has a prime model, namely the
real closure of 
, the ordered field of all real algebraic
numbers. Applying Theorem 5.1 once again confirms the completeness of RCF.
A theory T is called the model completion of a theory T0 of the same
language if T0 ⊆ T and 
 is complete for every 
. Clearly, T is then model complete; moreover, T is
model compatible with T0 (
 implies 
, since 
 is consistent). The existence of a model
complete extension is necessary for the existence of a model completion of T0,
but not sufficient. See Exercise 1.
Remark 2. A somewhat surprising fact is that a model completion of T is
uniquely determined provided it exists. Indeed, let T, T′ be model completions of
T 0. Both theories are model compatible with T 0, and hence with each other. T, T
′ are model complete and therefore inductive, so that T + T′ is model compatible
with T (Exercise 3 in 5.4). Thus, if 
 then there exist
some 
 with 
, and since T is model complete we
obtain 
. This implies 

, and consequently 
. For reasons of symmetry, 
 as well.
Therefore T = T′.
Example 3. ACF is the model completion of the theory T J of integral
domains, hence also of the theory T F of fields. Indeed, let 
. By Theorem 5.4, ACF is model complete,
hence also 
 (in 
). Moreover, T is complete, since by Example 1, T has a prime model, the closure
 of 
 in ACF. Using Theorem 5.5,
one shows analogously that RCF is the model completion of the theory of
ordered commutative rings with unit element. Each such ring is embeddable into
an ordered field (an algebraic standard construction) and hence into a real closed
field.
 is called existentially closed in T, or ∃-closed in
T for short, if 
 for each 
 with 
.
For instance, every a.c. field 
 is ∃-closed in the theory of
fields. For let 
 be any field and 
 be any a.c. extension of 
. Then 
 thanks to the model completeness
of ACF. Hence 
 by Lemma 4.8(ii).
The following lemma generalizes in some sense the fact that every field is
embeddable into an a.c. field. Similarly, a group, for instance, is embeddable into
a group that is ∃-closed in the theory of groups.
Lemma 5.6.
Let T be an ∀∃-theory of some countable language 
 . Then
every infinite model 
 of T can be extended to a model 
 of T such that 
, which is ∃-closed in T.
Proof. For the proof we assume, for simplicity, that 
 is
countable. Then 
 is also countable. Let 
 be an enumeration of the ∃-sentences

of 
 and 
. 5
Let 
 be an extension of 
in 
 such that 
, as long as such an
extension exists; otherwise simply put 
. Since T is inductive, 
. If α = α n is an ∃-sentence in 
 valid in some extension 
 of 
, then already 
 and thus also 
. Now we repeat this construction with 
 in place of 
 with respect to
an enumeration of all ∃-sentences in 
 and obtain an 
-structure 
.
Subsequent reiterations produce a sequence 
 of 
-structures 
.
Let 
 be the 
-reduct of 
and 
.
Assume 
, 
. Then 
 for suitable m. Hence
, and so 
.
With this lemma one readily obtains the following highly applicable criterion
for proving the model completeness of countable κ-categorial theories, which, by
Vaught’s criterion, are always complete.
Theorem 5.7 (Lindström’s criterion).
A countable κ-categorical ∀∃-theory T without finite models is model complete.

Proof. Since all T-models are infinite, T has a model of cardinality κ, and by
Lemma 5.6 also one that is ∃-closed in T. But then all T-models of cardinality κ
are ∃-closed in T, because all these are isomorphic. Thus 
, for all 
 of cardinality κ. Therefore, T
is model complete according to Remark 1 on page 196.
Examples of applications.
(a) The ℵ0-categorical theory of atomless Boolean algebras.
(b) The ℵ1-categorical theory of nontrivial 
-vector
spaces.
(c) The ℵ1-categorical theory of a.c. fields of given characteristic.
A few comments: A Boolean algebra 
 is called atomless if
for each a≠0 in B there is some b≠0 in 
 with b < a ( < is the
partial lattice order of 
). The proof of (a) is similar to that for
densely ordered sets. Also (b) is easily verified. Observe that a 
-vector space of cardinality ℵ1 has a base of cardinality ℵ1.
From (c) the model completeness of ACF follows in a new way: If 
 and 
 then both fields have the same
characteristic p ≥ 0. Since ACF p is model complete by (c), 
 follows. This obviously implies
that ACF itself is model complete.
Exercises
1. Prove that of the four theories DO ij only DO00 is model complete.
Moreover, show that DO is model completion of both DO and the theory
T ord of all orders, but not of the theory of all irreflexive relations which
is not model-compatible with T ord .
 
2. Let T be the theory of divisible torsion-free abelian groups. Show that T
is the model completion of the theory T 0 of torsion-free abelian groups.  
3. T ∗ is called the model companion of T provided T, T ∗ are model
compatible and T ∗ is model complete. Show that if T ∗ exists then T ∗ is
uniquely determined, provided it exists. Moreover, show that each 
 is ∃-closed in T.
 

4. Prove that an ∀ ∃-sentence valid in all finite fields is valid also in all a.c.
fields. This fact is highly useful in algebraic geometry.
 
5.6 Quantifier Elimination
Because ∃x(y < x ∧ x < z) ≡ DO y < z, in the theory of densely ordered sets the
quantifier in the left-hand formula can be eliminated. In fact, in some theories,
including the theory DO00 (see 5.2), the quantifiers can be eliminated from every
formula. One says that 
 allows quantifier
elimination if for every 
 there exists some open
formula 
 such that φ ≡ T φ′. Quantifier elimination
is the oldest method of showing that certain theories are decidable and
occasionally also complete. Some presentations demand additionally { free} φ′ =
{ free} φ, but one is not obliged to to do so. A theory T that allows quantifier
elimination is model complete by Theorem 5.2(iv), because open formulas are ∀-
formulas. Hence, T is an ∀ ∃-theory, which is a remarkable necessary condition
for quantifier eliminability.
In order to confirm quantifier elimination for a theory T it suffices to
eliminate the prefix ∃x from every formula of the form ∃xα, where α is open.
Indeed, think of all subformulas of the form ∀xα in a formula φ as being
equivalently replaced by ¬ ∃x ¬α, so that only the ∃-quantifier appears in φ.
Looking at the farthest-right prefix ∃x in φ one can write φ = ⋯ ∃xα⋯ with
some quantifier-free α. Now, if ∃xα is replaceable by an open formula α′ then
this process can be iterated no matter how long it takes for all ∃-quantifiers in φ
to disappear.
Thanks to the ∨ -distributivity of ∃-quantifiers we may moreover assume
that the quantifier-free part α of ∃xα from which ∃x has to be eliminated is a
conjunction of literals, and that x explicitly occurs in each of these literals:
simply convert α into a DNF and distribute ∃x over the disjuncts such that ∃x
stands in front of a conjunction of literals only. If x does not appear in any of
these literals, ∃x can simply be discarded. Otherwise remove the literals not
containing x beyond the scope of ∃x, observing that ∃x(α ∧ β) ≡ ∃xα ∧ β if x∉{
var} β.
Furthermore, it can be supposed that none of the conjuncts is of the form x =
t with x∉{ var} t. Indeed, since ∃x(x = t ∧ α) ≡ α\frac{t} {x}, the quantifier has
then already been eliminated. We may also assume x≠v 0 (using bound
renaming) and that neither x = x nor x≠x is among the conjuncts. For x = x can

equivalently be replaced by 
, as can x≠x by 
. Here one
may define 
 and 
 as v 0 = v 0 and v 0≠v 0, respectively.
Replacement will then introduce v 0 as a possible new free variable, but that is
harmless. If the language contains a constant c one may replace v 0 by c in the
above consideration. If not, one may add a constant or even 
 as a
new prime formula to the language, similar to what is proposed below for DO00.
Call an ∃-formula simple if it is of the form ∃x ∧ i α i , where every α i is a
literal with x ∈ { var} α i . Then the above considerations result in the following
theorem.
Theorem 6.1.
T allows quantifier elimination if every simple ∃-formula ∃x∧ \nolimits i α i is
equivalent in T to some open formula. Here w.l.o.g., none of the literals α i is x =
x, x≠x, or of the form x = t with x∉{ var} t.
Example 1. T = DO00 allows quantifier elimination. Because of
and since in general 
, we may suppose that the conjunction of the α i in Theorem 6.1 does not contain
the negation symbol. We are therefore dealing with a formula of the form
which is equivalent to 
 if x is one of
the variables y i , z j , or to 
 whenever m = 0 or k = 0, and in the
remaining case to 
. Who wants to
avoid the use of v 0 as an extra variable may also regard at 
 as an
additional 0-ary relation symbol.
DO itself does not allow quantifier elimination, since in α(y) : = ∃x x < y the
quantifier is not eliminable. Indeed, if α(y) were equivalent in DO to an open
formula then 
, 
, a ∈ A, and 
 would imply 
. But this is not so for the densely ordered
sets 
 with 
 and 
.

Choose a = 1. Quantifier elimination does however become possible if the
signature 
 is expanded by considering L, R as 0-ary predicate
symbols. The fact that {L, R} forms a Boolean basis for sentences in DO is not
yet sufficient for quantifier eliminability if looking at L and R as formulas, for
these contain quantifiers.
Also the theory SO does not allow quantifier elimination in the original
language, simply because it is not an ∀ ∃-theory as was noticed earlier. The same
is true for the expansions SO ij of SO.
Example 2. A classical and nontrivial result of quantifier elimination by
Presburger [Pr] refers to 
, with the
additional unary predicate symbols m (
), defined by m x ↔
∃y my = x, where my denotes the m-fold sum 
 of y. We shall
prove a related result with respect to the group ℤ in 
. Denote the k-fold sum 
 by k, and set 
.
Let ZGE be the elementary theory in 
 whose axioms subsume those
for ordered abelian groups plus the axioms
, 
, and 
for 
 The reducts of ZGE-models to 
 are called ℤ-groups. These are
ordered with the smallest positive element 1. The 
 state for a 
-group G that the
factor groups G ⁄ mG are cyclic of order m. Here 
. Let ZG denote the reduct theory of ZGE in 
. Its models are precisely the 
-groups.
ZGE is a definitorial, hence conservative extension of ZG (cf. 2.6). It will turn
out that 
-groups are just the ordered abelian groups
elementarily equivalent to the paradigm of a 
-group, 
. Let us notice that 
 for each n, where η n is the
formula 0 ≤ x < n → ∨ k < n x = k.
We are now going to prove that ZGE allows quantifier elimination. Observe
first that since 
, 

, and m t ≡ ZGE m − t, we may assume that the kernel of a simple ∃-formula is a
conjunction of formulas of the form 
, and 
, where 
. By multiplying these
formulas by a suitable integer and using t < s ≡ ZGE nt < ns and m t ≡ ZGE nm nt
for n≠0, one sees that all the n i , n i ′, n i ′, n i ′ can be made equal to some
number n > 1. Clearly, in doing so, 
 and the
“modules” m i all change. But the problem of elimination is thus reduced to
formulas of the following form, where the jth conjunct disappears whenever k j =
0 (j ≤ 3):
(1) 
With y for nx and m 0 = n, (1) is certainly equivalent in ZGE to (2) 
.
According to Theorem 6.1 we can at once assume that k 0 = 0, so that the
elimination problem, after renaming y back to x, reduces to formulas of the
following form, where 
:
(3) 
.
Let m be the least common multiple of 
.
Case 1: k 1, k 2 = 0. Then (3) is equivalent in ZGE to 
. Indeed, if an x such that 
exists at all, then so does some 
. For let j with 
 be given by axiom 
. Then also m x
− j and consequently m i x − j for all i ≤ k 3. Therefore 
also holds for 
 as was claimed.Case 2: k 1≠0 and j as
above. Then (3) is equivalent to(4) 
.

This is a case distinction according to the maximum among the values of the 
. From each disjunct in (4) certainly (3)
follows in ZGE (consider 
). Now
suppose conversely that x is a solution of (3). Then in the case 
the μth disjunct of (4) is also valid. To prove this we need only confirm 
, which
comes down to 
. Were x < t μ 1 +
j, i.e., 
, then 
 follows for some k < j by η j , that
is, 
. Thus, 
 for all i ≤ k 3. But
this yields the contradiction m j − k < m.Case 3: k 1 = 0 and k 2≠0. The argument
is analogous to Case 2 but with a distinction according to the smallest term
among the 
.
From this remarkable example we obtain the following
Corollary 6.2.
ZGE is model complete. Moreover, ZGE and ZG are both complete and
decidable.
Proof. Since 
 determines a prime model for ZGE,
completeness follows from model completeness, which in turn follows from
quantifier eliminability. Clearly, along with ZGE also its reduct theory ZG is
complete. Hence, as complete axiomatizable theories, both these theories are
decidable.
Remark 1. Also ZG is model complete; Exercise 1. Actually, ZG is the
model completion of the theory of discretely ordered abelian groups because
every such group is embeddable into some 
-group (which is
not quite easy to prove). This is a main reason for the interest in ZG. Although
model complete, ZG does not allow quantifier elimination.
We now intend to show that theories ACF and RCF of algebraically and real
closed fields respectively allow quantifier elimination, even without any
expansion of their signatures. Attacking the elimination problem in a direct
manner as above would fill a separate chapter. We therefore undertake a model-
theoretic proof in Theorem 6.4, applying thereby a variant of Theorem 2.3. Call 

 a Boolean basis for 
 in T if
every 
 belongs to 
,
the set of Boolean combinations of formulas from X. Let 
 be 
-models. Write 
 whenever 
,
for all φ ∈ X, and 
 whenever 
,
for all 
.
Theorem 6.3 (Basis theorem for formulas).
Let 
 be a theory, 
, and suppose that 
, for all 
 . Then X is a Boolean basis
for 
 in T.
Proof. Let 
 and 
, where 
 is defined as on page 180. One then shows
that 
 as in the proof of Theorem 2.3 by
arguing with a model 
 rather than a structure 
. The remainder of the proof proceeds along the lines of the
proof of the mentioned theorem and is therefore left to the reader.
A theory T is called substructure complete if for all 
 with 
 the theory 
 is complete. This generalizes model
completeness and is basically only a reformulation of ‘T is the model completion
of T ∀’. Indeed, let T be substructure complete and 
. Then by Lemma 4.1, 
 for some 
, hence 
 is complete. Thus, T is the model
completion of T ∀. Conversely, let T be the model completion of T ∀ and 
. Then 
, so that 

 is complete. Thus, T is substructure
complete. The criterion (ii) in the next theorem may therefore also be
reformulated. There exist yet other criteria, in particular the amalgamability of
models of T ∀; see e.g. [CK].
Theorem 6.4.
For every theory T in 
 the following are equivalent:
(i) T allows quantifier elimination, (ii) T is substructure complete.
Proof. (i) ⇒ (ii): Let 
 be a substructure of a T-model, 
, and 
 such that 
. Further, let 
 so that w.l.o.g. 
. Then also 
, because in view of (i) we may
suppose that φ is open. Since 
 was arbitrary, 
. Hence 
 is complete. (ii) ⇒ (i): Let X denote the set all of
literals of 
. It suffices to prove
for then X is a Boolean basis for 
 in T according to Theorem
6.3. This obviously amounts to saying that T allows quantifier elimination. Let 
, 
, 
, 
,
and 
. Let 
 be the substructure generated in 
 from E : = { a 1, …, a n }. By (ii), 
 is complete and consistent with 
, since 
 satisfies 
. Hence 
.
Moreover, 

by Exercise 5 in 5.1. Thus, by the finiteness theorem, there are literals 
with 
and 
.
Therefore 
,
because 
 do not appear in T. Since 
 and 
, also 
 and so
. The above holds for arbitrary
formulas 
 provided \vec{x}≠∅. These
include sentences as well which completes the proof of ( ∗ ).
Corollary 6.5.
An ∀-theory T permits quantifier elimination if and only if T is model complete.
Proof. Due to 
, (ii) in Theorem 6.4 is satisfied provided only that 
 is
complete for all 
. But this is granted if T is model
complete.
Example 3. Let T be the ∀-theory with two unary function symbols f, g
whose axioms state that f and g are injective, f and g are mutually inverse ( ∀x
fgx = x and ∀x gfx = x), and there are no circles, i.e., no sequences x 0, …, x n (n >
0) such that 
 and x 0 = x n . This implies in
particular ∀x x≠fx. Note that ∀y ∃xfx = y is provable (choose x = gy). Hence, f
and g are bijective. The T-models consist of disjoint countable infinite “threads,”
which occurred also in Example 3 in 5.2. Hence, T is ℵ1-categorical and thus
model complete by Lindstrm’s criterion. By the corollary, T permits the
elimination of quantifiers.
Theorem 6.6.
ACF and RCF allow quantifier elimination.

Proof. By Theorem 6.4 it is enough to show that ACF and RCF are substructure
complete, or put another way, ACF and RCF are the model completions of ACF∀
and RCF∀, respectively. Both claims are clear from Example 3 in 5.5 according
to which ACF∀ coincides with the theory of integral domains, and RCF∀ with the
theory of ordered commutative rings with a unit element.
Theorem 6.6 was originally proved by Tarski in [Ta3]. While thanks to a host
of model-theoretic methods the above proof is significantly shorter than Tarski’s
original, the latter is still of import in many algorithmic questions. Decidability
and eliminability of quantifiers in RCF have great impact also on other fields of
research, in particular on the foundations of geometry, which are not treated in
this book. Let us mention that both Euclidean and non-Euclidean geometry can
entirely be based on RCF and hence are decidable as well.
Remark 2. Due to the completeness of RCF, one may also say that the first-
order theory of the ordered field 
 allows quantifier
elimination. Incidentally, the quantifiers in RCF are not eliminable if the order,
which is definable in RCF, is not considered as a basic relation. Also 
, a (complete)
theory with the exponential function exp in the language, does not allow
quantifier elimination. Nonetheless, T is model complete, as was shown in [Wi].
Because of its completeness, the decision problem for T reduces to the still
unsolved axiomatization problem, whose solution hinges on the unanswered
problem concerning transcendental numbers, Schanuel’s conjecture, which lies
outside the scope of logic (consult the Internet). A particular question related to
the conjecture is whether e e is transcendental.
Exercises
1. Show that the theory ZG is model complete in its language, and even in
the language 
.
 
2. A structure elementarily equivalent to 
 is
called an 
 -semigroup. Axiomatize the theory of 
-semigroups and show (by tracing back to ZG) that it
allows quantifier elimination in 
.
 
3. Let RCF∘ be the theory of real closed fields without order as a basic
notion. Prove that ∃y is not eliminable in 
 

 in the frame of RCF∘.
4. Show that RCF is axiomatized alternatively by the axioms for ordered
fields and the continuity scheme in 3.3 page 110.
 
5. Show that the theory T of divisible ordered abelian groups allows
quantifier elimination.
 
5.7 Reduced Products and Ultraproducts
In order to merely indicate the usefulness of the following constructions consider
for instance 
, a direct power of the
additive group ℤ. By componentwise verification of the axioms it can be shown
that 
 is itself an abelian group (n ≥ 2).
But in this and similar examples we can save ourselves the bother, because by
Theorem 7.5 below a Horn sentence valid in all 
 is
also valid in the direct product 
, and
the group axioms are Horn sentences in each reasonable signature for groups.
Let 
 be a family of 
-structures and F a proper filter on a nonempty index set I
(see page 34). We define a relation ≈ F on the domain B of the product 
 by
This is an equivalence relation on B. Indeed, let 
. ≈ F is reflexive (since 
) and trivially symmetric, but also transitive, because 
, thanks to the
obvious fact 
.
Furthermore, ≈ F is a congruence in the algebraic reduct of 
. To see this let f be an n-ary function symbol and 
 (which for 
, 
 in B n
abbreviates 
). Then clearly 

belongs to F. Since certainly 
, we get 
 and hence 
.
Let 
, where 
 denotes the congruence
class of ≈ F to which a ∈ B belongs. Thus, 
. This C becomes the domain
of some 
-structure 
 in which first the
operations 
 are defined in a canonical
way. With 
we set 
. This definition is sound because ≈ F is a congruence. For constant symbols c let
 of course be 
.
Similar to the identity, the relation symbols are interpreted in 
 in a completely natural way as follows:
Also this definition is sound, since 
 and 
 imply I r\vec{b} ∈ F. Indeed, 
 is equivalent to 
 and it is readily verified that 
.
The 
-structure 
 so defined is called a
reduced product of the 
 by the filter F and is denoted
by 
 (some
authors denote it by 
). Imagining a
filter F as a system of subsets of I each of which contains “almost all indices,”
one may think of 
 as arising from 
 by identification of
those a, b ∈ B for which the ith projections are the same for almost all indices i.
Let 

.
For 
 the
valuation 
 to A i is denoted by w
i , so that 
.
Induction on t yields 
.
Define the valuation 
 by 
. This setting
generalizes inductively to
(1) 
, for all terms
t and valuations 
.
(1) follows from 
. It is easily seen that each 
 is of the
form w ⁄ F for a suitable valuation 
.
Let w { Var} → B and 
. Define 
. Then
(2) 
for some a ∈ B and 
.
Indeed, let 
, i.e., 
. Choose some a i ∈
A i with 
. For 
 pick up anya i ∈ A i
. Then clearly (2) holds with 
 and 
.
The case that F is an ultrafilter on I is of particular interest. By Theorem 7.1,
all elementary properties valid in almost all factors carry over to the reduced
product, which in this case is called an ultraproduct. If 
 for all i ∈ I then 
 is termed an
ultrapower of 
, usually denoted by 
.

(.)
(.)
The importance of ultrapowers is underlined by Shelah’s theorem (not
proved here) that 
 iff 
and 
 have isomorphic ultrapowers. The proof of Theorem 7.1
uses mainly filter properties; the specific ultrafilter property is applied only for
confirming 
.
Theorem 7.1 (Łoś’s ultraproduct theorem).
Let 
be an ultraproduct of the 
 -structures 
 . Then for all formulas 
and all 
 ,
Proof by induction on α. ( ∗ ) is obtained for equations t 1 = t 2 as follows:
One similarly proves ( ∗ ) for prime formulas 
. Induction steps:
Further, 
. Now let 
, a ∈ ∏ i ∈ I A i ,
and 
. Since 
, also 
. Hence, 
 by the induction hypothesis. a was
arbitrary, therefore 
. The
converse is, with β : = ¬α, equivalent to 
. This follows from (2), since ( ∗ ) holds by the induction hypothesis for α, hence
also for ¬α.
Corollary 7.2.
A sentence α is valid in the ultraproduct 
 iff α is valid in

“almost all” 
 , that is, 
 . In particular, 
, for all 
 . In words, an ultrapower of 
 is elementarily equivalent to 
 .
The last claim is clear, since the validity of α in a structure does not depend on
the valuation chosen. The ultrapower case can be further strengthened to 
 (Exercise
2), useful for the construction of special nonstandard models, for instance. From
countless applications of ultraproducts, we present here a very short proof of the
compactness theorem for arbitrary first-order languages. The proof is tricky, but
it is undoubtedly the most elegant proof of the compactness theorem.
Theorem 7.3.
Let 
 and let I be the set of all finite subsets of X.
Assume that every i ∈ I has a model 
 . Then
there exists an ultrafilter F on I such that 
, where 
 for
x ∈{ Var}. In short, if every finite subset of 
 has a
model then the same applies to the whole of X.
Proof. Let 
 for α ∈ X. The
intersection of finitely many members of 
 is ≠∅; for instance 
 belongs to 
. By the
ultrafilter theorem (page 35), there exists an ultrafilter F ⊇ E. If α ∈ X and 
 (that is, α ∈ i) then 
. Consequently, 
; hence 
. Therefore, 
by Theorem 7.1, as claimed.
A noteworthy consequence of these results is the following theorem in which

 denotes the class of all 
-
structures; by Shelah’s theorem mentioned above, condition (a) can be converted
into a purely algebraic one.
Theorem 7.4.
Let 
. Then
(a) K is 
 -elementary iff K is closed under elementary
equivalence and under ultraproducts,
 
(b) K is elementary iff K is closed under elementary equivalence and both
K and 
 are
closed under ultraproducts.
 
Proof. (a): A 
-elementary class is trivially closed under elementary
equivalence. The rest of the direction   ⇒   holds by Theorem 7.1. 
: Let 
 and 
, and let I be the set of all finite subsets of 
. For each 
 there exists some 
 such that 
,
for otherwise 
 which
contradicts 
. According to
Theorem 7.3 (with 
) there is a 
, and if 
 then so too 
.
Because of 
 we know
that 
, and therefore 
. This shows that 
. Hence 
, i.e., K is 
-elementary. (b): ⇒ is obvious by (a), because for K = Mdα we
have 
. 
: By
(a), K = MdS for some 
. Let I be the set of all
finite nonempty subsets of S. Claim: There is some i = { α0, …, αn} ∈ I with

Mdi ⊆ K. Otherwise let 
 such that 
 for all i ∈ I. Then there exists an
ultraproduct 
 of the 
 such that 
 and 
 for all i ∈ I;
hence 
. This is a contradiction to MdS ⊆ K. Thus,
the claim holds. Since also K = MdS ⊆ Mdi, we obtain 
.
Application. Let K be the (
-elementary) class of all fields of
characteristic 0. We show that K is not elementary, and thus in a new way that {
Th} K is not finitely axiomatizable. Let 
 denote the
prime field of characteristic p i (
) and let
F be a nontrivial ultrafilter on 
. We claim that the field 
 has
characteristic 0. Indeed, 
 is for a given
prime p certainly cofinite and belongs to F, so that 
for all p. Hence ∖ K is not closed under ultraproducts, and so by Theorem
7.4(b), K cannot be elementary.
We now turn to reduced products. Everything said below on reduced
products remains valid for direct products; these are the special case with the
minimal filter F = { I}. More precisely, 
. Filters are always supposed to be proper in the sequel.
Theorem 7.5.
Let 
be a reduced product, 
 , and α a Horn
formula from the corresponding first-order language. Then
In particular, a Horn sentence valid in almost all 
is also valid in 
 .

Proof by induction on the construction of Horn formulas. For prime formulas the
converse of ( ⋆ ) is also valid, because in the proof of ( ∗ ) from Theorem 7.1 for
prime formulas no specific ultrafilter property was used. Moreover, 
, provided α is prime. Hence, ( ⋆ ) is correct for all literals. Now suppose ( ⋆ ) for
a prime formula α and a basic Horn formula β, and let 
. We show that 
. Let 
. Then 
 since α is prime. 
leads to 
; hence 
 by the induction hypothesis. This shows
that 
 and proves ( ⋆ )
for all basic Horn formulas. Induction on ∧ and ∀proceeds as in Theorem 7.1
and the ∃-step easily follows with the help of (2) above.
According to this theorem the model classes of Horn theories are always
closed under reduced products, in particular under direct products. This result
strengthens Exercise 1 in 4.2 significantly. We mention finally that also the
converse holds: every theory with a model class closed with respect to reduced
products is a Horn theory. But the proof of this claim, presented in [CK], is
essentially more difficult than that for the similar-sounding Theorem 4.4.
Exercises
1. Show that 
is isomorphic to 
 for some i 0 ∈ I if F is
a trivial ultrafilter. This applies, for instance, to ultraproducts on a finite
index set (Exercise 3 in 1.5). Thus, ultraproducts are interesting only if
the index set I is infinite.
2. Prove that 
 is elementarily embeddable into an
ultrapower 
.
3. (Basic in nonclassical logics). Let 
be the consequence relation defined by a class K of L-matrices (page 49).
Show that ⊨ K is finitary provided K is closed under ultraproducts (which
is the case, for instance, if 
 with finite 

). Thus, 
 is finitary
for each finite logical matrix.
4. Let 
 be Boolean algebras. Prove that 
for all universal Horn sentences α. This holds in particular for identities
and quasi-identities. Every sentence of this kind valid in 2 is therefore
valid in all Boolean algebras.
5. Let 
 for each i ∈ I (≠∅) be an expansion of the 
-structure 
 to 
. Prove that the reduced product 
 is an 
-expansion of the reduced product 
 (the
“expansion theorem”).
References
CK.
C. C. CHANG, H. J. KEISLER, Model Theory, Amsterdam 1973, 3⟨{ rd}⟩ ed. North-Holland 1990.
Ho.
W. HODGES, Model Theory, Cambridge Univ. Press 1993.
Bue. S. BUECHLER, Essential Stability Theory, Springer 1996.
Mar. D. MARKER, Model Theory, An Introduction, Springer 2002.
Pz.
B. POIZAT, A Course in Model Theory, Springer 2000.
Rot.
P. ROTHMALER, Introduction to Model Theory, Gordon & Breach 2000.
Sa.
G. SACKS, Saturated Model Theory, W. A. Benjamin 1972.
She. S. SHELAH, Classification Theory and the Number of Nonisomorphic Models, Amsterdam 1978, 2⟨{
nd}⟩ ed. North-Holland 1990.
De.
O. DEISER, Axiomatische Mengenlehre, Springer, to appear 2010.
Ku.
K. KUNEN, Set Theory, An Introduction to Independence Proofs, North-Holland 1980.
HR.
H. HERRE, W. RAUTENBERG, Das Basistheorem und einige Anwendungen in der Modelltheorie,
Wiss. Z. Humboldt-Univ., Math. Nat. R. 19 (1970), 579–583.

1
2
3
4
5
MV.
R. MCKENZIE, M. VALERIOTE, The Structure of Decidable Locally Finite Varieties, Progress in
Mathematics 79, Birkhuser 1989.
Wae. B. L. VAN DER WAERDEN, Algebra I, Berlin 1930, 4⟨{ th}⟩ ed. Springer 1955.
Zi.
M. ZIEGLER, Model theory of modules, Ann. Pure Appl. Logic 26 (1984), 149–213.
Pr.
M. PRESBURGER, Über die Vollständigkeit eines gewissen Systems der Arithmetik ganzer Zahlen, in
welchem die Addition als einzige Operation hervortritt, Congrès des Mathématiciens des Pays Slaves
1 (1930), 92–101.
Wi.
A. WILKIE, Model completeness results for expansions of the ordered field of real numbers by
restricted Pfaffian functions and the exponential function, Journal Amer. Math. Soc. 9 (1996), 1051–
1094.
Dav. M. DAVIS (editor), The Undecidable, Raven Press 1965.
Ro1. A. ROBINSON, Introduction to Model Theory and to the Metamathematics of Algebra, Amsterdam
1963, 2⟨{ nd}⟩ ed. North-Holland 1974.
Wae. B. L. VAN DER WAERDEN, Algebra I, Berlin 1930, 4⟨{ th}⟩ ed. Springer 1955.
Ta3. _________ , A Decision Method for Elementary Algebra and Geometry, Santa Monica 1948,
Berkeley 1951, Paris 1967.
Footnotes
All these conditions are also equivalent (they all hold) if the inconsistent theory is taken to be complete,
which is not the case here, as we agreed upon in 3.3.
This assumption is equivalent to the assertion that 
 is complete; see the
subsequent proof. For refinements of the theorem we refer to [HR].
The “distance” d(a, b) between elements a, b of some SO-model is 0 for a = b, 1 + the number of
elements between a and b if it is finite, and d(a, b) = ∞ otherwise.
Moreover, the theory of all linear orders is decidable (Ehrenfeucht), and thus each of its finite extensions;
but the proof is incomparably more difficult than for DO or SO.
For uncountable 
 we have 
.

In this case one proceeds with an ordinal enumeration of 
 rather than an ordinary
one. But the proof is almost the same.

(1)
Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-
1-4419-1221-3_6, © Springer Science+Business Media, LLC 2010
6. Incompleteness and Undecidability
Wolfgang Rautenberg
1  
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
Wolfgang Rautenberg
Email: raut@math.fu-berlin.de
Abstract
Gödel’s fundamental results concerning the incompleteness of formal systems
sufficiently rich in content, along with Tarski’s on the nondefinability of the
notion of truth and Church’s on the undecidability of logic, as well as other
undecidability results, are all based on essentially the same arguments. A widely
known popularization of Gödel’s first incompleteness theorem runs as follows:
Consider a formalized axiomatic theory T that describes a given domain of
objects 
 in a manner that we hope is complete. Moreover, suppose that T is
capable of talking in its language  about its own syntax and proofs from its
axioms. This is often possible if T has actually been devised to investigate other
things (numbers or sets, say), namely by means of an internal encoding of the
syntax of 
. Then the sentence γ: “I am unprovable in T” belongs to 
, where
“I” refers precisely to the sentence γ (clearly, this possibility of self-reference has
to be laid down in detail, which was the main work in [Gö2]). Then γ is true in 
 but unprovable in T.
Gödel’s fundamental results concerning the incompleteness of formal systems
sufficiently rich in content, along with Tarski’s on the nondefinability of the
notion of truth and Church’s on the undecidability of logic, as well as other
undecidability results, are all based on essentially the same arguments. A widely
known popularization of Gödel’s first incompleteness theorem runs as follows:
Consider a formalized axiomatic theory T that describes a given domain of

objects 
 in a manner that we hope is complete. Moreover, suppose that T is
capable of talking in its language  about its own syntax and proofs from its
axioms. This is often possible if T has actually been devised to investigate other
things (numbers or sets, say), namely by means of an internal encoding of the
syntax of . Then the sentence γ: “I am unprovable in T” belongs to , where
“I” refers precisely to the sentence γ (clearly, this possibility of self-reference has
to be laid down in detail, which was the main work in [Gö2]). Then γ is true in 
 but unprovable in T.
Indeed, if we assume that γ is provable, then, like any other provable
sentence in T, γ would be true in 
 and so unprovable, since this is just what γ
claims. Thus, our assumption leads to a contradiction. Hence, γ is indeed
unprovable, that is, γ’s assertion conforms with truth; moreover, γ belongs to the
sentences from  true in 
, as it will turn out.
Put together, our goal of exhaustively capturing all theorems valid in 
 by
means of the axioms of T has not been achieved and is in fact not achievable, as
we will see. No matter how strong our axiomatic theory T is, there are always
sentences true in 
 but unprovable in T.
Clearly, the above is just a rough simplification of Gödel’s theorem that does
not speak at all about a domain of objects, but is rather a proof-theoretic
assertion the proof of which can be carried out in the framework of Hilbert’s
finitistic metamathematics. This in turn means about the same as being
formalizable and provable in Peano arithmetic 
, introduced in 3.3.
This result was a decisive point for a wellfounded criticism of Hilbert’s
program, which aimed to justify infinitistic methods by means of a finitistic
understanding of metamathematics. For a detailed description of what Hilbert
was aiming at, see [Kl2] or consult [HB, Vol. 1]. The paradigm of a domain of
objects in the above sense is, for a variety of reasons, the structure 
. Gödel’s theorem states that even for 
 a complete axiomatic
characterization in its language is impossible, a result with far-reaching
consequences. In particular, 
, which aims at telling us as much as possible
about 
, is shown to be incomplete. 
 is the center point of Chapter 7. It is of
special importance because most of classical number theory and of discrete
mathematics can be developed in it. In addition, known methods for
investigating mathematical foundations can be formalized and proved in 
.
These methods have stood firm against all kinds of criticism, leaving aside some
objections concerning the unrestricted use of two-valued logic, not to be
discussed here.
Some steps in Gödel’s proof require only modest suppositions regarding T,

namely the numeralwise representability of relevant syntactic predicates and
functions in T in the sense of 6.3. It was one of Gödel’s decisive discoveries that
all the predicates required in γ’s construction above are primitive recursive1 and
that all predicates and functions of this type are indeed representable in T. As
remarked by Tarski and Mostowski, the latter works even in certain finitely
axiomatizable, highly incomplete theories T and, in addition, covers all recursive
functions. This yields not only the recursive undecidability of T and all its
subtheories in  (in particular the theory 
), but also of all consistent
extensions of T.
From this it follows that the first incompleteness theorem as well as Church’s
and Tarski’s results can all be obtained in one go, making essential use of the
fixed point lemma in 6.5, also called the diagonalization lemma because it is
shown by some kind of diagonalization on the primitive recursive substitution
function. Its basic idea can even be recognized in the ancient liar paradox, and is
also used in the foregoing popularization of the first incompleteness theorem.
In 6.1 we develop the theory of recursive and primitive recursive functions to
the required extent. 6.2 deals with the arithmetization of syntax and of formal
proofs. 6.3 and 6.4 treat the representability of recursive functions in axiomatic
theories. In 6.5 all the aforementioned results are proved, while the deeper-lying
second incompleteness theorem is dealt with in Chapter 7. Section 6.6 concerns
the transferability of decidability and undecidability by interpretation, and 6.7
describes the first-order arithmetical hierarchy, which vividly illustrates the close
relationship between logic and recursion theory. At the end we consider special
Σ 1-formulas, important for Chapter 7.
6.1 Recursive and Primitive Recursive Functions
From now on, along with 
 we take 
 to denote natural numbers,
unless stated otherwise. Let F n denote the set of all n-ary functions with
arguments and values in 
, and put 
. For f ∈ F m and g1, …, gm ∈ F
n , we call 
 the (canonical) composition of f and the g i and
write h = f[g1, …, gm]. The arity of h is n. Analogously, let P[g1, …, gm] for a
given predicate 
 (m > 0) denote the n-ary predicate 
.
In an intuitive sense f ∈ F n is computable if there is an algorithm for

computing 
 for every  in finitely many steps. Sum and product are simple
examples. There are uncountably many unary functions on 
, and because of the
finiteness of every set of computation instructions, only countably many of these
can be computable. Thus, there must be noncomputable functions. This
existence proof brings to mind the one for transcendental real numbers, based on
the countability of the set of algebraic numbers. Coming up with concrete
examples is, in both cases, much less simple.
The computable functions in the intuitive sense obviously have the following
properties:
If h ∈ F m and 
 are computable, so too is the composition 
.
If g ∈ F n and h ∈ F n + 2 are computable then so is f ∈ F n + 1, uniquely
determined by the equations
These are called the recursion equations for f, and f is said to result from g,h
by primitive recursion, or f = Op(g, h) for short.
Let g ∈ F n + 1 be such that 
. If g is computable then so is f,
given by 
. Here the right-hand term denotes the smallest b
with 
. f is said to result from g by the so-called μ-operation.
Considering Oc, Op, and Oμ as generating operations for obtaining new
functions from already-constructed ones, we state the following definition due to
Kleene: Definition. The set of p.r. (primitive recursive) functions consists of all
functions on  that can be obtained by finitely many applications of Oc and Op
starting with the following initial functions: the constant 0, the successor
function S, and the projection functions 
 (1 ≤ ν ≤ n, 
). With
the additional generating schema Oμ one obtains the set of all recursive or μ-
recursive functions. A predicate 
 is called p.r. or recursive (also
decidable) provided the characteristic function 
 of P has the respective
property, defined by
Remark 1. It should at least be noticed that it was Dedekind who first

proved that Op defines exactly one function f ∈ F n in the sense of set theory.
Note also that for n = 0 the recursion equations reduce to f0 = c and fSb = h(b,
fb), where c ∈ F 0 and h ∈ F 2. If the condition 
 in Oμ is omitted,
then f is regarded as undefined for those  for which there is no b with 
. In this way the so-called partially recursive functions are defined. These are
very important for recursion theory. However, we will not require them.
The following examples make it clear that by means of the functions 
 our
stipulations concerning arity in Oc and Op can be extensively relaxed. In the
examples, however, we will still adjoin the normalized notation each time in
parentheses.
Examples. Let 
 and 
, so that clearly 
. By
Oc these functions are all p.r. The n-ary constant functions 
 can be
seen to be p.r. as follows: 
 for arbitrary 
, while 
 and 
. Thus, 
. For n
> 1 we have 
. Further, the recursion equations
show addition to be a p.r. function. Since
it follows that ⋅ is p.r. and entirely analogously so is 
. Also the
predecessor function Pd is p.r. since 
 and 
.
“Cut-off subtraction” 
, defined by 
 for a ≥ b, and 
otherwise, is p.r. because
The absolute difference | a − b | is p.r., for 
.
One sees easily that if f is p.r. (resp. recursive) then so too is every function
that results from f by swapping, equating, or adjoining fictional arguments. For
example, let f ∈ F 2 and 
. Then clearly f 1(a, b) = f(b, a). For 

 we have f 2 a = f(a, a), and for 
 we get f 3(a, b, c) =
f(a, b), for all a, b, c.
From now on we will be more relaxed in writing down applications of Oc or
Op; the 
 will no longer explicitly appear. If f ∈ F n + 1 is p.r. then so is the
function 
, since 
, and 
. Also 
, defined by 
 and 
 is p.r.
The δ-function is defined by δ0 = 1, δSn = 0 and hence is p.r. With δ we
easily obtain the characteristic function of the identity relation: 
 equals
δ | a − b |. This in turn implies that every finite subset E of  is p.r. because 
 and for 
 we have
The inequality relation ≠ is p.r. because 
 with the signum
function σ, defined by σ0 = 0, σSn = 1. Also ≤ is p.r. because 
, as is easily verified.
Almost all functions considered in number theory are p.r., in particular the
prime enumerationn↦p n (with 
). The same is true for standard
predicates like ∣ (divides) and { prim} (to be a prime number). This will all be
verified after some additional remarks.
Important is the closure of the set of p.r. functions with respect to definition
by p.r. (resp. recursive) case distinction: If P, g, h are p.r. (resp. recursive) then
so is f, given by 
, or
A simple example is (a, b)↦max(a, b), defined by max(a, b) = b if a ≤ b and
max(a, b) = a otherwise. Case distinction easily generalizes to more than two
cases. Sometimes equations in Op are given by p.r. case distinction as in a
defining equation for rem(a, b), the remainder of dividing a byb (≠0): rem(Sa, b)
= 0 if b∣Sa, and rem(Sa, b) = Srem(a, b) otherwise. Case distinction yields also
that a↦b i a is p.r., where b i a denotes the ith digit of the binary representation of
a ∈ ℕ, 
. Indeed, b i a = 0 if 
 and b i a = 1

otherwise.
Of fundamental importance is the hypothesis that recursive functions exhaust
all the computable functions over 
. This hypothesis is called Church’s thesis;
all undecidability results are based on it. Though it is not at all obvious from
looking at the definition of recursive functions that these functions exhaust all
computable functions no matter what the computation procedures look like, all
the variously defined computability concepts turned out to be equivalent,
providing evidence in favor of the thesis. One of these concepts is computability
by means of a Turing machine [Tu], a particularly simple model of automated
information processing. Also, programming languages may be used to define the
concept of computability, for instance PROLOG, as was seen in 4.6.
Below we compile a list of the easily provable basic facts about p.r. and
recursive predicates needed in the following. Further insights, above all
concerning the form of their defining formulas, will emerge in 6.3 and thereafter.
P, Q, R now denote exclusively predicates of . In order to simplify the notation
of properties of such predicates, we use as abbreviations in our metatheory the
prefixes ( ∃a < b), ( ∃a ≤ b), ( ∀a < b), and ( ∀a ≤ b) as in (B) below. Their
meaning is self-explanatory.
(A) The set of p.r. (resp. recursive) predicates is closed under forming the
complement, union, and intersection of predicates of the same arity, as well as
under insertion of p.r. (resp. recursive) functions, and finally under swapping,
equating, and adjoining fictional arguments. This is proved as follows: for 
, 
 is exactly the characteristic function of 
;
furthermore, 
 and 
 as well as 
. Since 
 is the same as χ = (f\vec{a},
b), graphf is p.r. provided f is (though the converse need not hold, see the end of
this section). All the other above-mentioned closure properties are simply
obtained from the corresponding properties of the characteristic functions.
(B) Let 
. Suppose that 
 and 
. Then we say that Q, R result from P by bounded
quantification. The same will be said if < in these definitions is replaced by ≤. If
P is p.r. so too are all these predicates, because 
, 
, and similarly if < is replaced by ≤. The proofs of
these equations are so simple that we may pass over them. Briefly, the set of p.r.
(resp. recursive) predicates is closed under bounded quantification. For instance,

since 
, also ∣ is p.r. So too is the predicate { prim},
because 
. Note that a∣p ⇒ a = 1 is
equivalent (at the metatheoretical level) to a∤p ∨ a = 1 and is therefore the union
of p.r. predicates. Hence, this predicate is indeed p.r.
(C) Suppose 
 satisfies 
 and let 
 be the
smallest k such that 
. Then by Oμ, if P is recursive so too is the function f,
because 
. This important generalization of Oμ will
henceforth likewise be denoted by Oμ. On the other hand, f need no longer be
p.r., provided P is. This does hold, though, for the boundedμ-operation: if 
 is p.r. so too is f defined by 
. Here let
Clearly 
, and 
 if 
, and 
 otherwise. To convert this into a p.r. case distinction we define a
p.r. function g by
Then 
 is readily confirmed. Therefore, f is indeed a p.r.
function.
Let h and P be p.r. and 
. Then also
 is p.r. A first application is the pairing function℘, a bijective
mapping from 
 to 
, defined by 
. It enumerates the pairs
(a, b) as in the figure (cf. Exercise 2). Using the formula ( ∗ ) : 
we obtain 
. From this equation one easily obtains a

definition of ℘ by means of the bounded μ-operation, for instance 
.
The famous formula ( ∗ ) was probably first considered by Pythagoras, who
counted the number t n of points of a triangle with n base points as in the right-
hand figure in two ways. t n equals 
 if one counts vertically. But
two such triangles, put together appropriately, form a rectangle with n ⋅ (n + 1)
points. Thus, in fact 
.
Clearly, the bounded μ-operation is not really needed in order to see that ℘ is
p.r. A more convincing application of this operation is a rigorous proof that the
prime number enumeration is p.r. If p is prime then p! + 1 is certainly not
divisible by a prime q ≤ p. Indeed, if q∣p! + 1 and q∣p!, we obtain 
 and hence the contradiction q∣1. Thus, a prime divisor of p! + 1
is necessarily a new prime. Therefore, the function n↦p n is uniquely
characterized by the equations
( ⋆ ) 
.
( ⋆ ) is an application of Op, because with f (a, b)↦μq ≤ b[{ prim}q & q > a],
g : a↦f(a, a! + 1) is p.r. as well, and the second equation in ( ⋆ ) can be written 
, as is easily verified. Hence, n↦p n is indeed p.r.
Remark 2. Unlike the set of p.r. functions, the set of μ-recursive functions
can no longer be effectively enumerated, indeed, not even all unary ones: if 
 were such an effective enumeration then f n↦f n (n) + 1 would be
computable and hence recursive by Church’s thesis. Thus, f = f m for some m, so
that 
, a contradiction. While this seemingly speaks
against the thesis, it can in fact be eliminated from the argument using some
basic recursion theory. (C) clarifies the distinction between p.r. and recursive
functions to some extent. The former can be computed with an effort that can in
principle be estimated in advance, whereas the existence condition in the
unbounded μ-operation may be nonconstructive, so that even crude estimates of
the effort required for computation are impossible. It is not very hard to

construct a computable unary function that each p.r. unary function eventually
overtakes. Nonetheless, also a p.r. function may grow extremely fast. For
instance, it is physically impossible to compute the digits of f6 for the p.r.
function 
. While f5 has “only” 19 729 digits, the number of digits
of f6 is already astronomical.
The following considerations are required in 6.2. They concern the encoding
of finite sequences of numbers of arbitrary length. There are basically several
possibilities for doing this. One of these is to use the pairing function  (or a
similar one, cf. [Shoe]) repeatedly. Here we choose the particularly intuitive
encoding from [Gö2], based on the prime enumeration 
 and the unique
prime factorization.
Definition. 
 is called the Gödel
number of 
. The empty sequence has the Gödel number 1, also
denoted by ⟨⟩. Let { GN} denote the set of all Gödel numbers.
Clearly, ⟨a 0, …, a n ⟩ = ⟨b 0, …, b m ⟩ implies m = n and a i = b i for i = 1, …,
n. Also, 
 is certainly p.r. and by (A), (B) above, so is {
GN}, since
We now create a small provision of p.r. functions useful for the encoding of
syntax in 6.2. Using (C) we define a p.r. function 
 as follows:
We call ℓa for a Gödel number a the “length” of a, since clearly ℓ1 = 0, and
for 
 is 
, because 
 is the smallest
index such that p k ∤a. Note that always ℓa ≤ a and for a≠0 even ℓa < a, because
p a − 1 ∤a in view of p a − 1 > a. Also the binary operation (a, i)↦(a) i is p.r.,
where the term (a) i is defined by
This is the “component-recognition function.” 
 and 
 imply k
= (a) i , hence 
 for all i ≤ n. This function, printed bold in order
to catch the eye, always begins counting the components of a Gödel number with

i = 0. Therefore, 
 is the last component of a Gödel number a≠1,
while (1){ last} = 0. Which values (a) i and ℓ have for arguments outside { GN}
is irrelevant.
From the above definitions it follows that 
 for Gödel
numbers a including a = 1 (because the empty product equals 1). Next we define
the arithmetical concatenation 
 by
 for a, b ∈ { GN} and a ∗ b = 0 otherwise.
Obviously, 
, so that { GN} is
closed under ∗. Moreover, 
, as immediately follows from
the definition. Note also that a ∗ b ∈ { GN} ⇒ a, b ∈ { GN}, for all a, b. Clearly,
∗ is p.r., for its definition is based on p.r. case distinction.
The arithmetical function ∗ is useful for, among other things, a powerful
generalization of Op, the course-of-values recursion explained now. To every f ∈
F n + 1 corresponds a function 
 given by
 encodes the course of values of f in the last argument. Let F ∈ F n + 2.
Then just as for Op there is one and only one f ∈ F n + 1 that satisfies
Namely, 
, 
, etc. In Oq, 
 in general depends for b > 0 on
all values 
, not just on 
 as in Op. Hence Oq is
called the schema of course-of-values recursion. A simple example is the
Fibonacci sequence 
, defined by 
, and 
 for n ≥ 2. The F in “normal form” Oq is given here by
F(b, c) = b for b ≤ 1 and 
 otherwise. Indeed, 
, 
, and for n ≥ 2, we have 
.
Op is a special case of Oq. If f = Op(g, h) and F is defined by the equations 
 and 
, then f satisfies Oq with this F, as may

straightforwardly be checked while observing that 
 equals 
.
Theorem 1.1.
Let f satisfy Oq. If F is p.r. then so too is f.
Proof. Since 
 for b > 0, our  satisfies
The second equation can be written 
, where h is defined by
. With F also the function h is p.r. Hence, by Op,  is p.r.
But then so is f, because in view of Oq, f is a composition of p.r. functions. ___
We now make precise the intuitive notion of recursive (or effective)
enumerability. 
 is called r.e. (recursively enumerable) if there is some
recursive 
 such that 
. In short, M is the range
of some recursive relation. Since 
, where 
, M is
at the same time the domain of some recursive relation. More generally, 
is called r.e. if 
 for some (n + 1)-ary recursive predicate Q.
Note that a recursive predicate P is r.e. Indeed, 
; here 
 (adjoining a fictional variable). It is not quite easy to present an ad
hoc example of an r.e. predicate that is not recursive. But such examples arise
naturally in 6.5, where we prove the undecidability of several axiomatic theories.
It is readily shown that M≠∅ is r.e. if and only if M = { ran} f for some
recursive f ∈ F 1; Exercise 5. This characterization corresponds perfectly to our
intuition: stepwise computation of 
 provides an effective enumeration
of M in the intuitive sense. This enumeration can be carried out by a computer
that puts out 
 successively and does not stop its execution by itself.
The empty set is r.e. because it is the domain of the empty binary relation,
which is recursive, and even p.r., since its characteristic function is the constant
function 
. In view of the above characterization of r.e. sets M≠∅, one could
have defined these from the outset as the ranges of unary recursive functions.
But the first definition has the advantage of immediately expanding to the n-
dimensional case.
It is easily seen that a function f ∈ F n is recursive provided graphf is, simply

because 
 (or in strict terms of property Oμ, 
), that is, f can immediately be isolated from graphf
with the μ-operator. Conversely, if f is recursive then so is graphf, because 
. This equation also shows that graphf is p.r. whenever f
is p.r. The converse need not be true. There are functions f whose graph is p.r.
although f itself is not. A famous example is the (modified) Ackermann function
∘ ∈ F 2, defined by
(see e.g. [Fel1, pp. 76–84]). Thus, not every recursion is primitive recursive.
Exercises
1. Let a ≤ fa for all a. Prove that if f is p.r. (resp. recursive) then so is {
ran} f. The same holds for f ∈ F n if 
 for 
.
 
2. Prove in detail that the pairing function 
 is bijective and that
its diagram in the figure on page 222 is correct.
 
3. Since 
 is bijective, there are functions 
 with 
, for all n. Prove that 
 are p.r. (One need not exhibit
explicit terms for ϰ 1, ϰ 2, although this is not difficult.)
 
4. Let 
 be the least common multiple of f0, …, fn with f ∈ F
1. Show that 
 is p.r. provided f is.
 
5. Let 
 be nonempty. Show that M is r.e. iff M = { ran} f for some
recursive f ∈ F 1.
 
6.2 Arithmetization
Roughly put, arithmetization (or Gdelization) is the description of the syntax of a
formal language 
 and of formal proofs from an axiom system by means of
arithmetical operations and relations on natural numbers. It presupposes the
encoding of strings from the alphabet of 
 by natural numbers. Syntactic
functions and predicates correspond in this way to welldefined functions and

predicates on 
.
Thus many goals at once become attainable. First of all, the intuitive idea of
a computable word function can be made more precise using the notion of
recursive functions. Second, syntactic predicates such as, for instance ‘x ∈ {
var} α’, can be replaced by corresponding predicates of 
. Third, using
encoding, statements about syntactic functions, predicates, and formal proofs can
be formulated in theories 
 able to speak about arithmetic, and perhaps be
proved in T.
We demonstrate the arithmetization of syntax using as an example the
language 
, whose extralogical symbols are 0, S, +, ⋅. This is the
language of Peano arithmetic 
. However, the same procedure can be carried
out analogously for other formal languages, as will be apparent in the course of
our considerations.
The first step is to assign uniquely to every basic symbol ζ of  a number 
,
its symbol code. The following table provides an example for 
:
Next we encode the string ξ = ζ0⋯ζ n by its Gödel number, which is the
number 
. The empty string gets Gödel number
1. This is Gdel’s original encoding, but there are several other possibilities to
encode syntax by natural numbers.
Example. The term 0 and the prime formula 0 = 0 have the still
comparatively small Gödel numbers 
 and 214 ⋅ 32 ⋅ 514, respectively.
The term \underline{1} has Gödel number 216 ⋅ 314. This encoding is not
particularly economical, but that need not concern us here. Nor is it a problem
that the symbol code of = is just the Gödel number of the empty string. For note
that =, considered as an atomic string or a string of length 1, has Gödel number 
.
Let  be the Gödel number of 
, and  and  therefore those of the
term t and the formula α, respectively. If we write ξη for the concatenation of 
, then obviously 
, where ∗ is the arithmetical concatenation
from 6.1. 
 is a p.r. subset of the set of all Gödel numbers.
Indeed, since -symbols are encoded by odd numbers, 
.

When arithmetizing the syntax one has to carefully distinguish a symbol s
from its corresponding atomic string, although these are normally denoted
identically (see also the Notation). 
 is the symbol code of the symbol s, while 
 is the Gödel number of the atomic string . For example, the symbol 0
has the symbol code 13, while the prime term 0 has Gödel number 
. We must also distinguish between the symbolv i and the prime
termv i . Thus, the set of prime terms 
, denoted by , cannot simply be
identified with { Var}.
Remark 1. Nonetheless, one could, right from the beginning, identify
symbols with their codes and strings with their Gödel numbers, so that 
and 
 for formulas φ and terms t. Then syntactic predicates are arithmetical
from the outset. This would even alleviate some of the following considerations
in technical respect. However, we postpone this until we have convinced
ourselves that syntax can indeed adequately be encoded in arithmetic. Further,
the alphabet of 
 could easily be replaced by a finite one, consisting, say, of
the symbols 
, in that v 0 is replaced by the string v0, v 1 by vS0, etc.
Other encodings found in the literature arise from the identification of the letters
in such alphabets with the digits of a suitable number base.
In the following, let 
 for sets 
 of words. A
corresponding notation will be used for many-place word predicates P. We call P
p.r. or recursive whenever 
 is p.r. or recursive, respectively. So, for example, if
we talk about a recursive axiom system 
, it is always understood that 
 is
recursive. Other properties, such as recursively enumerable or representable, can
be transferred to word predicates by means of the above or a similar
arithmetization.
All these remarks refer not just to 
, but to an arbitrary arithmetizable
(or Gdelizable) language 
, by which we simply mean that  possesses finitely
or countably many specified basic symbols, so that each string can be assigned a
number code in a computable way. In this way, the concepts of an axiomatizable
or decidable theory, already used in 3.3, obtain a precise meaning. Of course,
one must clearly distinguish between the axioms and theorems of an axiomatic
theory; the axiom systems of familiar theories like 
 and 
 are readily seen
to be p.r., while these theories considered as sets of theorems are shown in 6.5 to
be undecidable and cannot even be extended in any way to decidable theories.
The main goal now is the arithmetization of the formal proof method. We use
 from now on to denote the Hilbert calculus of 3.6 consisting of the axiom
system Λ with the axiom schemata Λ1–Λ10 given there and the Modus Ponens

MP as the only rule of inference. Here everything refers to a fixed arithmetizable
language 
, which, as a rule, will be the arithmetical language 
. Just as for
strings, for a finite sequence 
 of 
-formulas we call 
 its Gödel number. This includes in particular the case that Φ is a
proof from 
 in the sense of 3.6, which in the general case also contains
formulas from Λ. Note that 
 for all 
, because 
 (a proof is not
empty), whereas 
 because the symbol codes are odd. This is the case in
our example language 
 and may actually be presupposed throughout. Thus,
we can comfortably distinguish the Gödel numbers of formulas and terms from
the Gödel numbers of finite sequences of formulas.
Now let 
 be a theory axiomatized by some fixed axiom system X ⊆
T. Examples are 
 and 
. 2 A proof 
 from X is also called a
proof in T. Here and elsewhere the axiom system X is tacitly understood to be an
essential part of T, which is originally understood as a set of sentences. First
define the p.r. functions 
 as follows: 
, 
 and 
 (argument parentheses in the last expression should not be
mixed up with parentheses belonging to the alphabet of 
). Clearly, both 
 and
 are closed under these operations.
Let { proof} T denote the unary arithmetical predicate that corresponds to the
syntactic predicate ‘Φ is a proof in T ’. We denote the arithmetical predicates
corresponding to ‘Φ is a proof in T for φ’ (the last component of Φ) and to ‘there
is a proof for φ in T’ by { bew} T and { bwb} T , respectively (coming from
beweis = proof and beweisbar = provable). Precise definitions of these predicates
look as follows:
(1)
 
,
 
(2)
 ,
 
(3)
.
 

Since { bwb} T is a unary predicate that will be met several times in the
sequel, we dropped the argument parentheses in writing { bwb} T a. Easily
obtained from (1), (2), and (3) are
(4)
 (
),
 
(5)
, for all a, b, c, d, 
(6)
, for all a, b,
 
(7)
, for all 
.
 
The equivalence (4) is clear, for if 
 then there is a proof Φ for α, hence 
 with 
, and conversely. (5) tells us in arithmetical terms the
familiar story that concatenating proofs for α, α → β and tacking on β yields a
proof for β. (5) immediately yields (6) by particularization, and (6) implies (7)
since 
.
Remark 2. We will not need (5)–(7) until 7.1. But it is very instructive for
our later transfer of proofs to 
 to verify (5) first naively. This is simple when
we refer to the following facts: 
, 
 and 
, for all 
. Note also that ( c )0 = c for all c. Since it
would impede the proof of (5), 
 was not added to the right-hand
side of (1). This is in fact dispensable, for induction on the length ℓb of the
Gödel number b readily shows that { proof} T (b) implies 
. Here
we need 
, for all 
 (Exercise 3 in 2.3).
Now we really get down to work and show that the syntactic basic notions up
to the predicate { bew} T are p.r. In 6.5 only their recursiveness is important; not
until Chapter 7 do we make essential use of their p.r. character, which ensures
that all involved functions are definable in 
. We shall return to our example 
 only at the end of 6.3, because the proofs of the following lemmas are
not entirely independent of the language’s syntax and the selected encoding,
though they can be proved for other arithmetizable languages in nearly the same
way.
In addition to the already-defined functions , , and \tilde{ →}, we define 

(.)
 
 and 
.  is defined similarly. Finally, for the
operations S, +, ⋅ define 
, 
, and similarly for ⋅. Then, for
example, 
 and 
 for terms s, t, as well as 
 
. All these functions are obviously primitive recursive.
For arbitrary strings ξ, η let ξ ≤ η mean 
 (correspondingly for ¡). For
example, ξ ≤ η holds if ξ is a substring of η, in particular if ξ is a subformula of
the formula η. This follows immediately from the property a, b ≤ a ∗ b for
Gödel numbers a, b, mentioned already on page 224.
Lemma 2.1.
The set 
 of all terms is primitive recursive.
Proof. The set  of terms v i is p.r. since 
. Thus 
, the set of all prime terms, is p.r. as well. By the recursive
definition of 
, 
 if and only if
Therefore the corresponding arithmetical equivalence holds as well:
( ∗ ) 
,
where 
. We now show how to convert
this “informal definition” of 
, which on the right-hand side makes use of
elements of 
 smaller than n only, into a course-of-values recursion for the
characteristic function 
, whence 
, and so 
 would turn out to be p.r.
Consider the p.r. predicate P defined by
We claim that 
 satisfies 
, where 
 equals 
, and hence f is p.r. by Theorem 1.1. Indeed, since 
, we obtain in view of ( ∗ )

From this it clearly follows that 
, which in turn
implies Oq, since both f and 
 take values from {0, 1} only. ___
Lemma 2.2.
The set 
 of all formulas is primitive recursive.
Proof. 
 is p.r. simply because
If we consider 
 for every 
 with x ∈ { var} ξ (because then ξ = ηxθ
for some strings 
), then the predicate ‘
’ clearly satisfies
This “informal definition” can then be transformed just as in Lemma 2.1 into
a course-of-values recursion of the characteristic function of  using the
characteristic function of the certainly p.r. predicate P given by
We now define a ternary p.r. function 
 such that
( ∗ ) 
 for all 
, 
, and 
.
 will be constructed essentially by course-of-value recursion on m in
two steps, first for 
 and then for 
. Step 1: Put 
, if 
 or 
 or 
. Otherwise let first 
. In case m = i set 
(remember that 
), and if m≠i set 
. Now let 
.
According to the case distinction in Exercise 2, let first 
. Put 
 (m 0 is unique). If 
, set 
, and similarly
for . Thus, 
 is now well defined and p.r., but ( ∗ ) holds currently only for 

(.)
, since 
 for 
. Step 2: We modify the step 1 definition of 
 for the case 
, 
, and 
 in a p.r. way. For 
 put 
 (
, prime formula case). Otherwise, 
 or 
 or 
 for suitable m 0, m 1 < a from 
 and 
according to Exercise 3. We then explain 
 for all 
 by course-of-value
recursion similarly to what we did in Step 1 and according to the definition of a
substitution on formulas. This results in a p.r. function that satisfies ( ∗ ).
As was already noticed, the predicate ‘x occurs in ξ’, or ‘
’ for short,
is p.r., since 
. Replacing here 
 by 
 makes it clear that ‘x ∈ { bnd} α’ is p.r. as well. The binary predicate ‘x ∈
{ free} α’ is also p.r. because x ∈ { free} α if and only if 
. Consequently 
 is p.r. With these
preparations we now prove
Lemma 2.3.
The set Λ of logical axioms is primitive recursive.
Proof. Λ1 is p.r. because φ ∈ Λ1 if and only if
To characterize the corresponding arithmetical predicate we use the p.r. function 
. One reasons similarly for Λ2–Λ4. For a p.r. characterization of Λ5 use the
fact that the ternary predicate ‘
 collision-free’ is p.r. For ‘
 collision-
free’ holds iff 
. Further, the predicate ‘
’, which depends on 
, is p.r., as can be seen by applying 
. Hence, Λ5 is p.r. as well, because 
 if and only if
Similarly it is shown that Λ6–Λ10 are p.r. Thus, each of the schemata Λi is p.r.
and therefore so is Λ 0 : = Λ1 ∪⋯ ∪Λ10. But then the same holds for Λ itself,
because 
 is surely p.r. and every α ∈ Λ can be written 
 with

(.)
some (possibly empty) prefix 
 and for some α0 ∈ Λ 0, and then it must hold
that
The second line of this formula tells us that m is the Gödel number of a prefix ∀x
1⋯ ∀x l . This is a string of length m = 2l. __ __
All of the above holds completely analogously for every arithmetizable
language. Hence, given a p.r. or recursive axiom system X, X ∪Λ is p.r. (resp.
recursive) as well. This applies in particular to the axiom systems of 
 and 
. These are p.r. like every other common axiom system, despite the difference in
their strengths. The proof is carried out in a manner fairly similar to that of
Lemma 2.3.
The main result of this section, which now follows, is completely
independent of the strength of an axiomatic theory T. The strength of a theory T
first comes into the picture when we want to prove something about { bew} T and
{ bwb} T within T itself.
Theorem 2.4.
Let X be a p.r. axiom system for a theory T of an arithmetizable language. Then
the predicate { bew} T is p.r. The same holds if we substitute here “recursive” for
“primitive recursive.” T is in either case recursively enumerable.
Proof. Definition (2) on page 229 shows that { bew} T is p.r. Because of (3) on
the same page, 
 is the range of a (primitive) recursive
relation and thus is r.e. Clearly, the last part of the theorem is proved in the same
manner. ___
Theorem 2.4 can be strengthened only under particular circumstances, for
example if T is complete. Although { bew} T is a (primitive) recursive predicate
for each axiomatic arithmetizable theory T, { bwb} T need not be recursive, as,
for example, in the case T = Q. This is a famous finitely axiomatizable theory
presented in the next section, whose particular role for applied recursion theory
was revealed in [TMR].
Exercises
1. Prove that if a theory T has a recursively enumerable axiom system X,
then T also possesses a recursive axiom system (W. Craig).
 

(.)
2. Let 
. Show that there are 
 with 
 or 
 or 
. Moreover, b, c are unique and < a in each case.
 
3. Let 
. Show that there are 
, 
 with 
 or 
 or 
. b, c, d are unique and < a in each case.
 
4. Let T be axiomatizable and 
. (a) Define a binary p.r. f such that 
 (the arithmetized deduction
theorem). (b) Show that 
.
 
6.3 Representability of Arithmetical Predicates
First of all we consider the finitely axiomatized theory Q with the axioms
The axioms characterize Q, also called Robinson’s arithmetic, as a modest
subtheory of 
. Both theories are formalized in 
 and are subtheories of 
, where 
 as always denotes the standard model 
. In Q, 
and related theories in 
, ≤, and < are defined by the formulas 
 and 
 according to our convention on
page 105. The term S n 0 is denoted by .
From the results of this and the next section, not only will the recursive
undecidability of Q be derived, but also that of every subtheory and every
consistent extension of Q; see 6.5. If we were interested only in undecidability
results, we could simplify the proof of Theorem 4.2 by noting that all recursive
functions can already be obtained with Oc and Oμ from the somewhat larger set
of initial functions 
. But even ignoring the considerable effort
required to prove the eliminability of the schema Op at the price of additional
initial functions, such an approach would blur the distinction between primitive
recursive and μ-recursive functions, relevant for some details in Chapter 7.
∀x x≠Sx is easily provable in 
 by induction, but Q is too weak to allow a
proof of this sentence. Its unprovability follows from the fact that 

 satisfies all axioms of Q, but not ∀x x≠Sx. Here ∞ is a new
object and the operations S, +, ⋅ are extended to 
 by putting S∞ = ∞, ∞ ⋅
0 = 0, and for all n and all m≠0,
This model shows the unprovability in Q of many familiar laws of arithmetic,
which tell us that 
 is the nonnegative part of a discretely ordered commutative
ring with unit element 1 : = S0. These laws are collected in the following axiom
system of a finitely axiomatizable theory 
, with the order defined as in Q
above:
∀-quantifiers in the axioms are omitted. N is also denoted by 
 in the
literature, and like Q, a subtheory of 
. All Q-axioms are derivable in N (a
recommendable exercise), so that 
. Reflexivity, transitivity, and
antisymmetry of ≤ are provable in N, as are the strong and weak monotonicity
laws for + and ⋅.
In this section we mostly write 
 for 
 and 
 for 
 etc. We also
write occasionally α ⊢ β ⊢ γ for 
 & 
, and apply further self-
explanatory abbreviations such as 
 for 
, and 
 for ‘ 
 and α is equivalent to β ’. The use of  in the subtle
derivations carried out below helps one see what is going on and makes the
metainduction used there more vivid. Some of the proofs can be seen as
“transplanting inductions from 
 into the metatheory.” For instance, ∀x x≠Sx is
provable in 
, but not in Q, as was just shown. Nonetheless, we still can prove 
 for all n, as is seen by metainduction on n. Indeed, ⊢ 0≠S0 is clear by
Q1. The induction step 
 derives from 
.
This in turn easily follows with MP from 
, an application of
Q2. We now prove

(.)
(.)
From C5 follows 
 (
), which is ⊥
for n = 0. The proofs of C0–C6 will be carried out by induction (more precisely,
metainduction) on n.
C0: Clear for n = 0, because 
 by Q4 and Q5.
Our induction hypothesis is 
. It yields, again by Q5, the
induction claim 
.
C1: Clear for n = 0, because 
 by Q4. The induction
hypothesis 
 yields 
, by Q5, and the
last term is the same as 
. This proves the induction step. Analogously we
derive 
 with Q6, Q7, and what was shown already.
C2: Clear for n = 0, since then m = Sk for some k, and so 
 by Q1. Let
Sn≠m. By Q1, 
 in case m = 0. Otherwise, m = Sk for some k, so that
n≠k; hence 
 by the induction hypothesis. Thus, 
 by Q2.
C3: m ≤ n implies 
 for some k, hence 
. Thus, 
by C1. Therefore 
, i.e., 
.
C4: m≰n ⇒ m≠0, hence m = Sk for some k. Let m≰ 0. Then 
 because
 by Q5, Q1. Let 
. Then k≰n, and so 
 by the induction hypothesis. Hence 
, for 
 (the
latter needs only Q5 and Q2).
C5: Clear for n = 0, because 
 by Q3, Q5, Q1. The
induction claim is equivalent to 
. It is derived as
follows:
C6: Clear for n = 0. Further, 
, by Q3, C0,
and 
 by C1. Thus, 
. Now, C5 and C3 easily lead to 

. This and the former yield the inductive step, because 
.
With these preparations we now give the following crucial definition, in
which T ⊇ Q is supposed. This will cover all our applications.
Definition. Call a predicate 
 numeralwise representable or simply
representable in T ( ⊇ Q) 3 if there is some 
, called a representing
formula, such that
{ R}+: 
 ; { R}−: 
.
Examples. The identity relation 
 is represented by x = y,
because 
 is trivial if a = b, and 
 is derivable for a≠b by C2. By
C3 and C4 the formula x ≤ y represents the ≤ -predicate. x≠x represents the
empty set, represented as well by any α with ¬α ∈ Q.
For consistent T ⊇ Q, whenever R+, R− are valid then so too are their
converses, so that in fact 
 and 
. Note also that a
representable 
 is recursive by Church’s thesis. For let P be represented
by 
. Simply turn on the enumeration machine for Q and wait until 
 or 
 appears. The set of n-ary representable predicates is closed under union,
intersection, and complement, as well as swapping, equating, and adjoining
fictional arguments. If P, Q are represented respectively by 
, then so
too are P ∩ Q by 
 and ¬P by 
. Consequently, P ∪Q by 
, etc.
A predicate P represented in Q by α is clearly representable by the same α in
any consistent extension of Q, in particular in 
. But this just means
definability of P in 
 by α in the sense of 2.3, because 
 is equivalent to
. In short, definability of P in 
 and representability of P in 
coincide. In the main, however, we consider representability in Q to obtain some
strong results needed in 6.5. We always have to look carefully at the representing
formulas.
One could define f ∈ F n to be representable if graphf is representable.
However, it turns out that this definition is equivalent to a stronger notion of
representability for functions that will be introduced after some additional
preparation.
Predicates and functions definable in 
, that is, by 0, S. +, ⋅, are called

(.)
arithmetical after [Gö2]. From now on this word will always have this meaning.
The arithmetical predicates encompass the representable ones. In order to
discover more about these objects we consider their defining formulas more
closely. Prime formulas in 
 are equations, also called Diophantine equations
for traditional reasons. If 
 is such an equation and 
, then
P is called Diophantine. A simple example is ≤, because 
. 4In
fact, all predicates definable in 
 by ∃-formulas ∃\vec{y}φ from 
 with
kernel φ are Diophantine. The proof is not difficult: Think of φ as being
constructed from literals by means of 
 (cf. Exercise 4 in 2.4), and use the
following easily provable equivalences in an inductive proof on φ of what has
been claimed:
A classification of arithmetical formulas and predicates helpful not
only for the sake of representability is given by the following definition, to be
generalized in 6.7: Definition. A formula is called Δ0 or a Δ0-formula if it is
generated from prime formulas of 
 by ∧, ¬, and bounded quantification, i.e.,
if α is Δ0 then so is ( ∀x ≤ t)α; here t is any 
-term with x∉{ var} t. (It is not
important that x ≤ t is not a prime formula of 
). Let φ be Δ0 and  arbitrary.
Then 
 is called a Σ 1 -formula, and 
 a Π1-formula. Further, 
 is
said to be Δ0, Σ 1, or Π1 whenever P is defined in 
 by a Δ0-, Σ 1-, or Π1-
formula, respectively. Δ0, Σ 1, and Π1 denote the sets of Δ0-, Σ 1-, and Π1-
predicates. In addition, Δ1 : = Σ 1 ∩ Π1. There are no Δ1-formulas, for there is
no meaningful definition of such formulas. φ is called Δ0, Σ 1, or Π1 also if it is
equivalent to an original Δ0-, Σ 1-, or Π1-formula, respectively. In this sense, if α
is Δ0 then so too are 
 and 
.
Clearly, Π1 consists of the complements of the P ∈ Σ 1. The P ∈ Δ1 are both
Σ 1- and Π1-definable, with possibly distinct formulas. By Exercise 3 in 2.4, Σ 1
and Π1 are closed under union and intersection of predicates of the same arity,
and Δ1 like Δ0 moreover under complements. If 
 and g 1, …, g m ∈ F n
are Σ 1, so too is Q = P[g 1, …, g m ], simply because 
.
Note also that if graphf is Σ 1 then it is automatically Δ1, for 

, so that the complement of graphf is again Σ 1. Here
some examples of Δ0-and Σ 1-formulas and sentences. Interesting Π1-sentences
are found at the end of 6.5.
Examples. Diophantine equations are the simplest Δ0-formulas. To these
belong the formulas 
 with 
, which define the term functions 
. Since 
, divisibility and thus also the
predicate { prim} are Δ0. Because 
, 
 is
Δ0. The same holds for the relation of being coprime, denoted by 
 and defined
by 
. Diophantine predicates are trivially Σ 1.
Surprisingly, by Theorem 5.6 the converse holds as well, although it had
originally been conjectured that, for instance, the set 
 of all powers of 2 was not Diophantine.
This set is Δ0. Even the graph of 
 is Δ0.
Remark 1. More generally, the predicate ‘a b = c’ is Δ0, though it is difficult
to prove this fact. Indeed, even the proof in 6.4 that this predicate is arithmetical
requires effort. Earlier results from Bennet, Paris, Pudlak, among others, are
generalized in [BA] as follows: if f ∈ F n + 1 (more precisely, graphf) is Δ0 then
so is 
, and the recursion equation 
 is
provable in IΔ0. This theory is an important weakening of 
. It results from Q
by adjoining the induction schema restricted to Δ0-formulas. IΔ0 plays a role in
various questions, e.g., in complexity theory ([Kra] or [HP]). Induction on the
Δ0-formulas readily shows that all Δ0-predicates are p.r. The converse does not
hold; an example is the graph of the very rapidly growing hyperexponentiation,
recursively defined by 
 and 
. Stated more
suggestively, 
.
According to Theorem 3.1 below, already the weak theory Q is Σ 1-complete,
i.e., each Σ 1-sentence true in 
 is provable in Q. This can be confirmed in
various ways. For instance, one may use that by C1 and C2, 
 is a prime model
of Q in the sense of 5.1Elementary Extensionssection.5.1.233; in addition,
each 
 is an end extension of 
 as defined in 3.3. But we choose here a
constructive approach, which provides some additional information.

Theorem 3.1 (on the Σ 1-completeness of Q).
Every Σ 1 -sentence true in 
 is already provable in Q and hence in each
extension T ⊇ Q .
Proof. We claim that it suffices to prove
( ∗ ) Either 
 or 
, for each Δ0-sentence α.
Indeed, let 
 with the Δ0-formula 
, say 
. Then ⊢
Q α by ( ∗ ), for 
 is impossible. Hence 
. We verify ( ∗ ) first for
prime sentences. If t is a variable-free term then C1 readily yields 
.
For example, 
. Thus, if α is the prime sentence t 1 = t 2, then 
 or 
 by C2, which confirms ( ∗ ) for α. The induction
steps over ∧, ¬are simple. For instance, 
 if 
, and 
 otherwise, i.e. if 
 or 
. These steps suffice
already to prove ( ∗ ), because bounded quantifiers are eliminable from a Δ0-
sentence α modulo Q. Indeed, let ( ∀x ≤ t) be the first bounded quantifier in α
from the left, with the scope β. Then { var} t = ∅, because x∉{ var} t and any y
∈ { var} t must have been bounded further to the left. Moreover, ( ∀x ≤ t)β(x) ≡
Q ( ∀x ≤ \underline{n})β(x) with 
, since 
. We then easily get 
 with C3 and C5. Thus, ( ∀x ≤ t) can be
eliminated from α and this process can be repeated if necessary. ___
If 
 is Δ0 then 
 and 
 by the
theorem, because both 
 and 
 are trivially Σ 1. Thus, we obtain a first
important result on representing formulas:
Corollary 3.2.
A Δ0-formula represents in Q the predicate that it defines in 
 .
Lemma 3.3.
Let 
 represent 
 . Then 
 and 
represent the predicates Q and R, respectively, where
The same is true if < is replaced by ≤ in this lemma.

Proof. { R}+: Suppose 
, that is, 
 for some c < b. Then 
. Consequently, 
. To prove { R}− suppose 
, hence 
, i.e., 
 for all i < b. We thus obtain 
. By C5, 
 and so 
. Therefore, 
. This proves { R}−. For handling the
predicate R simply notice that 
. This proof is literally
the same if < is replaced by ≤ in the lemma. ___
Following [Gö2] and [TMR], we now define the notion of a representable
function. Although representability of f is much stronger a notion than
representability of graphf, Lemma 3.4(b) will show that both properties coincide,
provided the axioms of Q are available.
Definition. f ∈ F n is representable in T if there is a formula 
such that for all 
,
If φ is Δ0 (respectively Σ 1 or Π1) then f is said to be Δ0-representable
(respectively Σ 1- or Π1-representable).
For some purposes it is useful to refine this definition: f ⊆ F n is said to be
Δ1-representable if f is both Σ 1- andΠ1-representable, with usually distinct
formulas. Corresponding phrases will be used for predicates P instead of
functions f.
Since { R}+ is equivalent to 
, it is obvious that R+ and R =
together are replaceable by the single condition 
 for all . If f is
represented by 
 then graphf is represented by the same formula, because if
 and hence 
 by C2, then 
 by R =, so that the condition R−
holds. The following lemma will show in particular that f is representable
provided graphf is representable.
Lemma 3.4.
(a) Let 
 be represented by 
 and suppose that 
 . Then 
 represents the function 
 . If P
is Δ0-representable then so too is f. If P is Δ1-representable then so is f. (b) f is
representable provided graph f is representable. (c) If f is Σ 1 -representable then

f is Π1-representable as well. (d) If χP is Σ 1 -representable then P is Δ1-
representable.
Proof. By Lemma 3.3, 
 represents the predicate defined by 
 and this
is clearly graphf. Hence, R+ holds. We verify 
 by proving
Suppose 
. Then 
), because 
. Contraposition
yields 
. By C5 and { R}− we have 
. Hence 
 and so, by C6,
This confirms ( ∗ ). Clearly, φ in (a) is Δ0 if α is Δ0. Let P be represented at the
same time by the Π1-formula β. Repeating the above with 
 (a Σ 1-formula by Exercise 2) in place of φ shows that f
is Σ 1-representable. It is then also Δ1-representable by item (c). (b) follows from
applying (a) to P = graphf while noting that 
. (c): Let the Σ 1-
formula 
 represent f and z∉{ var} φ. Then 
 is
a Π1-formula that represents f as well: Application of R = results in ⊢ φ
′(\underline{\vec{a}}, \underline{f\vec{a}}), which confirms R+ for φ′, and
because of 
, we obtain R = for φ′ from
.
(d): Let 
 be Σ 1-represented by 
. Then P is Σ 1-represented by 
 and Π1-represented by 
, as is easily confirmed. ___
Remark 2. 
 represents 
 in Q.
Thus, the Δ0-formula 
 represents  according to
Lemma 3.4(a). We mention that in 
 (but not in Q) the function  is
represented even by the open formula α.
Lemma 3.5.
Let 
 be represented by 
 , and g i ∈ Fn represented by γi for 
 . Then 
 represents the predicate Q :=
P[g1,…,gk]. If the γ i are Σ 1 and P is Σ 1 -representable or Δ1-representable,

then the corresponding holds for Q.
Proof. Let 
, so that 
, and 
. If 
 holds, hence 
,
then 
, whence 
, and so 
. But if 
 and thus 
, then clearly 
. Using 
 for the γ i , this then yields 
. Hence 
. If the γ i
and also α are Σ 1, then so too is β. If P is represented by the Π1-formula 
 at
the same time, then the Π1-formula 
 represents Q as well.
___
From this lemma, applied to graphh, follows without difficulty
Corollary 3.6.
If h ∈ F m is representable by β and the gi ∈ Fn by γi, then 
 represents f = h[g 1 ,…,g m ].
Exercises
1. Let 
 be Σ 1 and 
 be Π1. Construct Δ0-formulas β and γ such that 
 and 
 (quantifier compression). Since a Δ0-
predicate is p.r. (Remark 1), each Σ 1-predicate is r.e. and w.l.o.g. of the
form 
 with Q ∈ Δ0.
 
2. Show using (c) from Exercise 4 in 3.3 that Σ 1 is closed under bounded
quantification, that is, if 
 and 
 is Σ 1 (defines an (m + 1)-
ary Σ 1-predicate), then also 
 and 
 are Σ 1.
Derive the corresponding for Π1 and Δ1.
 
3. Prove that 
 represents 
 provided α
represents P.
 
4. Show that every Δ0-formula is equivalent to a Δ0-formula built up from
literals by means of 
, and the bounded quantifiers ( ∀x ≤ t) and ( ∃x
≤ t), a correspondence to Exercise 4 in 2.4.
 

6.4 The Representability Theorem
For the representability of all recursive or just all p.r. functions, it is helpful to
have a representable g ∈ F 2 that satisfies the following: for every n and every
sequence 
 there exists a number c such that g(c, i) = c i for all i ≤ n. In
short, c can be chosen such that the values 
 are the given
ones. Now, there are many p.r. functions g that can do this, for example 
 for choosing 
. Initially there is no obvious way to
show the representability of such a function g in Q or in some extension of Q
within the language 
. Therefore, K. Gödel, who around 1930 was working on
this and related problems, in the words of A. Mostowski “phoned with God.”
Although nowadays several possibilities are known, we follow the original,
which has not lost any of its attraction.
Let 
, where rem(a, d) denotes the remainder of
a divided by d (≠0). In addition, rem(a, 0) : = 0. Note that r = rem(a, d) is well
defined, since for any a and d≠0 there are unique 
 with 
 and r <
d (readily shown by induction on a). Clearly, graphα has the Δ0-definition
Hence, the function α is Δ0-representable by Lemma 3.4(b). The same holds for
the pairing function . Because  is bijective there are unary functions ϰ 1, ϰ 2
such that 
 for all k. Their explicit form is insignificant; we just
require the obvious property ϰ 1 k, ϰ 2 k ≤ k. The function 
is called the β-function. Since
graphβ is Δ0. Hence, according to Lemma 3.4, β is represented by a Δ0-formula,
which will be denoted by { beta}. Omitting the argument parentheses in { beta},
this means that (1) 
, for all 
.
Clearly, { beta} also defines the β-function in 
. The following simple number-
theoretic facts known for ages will be applied in proving the main property of
the β-function stated in Lemma 4.1 below.
Euclid’s lemma. Let a,b be positive and coprime (a⊥ b). Then there exist 
 such that 
 . (The converse of this claim is obvious: c∣a, b
implies 
 and hence c = 1.)
Proof by < -induction on 
. Trivial for s ≤ 2, i.e., 
. Let s > 2.

Then a≠b, say a > b, and clearly a − b⊥b as well. Since 
, there are
x, y ∈ ℕ with 
 by the induction hypothesis. Hence, 
with 
. In the case a < b consider 
, so that 
 for
some x, y by the induction hypothesis. Hence 
. __ __
Chinese remainder theorem. Let c i < d i for 
 and let 
 be
pairwise coprime. Then there exists some 
 such that rem (a,d i ) = c i for 
.
Proof by induction on k. For k = 0 this is clear with a = c 0. Let the
assumptions hold for k > 0. By the induction hypothesis, rem(a, d i ) = c i for
some a and all i < k. Since 
 are coprime, 
 and d k are
coprime (Exercise 1c). Thus, by Euclid’s lemma, there are 
 such that 
. Multiplying both sides by 
, we obtain 
 with new values 
. Put 
. Then 
 for all i < k, since d
i ∣m. But also rem(a′, d k ) = c k , because c k < d k . __ __
Unlike those in most textbooks of number theory, the proof above is
constructive and easily transferable to 
, as will be shown in 7.1. In logic it is
occasionally not just important what you prove, but how you prove it. The claim
of Euclid’s lemma can also be shown by means of the Euclidean algorithm for
determining lcm(a, b).
Lemma 4.1 (on the β-function).
For every n and every sequence 
 there exists some c such that β(c,i) = c
i for i = 0,…,n.
Proof. It suffices to provide numbers a and b such that α(a, b, i) = c i for all i ≤ n.
Because of 
 the claim is then satisfied with 
. Let 
 and 
. We claim that the numbers 
 (i ≤ n) are pairwise coprime. For otherwise let p be a
common prime factor of d i , d j with i < j ≤ n. Then 
; hence
p∣j − i or p∣b. But since j − i∣b in view of j − i ≤ n ≤ m, it follows that p∣b in any
case. Since b∣d i − 1 this yields p∣d i − 1, contradicting p∣d i . Hence, 
 are

indeed pairwise coprime. By the Chinese remainder theorem there is an a such
that rem(a, d i ) = c i , that is, α(a, b, i) = c i for i = 0, …, n. __ __
Remark 1. Already at this stage we gain the interesting insight that the
exponential function (a, b)↦a b is explicitly definable in 
, namely by
This is a Σ 1-formula, more precisely, the description of a Σ 1-formula arising
after the elimination of the occurring β-terms by means of (1), using some
further ∃-quantifiers instead. By induction on b one sees that 
implies a b = c. Suppose conversely that a b = c. Then Lemma 4.1 guarantees a
suitable u such that 
: choose u such that β(u, i) = a i for all i ≤ b.
This argument is generalized in Theorem 4.2 below. It tells us in particular that
each recursive function is explicitly definable in 
.
For simplicity, we assume T ⊇ Q in Theorem 4.2 below, though it holds as
well if Q is merely interpretable in T in the sense of 6.6. For the derivation of
undecidability results or a simplified version of the first incompleteness theorem,
the theorem’s “Moreover” part is not needed.
Theorem 4.2 (Representability theorem).
Each recursive function f—and hence every recursive predicate—is
representable in an arbitrary consistent axiomatic extension T ⊇ Q . Moreover, f
is Σ 1 -representable.
Proof. It suffices to construct a Σ 1-formula that represents f in Q. For the initial
functions 0, S, 
 we may choose the formulas v 0 = 0, v 1 = Sv 0, and 
.
As regards Oc, let f = h[g1, …, gm] and suppose 
 and 
 are Σ 1-
formulas representing h and the g i . Then by Corollary 3.6, 
 is such a formula for f. Next let f = Op(g, h),
with g, h both being Σ 1-representable. Define
According to Lemmas 3.5 and 3.3, P is Δ1-representable (use composition,
instantiation of Σ 1-representable functions, bounded quantification, and
conjunction). Clearly 
 is equivalent to ( ∗ ) : 
 for all i ≤ b.
By Lemma 4.1, for given \vec{a}, b there is some c satisfying ( ∗ ) ; hence we

know that 
. Thus, 
 is Σ 1-representable;
Lemma 3.4(a). Since 
, ( ∗ ) holds with 
, which yields 
 for i = b. Thus, as a composition of Σ 1-representable
functions, f is Σ 1-representable. Finally, let f result from g by Oμ, 
, where P(\vec{a}, b) ⇔ g(\vec{a}, b) = 0 and g is Σ 1-
representable. By Lemma 3.4(c), g is Π1-representable, too. This clearly implies
that P is Δ1-representable. Hence, f is Σ 1-representable by Lemma 3.4(a). ___
Let T ⊇ Q be a theory in 
. To 
 corresponds withinT the term
\underline{n} with 
, which will be denoted by 
 (or ) and called the
Gdel term of φ. For example, 
 is 
). Analogously ⌜ t
⌝ is defined for terms t. For instance, 
. If T is axiomatized,
also 
 for proofs Φ in T is well defined. For instance, (v 0 = v 0) is for
such a T a trivial proof of length 1 by axiom Λ9 in 3.6. Its Gödel term is 
. The predicate { bew} T is p.r. (Theorem 2.4), hence Σ 1-representable (Theorem
4.2), by the formula 
, say. Define 
. Then
Theorem 4.2 and (4) from page 230 obviously yield the following important
Corollary 4.3.
If T ⊇ Q is axiomatizable then 
 for some n, and 
 for all n. Hence, 
 implies 
 in any
case.
The converse 
 need not hold; see 7.1. Theorem 4.2 has
several important consequences, for example Theorem 4.5 below. Before stating
it we will acquaint ourselves with a method of eliminating Church’s thesis from
certain intuitively clear arguments that demand justification when ‘decidable’ is
identified with ‘recursive’. Of course, such an elimination must in principle
always be possible if the thesis is to retain its legitimacy. For instance, Church’s
thesis was essentially used in the proof of Theorem 3.5.2. We reformulate the
theorem and will give a rigorous proof.
Theorem 4.4.

A complete axiomatizable theory T is recursive.
Proof. Because of completeness, the function
is well defined. Indeed, let P(a, b) denote the recursive predicate in square
brackets. Then ∀a ∃bP(a, b) (note that P(a, 0) if 
). By Oμ, then, f is
recursive. We claim ( ∗ ) : 
. This clearly implies the
recursiveness of T. In order to prove ( ∗ ) let 
, so certainly 
. Then for
b = fa, the smallest b such that 
, the first disjunct must
hold, because due to the consistency of T, no 
 with 
 can exist at
all. Hence, { bew} T (fa, a). The ⇐ -direction in ( ∗ ) is obvious. ___
This proof illustrates sufficiently well the distinction between a primitive
recursive and a recursive decision procedure. Even when X and thus the
predicate P in the proof above are primitive recursive, the defined recursive
function f need not be so, because the completeness of T may have been
established in a nonconstructive way. The use of Church’s thesis in the proofs of
(i) ⇒ (ii) and (iii) ⇒ (ii) of the following theorem can be eliminated in almost
exactly the same manner as above, although then the proof would lose much of
its transparency.
Theorem 4.5.
For a predicate 
 and any consistent axiomatizable theory T ⊇ Q the
following are equivalent: (i) P is representable in T, (ii) P is recursive, (iii) P is
Δ1.
Proof. (i) ⇒ (ii): Suppose P is represented in T by 
. Given  we set going
the enumeration machine of T and wait until 
 or 
 appears. Thus, P is
decidable and hence recursive by Church’s thesis. (ii) ⇒ (i),(iii): By Theorem
4.2, 
 is representable in T by a Σ 1-formula; hence P is Δ1-representable by
Lemma 3.4(d) and of course by the corresponding formulas also defined in 
.
Thus, P ∈ Δ1. (iii) ⇒ (ii): Let P be defined by the Σ 1-formula 
 and the Π1-
formula 
. Given \vec{a} we start the enumeration machine for Q and wait
until the Σ 1-sentence 
 or 
 appears. In the first case P\vec{a} holds; in

the second it does not. This procedure terminates because Q is Σ 1-complete by
Theorem 3.1. ___
This theorem tells us that in all consistent axiomatic extensions of Q exactly
the same predicates are representable, namely the recursive ones. Moreover, Δ1
contains precisely the recursive predicates, from which it easily follows that Σ 1
consists just of all r.e. predicates. Theorem 4.5 clarifies fairly well the close
relationship between logic and recursion theory. It is independent of Church’s
thesis. Even if the thesis for certain theoretical or practical reasons had to be
revised, the distinguished role of the μ-recursive functions would not be affected.
Remark 2. The above results allow us to define recursive or decidable
predicates directly as follows: 
 is recursive iff there is some finitely
axiomatizable theory in which P is representable. We need only to notice that a
predicate representable in any finitely axiomatizable theory in which
representability makes sense is recursive by Church’s thesis. In this and the
previous section we met several formulas or classes of those that represent
predicates in Q and hence are recursive. It would of course be nice to provide a
somewhat more uniform system of formulas that represent the recursive
predicates, or at least that define them in 
. Unfortunately, such a system of
formulas cannot be recursively enumerated. Indeed, suppose there is such an
enumeration. Let 
 be the resulting subenumeration of its members in 
. These define in 
 the recursive sets. Then also 
 is
recursive, hence is defined in 
 by 
, say, so that 
.
However, this equivalence yields for n = m the contradiction 
.
In 6.5 we need a p.r. “substitution” function and in 7.1 a generalization. Let 
 denote the Gödel number of the “cipher term” 
. Then 
 is p.r., since 
 and 
. Let 
 and define 
 inductively on the length n of 
 by sb ∅ (m) = m and 
. Here x 1, …, x n , x denote distinct variables.
Clearly, the 
 are all p.r.
Let 
 denote the Gödel number of the formula 
 that arises from φ
by stepwise substituting 
 at the free occurrences of x i in φ for i = 1, …, n (cf.

also page 60). Then the main property of the p.r. functions 
 is expressed by
Theorem 4.6.
 , for arbitrary 
 and all 
 .
Proof. Since 
 results from applying simple substitutions stepwise, we need
only show that 
 for all 
, and 
. This holds,
since 
 in view of ( ∗ ) from page 232.
___
Example. Let α be Sx = y. Then 
 for 
. Further,
. This equation will be
generalized in Exercise 3c.
Exercises
1. Let a, b, a0, …, an (n > 0) be positive natural numbers and p a prime.
Prove (a) 
, (b) 
 for some ν ≤
n, and (c) lcm{aν∣ν < n} and a n are coprime provided a0, …, an are
pairwise coprime.
 
2. Provide a defining Σ 1-formula for the prime enumeration n↦p n .
 
3. Expand the 
-structure 
 by the functions 
, and all sb\vec{x},
and if necessary, further p.r. base functions to a structure 
. Then the
terms 
 for 
 are well defined in the theory of 
. 5
Verify for arbitrary 
 the following equations in 
:
(a)
, and analogously for ¬, →,
and ∀.
 
(b)
, where \vec{x} ′ covers all 
 such
that 
.
 
 

(c) Let 
. Then 
 for t ∈ { 0,
y, Sy} in case x ∈ { free} φ and 
; otherwise 
.
 
6.5 The Theorems of Gödel, Tarski, Church
Call a theory 
 arithmetizable if 
 is arithmetizable and a sequence
(\underline{n}) n ∈ ℕ of constant terms is available such that 
 for n≠m
and 
 is p.r. These are minimal requirements that representability of
arithmetical predicates in T make sense. They are trivially satisfied for T ⊇ Q,
but also for 
 with respect to ω-terms (page 115). Terms and formulas are
coded within T, similarly to what is done in theories in 
. In particular, 
 denotes the already defined Gödel term of a formula α. In order to
evoke a concrete picture of the following two fairly general lemmas, take 
 and 
 as standard examples.
A sentence γ is called a fixed point of α = α(x) in T if 
;
equivalently, 
. In intuitive terms, γ then says “α applies to me.”
The p.r. function 
 from 6.4 is representable in T under relatively weak
assumptions by Theorem 4.2. Hence, the lemmas below have a large spectrum of
application.
Fixed point lemma. Let T be an arithmetizable theory and suppose that 
is representable in T. Then for each 
 there is some 
 such that
(1) 
.
Proof. Let x 1, x 2, y≠x and 
 be a formula representing sb x in T.
Then 
 for all 
 and all n. With 
 we then
get
(2) 
.
Let 
. Then 
 yields what we require.
Indeed,

A fixed point can in the most interesting cases of α be constructed fairly
easily; see 7.5. The following lemma also formulates a frequently appearing
argument.
Nonrepresentability lemma. Let T be a theory as in the fixed point lemma.
Then T (more precisely ) is not representable in T itself.
Proof. Let T be represented by the formula τ(x). We show that even the
weaker assumption (a): 
 leads to a contradiction.
Indeed, let γ be a fixed point of ¬τ(x) according to (1), so that (b): 
. Choosing α = γ in (a) clearly yields with (b) the
contradiction 
. __ __
We now formulate Gödel’s first incompleteness theorem, giving three
versions, of which the second corresponds essentially to the original. For
simplicity, let henceforth 
 and T ⊇ Q, ensuring the applicability of the
two lemmas above. However, all of the following holds for theories T, such as 
, in which Q is just interpretable in the sense of 6.6.
Theorem 5.1 (the popular version).
Every consistent (recursively) axiomatizable theory T ⊇ Q is incomplete.
Proof. If T is complete then it is recursive by Theorem 4.4, hence representable
in T by Theorem 4.2, which is impossible by the nonrepresentability lemma. __
__
Unlike the proofs of Theorems 5.1′ and 5.1′, the above proof is
nonconstructive, for it does not explicitly provide a sentence α such that 
and 
.
Stronger than the consistency of T is the so-called ω-consistency of 
, i.e., for all φ = φ(x) such that 
 we have 
 for at least one n, or
equivalently, if 
 for all n, then 
. Clearly, if 
 then T is
surely ω-consistent, because the supposition 
 and 
 for all n
implies the contradiction 
. Thus, from a semantic perspective the
theories Q and 
 are certainly ω-consistent, hence also consistent.6
Theorem 5.1′ (the original version). For every ω-consistent theory T ⊇ Q

axiomatized by a p.r. axiom system X, there is a Π1-sentence α such that neither
⊢T α nor ⊢T ¬α, i.e., α is independent in T. There is a p.r. function that assigns
such an α to a formula representing X.
Proof. Let bew T be represented in T by the Σ 1-formula 
, see page
247. For 
 from Corollary 4.3 we obtain (a): 
, for all φ. Let γ be a fixed point of 
 according to
the fixed point lemma, so that (b): 
. The assumption 
 yields
 by (a), but 
 by (b), contradicting the consistency of
T. Thus, 
. Now assume 
, so that 
 by (b); hence (c): 
. Obviously 
, because T is consistent. Applying Corollary
4.3 once again, we infer that 
 for all n. However, this and (c)
contradict the ω-consistency of T. Consequently 
 is impossible as well.
Thus, γ is independent in T. But then too is the Π1-sentence 
,
which is equivalent to γ in T. The claim of the p.r. assignment follows evidently
from the construction of γ in the proof of (1). ___
This theorem remains valid without restriction if the axiom system X is just
r.e. In this case X can be replaced by some recursive X′ (Exercise 1 in 6.2), so
that { bew} T is still recursive according to Theorem 2.4.
Theorem 5.1′ (Rosser’s strengthening of Theorem 5.1′ ). The assumption
of ω-consistency in Theorem 5.1′ can be weakened to the consistency of T.
Proof. Instead of bew(y, x) we consider the arithmetical predicate
where { bew} is { bew} T and T is consistent. We think here of the p.r. function 
 as having been eliminated in the usual way by a formula representing it.
Because of the consistency of T, { prov}(x) says essentially the same as bwb(x)
and has the following fundamental properties: (a) ⊢ T α ⇒ ⊢ T { prov}( ⌜ α ⌝ ),
(b) ⊢ T ¬α ⇒ ⊢ T ¬{ prov}( ⌜ α ⌝ ). 7
Indeed, suppose ⊢ T α, so that 
 for some n (Corollary 4.3). Since
⊬T ¬α, it follows that 
 for all k. Therefore, C5 in 6.3 gives 
, and so
whence particularization yields the claim ⊢ T { prov}( ⌜ α ⌝ ). Proof of (b):
Suppose ⊢ T ¬α, say 
. Then 
 by C5,

since ⊬T α. This gives 
 by C6. Because of 
 (choose 
 for z) we clearly obtain that 
. This confirms (b). Now
let (c): γ ≡ T ¬{ prov}( ⌜ γ ⌝ ) by (1). The assumption ⊢ T γ yields with (a) and
(c) the contradiction ⊢ T { prov}( ⌜ γ ⌝ ), ¬{ prov}( ⌜ γ ⌝ ), and the assumption
⊢ T ¬γ yields with (b) and (c) the same contradiction. Thus, neither ⊢ T γ nor ⊢
T ¬γ. __ __
 is called ω-incomplete if there is some φ = φ(x) such that 
for all n and yet 
. We claim that 
 is not only incomplete but ω-
incomplete. Let 
 and 
. By Theorem
5.1′, 
, that is, 
. On the other hand, since
⊬PA γ, we know that 
 for all n (Corollary 4.3) which
confirms our claim. Note that φ(x) is even a Π1-formula, which is particularly
interesting.
 is said to be true in 
 if 
. In particular, 
 is called true
(more precisely, true in 
 or true in reality, as some people like to say) if 
.
If there is some 
 with a single free variable such that 
, for all α ∈ ℒ0, it is said that truth of 
 is definable in 
.
Clearly, this is equivalent to the representability of 
 in 
. For 
,
however, such a possibility is excluded by the nonrepresentability lemma. We
therefore obtain
Theorem 5.2 (Tarski’s nondefinability theorem).
The notion of truth in 
 is not definable in 
 ; in other words, 
 is not
arithmetical.
In this theorem lies the origin of a highly developed theory of definability in 
(see also 6.7). The theorem holds correspondingly for every domain of objects 
whose language is arithmetizable and in which the function sb x is representable
for some variable x.
We now turn to undecidability results. First of all we prove the claim in
Exercise 1 in 3.5 in a somewhat stronger framework: ‘decidable’ will now have
the precise meaning of ‘recursive’.
Lemma 5.3.

Every finite extension T′ of a decidable theory T of one and the same
(arithmetizable) language  is decidable.
Proof. Suppose T′ extends T by α0, …, αn. Put 
, so that 
.
Since 
, we obtain
Now, 
, 
, and 
 are recursive. Hence the same applies to 
. __ __
That T′ belongs to the same language as T is important. A decidable theory T
axiomatized by 
 but considered as a theory in 
 with the same
axiom system X may well be undecidable, due to a higher complexity of the
additional tautologies of ℒ′.
 is called strongly undecidable if T0 is consistent and each theory 
 compatible with T0 (i.e., T + T0 is consistent) is undecidable. Then each
T compatible with T0 in a language 
 is also undecidable, for otherwise 
 would clearly be decidable. If T0 is strongly undecidable then so is every
consistent T1 ⊇ T0, for if T is compatible with T1 then it is also compatible with
T0. Moreover, each subtheory of T0 in 
 is then undecidable, or T0 is
hereditarily undecidable in the terminology of [TMR]. The weaker a strongly
undecidable theory, the wider the scope of applications. This will become plain
by means of examples in the next section. The following theorem is the main
result from [TMR].
Theorem 5.4.
Q is strongly undecidable.
Proof. Let T ∪Q be consistent. Assume T is decidable. Then the same does hold
for the finite extension 
 of T; Lemma 5.3. Thus, by Theorem 4.2, T′ is
representable in itself, which is impossible by the nonrepresentability lemma. __
__
Theorem 5.5 (Church’s undecidability theorem).
The set 
 of all tautological sentences is undecidable for 
 .
Proof. 
 is surely compatible with Q and hence is undecidable by Theorem
5.4. ___
This result readily carries over to the language with a single binary relation,

as will be shown in the next section, and hence to all expansions of this
language. Indeed, it carries over to all languages with the exception of those
containing unary predicate symbols only and at most one unary function symbol.
For the tautologies of these languages there exist various decision procedures;
see [ML, vol. I].
By Theorem 5.4, in particular 
 is undecidable; likewise is every
subtheory of 
, for instance Peano arithmetic 
 and each of its
subtheories, as well as all consistent extensions of 
, because these are all
compatible with Q. 
 is not even axiomatizable, since an axiomatizable
complete theory is decidable. Further conclusions concerning undecidable
theories will be drawn in 6.6.
Alongside undecidability results concerning formalized theories, numerous
special results can also be obtained in a similar manner; for instance negative
solutions to word problems of all kinds, and halting problems (see e.g. [Rog] or
[Bar, C2]). Of these perhaps the most spectacular was the solution to Hilbert’s
tenth problem: Does an algorithm exist that for every polynomial 
 with
integer coefficients decides whether the Diophantine equation 
 has a
solution in ? The answer is no, as Matiyasevich proved in 1970.
We briefly sketch the proof. Note first that it suffices to show that no
algorithm exists for the solvability of all Diophantine equations in 
. Indeed, by
a famous theorem from Lagrange, every natural number is the sum of four
squares of integers. Consequently, 
 is solvable in  iff 
is solvable in . Thus, if
we could decide the solvability of Diophantine equations in , then we could
solve as well the corresponding problem in . For the latter notice first of all
that the question of solvability of 
 in natural numbers is equivalent to the
solvability of a Diophantine equation of 
 (i.e., an equation 
), by
simply bringing all terms of 
 preceded by a minus sign “to the other side.”
Thus, Hilbert’s problem is reduced to the question of a decision procedure for
the problem 
, where 
 runs through all Diophantine equations 
) in 
.
The negative solution to the last question follows easily from the much
further-reaching Theorem 5.6, which establishes a surprising connection
between number theory and recursion theory, proved in detail for instance in
[Mat]. This theorem is a paradigm of the experience that the solution of certain

mathematical questions lead to results whose significance extends way beyond
that of an answer to the original question.
Theorem 5.6.
An arithmetical predicate P is Diophantine if and only if P is recursively
enumerable.
To give at least an indication of the proof, let the Diophantine predicate 
be defined by 
, with the equation 
, 
.
The defining formula for P is Σ 1, and since 
 is recursive by Theorem
4.5, P is r.e. This is the trivial direction of the claim. The converse—every r.e.
predicate is Diophantine—is too large in scope to be given here. Much tricky
inventiveness is used in order to show that certain arithmetical predicates and
functions are Diophantine, among them the ternary predicate ‘a b = c ’, which for
a long time resisted the proof of being Diophantine. Theorem 5.6 yields
Corollary 5.7.
(a) Hilbert’s tenth problem has a negative answer. (b) For every axiomatizable
theory T ⊇ Q , in particular 
 , there is an unsolvable Diophantine
equation whose unsolvability is provable in T.
Proof. 
 is r.e. by 6.2. Hence, by Theorem 5.6, there is a Diophantine
equation 
 such that 
. We claim that even for the
set 
 of equations it is undecidable whether 
.
Otherwise, 
 and hence also { bwb} Q would be recursive.
This is a contradiction to Theorem 5.4 and proves (a). (b): If the unsolvability of
every unsolvable Diophantine equation 
 were provable in T, then either 
 (provided 
 is unsolvable) or else 
, for T is Σ 1-
complete. Since the theorems of T are r.e., one would then have a decision
procedure for the solvability of Diophantine equations, which contradicts part
(a). ___
Theorem 5.6 can be yet further strengthened; namely, it can be proved within
. Thus, one obtains the following theorem, whose name stems from
Matiyasevich, Robinson, Davis, and Putnam, all of whom made significant
contributions to the solution of Hilbert’s tenth problem. Because of its lengthy
proof, we shall not use this theorem, though in fact many things would thereby
be simplified.

MRDP theorem. For every Σ 1 -formula α there exists an ∃-formula φ in 
 such that 
 . Here φ is without loss of generality of the form 
with certain 
 -terms s,t.
Π 1-formulas and -sentences have a corresponding simple representation. A
famous example of a Π1-sentence is Goldbach’s conjecture 
.
(each even number ¿2 is a sum of two primes). ( ∗ ) represents an example
of a Π1-sentence in 
 whose truth in 
 is still unknown, hence may be
independent in 
. Clearly, if ( ∗ ) is false then its negation is provable, even in
Q. But ( ∗ ) may be true and nevertheless unprovable.
Fermat’s conjecture, which has a still longer history and was finally proved
at the end of the twentieth century, is the statement 
.
This is equivalent to a Π1-sentence, because 
 is not only Σ 1- but
even Δ0-definable in 
, as was noticed in Remark 1 in 6.3. Hence, ( † ) is a
candidate for a sentence that may be independent in 
.
Remark. It would be interesting to discover whether the proof of Fermat’s
conjecture or a suitable modification of this proof can be carried out in 
. A
demonstration that this is not the case would hardly be less spectacular than the
solution of the problem itself. However, it seems that the proof can be carried out
in a suitable conservative extension of 
 (communicated by
G. Kreisel). Note also the following: Since 
 is ω-incomplete
already for Π1-formulas (see page 253), it may even be the case that 
for every single n > 2, although ( ∗ ) is not provable in 
.
Similarly, it may well be that 
 is true for
each n > 1 but ( † ) is still unprovable. This would be no less sensational than a
proof of Goldbach’s conjecture itself.
Exercises
1. Show that an ω-incomplete theory in 
 has
a consistent but ω-inconsistent extension.
 
2. Suppose T is complete; prove the equivalence of
 

(i) T is strongly undecidable, (ii) T is hereditarily undecidable.
3. A consistent theory 
 is
called essentially undecidable if each consistent T ⊇ T 0 is undecidable.
Show that a finitely axiomatizable theory T is essentially decidable iff T
is strongly undecidable.
 
4. Let Δ be a finite list containing explicit definitions of new symbols in
terms of those occurring in 
. Show that if T is
decidable then so is T + Δ (independent of whether all definitions in Δ
are legitimate in T; in the worst case T + Δ is inconsistent).
 
5. Construct a primitive recursive function 
 such that { ran} f is not
recursive (although it is surely recursively enumerable).
 
6.6 Transfer by Interpretation
Interpretability is a powerful method to transfer model-theoretic and other
properties, such as undecidability, from one theory to another. Roughly speaking,
interpreting a theory 
 into a theory 
 means to make the basic notions
of T0 understandable in T1 via explicit definitions. ‘for all x’ from T0 is replaced
in T1 by ‘for all x ∈ P ’, where P is a new unary predicate symbol for the
domains of T0-models, i.e., the T0-quantifiers run over the subdomains 
 of the domains

of the T1-models 
. We consider the most important concepts,
interpretability from Tarski (also called relative interpretability) and
interpretability from Rabin, called model interpretability. All theories considered
in this section are supposed to be consistent.
Let P be a unary predicate symbol not occurring in T1. The formula φP, the
P-relativized of a formula φ, results from φ by replacing all subformulas of the
form ∀xα by ∀ x(Px → α). A precise definition of φ P runs by induction: φ P = φ
if φ is a prime formula, ( ¬φ) P = ¬φ P , (φ ∧ ψ) P = φ P ∧ ψ P , and ( ∀ xφ)P =
∀x(P x → φP), so that φP = φ for open φ. One readily confirms 
. The right-side formula shows clearly what relativation is intending to mean. Set
for 
.
Example. 
. The last equivalence results from 
, cf. (12) in 2.4.
Definition. 
 is called interpretable in
 (where for simplicity we assume that T0
has finite signature) if there is a list Δ of explicit definitions legitimate in T1 of
the symbols of T0 not occurring in T1 and of a new unary predicate symbol P
such that T0P ⊆ T1 + Δ, the definitorial extension of T1 by Δ.
This definition expresses only that all notions of T0 “are understood” in T 1 ,
and what is provable in T0 is also provable in T 1 . Examples will be given later.
The theory T + Δ ( 
 ) will henceforth be
denoted by 
 , and its language by 
 . Interpretability generalizes
the notion of a subtheory: If T 0 ⊆ T 1 then T0 is trivially interpretable in T 1 ,
i.e., only the trivial relativation 
 belongs to Δ. In this case α
P ≡ α.
Let 
 denote the set of the so-called closure axioms ∃xP x,
Pc, 
).
These are equivalent to ( ∃x x = x)P, ( ∃x x = c) P , and 

, respectively. Thus, 
 is up to equivalence a set of the form F
P for some finite set F of 
 -tautologies, so that 
 for
each theory 
. The sentences of 
 guarantee that for a given 
-structure 
 there is a welldefined 
-structure 
 whose domain is 
. The
relations and operations of 
 are the ones defined by Δ but
restricted to A. This structure 
 will be denoted by 
. It is a substructure of the 
-reduct of 
, whose role will
become clear in the next lemma.
Lemma 6.1.
Let 
 . Then 
, for all sentences α of the language 
.
Proof. 
 is an 
-structure. Claim: 
, for any w { Var} → A. This proves the lemma, since α is a sentence. We prove
the claim by induction on 
 . It is clear for
prime formulas π since α P = π. The induction steps for ∧, ¬proceed without
difficulty, and the one for ∀is obtained as follows:
Remark 1. If T0 is axiomatized by X 0 then in the definition of interpretability it
suffices to require just 
instead of 
. That is, we have only to check 
for the axioms α of T0, and 
 (cf. the example
below). This fact is highly important. It follows immediately from

( ∗ ) 
(
).
For proving ( ∗ ) let 
 and 
. Then 
 by the lemma. Thus, 
 because 
 ,
and so 
 .
Since 
was arbitrary, we get 
.
Theorem 6.2.
Let T0 be interpretable in T1. If T0 is strongly undecidable so is T1.
Proof. Let 
 be compatible with T1. Then
T + T1 is consistent and so is 
. Now, 
is a theory, for 
and ( ∗ ) yield S ⊢ α ⇒ T Δ ∪CA ⊢ αP ⇒ α ∈ S. Let 
. Thus, 
 by Lemma 6.1; hence S is
compatible with T0 and so undecidable. If T were decidable, then so would be 
 (Exercise 3 in 6.5). Hence also 
 (Lemma 5.3), and so
clearly S. This is a contradiction. ___
Example. Q is interpretable in the theory T d of discretely ordered rings 
 . These have a smallest positive element
e, defined by 
,
that need not be a unit element of 
. Ring multiplication is
denoted by ×, to distinguish it from multiplication in Q. Here are the definitions
for P, S, ⋅ (0, + remain unaltered):

With some patience, all P-relativized Q-axioms can be proved in 
, with the list Δ of the above definitions.
Thus, by Remark 1, Q is interpretable in T d, and T d is strongly undecidable
according to Theorem 6.2.
While Q is not directly interpretable in the theory T F of fields, it is in a
certain finite extension of T F, whereby T F is shown to be undecidable (Julia
Robinson). The same also holds for the theory of groups T G [TMR]. However,
none of these theories is strongly undecidable.
Q and also 
 are interpretable in 
 ,
as is nearly every other theory. Let 
 , and define S, +, ⋅
within ZFC such that their restrictions to ω coincide with the usual operations.
In particular, S is defined by 
 . This immediately yields
the incompleteness and the undecidability of 
 , assuming of
course its consistency. Q is also interpretable in weak subtheories of 
 , e.g., in the Tarski fragment TF, by which we mean the
theory in 
 with the following three axioms.8 Hence, like
Q, the theory TF is strongly undecidable.
In particular, the set of tautologies in a binary relation is undecidable, even
without identity in the language; for = can conservatively be eliminated from 
 by means of 
. Q is surely
interpretable in 
, and 
 in turn in 
 with 
. This is a consequence of
Lagrange’s theorem. Hence, 
 is strongly
undecidable, and thus every subtheory is undecidable, e.g., the theory of
commutative rings. 
 and 
 have the same degree of complexity,
because 
 is (in various ways) interpretable in 
; Exercise 3.

Remark 2. Remarkable is the mutual interpretability of 
 and 
 , the theory of
(hereditarily) finite sets. It arises from 
 by replacing AI
by the schema 
and 
 by the schema of foundation 
; see [De] or [Ra4]. A surprisingly simple interpretation of 
 in 
 was given by
Ackermann in [Ac]: Each natural number represents a finite set in 
 (relativation is trivial), and the ∈ -relation is defined by 
. 9 For instance, 0
represents ∅, since b i 0 = 0 for all i. 1, 2, and 3 (more precisely, 
, 
 , and 
 ) represent
 , 
 , and 
 , respectively. To see that 3
represents 
 , notice that b 0 3 = b 1 3 = 1 and b i 3 = 0 for i > 1.
See also [Fi] for more details.
We now describe a related notion of interpretability. For simplicity, we omit
some details. Let K 0 and K be nonempty classes of 
 -
and 
 -structures, respectively. Further, let Δ be a list of
definitions of the 
-symbols and a predicate symbol P,
and let 
, 
, and 
 for 
 be
defined as above. 
 denotes the
expansion of 
 in 
 according to Δ (the Δ-expansion
of 
). If 
 is a sentence, let 
 denote the class of all 
 for 
 such
that 
.
Definition. K 0 (or { Th} K 0) is called model interpretable in K (or in { Th}
K respectively) if for suitable Δ and a suitable sentence 
,

(1)
 and 
 for each 
,
 
(2) For every 
 there is some 
 such that 
.
 
Clearly, we can construct as in 2.6 for each sentence 
 a reduced sentence 
 such that
(3)
, for all 
.
Theorem 6.3.
Let K 0 be model interpretable in K. If { Th} K 0 is undecidable then so too is {
Th} K.
Proof. Put 
for 
. It suffices to prove 
, because a decision procedure for { Th} K then clearly extends to { Th} K 0. ⇒ :
Let K 0 ⊨ α, 
 , 
 so that 
 by (3), and 
. By (1), 
 . Thus, 
 . Hence 
 by
Lemma 6.1, and so 
. This confirms 
 for all 
 , in other words, 
 . ⇐ :

Assume that K 0 ⊭ α, say 
 . Choose some ℬ
∈ K γ according to (2), so that 
 .
Then 
; hence 
. This confirms ( ∗ ). __ __
Example. The class K 0 of all graphs ( A, R) is model interpretable in the
class K of simple graphs (B, S), i.e. S is irreflexive and symmetric. The figure
below shows some 
 with aRa, aRb, bRa, bRc,
and on the right some 
 , called the encoding structure of 
 because it completely describes 
 in order
to satisfy (2). Roughly put, a set N of new points is adjoined to A so that B = A
∪N. Since the edges in 
 are undirected, we need new points
for coding the edge directions of 
.
The “old” points in 
 , the larger dots in the figure, are the
ones from 
 .
Such a point neighbors three or two endpoints (i.e., points in which only one S-
edge ends), depending on whether the point is reflexive in 
 or
not (only a is reflexive). Informally, the definition for R in 
reads as follows: “xRy iff 
 and
either x = y and x neighbors three endpoints, or there exists exactly one new
point z such that xSzSy, or there are exactly two new points u, v such that
xSuSvSy and uSy.” γ is rendered informally into “ ∃x P x and all new points are
either endpoints or neighbor precisely two old points or one old and one new
point.”
In the example, { Th} K 0 is the logical theory of a binary relation, already
established as undecidable. Accordingly the theory of all simple graphs is
undecidable. The latter can be used to show, for instance, that the theory { SL}
of semilattices is undecidable. By Theorem 5.4 the same then follows for the
theory { SG} of semigroups, for { SL} is a finite extension of { SG}. In order to
show that { Th} K 0 is model interpretable in { SG}, it suffices to provide,

similarly to the last example, for a simple graph (A, S) the encoding semilattice
(B, ∘). The figure on the left shows the ordering diagram of B for 
 and 
; here S is understood as
a set of edges. The old points are precisely the maximal points of B. By
construction, B has a smallest element 0 and is of depth 3, that is, there are at
most three consecutive points in B with respect to <. This must now be expressed
by the sentence γ required in the definition of model interpretability.
The theory of finite simple graphs with or without some additional feature
(for instance planarity) is undecidable; see e.g. [RZ]. The above construction
shows that the undecidable theory of finite simple graphs is model interpretable
in the theory of finite semilattices, which hence is undecidable. This clearly
implies the undecidability of the theory FSG of finite semigroups. Setting an
element on top of the maximal elements in the last figure results in the order
diagram of a finite lattice, so that the theory of finite lattices turns out to be
undecidable. The same holds for the theory FPO of finite partial orders because
for the description of ( A, S) only the partial order of B is relevant.
Remark 3. Somewhat more mathematics is required to prove the
undecidability of the theory FDL of all finite distributive lattices. The previous
figure illustrates that also FPO is undecidable. But FPO is model interpretable in
FDL, in that one identifies the elements of g with the ∩ -irreducible elements of
the lattice, 
 say. Here we need to know that 
’s structure is completely determined by the partial order of
its irreducible elements and that this order can be given completely arbitrarily.
Positive results are also transferable. For instance, the (logical) theory of a
unary function is interpretable in the first-order theory of (undirected) trees
[KR], and with the latter the former is also decidable. The decidability of the
theory of a single unary function was first proved by Ehrenfeucht with a
different method. We mention that the theory of two or more unary functions is
undecidable. Decidability of the theory of simple trees also follows from the
decidability of the second-order monadic theory of binary trees [Bar, C3], a very
strong result with an immense scope of applications. One of these applications is
a simple proof of decidability of all modal systems considered in Chapter 7 (see
e.g. [Ga]).
Exercises
1. Show that if a theory T 0 is essentially undecidable and interpretable in 
 then T1 is essentially
 

undecidable as well.
2. Show (informally) that 
 is interpretable not only in 
 but also in 
 .
( Attention: ω is no longer a set in 
.)
 
3. Show in detail that 
 is interpretable in 
.
 
4. Prove that all axioms of 
 derive
from TF + Sfin + Sfnd. This makes interpretability of 
 in 
 an easy
task.
 
6.7 The Arithmetical Hierarchy
We now add a little more on the complexity of predicates of ℕ including subsets
of 
 . The set of the Gödel numbers of all sentences valid in 
 is an example of a rather simply defined nonarithmetical
subset of 
 ; by Theorem 5.2 it has no definition in 
. 10 However, relatively simply defined arithmetical
sets and predicates may be recursion-theoretically highly complicated. It is
useful to classify these according to the complexity of the defining formulas.
The result is the arithmetical hierarchy, also called the first-order Kleene–
Mostowski hierarchy. The following definition builds upon the one in 6.3 of the
Σ 1- and Π1-formulas and the Σ 1-, Π1-, and Δ1-predicates defined by these.
Definition. A Σ n + 1-formula is a formula of the form 
 , where α is a Πn-formula from 
; analogously, we call 
 a 
-formula if
β is a Σn-formula. Here 
 are arbitrary tuples of variables.
A Σn-predicate (resp. Πn-predicate) is an arithmetical predicate P defined in 
 by a Σn-formula (resp. Πn-formula). If P is both Σn and Πn
(i.e., a Σn- and Πn-predicate) then we say that P is a Δn-predicate, or P is Δn for
short. We denote by Σn, Πn, and Δn the sets of the Σn-, Πn-and Δn-predicates,

respectively. In addition, 
.
According to this definition, a Σn-formula is a prenex formula φ with n
alternating blocks of quantifiers, the first of which is an ∃-block, which may also
be empty. φ’s kernel is Δ0. Clearly, each φ ∈ ℒar is equivalent to a Σn-or Πn-
formula for a suitable n, for φ can be brought into prenex normal form and the
quantifiers can be grouped into blocks of the same quantifiers. Obviously, Δn ⊆
Σn, Πn. When considering the hierarchy it is convenient to have Σn-and Πn-
formulas closed under equivalence in 
. Hence, we say that α
isΣ n or Π n to indicate that α is equivalent to an originalΣ n - or Π n -formula,
respectively. Note that since 
 in case 
,
every Σ n - or Π n -formula is also both Σ n + 1 and Π n + 1. Therefore 
. This yields the following
inclusion diagram, where all the inclusions, indicated by lines, are proper:
We have already come across Σ 1 -, Π1-, and Δ1-predicates; for instance, the
solvability claims of Diophantine equations are Σ 1, and the unsolvability claims
are Π1. Below we provide an example of a Π 2-predicate. It is also convenient to
say that Σ n - and Π n -sentences define 0-aryΣ n - and Π n -predicates,
respectively. In this sense the consistency of 
 (in
arithmetical terms 
)
is Π 1 , the incompleteness of 
 is Σ 2, and the ω-consistency
is Π 3 (Exercise 3). The hierarchy serves various purposes. More recent
investigations have considered also Δ0-or Σn-or Π n-induction. Here the schema
IS is restricted to the corresponding class of formulas. An example is IΔ0
mentioned on page 239.
As already shown in 6.4, the Σ 1 -predicates are the recursively enumerable
ones, the Π1-predicates their complements, and the Δ1-predicates are exactly the
recursive predicates, which are the ones whose complements are r.e. as well.
Thus, we are provided with a purely recursion-theoretic way of regarding Σ 1,
Π1, and Δ1. This underscores the importance of the arithmetical hierarchy,
which is fairly stable with respect to minor changes in the definition of Δ0.

In view of Theorem 5.6 one could begin, for instance, with a Δ0 consisting of
all polynomially (or equivalently, quantifier-free) definable relations. In some
presentations, a system of formulas is effectively enumerated (and denoted by
Δ0) that define exactly the p.r. predicates in 
 . Section 7.1 will
indicate how such a system can be defined. Between these and the Δ0-formulas
(which themselves may still be classified) lie many r.e. sets of formulas that
define computable functions significant in both the theory and practice of
computability, e.g., the elementary functions mentioned in the introduction to
this chapter.
However, by Remark 2 in 6.4 we know that there exists no effectively
enumerable system of formulas in 
 through which
all recursive, or equivalently all Δ1-predicates, are defined, so that the definition
of the arithmetical hierarchy cannot start in a feasible manner with a
representative “set of Δ1-formulas.”
Remark.We mention that the first-order arithmetical hierarchy considered so
far extends in a natural way to the second-order arithmetic. Also this extended
hierarchy is closely related to recursion theory (see e.g. [Shoe]). A treatment lies
outside the scope of this book.
Similarly to the case n = 1, one readily shows that a conjunction or
disjunction of Σn-formulas is equivalent to some other Σn-formula; likewise for
Πn-formulas. The negation of a Σn-formula is equivalent to a Πn-formula, and
vice versa; this is certainly correct for n = 1, which initiates an easy induction on
n. The complement of a Σn-predicate is therefore a Πn-predicate, and vice versa.
From this it easily follows that Δn is closed under all the mentioned operations,
including complementation.
By “compression of quantifiers,” the idea of which was illustrated in
Exercise 1 in 6.3, one obtains a somewhat simpler presentation of the
quantifier blocks. The ∃- and ∀-blocks can each be collapsed into one
quantifier. This procedure is fairly easy, provided we are dealing with
equivalence in 
 as is the case here, and not in a possibly
too weakly axiomatized theory over 
 (in fact, 
 would suffice for the proof):
Theorem 7.1.
A (proper) Σ n -predicate is definable by a prenex formula 
with a Δ0-formula α, where 
 is either the ∀- or ∃-
quantifier, depending on whether n is even or odd. Similarly, a Π n -predicate is

definable by a formula ∀x 1 ∃x 2 ⋯Q n x nα.
Proof by (simultaneous) induction on n. Exercise 1 in 6.3 formulates the case for
Σ 1 - and for Π1-predicates. Assume that this is the case for n and let ∃\vec{ x}α
be the defining formula of a Σ n + 1 -predicate, where α defines a Π n -predicate
and 
 is a block of length m ≥ 1. By using the (defining Δ0-
formula of the) pairing function, 
 can be compressed
stepwise to a single ∃-quantifier ∃x. The arising bounded quantifiers commute
with the following ∀-block (Exercise 1). The case m = 0 can also be included in
the argument, using a “vacuous quantifier” ∃x (i.e., x∉{ var} α). The Π n + 1-
formulas are treated completely analogously. ___
It is quite often a nontrivial task to determine a welldefined predicate’s exact
position in the arithmetical hierarchy, or better, like every fastidious game, it
requires sufficient training. In the example below, we consider a set that is
neither recursive nor r.e. For the sake of simplicity, we apply Church’s thesis in
one place, although it can be eliminated using a little recursion theory, as was
demonstrated previously in the proof of Theorem 4.4. The example is also a
good preparation for 7.6.
Example. Let 
 denote the set of the 
 that represent in Q recursive
subsets of 
 . Thus, the 
have at most one free variable, namely the first one. For instance, all Δ0-
formulas in 
 belong to this 
. Since 
 and 
 are
recursive and 
, all
members of the set 
also belong to 
, because each α ∈ Q trivially
represents ℕ, and each α with 
 represents ∅.
Conversely, each closed formula of 
 belongs to 
. Obviously then, 
.
We now show that 
 is arithmetical; more precisely,
it is Π 2 , and indeed properly Π 2, that is, neither Σ 1 nor Π 1. By definition,

is a proof for 
 or for 
.
This equivalence readily yields a definition of ℒ r (more exactly, of 
) by a Π 2 -formula φ( x) (that is, a ∀-formula
∀xα such that α is Σ 1). Let the p.r. predicate ‘
’ be Σ 1 -defined by the formula λ 1(x). With
, we then set
More precisely, φ should be the reduced in ℒar after eliminating the
occurring p.r. function terms using more ∃-quantifiers inside the brackets. Thus,
φ describes a Π 2-formula, that is, 
 is Π 2 . It is not Σ 1,
because ℒ r is not r.e. by Remark 2 in 6.4, nor is it Π1. Indeed, assume this were
the case; then 
would also be Π1, for 
 is Δ1. Now Q∗ is
certainly r.e. and thus Σ 1 , and so by Theorem 4.5, Q ∗ would be recursive. But
then we obtain a decision procedure for Q (hence a contradiction) as follows:
Let α ∈ ℒ0 ar be given. If α∉Q ∗ then also α∉Q; if α ∈ Q ∗, we turn on the
enumeration machine for Q and wait until either α or ¬α appears. This obviously
is a decision procedure for Q.
Special Σ 1 -formulas. We end this chapter with a result useful for
proving the Σ 1-completeness of 
 inside 
 in 7.2, the so-called provable Σ 1 -completeness. It will be
shown that the Σ 1 -predicates are definable without reference to Δ0, using
special Σ 1 -formulas. To this end somewhat stronger axioms are considered than
those of Q, namely the axioms of the theory 
 presented in 6.3.
All axioms of N are derivable in 
, as was pointed out
already on page 235.
Definition. SpecialΣ 1-formulas are defined as follows:
(a) Sx = y, x + y = z, and x ⋅ y = z are special Σ 1 -formulas, where x, y, z
denote distinct variables (the special prime formula condition);
 

(b) if α, β are special Σ 1 -formulas then so too are 
 , 
 , 
 , and 
 , where x, y are
distinct and not in { bnd} α (prime-term substitution), as well as ∃ xα
and ( ∀x < y)α for y∉{ var} α.
 
Theorem 7.2.
Every original Σ 1 -formula is equivalent to a special Σ 1 -formula in the theory
N , thus in 
 and a fortiori in 
 .
Proof. It suffices to verify the claim for all Δ0-formulas, since the set of
special Σ 1-formulas is closed under ∃-quantification. Since
s = t ≡ ∃ x(x = s ∧ x = t) with x∉{ var} s, t,
it is enough to consider prime formulas of the form x = t. For prime terms t
this clearly follows from 
 and 
 . The
induction steps on the operations S, + are obtained as follows:
and similarly for ⋅. The claim holds for all literals because 
 , 
.By Exercise 4 in 6.3 we need only carry out induction on 
 and ( ∃x ≤ t). For ∧, ∨ this is clear. For the
remainder note that ( ∀x ≤ t)α and ( ∃x ≤ t)α are N-equivalent respectively to 
and 
. __ __
Exercises
1. Show that Σn and Πn and hence Δ n are closed under bounded
quantification (bounded quantifiers “commute” with the next ∃- or ∀-
block, Exercise 2 in 6.3).
 

2. Confirm that Δ0 ⊂ Δ1 ⊂ Σ 1 , Π1, which therefore shows that these four
classes of arithmetical predicates are distinct.
 
3. Prove that ω-inconsistency is (at most) Σ 3 . Theorem 7.6.2 will show
that ω-inconsistency is properly Σ 3. Hence, ω-consistency is Π 3.
 
References
Gö2.
_________ , Über formal unentscheidbare Sätze der Principia Mathematica und verwandter
Systeme I, Monatshefte f. Math. u. Physik 38 (1931), 173–198, also in [Gö3, Vol. I, 144–195], [Hei,
592–617], [Dav, 4–38].
Kl2.
_________ , Mathematical Logic, Wiley & Sons 1967.
HB.
D. HILBERT, P. BERNAYS, Grundlagen der Mathematik, I, II, Berlin 1934, 1939, 2⟨{ nd}⟩ ed.
Springer, Vol. I 1968, Vol. II 1970.
Mo.
D. MONK, Mathematical Logic, Springer 1976.
Tu.
A. TURING, On computable numbers, with an application to the Entscheidungsproblem,
Proc. London Math. Soc., 2⟨{ nd}⟩ Ser. 42 (1937), 230–265, also in [Dav, 115–154].
Shoe. J. R. SHOENFIELD, Mathematical Logic, Reading Mass. 1967, A. K. Peters 2001.
Fel1.
W. FELSCHER, Berechenbarkeit, Springer 1993.
TMR. A. TARSKI, A. MOSTOWSKI, R. M. ROBINSON, Undecidable Theories, North-Holland 1953.
Hei.
J. VAN HEIJENOORT (editor), From Frege to Gödel, Harvard Univ. Press 1967.
Kl1.
S. KLEENE, Introduction to Metamathematics, Amsterdam 1952, 2⟨{ nd}⟩ ed. Wolters-Noordhoff
1988.
BA.
A. BERARDUCCI, P. D’AQUINO, Δ0-complexity of the relation y = ∏ i ≤ n F(i), Ann. Pure Appl. Logic
75 (1995), 49–56.
Kra.
J. KRAJÍČEK, Bounded Arithmetic, Propositional Logic, and Complexity Theory, Cambridge
Univ. Press 1995.
HP.
P. HÁJEK, P. PUDLÁK, Metamathematics of First-Order Arithmetic, Springer 1993.
Tak.
G. TAKEUTI, Proof Theory, Amsterdam 1975, 2⟨{ nd}⟩ ed. Elsevier 1987.
ML.
G. MÜLLER, W. LENSKI (editors), The Ω-Bibliography of Mathematical Logic, Springer 1987.

1
2
3
Rog.
H. ROGERS, Theory of Recursive Functions and Effective Computability, New York 1967, 2⟨{
nd}⟩ ed. MIT Press 1988.
Bar.
J. BARWISE (editor), Handbook of Mathematical Logic, North-Holland 1977.
Mat.
Y. MATIYASEVICH, Hilbert’s Tenth Problem, MIT Press 1993.
De.
O. DEISER, Axiomatische Mengenlehre, Springer, to appear 2010.
Ra4.
_________ , Einführung in die mathematische Logik, Wiesbaden 1996, 3⟨{ rd}⟩ ed.
Vieweg+Teubner 2008.
Ac.
W. ACKERMANN, Die Widerspruchsfreiheit der Allgemeinen Mengenlehre, Mathematische Annalen
114 (1937), 305–315.
Fi.
M. FITTING, Incompleteness in the Land of Sets, College Publ. 2007.
RZ.
W. RAUTENBERG, M. ZIEGLER, Recursive inseparability in graph theory, Notices Amer. Math. Soc. 22
(1975), A–523.
KR.
I. KOREC, W. RAUTENBERG, Model interpretability into trees and applications, Arch. math. Logik 17
(1976), 97–104.
Ga.
D. GABBAY, Decidability results in non-classical logic III, Israel Journal of Mathematics 10 (1971),
135–146.
Footnotes
All these predicates are also elementary in the recursion-theoretic sense, see e.g. [Mo], although it
requires much more effort to verify this. Roughly speaking, the elementary functions are the “not too
rapidly growing” primitive recursive functions. The exponential function (m, n)↦m n is still elementary;
however, the hyperexponential function defined on page 239 is not.
The language 
 of 
 is obviously simpler than 
. It contains no composed terms and hence only the simplest possible
equations, which of course simplifies encoding.
‘in T’ will often be omitted; we then always mean ‘in T = Q’. Representable predicates are called
entscheidungsdefinit in [Gö2] (translated as decidable in [Hei]), in [HB] vertretbar, in [Kl1] numeralwise
expressible, and in [TMR] definable.

4
5
6
7
8
9
10
The right side of this equivalence is an informal and more easily readable substitute for the somewhat
lengthy notation 
.
The expansion of 
 alleviates the later transfer of this exercise to 
.
In 7.1 it will be shown that the additional functions of 
are explicitly definable already in 
.
There are famous (relative) consistency proofs for 
 that presuppose considerably less
than the full semantic approach; cf. e.g. [Tak].
In particular 
. That the latter is not the case if
we write bwb instead of { prov} is the import of Gödel’s second incompleteness theorem, Theorem 7.3.2.
Thus, bwb and { prov} behave withinT very differently, although 
.
Claimed in [TMR, p. 34]. The lengthy proof is presented in [Mo, pp. 283–290].
 (the ith binary digit of a; see 6.1) is a p.r. function and hence
explicitly definable in 
, as is every p.r. function according to Theorem 7.1.1.
 is definable only in second-order arithmetic, which along with
variables for numbers has variables for sets of natural numbers. However, the set of sentences α from 
 with bounded quantifier rank, i.e. qrα ≤ n, is definable for each n. In
this sense, 
 has an “approximate” elementary definition.

(1)
Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-
1-4419-1221-3_7, © Springer Science+Business Media, LLC 2010
7. On the Theory of Self-Reference
Wolfgang Rautenberg
1  
Fachbereich Mathematik und Informatik, 14195 Berlin, Germany
Wolfgang Rautenberg
Email: raut@math.fu-berlin.de
Abstract
By self-reference we basically mean the possibility of talking inside a theory T
about T itself or related theories. Here we can give merely a glimpse into this
recently much advanced area of research; see e.g. [Bu]. We will prove Gödel’s
second incompleteness theorem, Löb’s theorem, and many other results related
to self-reference, while further results are discussed only briefly and elucidated
by means of applications. All this is of great interest both for epistemology and
the foundations of mathematics.
The mountain we first have to climb is the proof of the derivability
conditions for 
 and related theories in 7.1, and the derivable Σ 1-completeness
in 7.2. But anyone contented with leafing through these sections can begin
straight away in 7.3; from then on we will just be reaping the fruits of our labor.
However, one would forgo a real adventure in doing so, namely the fusion of
logic and number theory in the analysis of 
. For a comprehensive
understanding of self-reference, the material of 7.1 and 7.2 (partly prepared in
Chapter 6) should be studied anyway.
By self-reference we basically mean the possibility of talking inside a theory T
about T itself or related theories. Here we can give merely a glimpse into this
recently much advanced area of research; see e.g. [Bu]. We will prove Gödel’s
second incompleteness theorem, Löb’s theorem, and many other results related
to self-reference, while further results are discussed only briefly and elucidated

by means of applications. All this is of great interest both for epistemology and
the foundations of mathematics.
The mountain we first have to climb is the proof of the derivability
conditions for 
 and related theories in 7.1, and the derivable Σ 1-completeness
in 7.2. But anyone contented with leafing through these sections can begin
straight away in 7.3; from then on we will just be reaping the fruits of our labor.
However, one would forgo a real adventure in doing so, namely the fusion of
logic and number theory in the analysis of 
. For a comprehensive
understanding of self-reference, the material of 7.1 and 7.2 (partly prepared in
Chapter 6) should be studied anyway.
Gödel himself tried to interpret the notion “provable” using a modal operator
in the framework of the modal system S4. This attempt reflects some of his own
results, though not adequately. Only after 1970, when modal logic was
sufficiently advanced, could such a program be successfully carried out. A
suitable instrument turned out to be the modal logic denoted by G (or GL). The
Kripke semantics for G introduced in 7.4 is an excellent tool for confirming or
refuting self-referential statements. Solovay’s completeness theorem and the
completeness theorem of Kripke semantics for G in 7.5 are fortunately of the
kind that allows application without knowing the completeness proof itself,
which in both cases are not quite easy and use several technical tricks.
There are several extensions of G, for example, the bimodal logic GD in 7.6.
This logic is related to Hilbert’s famous ω-rule. A weakening of it can expressed
by the modal operator 
 of GD. A comprehensive survey can be found in [Bu,
Chapter VII]; see also [Vi2]. In 7.7 we discuss some questions regarding self-
reference in axiomatic set theory.
7.1 The Derivability Conditions
Put somewhat simply, Gödel’s second incompleteness theorem states that 
 cannot hold for a sufficiently strong and consistent axiomatizable
theory T. Here Con T is a sentence reflecting the metatheoretic statement of
consistency of TinsideT, more precisely, inside the (first-order) language 
 of T.
In a popular formulation: If T is consistent, then this consistency is unprovable in
T. As was outlined by Gödel and will be verified in this chapter, the italicized
sentence is not only true but also formalizable in  and even provable in the
framework of T.
The easiest way to obtain Gödel’s theorem is first to prove the derivability

conditions stated below. Their formulation supposes the arithmetizability of T,
which includes the distinguishing of a sequence 
 of ground terms; see
page 225. Let 
 be a formula that represents the recursive predicate bew
T in T as in 6.4. For 
 we write □ (x), and □ α is to mean 
. We may read □ α as “box α” or more suggestively “α is provable in
T,” because □ α reflects the metatheoretic property ⊢ T α inT. If □ refers to some
theory T′≠T then □ has to be indexed correspondingly. For instance, 
 for 
 can easily expressed also in 
. Note that □ α is always a sentence,
even if α contains free variables.
Further, set 
 for α ∈ ℒ. If α is a sentence, 
 may be read as α is
compatible with T, because it formalizes ‘
’, which is, as we know,
equivalent to the consistency of T + α. First of all, we define Con T in a natural
way by
where 
 is a contradiction, 0≠0, for instance. We shall see in a moment that
Con T is independent modulo T of the choice of 
. The mentioned derivability
conditions then read as follows:
D1: 
,
D2: 
,
D3: 
.
Here α, β run through all sentences of . These conditions are due to Löb,
but they were considered in a slightly different setting already in [HB].
Sometimes D2 is written in the equivalent form 
, and D3
as 
.
A consequence of D1 and D2 is D0: 
. This results from
the following chain of implications:
From D0 it clearly follows that 
. In particular, the
choice of 
 in Con T is arbitrary as long as 
.
Remark 1. Any operator 
 satisfying the conditions d1: 
 and d2: 
 thus satisfies also d0: 

, and hence d00: 
, for all 
. It
likewise satisfies 
: 
, for 
, hence 
 in view of d0. The converse direction 
 readily follows from 
 by first applying d0 and
then d2.
Whereas D2 and D3 represent sentence schemata in T, condition D1 is of
metatheoretic nature and follows obviously from the representability of bew T in
T. Thus, D1 holds even for weak theories such as T = Q. On the other hand, the
converse of D1, D1∗: 
, for all 
,
may fail. Fortunately, it holds for all ω-consistent axiomatic extensions T ⊇
Q such as 
. Indeed, ⊬T α implies 
 for all n (Corollary
6.4.3). Hence, 
 in view of the ω-consistency of T, that is, 
.
Unlike D1, the properties D2 and D3 are not so easily obtained. The theory T
must be able not only to speak about provability in T (perhaps via
arithmetization), but also to prove basic properties about provability. D3 is
nothing else than condition D1 formalized within T, while D2 formalizes (7)
from page 206, the closure under MP in arithmetical terms. Let us first realize
that D2 holds, provided it has been shown that D2∗: 
,
where the p.r. functions 
, ∗, and 
 appearing in D2∗ must either be
present or definable in T. Generally speaking, f ∈ F n is called definable in an
arithmetizable theory 
 (with respect to the sequence of terms 
 in T)
if there is a formula 
 such that
(1) (a) 
 for all , (b) 
.
Clearly, f is then also represented by 
. For 
 and related theories,
(1) means that f is explicitly definable in T in the sense of 2.6 and may be
introduced in T (using a corresponding symbol). From now on we will no longer
distinguish between T and its definitorial extensions and apply 
 without comment. This and (1) easily imply 
, e.g. 
. With 
, ⌜ β ⌝ for x, y, we thus obtain from D2∗ in view of 
,

Particularization yields D2. But the real work, the definability of the
functions appearing in D2∗ in theories like 
, still lies ahead.
In order to better keep track of things, we restrict our considerations to the
theories ZFC and 
, which are of central interest in nearly all foundational
questions. ZFC is only briefly discussed. Here the proofs of D2 and D3 (with □
= □ ZFC) are much easier than in PA and need only a few lines as follows: D2∗
and hence D2 are clear, because the naive proof of D2∗ above with bew T = bew
ZFC can easily be formalized inside ZFC. This includes the definability of all
functions occurring in D2∗, for we did define them; for instance, the operation ∗
on page 201 may be defined by setting a ∗ b = ∅ if a∉ω or b∉ω. We
arithmetize 
 according to the pattern in 6.2, encoding formulas with Gödel
numbers, 1 so that 
-formulas are encoded within ZFC by certain ω-terms,
defined in 3.4. Formulas from 
 are identified with their ω-relativized in 
,
called the arithmetical formulas of 
. Moreover, the arithmetical predicate
bew ZFC is certainly representable in ZFC by Theorem 6.4.2, since this theorem
can be viewed, just like every theorem in this book, as a theorem within ZFC.
Thus, the naive proof of D1 based on this theorem (up to Corollary 6.4.3) can as
a whole be carried out in ZFC, and so D3 is proved.
Roughly speaking, D2 and D3 hold for ZFC because ordinary mathematics,
in particular the material in Chapter 6, is formalizable in ZFC. In all of the
above, no special set-theoretic constructions such as transfinite recursion are
needed. Only relatively simple combinatorial facts are required. Hence there is
some hope that the proofs of D2 and D3 can also be carried out in sufficiently
strong arithmetical theories like PA. This is indeed so. The proof of D3 for PA
will need the most effort and will be completed only in 7.2. Our first goal will be
to show that the p.r. functions occurring in D2∗, and in fact all p.r. functions, are
explicitly definable in PA. 2 They turn out to be definable even in a sense
stronger than required by (1) from the previous page.
Definition. An n-ary recursive function f is called provably recursive or Σ1-
definable in PA if there is a Σ 1-formula 
 in ℒar such that
(2) (a) 
 for all 
; (b) 
.
Since PA is Σ 1-complete, 2(a) is equivalent to 
 for all , which
is often more easily verified than 2(a) and could replace 2(a). We will show that
all p.r. functions are Σ 1-definable in PA, which strengthens their explicit

(.)
definability in PA. Thereafter we may treat all occurring p.r. functions in PA as if
they had been available in the language right from the outset. Essentially this
fundamental fact allows a treatment of elementary number theory and
combinatorics within the boundaries of PA and hence is particularly interesting
for a critical foundation of mathematics.
If 
 in (2) is Δ0 then f is called Δ0-definable. An example is the β-
function (Exercise 1), which from now on may be supposed to be present in PA.
Basic for the Σ 1-definability of all p.r. functions is β’s main property, Lemma
6.4.1, of which we need, of course, some provable version in PA. Since Euclid’s
lemma and the Chinese remainder theorem are involved here, these should be
derived first. Clearly, the basic arithmetical laws applied in their proofs in 6.3
should be at our disposal, including those on the order relation and on a − b for a
≥ b, all provable in N.
The proof of Euclid’s lemma is straightforward, Exercise 2. As for the
Chinese remainder theorem, we avoid the quantification over finite sequences
for the time being, by stating the theorem as a scheme. Let c, d denote unary
provably recursive functions, which may depend on further parameters. Each
such c determines for given n the sequence 
, with cν = c(ν) for ν ≤ n. For
suggestive reasons from now on also letters such as 
 may denote variables
in 
. With the Δ0-definable relation 
 of coprimeness, the Chinese remainder
theorem can provisionally be stated as follows: for arbitrary c, d as arranged
above, we get
To convert the original proof of the remainder theorem to one for (3) we
require, for given provably recursive d, the term 
, the least
common multiple of 
. Claim: 
 is defined in PA by
the Σ 1-formula
More precisely, δ f (x, y) describes a Σ 1-formula in ℒar that is even Δ0, provided
d is Δ0-definable. Clearly 
 for all n. Thus, 2(a) holds.
With the minimum schema (Exercise 4 in 3.3) applied to 
,
we obtain 
, provided it has been shown that ⊢ PA ∃yβ(x, y) (‘
 have a common multiple’), which is easily derived by induction on x;
see Example 1 in 2.5. This proves the claim. After having derived Euclid’s

lemma in PA (Exercise 2) we confirm (3) by following the proof of the
remainder theorem in 6.2, and, writing βst for β(s, t), a suitable version of
Lemma 6.4.1 as follows: (4) 
, for any given provably
recursive c.
Theorem 1.1.
Each p.r. function f is provably recursive. Moreover, the recursion equations for f
are provable in PA whenever f = Op (g,h).
Proof. For the initial functions and +, ⋅ the formulas v 0 = 0, v 1 = Sv 0, 
along with 
 and v 2 = v 0 ⋅ v 1 are obviously defining Σ 1-formulas. For
the composition f = h[g1, …, gm], let 
 be the formula 
.
In this case (2) is clear, because we might think of 
 as being already
introduced in PA, so that 
 belongs to the expanded language. Only the
construction of δ f for the case f = Op(g, h) requires some skill. We may assume
that besides β also g, h have already been introduced in the language. Consider
(5) 
δ f is similar to δexp from Remark 1 in 6.4. It is Σ 1, because β, g, h are Σ 1-
definable. Lemma 6.4.1 applied with 
 for i ≤ b shows that 
, equivalently 2(a). Uniqueness in 2(b), that is,
derives easily from 
, which clearly follows
from 
. This is easily shown by
induction on y. Also, 
 will be shown inductively on y. We get 
 (hence 
) from (4), choosing c therein such that 
 and 
 for ν≠0. c is provably recursive, for the term 
 is Σ 1-
definable. The inductive step will be verified informally, that is, we shall prove
(*) 
Suppose 
. Consider the provably recursive 
 defined by 
 for ν ≤ Sy and 
. Here 
 are parameters in the
defining Σ 1-formula for c. So by (4) (taking Sy for n) there is some u′ with 

 for all ν ≤ y and 
. With this u′ and z′ = βu
′Sy we obtain 
, and so 
. This confirms ( ∗ ) and hence
2(b). Thus, f is provably recursive and may now be introduced in PA. We finally
sketch a proof of the recursion equations for f in PA, which also in PA may be
written as usual, i.e., (A) 
, (B) 
.
(A) holds because 
 and
clearly 
. (B) follows by < -induction on
y applied to 
. Assume that 
.
Choosing u in (5) such that 
, we readily obtain 
, so that
This confirms 
, hence ⊢ PA ∀yα by < -induction.
We thus have achieved our first goal. Next observe that the properties of 
 from the remark on page 206 along with the basic property (5) stated
there are also readily proved within PA. This is a little extra program that
includes the proof of unique prime factorization, see Exercise 4. Thus, D2∗ and
hence D2 are indeed provable for T = PA. In particular, the property (6) from
page 206 carries over to PA, so that (6) 
.
We mention that □ in (3) may even denote the formula bwb T for any
axiomatizable (and arithmetizable) theory T. D3 will be proved in the next
section in a somewhat broader context.
Remark 2. The formalized equations of Exercise 3 in 6.4 are now also
provable in PA. For instance, item (b) reads 
 for 
, where 
 enumerates the free variables of φ. As regards (c),
consider first a special case. Let φ be Sx = y. Then 
,
formalized 
. For the proof of this equation in PA,
just 
 is required, which holds by Theorem 1.1. Whoever wants to
write down a detailed proof should follow the example on page 223.
Exercises
1. Prove in PA the Δ0-definability of the remainder function rem, the
pairing function, and the β-function; see 6.4. In particular, rem is defined 

by 
. The laws of
arithmetic as given by N (page 211) may be used.
2. Prove in PA (a) 
), that is, Euclid’s
lemma. (b) 
 (‘each number ≥ 2 has a prime
divisor’), (c) 
.
 
3. Show that 
, required for
carrying out the proof of the Chinese remainder theorem in PA.
 
4. One of several possibilities of formalizing the prime factorization in PA
is 
, where m serves as a variable for the
sequence of prime exponents. Prove this in PA, as well as its uniqueness,
which is essentially based on Exercise 2.
 
5. Let 
 and T satisfy D1–D3. Show that
(a)
 (the formalized deduction theorem), 
(b) D1–D3 hold also for T′.
 
 
3
7.2 The Provable Σ 1-Completeness
D3 is a special case of the provableΣ1-completeness. This is essentially the
statement 
 for Σ 1-sentences α. The proof demands still additional
preparation, and even good textbooks do not carry out all proof steps. All steps
described in this section and not handled in detail can easily be completed in full
by the sufficiently assiduous reader. Life could be made easier through the
mutual interpretability of PA and ZFCfin mentioned in 6.6. Let □ = □ (x) denote
the formula 
 till the end of this section. We first introduce an additional
notation. Let 
.
Definition. 
.

By Remark 2 in 7.1, 
, where \vec{x} ′
enumerates free φ. Hence, we may assume w.l.o.g. that 
.
Moreover, for 
 we have 
, hence 
and 
 may be identified. ‘
 for all 
’ is reflected in PA by ‘
’. The latter thus reflects in PA the existence of a collection of proofs
which, due to the ω-incompleteness of PA, may be less than 
, or what
amounts to the same, 
.
Example. Let φ = φ(x, y) be Sx = y. We prove φ ⊢ PA □ [φ], or equivalently,
, where w.l.o.g. x, y do not occur bound in □ (x). In order to prove 
 observe that in view of Remark 2 in 7.1,
with 
. Thus, it suffices to verify ⊢ PA □ [α(x)] (equivalently,
⊢ PA ∀x □ [α(x)]). This reflects in PA ‘for arbitrary n, 
 ’. We verify
⊢ PA □ [α(x)] in detail. Consider the p.r. function 
 (the Gödel
number of 
). By axiom Λ9, 
 is for each n a trivial arithmetized proof of
length 1. Stated within PA, 
. This clearly yields 
.
Next we prove some modifications D1, D2 for 
 and 
:
(1) (a) ⊢ PA α  ⇒   ⊢ PA □ [α]; (b) □ [α → β] ⊢ PA □ [α] → □ [β].
To see (a) let ⊢ PA α, hence also 
 and so 
. Just as in the
above example, a proof for 
 provides one for 
 in a p.r. way, or stated
within PA: 
 ( = □ [α]; thus, ⊢ PA □ [α]). (b) follows from
(6) in 7.1 with 
 for x, y, observing that 
, see Exercise 3 in 6.4. (c) of this
exercise yields for all not necessarily distinct x, y (2) 
 (t ∈ { 0,
y, Sy} and y∉bnd α).
Now, D3 is only a special case of the provable Σ1-completeness of PA, stated
not only for sentences, but for arbitrary formulas as follows: (3) φ ⊢ PA □ [φ]
(equivalently, ⊢ PA φ → □ [φ]), for all Σ 1-formulas φ.

Indeed, choose in (3) for φ the Σ 1-sentence □ α for any 
. Then □ α
⊢ PA □ [ □ α] ≡ □ □ α, and D3 is proved. We obtain (3) from Theorem 2.1
below, since by (1), (2), and since w.l.o.g. free α = free □ [α], the operator 
 satisfies the conditions of the theorem.
Theorem 2.1.
Let 
 be any operator with 
 satisfying
d1 : 
,
d2 : 
,
ds : 
Then ⊢PA φ → ∂φ holds for all Σ1-formulas 
 .
Proof. ∂ satisfies also d0, d00, and d ∧ (see Remark 1 in 7.1). Hence, by
Theorem 6.7.2 and d00 we need to carry out the proof only for special Σ 1-
formulas. First let φ be Sx = y. Clearly, 
 is equivalent to 
,
and this to ⊢ PA ∂ Sx = Sx by ds, which is obvious from d1. Now let φ be 
. We shall prove ⊢ PA ∀yz(φ → ∂φ) by induction on x. Observing that y
= z ⊢ PA ∂ y = z (equivalently ⊢ PA ∂z = z), we obtain 
. Thus, 
. Now 
; hence 
, by d00, ds. The induction step 
 follows then from
.
The formula x ⋅ y = z is left to the reader, who should observe d ∧, d2, the
induction steps for ∧ and ∃, and 
.
We now treat the logical connectives. The induction steps for ∧, ∨, ∃are
simple. Indeed, from d ∧ we obtain
For ∨ note that α ⊢ PA ∂α ⊢ PA ∂(α ∨ β), and similarly for β. Further, since 
 we get φ ⊢ PA ∂φ ⊢ PA ∂ ∃xφ by d0, and from x∉ free ∂ ∃xφ follows
∃xφ ⊢ PA ∂ ∃xφ. The prime-term substitution step (t is prime in 
) also runs
smoothly: φ ⊢ PA ∂φ yields 
 by ds.
It remains to verify the step for bounded quantification. Suppose that α ⊢ PA ∂α

(.)
and y∉ var α. We prove φ : = ( ∀x < y) α ⊢ PA ∂φ by induction on y. The initial
step is obvious: 
, and therefore
Clearly, 
. Hence 
 because of α ⊢ PA ∂α.
That leads to
Thus, 
, which is obviously equivalent to the
inductive step.
Remark 3. D1–D3 are also provable for much weaker theories than PA, e.g.,
for the so-called elementary arithmetic 
. Here IΔ0 is
defined in Remark 1 in 6.3 and δ exp is a defining Δ0-formula for exp, see also
[FS]. Also Theorem 1.1 can essentially be strengthened and has many variants.
For instance, the provably recursive functions of IΣ1 (like PA but IS restricted to
Σ 1-formulas) are precisely the p.r. ones, [Tak]. The same provably recursive
functions has EA augmented by the Π 2-induction schema without parameters,
[Be4]. It is noteworthy that the provable recursive functions of EA itself are
precisely the elementary ones, [Si]. For more material on the metatheory of PA
and related theories see [Bar, Part D], and in particular [HP].
7.3 The Theorems of Gdel and Lb
We are now in a position to harvest the yields of our efforts. As long as not
stated otherwise, let T denote any arithmetizable axiomatic theory in 
, that
satisfies the derivability conditions D1–D3 of 7.1 along with the fixed point
lemma of 6.5. We direct attention straight away to the uniqueness statement of
Lemma 3.1(b) below. According to this claim, up to equivalence in T at most □ α
→ α can be the fixed point of the formula □ (x) → α. The proof of Theorem 3.2
will show that ¬ □ (x) too has only one fixed point modulo T. Beneath all this
lies, as we shall see from Corollary 5.6, a completely general result.
Lemma 3.1.
Let T be as arranged above, and let α,γ ∈ℒ0 be such that γ ≡ T □γ → α. Then (a)
□γ ≡ T □α and (b) γ ≡ T □α → α.

Proof. The supposition yields 
, by D0 and D2.
Now by D3, we clearly obtain 
, hence 
. Since 
 and so 
, it follows that 
 by D0. Together with
the already verified 
 we get (a). Using (a) we may replace □ γ with □ α
in γ ≡ T □ γ → α, which results in (b).
Theorem 3.2 (Second incompleteness theorem).
PA satisfies alongside the fixed point lemma also D1–D3. Every theory T with
these properties satisfies the conditions (1) ⊬ T { Con} T provided T is
consistent, (2) 
 .
Proof. D1–D3 were proved for PA in 7.1. (1) follows from (2). Assume 
. Then 
 by D1, as well as 
 by (2). Thus, T is inconsistent.
To verify (2), let γ be a fixed point of ¬ □ (x), i.e., ( ∗ )   
.
By Lemma 3.1(b) with α = ⊥, we obtain 
.
Replacing γ in ( ∗ ) with Con T gives Con T ≡ T ¬ □ Con T . Half of this is the
claim (2).
Thus, by (1), no sufficiently strong consistent theory can prove its own
consistency. In particular, ⊬ PA ConPA as long as PA is consistent which is
assumed throughout this book and is a minimal assumption for a far-reaching
metamathematics. The above proof shows that Con T is the only fixed point of ¬
bwb T modulo T. Actually, it shows a bit more, namely
(3) Con T ≡ T ¬ □ Con T .
This strengthens (2), but only by a little: 
 is just a special
case of
(4) 
 (equivalently, 
), for every 
.
This follows from 
, since 
 by D0. (4) reflects in T
‘If T is inconsistent then every formula is provable’. From (1) and (3) we get in
particular 
, although ‘ ConPA is unprovable in PA’ is true
according to (1) (again we tacitly use the consistence of PA). ¬ □ PA ConPA
reflects ‘ ConPA is unprovable in PA’; hence 
 is just another
formulation of the second incompleteness theorem.
The above claims hold independently of the “truth content” of the sentences

provable in T. Namely, a consequence of the second incompleteness theorem is
the existence of consistent theories T ⊇ PA in which along with claims true in 
also false ones are provable, i.e., in which truth and untruth live in peaceful
coexistence with each other. Such “dream theories” are highly rich in content,
for all of them include ordinary number theory. An example is 
. This theory is consistent because the consistency of 
is equivalent to the unprovability of { Con} PA in PA. The italicized sentence is
even provable in PA, as (5) below will show. By the formalized deduction
theorem (Exercise 5 in 7.1), 
; hence 
, and consequently,
(5) 
 (in particular, 
).
The special cases under (5) and (3) for T = PA now clearly yield
(6) 
 (hence also 
).
Put together, 
 contains ordinary number theory as known to us, but also
proves the indubitably false sentence 
. Moreover, because of 
 and hence 
 by (6), 
 proves (the reflection of) its
own inconsistency, although along with PA also 
 is consistent. It claims to
have a mysterious proof of 
. Thus, consistency of T can have a different
meaning within T and seen from outside, just as the meanings of countable
diverge, depending on whether one is situated in ZFC or is looking at it from
outside. One may even say that 
 is lying to us with the claim 
.
We learn from the preceding that the extension T + Con T of a consistent
theory T need not be consistent. 
 is a concrete example, and in fact only
one of arbitrarily many others. More on the meaning of ¬ Con T will be said in
Theorem 3.4.
We now discuss what is, along with (3), the most famous example of a self-
referential sentence. Clearly, a fixed point α of □ (x) claims just its own
provability, that is, α ≡ T □ α. A trivial example is 
, because 
,
and since 
, clearly 
, so that 
. What is surprising here is that
⊤ turns out to be the only fixed point of □ (x) modulo T. By D4∘ below, 
 implies ⊢ T α and so 
 (which confirms the uniqueness),
although one might perhaps expect 
 for all 
 because □ α → α is
intuitively true.

Theorem 3.3 (Löb’s theorem).
Take T to satisfy D1–D3 and the fixed point lemma. Then T has the properties
D4 : ⊢T □(□α → α) →□α, 
.
Proof. Let γ be a fixed point of □ (x) → α, i.e., γ ≡ T □ γ → α. Then γ ≡ T □ α →
α by Lemma 3.1(b). This and D0 imply □ γ ≡ T □ ( □ α → α). Lemma 3.1(a)
states □ γ ≡ T □ α, hence □ α ≡ T □ ( □ α → α). Half of this is D4. Now suppose 
. Then by D1, 
. Using D4 results in 
, and 
 yields 
, thus proving D4∘.
D4 reflects just D4∘ in T. One application of Löb’s theorem is an extremely
easy proof of 
. Indeed, 
 implies 
 by D4∘.
That’s all. Similarly, D4 implies (2) for 
 by contraposition. Thus, Lb’s
theorem is stronger than Gödel’s second incompleteness theorem, which is not
obvious at first glance.
Unlike 
, PA + ConPA conforms to truth (in 
). Unfortunately it is not
quite clear what ConPA means in number-theoretic terms. This is clear, however,
for an arithmetical statement discovered by Paris and Harrington (see [Bar]) that
implies ConPA; this statement is provable in ZFC but not in PA. Since then,
many such sentences have been found, mostly of a combinatorial nature. A
popular example is Goodstein’s theorem. Every Goodstein sequence ends in 0.
A Goodstein sequence is a number sequence 
, with arbitrary a 0 given
in advance, such that a n + 1 is obtained from a n as follows: Let 
, so
that b 0 = 2, b 1 = 3, etc. Expand a n in b-adic base for b : = b n , so that for
suitable k,
Also the powers k − i are represented in b-adic form, so too the powers of
powers, and so on. Now replace b everywhere with 
 and subtract 1
from the output. The result is a n + 1. The table below gives an example
beginning with a 0 = 11; already a 6 has the value 134 217 727.

(802)
As one sees from this example, a n initially increases enormously, and it is
hardly believable that the sequence ever starts to decrease and ends in 0. But the
proof of the theorem is not particularly difficult; one estimates a n from above by
the ordinal number λ n , which, crudely put, results from a n on replacing the
basis b in ( ∗ ) by ω. With some ordinal arithmetic it can readily be shown that λ
n + 1 < λ n as long as λ n ≠0. Since there is no properly decreasing infinite
sequence of ordinal numbers (these are well-ordered), the sequence 
 must
eventually end in 0. For more detailed information see for instance [HP].
Many metatheoretic properties can be expressed using the provability
operator □ in T, often using sentence schemata. The following ones turn out to
be equivalent and facilitate a better understanding of the meaning of ¬Con T
within T. None of these properties hold for a consistent T from the outside
(Theorem 6.5.1′), but all of them are provable in T = PA⊥.
Theorem 3.4.
The properties (i)–(iv) are all equivalent in a theory T satisfying the properties
named at the beginning of this section.
Proof. By (4) (i) ⇒ (ii),(iii),(iv) are clear. (ii) ⇒ (i): By Rosser’s theorem
formulated in T (see 7.5), 
 for some α. Thus, 
. (iii) ⇒ (i): For α : = Con T , SeComp and (2) yield α ⊢ T
□ α, ¬ □ α and so 
. (iv) ⇒ (i): By (3) in 7.2}, we obtain 
, for 
 is Σ 1. Hence,
ω{ -Comp} and (2) yield 
.
Therefore, 
.

Remark. Con T is also equivalent in T to other properties, for example to the
schema □ α → α for Π1-formulas α (the localΠ1-reflection principle) as well as
the uniformΠ1-reflection principle ∀x □ [α(x)] → ∀xα(x) for Π1-formulas α.
Both the theorems of Paris–Harrington and of Goodstein are equivalent in PA to
the uniform Σ 1-reflection, or equivalently, to the consistency of PA plus all true
Π1-sentences; see e.g. [Bar, D8].
Define inductively T0 = T and 
. This n-times-iterated
consistency extensionTn can be written as 
 with □ = bwb T , □
0α = α and 
 (Exercise 3). Thus, the consistency of T n can be
expressed by an iterated consistency statement on T. Let 
. Since
T n ⊆ T n + 1 and 
 (hence 
), the
following three items are equivalent:
(i) 
 is consistent, (ii) T n is consistent for all n, (iii) 
 for all n.
Like 
, also 
 conforms to truth looking at PA from
outside. When considered more closely, this means only that 
 is relatively
consistent with respect to ZFC. In other terms, 
. The argument (to
be formalized in ZFC) runs as follows: 
 implies 
 for some n, as
was noticed above, hence 
. But this is impossible, as is seen by a
repeated application of D1∗ (p. 7) on PA.
Exercises
1. Prove D4∘ for T by applying Theorem 3.2 to 
.
 
2. Show by means of Löb’s theorem that ConPA → ¬ □ ¬ ConPA is
unprovable in PA, although this formula is true if seen from outside.
 
3. Let Tn recursively be defined as in the text above. Prove that 
 and 
, where □ is bwb T .
 
4. Show that ⊢ ZFC □ PAα → α for all arithmetical sentences α from 
(the 
-sentences relativized to ω).
 

7.4 The Provability Logic G
In 7.3 first-order logic was hardly required. It comes then as no surprise that
many of the results there can be obtained propositionally, more precisely, in a
certain modal propositional calculus. This calculus contains alongside ∧, ¬the
falsum symbol 
, and a further unary connective □ to be interpreted as the proof
operator in 
, denoted by □ as well. First we define a propositional language 
, whose formulas are denoted by H, G, F: (a) the variables 
 from 
 (page 3) and 
 belong to 
; (b) if H, G belong to 
 then so too (H ∧
G), ¬H, and □ H.
No other strings belong to 
 in this context. 
, H → G, and 
are defined as in 1.4, 
. Further, set ◊ H : = ¬ □ ¬H and define
recursively □ 0 H = H, 
. Let G be the set of those formulas in 
 derivable using substitution in ℱ □, modus ponens MP, and the rule MN: H ⁄
□ H from the tautologies of two-valued propositional logic, augmented by the
axioms (called also the G-axioms) □ (p → q) → □ p → □ q,  □ p → □ □ p, 4  □ (
□ p → p) → □ p.
For H ∈ G we mostly write 
 (read “H is derivable in G”). Rule MN
corresponds to D1. The first G-axiom reflects D2, the middle D3, and the last
(called Lb’s formula) D4, hence the name provability logic. The connection
between G and PA is described in 7.5. Here we are concerned with the modal
logic G and its Kripke semantics. For simplicity, we restrict ourselves to finite
Kripke frames, which are just finite directed graphs. We can do so, since all
modal logics considered here have the finite model property. We begin without
further ado with the following Definition. A G -frame or Kripke frame for G is a
finite poset (g, < ). A valuation is a mapping w that assigns to every variable p a
subset wp of g. The relation 
, dependent on w, between points P ∈ g and
formulas 
 (read “P accepts H”) is defined inductively by
These conditions easily imply 
 iff 
 for some P′ > P, and 
 iff 
. If 
 for all w and all P ∈ g, we write g ⊨
H and say H holds in g. If g ⊨ H for all G-frames g, we write ⊨ G H and say H
is G -valid. The G-frame on the right, consisting of two points P, P′ with P < P′,
shows that 
. Indeed, let 
. Then P ⊩ p, but 
 because P
′⊮p. Note also ⊭ G □ p → p, for 
 but P′ ⊩ □ p because there is no P′ > P′.
We may tacitly assume that G-frames are initial (have a smallest point), for g

⊨ H is verified pointwise. We write H ≡ G H′ for 
. It is readily seen
that ≡ G is a congruence in ℱ □ that extends the usual logical equivalence
conservatively. For instance, ¬ □ H ≡ G ¬ □ ¬ ¬H ≡ G ◊ ¬H. Many more
equivalences are presented in the following examples. These will later be
translated into statements about self-reference.
Examples. (a) Let g be an arbitrary G-frame. Although always 
, we
have 
, provided P is maximal in g, that is, no Q > P exists. Likewise, 
 is accepted precisely at the maximal points of g. Thus, 
,
or equivalently, 
. This reflects in G the second
incompleteness theorem, as will be seen in 7.5.
(b) Let 
 be the ordered G-frame with P n < ⋯ < P 0. Clearly, 
 for each m > 0. Induction on n shows that 
 for all m > n,
but 
, and therefore 
. Hence, 
, and a fortiori 
 and 
, for all n.
(c) ⊨ G □ ( □ p → p) → □ p. For take an arbitrary g and P ∈ g. If 
then there is, since g is finite, some Q > P with 
 and 
 for all Q′ > Q.
Thus 
; hence 
 and so 
. Consequently, 
, which proves our claim. Note also that ⊨ G □ p → □ □ p.
Only the transitivity of < is relevant for the proof.
(d) ⊨ G ¬ □ n + 1 ⊥ → ◊ R n , where 
. For let P ∈ g, 
. Then there must be a chain 
 in g. Now, it
is a nice separate exercise to verify that each conjunct of R n fails to be accepted
by at most one of the n + 1 points P1, …, Pn + 1. Thus, at least one of these
accepts all conjuncts. In other words, 
 for some i > 0; hence 
.
This nontrivial example will essentially be employed in the proof of Theorem
7.1.
By induction on 
 one easily proves 
 (soundness of
Kripke semantics for ⊢ G). Example (c) is a part of the initial step. The
induction steps over the rules are easy. For instance, g ⊨ H clearly implies g ⊨
□ H. The converse, ⊨ G H ⇒ ⊢ G H, holds as well. Thus, ⊢ G H can be
confirmed by proving ⊨ G H, and vice versa. This is the content of
Theorem 4.1 (Completeness of Kripke semantics for G).

For each formula H from 
 it holds that 
 .
The nontrivial direction ⇐ follows directly from the finite model property of G,
i.e., each H∉G is falsified or refuted by some finite G-frame, proved, for
example, in [Boo], [Ra1], and [CZ]. For the relatively simple formulas
considered here, ⊨ G H is in general more easily checked than ⊢ G H.
Both the formulas provable in G and those refutable are clearly recursively
enumerable, thanks to the finite model property of G. Thus, in analogy to
Exercise 2 in 3.6, we obtain
Theorem 4.2.
G is decidable.
Remark. The finite model property, decidability, and some other properties
such as interpolation can all be proved in one move, see e.g. [Ra2]. An important
fragment of G is 
, where 
 denotes the set of variable-free
formulas of ℱ □. The formulas 
 form a Boolean base in G0.
One proves this most easily by showing that G0 is complete with respect to all
(totally) ordered G-frames, including the infinite ones, and applying Theorem
5.2.3 accordingly.
Exercises
1. Let g be any finite Kripke frame (a graph) that satisfies the axioms of G.
Show that g is necessarily a poset. Only this fact justifies the
identification of G-frames with posets.
 
2. Prove ⊢ G □ p → □ ( □ p → p), the inverse of Lb’s formula. (Only the
first of the three G-axioms is needed in the proof.)
 
7.5 The Modal Treatment of Self-Reference
Let T be a theory as in 7.3. A mapping ı from 
 to 
 with 
 is called
an insertion. ı can be extended to the whole of 
 by the clauses 
, 
, 
, and 
 
.
Briefly speaking, H ı results from H(p1, …, pn) by replacing the 
 by the
sentences 
 from . For instance, if 
 then 
,

and 
. The following lemma shows that ⊢ G is “sound”
for ⊢ T. Already this simple fact considerably simplifies proofs about self-
referential statements.
Lemma 5.1.
For each H with ⊢G H and each insertion ı, 
 .
Proof by induction on ⊢ G H. If H is a propositional tautology then 
. If H is one of the modal axioms of G, then ⊢ TH ı by D2, D3,
or D4. If ⊢ G H and 
 is a substitution, then 
, since 
 with 
, and ⊢ TH ı′ holds by the induction hypothesis. As
regards the induction step over MP, consider (F → G) ı = F ı → G ı . Finally, if
MN is applied, and ⊢ TH ı by the induction hypothesis, then ⊢ T □ H ı = ( □ H) ı
, due to D1.
Example 1. We prove (3) of Theorem 3.2 with the calculus ⊢ G. By Lemma
5.1 and Theorem 4.1 it suffices to show that 
. This holds by
Example (a) in 7.4. Next example: ⊨ G □ (p ↔ ◊ p) → ¬ ◊ p is easily
confirmed. Thus, ⊢ T □ (α ↔ ◊ α) → ¬ ◊ α. This tells us (if everything is related
to T = PA) that a sentence claiming its own consistency with PA is incompatible
with PA, which hardly seems plausible. Even the converse is provable in PA
since ⊨ G ¬ ◊ p → □ (p ↔ ◊ p).
We now explain certain facts that expand upon the reasoning of above. For
PA and related theories, the converse of Lemma 5.1 holds as well. That is to say,
the derivability conditions and Lb’s theorem already contain everything worth
knowing about self-referential formulas or schemes. This is essentially the
content of Theorem 5.2. For the subtle proofs of Theorems 5.2, 5.4, and 5.5, the
reader is referred to [Boo].
Theorem 5.2 (Solovay’s completeness theorem).
For all 
 : ⊢G H (equivalently ⊨G H) if and only if 
 for all
insertions ı.
Example 2 (applications). (a) 
 because by Example (b) in
7.4, 
. In particular, 
 
). (b) 
, since 
. (c) It is easily verified with the 2-point frame on page 38

that ⊭G ¬ □ p → □ ¬ □ p, in particular 
. Therefore, 
. (d) 
 is consistent for n > 0 by (b), but is
ω-inconsistent. Otherwise, by D1∗ (page 7), 
, contradicting 
. Since 
 by D3, we get PA n ⊇ PA n + 1, and since PA n ≠PA n + 1 by
(a), we have PA0 ⊃  PA1  ⊃  ⋯  ⊃  PA. Observe that PA1 is just 
.
Note also the following: Since ⊭G □ p → p, there must be some 
such that ⊬PA □ α → α. Indeed, choose 
. The above examples point out
that Theorem 5.2 and the decidability of G are very efficient tools in deciding the
provability of self-referential statements.
Many other theories have the same provability logic as PA, where in general
a modal propositional logic H is the provability logic for T when the analogue of
Theorem 5.2 holds with respect to T and H. For some theories, the provability
logic may be a proper extension of G. For example, the ω-inconsistent theory PA
n from Example 2(d) has the provability logic 
, the smallest
extension of G closed under all rules of G with the additional axiom 
(Exercise 1; note that G0 is inconsistent). By the following theorem, which will
be proved in 7.7, other extensions of G to be considered as provability logics are
out of the question.
Theorem 5.3 ([Vi1]).
Let T be at least as strong as PA. Then
If 
 (page 34) is consistent, then G is the provability logic of T;
if 
 and n is minimal such that 
 , then T’s provability logic is G
n.
The formulas 
 such that 
 for all insertions ı in 
 can also be
surprisingly easily characterized. All H ∈ G are obviously included; but in
addition also □ p → p belongs to this sort of formula, because 
 for 
. Indeed, if 
 then there is some n that codes a proof of α in PA,
hence 
.
Let GS ( ⊇ G) be the set of all formulas in 
 that can be obtained from
those in G ∪{ □ p → p} using substitution and modus ponens only. Induction in
GS readily yields 
 for all ı. Again, the converse holds as well:

Theorem 5.4 ([So]).
H ∈ GS if and only if 
 for all insertions ı.
GS is decidable as well, because it can be shown that 
, where 
. Here 
 is the set of subformulas of H of
the form □ G. Thus, Theorem 5.4 reduces the decidability of GS to that of G.
Using this theorem, many questions concerning the relations between provable
and true are effectively decidable. For instance,
is readily verified. Hence 
 for some 
 by Theorem 5.4. Translated into English: It is provable in PA that the
consistency of PA implies the independence of α for some sentence α. This is
exactly Rosser’s theorem, which in this way turns out to be provable in PA. As
was shown in [Be1], the box in the formulas H ∈ GS in Theorem 5.4 may
denote bwb T for any axiomatizable T ⊇ PA, provided 
. However, if T
proves false sentences (as does e.g. 
) then GS has to be redefined in a
feasible manner and is always decidable.
A variable p in H is called modalized inH if every occurrence of p is contained
within the scope of a □, as is the case in ¬ □ p, ¬ □ ¬p, and □ (p → q). By
contrast, p is not modalized in □ p → p. Another particularly interesting theorem
is
Theorem 5.5 (DeJongh–Sambin fixed point theorem).
Let p be modalized in 
 , n ≥ 0. Then a formula 
 from 
can effectively be constructed such that
(a) 
 , (b) 
 .
This theorem easily yields a corresponding result for theories T:
Corollary 5.6.
Let p be modalized in 
 and suppose T satisfies D1–D4. Then there is
an 
 with 
 for all 
 , 
 . For
each  there is only one 
 modulo T such that 
 .
Proof. Choose F as in (a) of the theorem. Then 
 by Lemma 5.1
(
). To prove uniqueness let 
 for i = 1, 2. By D1, 

. Inserting β i for p i and α i for q i in the
formula under (b) in the theorem then yields 
 by Lemma 5.1.
Example 3. For H = ¬ □ p (n = 0), 
 is a “solution” of (a) in
Theorem 5.5 because 
. According to Corollary 5.6, 
 is modulo T the only fixed point of ¬ bwb T . This is just the
claim of (3) from 7.3.
Many special cases of the corollary represent older self-reference results
from Gödel, Lb, Rogers, Jeroslow, and Kreisel, which,stated in terms of modal
logic, concern fixed points of ¬ □ p, □ p, ¬ □ ¬p, □ ¬p, and □ (p → q) in PA.
Incidentally, one gets the fixed points of these formulas—namely 
, 
, 
, 
, and □ q—according to a simple recipe. All first listed formulas are of the
form 
, where p is not modalized in G(p, \vec{q}) and 
 is
chosen appropriately. In this case, 
 is the fixed point of H, as is seen
after some calculation. For H = ¬ □ p from Example 3 is G = ¬p. Thus,
according to the recipe the fixed point is
For Kreisel’s formula □ (p → q) is G = p. Hence, it has the fixed point
The recipe also works for H = □ p → q, by choosing G = p → q. Hence 
 is the only fixed point of H
modulo T. Exactly this is the claim of Lemma 3.1(b), used in Gödel’s second
incompleteness theorem.
Exercises
1. Prove that the theory PA n from Example 2(d) has the provability logic G
n .
 
2. Show that 
 equals 
 and
that it has the provability logic 
. Here □ means □ PA.
 
3. Prove that 
, 
, and 
 are the fixed points of □ p, ¬ □ ¬p, and □ ¬p.  
4. (Mostowski). Let T ⊇ PA be axiomatizable and suppose 
. Show
that there are two mutually independent Σ 1-sentences α, β in T, that is, α 

→ β, α → ¬β, β → α, β → ¬α (hence also α, β, ¬α, and ¬β) are
unprovable in T.
7.6 A Bimodal Provability Logic for PA
Hilbert remarked jokingly that the incompleteness phenomenon can be
forcefully removed from the world by use of the so-called ω-rule
 has infinitely many premises. It is an easy exercise to derive with the aid of 
 every sentence α valid in 
 from the axioms of PA, even from those of Q.
Indeed, all sentences can (up to equivalence) be obtained from variable-free
literals with 
, bypassing formulas with free variables. Due to the Σ 1-
completeness of Q, all valid variable-free literals are derivable. The inductive
steps for ∧, ∨, ∃are simple, applying Σ 1-completeness in the ∃-step once again.
Only in the ∀-step is 
 used.
Clearly, an unrestricted use of the infinitistic rule 
 (in spite of its relevance for
higher order arithmetic) contradicts Hilbert’s own intention of giving
mathematics a finitistic foundation. However, things look different if we restrict 
 each time to a single application. In view of Remark 1 in 6.2, we no longer
distinguish between φ and , so that φ itself is a number and 
 is the
corresponding Gödel term. Let us define
1bwbPA is arithmetical, in fact it is Σ 3, for bwb PA is Σ 1 and 
is Π1. We read 1bwbPA(α) as “α is 1-provable.” Let 1bwb(x) be the Σ 3-formula
in 
 defining 1bwbPA. Here let x be v 0. Write 
 for 1bwb( ⌜ α ⌝ ) and
 for 
 ¬α. Clearly, □ α for 
 ( □ = □ PA) can be read ‘PA + ¬α is
inconsistent’, while 
 α, by Lemma 6.1, formalizes ‘PA + ¬α is ω-inconsistent’.
Thus, 
 ⊥) means ‘PA 
 is ω-consistent’. This explains the interest
in the operator 
.
If bwb PA (α) then certainly 1bwbPA (α) (choose α for φ). The italicized

statement is reflected in PA as ‘
 α for every 
’. The
converse fails, since ⊬PA ConPA, while ConPA is easily 1-provable: ⊢ PA
φ(\underline{n}) for all n, with 
, and trivially 
. In what follows, some claims will not be proved in detail.
Define 
. By its definition, Ω and hence
also 
 are formally Σ3. As Theorem 6.2 will show, 
 is
properly Σ3 and hence is no longer recursively axiomatizable.
Lemma 6.1.
The following properties are equivalent for 
 :
(i) 1bwbPA (α), (ii) 
 , (iii) PA + ¬α is ω-inconsistent.
Proof. (i) ⇒ (ii) follows with a glance at the definitions (read (i) naively). (ii) ⇒
(iii): Let 
. Since Ω is closed under conjunctions, there is some ∀xφ(x) ∈ Ω
with ∀xφ ⊢ PA α, hence ⊢ PA ¬α → ∃x ¬φ and so 
. Now, ∀xφ ∈ Ω,
therefore 
 and a fortiori 
, for all n. Thus, PA + ¬α is ω-
inconsistent. (iii) ⇒ (i): Let 
 for all n, but 
. Then 
. With 
 clearly 
 for all n. Now, 
. Hence 
. Thus, altogether 1bwbPA(α).
Theorem 6.2 (the 1-provable Σ3-completeness of PA).
All true Σ3-sentences are 1-provable. Moreover, for every β of this kind, 
 .
Proof. Let 
 where γ(y, x) is Σ 1. Then there is some m such
that 
 for all n. Therefore, 
 for all n, because PA is Σ 1-
complete. Hence, 
 and so 
, or equivalently, 1bwbPA(β)
by Lemma 6.1. Because of the provable Σ 1-completeness of PA, this
argumentation is comprehensible in PA, so that also ⊢ PA β →□1 β.
D1–D4 are also valid for the operator 
. Indeed, D1 holds
because 
 α, and D2 formalizes (or reflects) ‘

’ in PA (observe Lemma 6.1). D3 follows from Theorem
6.2 with β = 
 α. The proof of D4 in 7.3 uses, along with the fixed point lemma,
only D1–D3; so D4 holds as well. Therefore, nearly everything said in 7.3 on □
applies also to 
, including Theorem 3.2, which now reads 
┬). To put it more concisely, although the consistency of PA is provable with
the extended means, ω-consistency is not. Hence, this property, which is Π 3-
definable according to Exercise 3 in 6.7, cannot be Σ3 by Theorem 6.2, and must
therefore be properly Π 3. Equivalently, ω-inconsistency is properly Σ 3.
Alongside 
 α, there are other noteworthy interactions between □ and 
, in particular 
 ¬α. This formalizes ‘If ⊬PA α then ¬ □ α is 1-
provable’. To verify the latter notice that ⊬PA α implies 
 for all n,
where φ(x) is 
, and since ⊢ PA ∀xφ → ¬ □ α, we get ⊢ PA 
 ¬ □
α. On the other hand, 
 fails in general; Example 2(c) in 7.5
yields a counterexample.
The language of the bimodal propositional logic GD now to be defined
results from 
 by adding a further connective 
, which is treated
syntactically just as □. The axioms of GD are those of G stated both for □ and 
, augmented by the axioms
The rules of GD are the same as those for G. Insertions ı to 
 are defined
as in 7.5, but with the additional clause 
 
, that is, 
. By the reasoning above, all axioms and rules of GD are
sound. This proves (the easier) half of the following remarkable theorem from
Dzhaparidze (1985):
Theorem 6.3.
 for all insertions ı as defined above. Furthermore, GD is
decidable.
Thus, the modal system GD completely captures the interaction between bwb
PA and 1bwbPA; also Theorem 5.5 carries over. However, GD no longer has an
adequate Kripke semantics, which complicates the decision procedure. For
further references see [Boo] or [Be3].

As an exercise, the reader should derive 
 from the axioms of GD.
Thus, 
 for every 
, while ⊢ PA □ ( □ α → α) is the case
only provided ⊢ PA α. In other words, the local reflection principle 
 is 1-provable in PA. Be careful: GD expands G
conservatively, so that 
.
7.7 Modal Operators in ZFC
Considerations regarding self-reference in ZFC are technically sometimes easier,
but from the foundational point of view more involved because there is no
superordinate theory. If ZFC is consistent, as we assume it is, then ConZFC is a
true arithmetical statement that is unprovable in ZFC. Thus, true arithmetical
statements may even be unprovable in ZFC, not only in PA or similarly strong
arithmetical theories. It makes sense, therefore, to consider 
, because after all, we want set theory to embrace as
many facts about numbers and sets as possible from which interesting
consequences may result.
As 7.3 shows, the consistency of ZFC alone does not guarantee that ZFC+ is
consistent. The second incompleteness theorem clearly excludes 
but does not preclude 
. In this case 
,
and so 
 by the same theorem. On the other hand, from certain
assumptions about the existence of large cardinals, the consistency of ZFC+
readily follows. These assumptions would have to be jettisoned in case 
,
i.e. 
. Moreover, the consistency of ZFC would then not correctly be
reflected in ZFC, and ZFC proves along with true arithmetical facts also false
ones. This sounds strange, but there is hardly a convincing argument that this
cannot be so.
Even if ZFC+ is consistent, i.e. 
, it may still be that one of the
sentences from the sequence 
 is provable in ZFC
(where □ denotes □ ZFC as long as it is not redefined). The latter is excluded only
if we assume that the ω-iterated consistency extension 
 is consistent,
hence 
, for all n (see page 34), so that by Theorem 5.3, G would be
the provability logic of ZFC.

In fact, the assumption 
 is equivalent to G’s being the
provability logic of ZFC, by the general Theorem 7.1 below. Therein 
 denotes the already encountered reflection principle.
Also Theorem 5.3 is a corollary of the theorem, simply because 
 is equivalent to the consistency of 
.
Theorem 7.1.
For a sufficiently expressive theory T 5 the following conditions are equivalent:
(i)
T ω is consistent,
 
(ii) T + Rf T is consistent,
 
(iii) G is the provability logic of T. 
Proof. (i) ⇒ (ii) indirect: Suppose that T + RfT is inconsistent. Then there are
formulas α0, …, α n such that ⊢ T ¬φ, 
. Hence 
. Now, because 
, by Example (d) in 7.4 and
Lemma 5.1, we get 
 (
). Clearly, 
 and so 
. Since
also 
, 
 is inconsistent. (ii) ⇒ (iii): The proof of Theorem 5.2 for PA,
as presented in [Boo], runs nearly in the same way for T, because PA is
transgressed in one place only: one uses the fact that 
. However, the
existence of a corresponding T-model is ensured by (ii). (iii) ⇒ (i): 
,
hence 
 for all n, and so 
 is consistent.
The equivalence (i) ⇔ (ii) is a purely proof-theoretic one. It is called
Goryachev’s theorem; see [Gor] or [Be2]. We obtained it using essentially some
elementary modal logic. For T = ZFC, perhaps a bit more interesting than (i) or
(ii) is the assumption of the ω-consistency of ZFC, that is, ( ∗ ) 
 for some n (
).
This assumption implies D1∗, which in turn ensures 
 for all n,
that is, (i), and hence all other conditions in Theorem 7.1 hold for T = ZFC. It is
worthwhile to observe that the consistency of ZFC + Rf ZFC and thereby the
proof of Solovay’s completeness theorem for ZFC follow directly from ( ∗ ),

without appealing to Goryachev’s theorem. What is needed to see that the latter
is the case is the following Lemma. Suppose that ZFC is ω-consistent. Then
there exists a model 
 such that 
 .
Proof. Let 
. Then ZFC +
Ω is consistent. Indeed, otherwise ⊢ ZFC ¬( ∀x ∈ ω)α ≡ ( ∃x ∈ ω) ¬α for some (
∀x ∈ ω)α ∈ Ω (since Ω is closed under conjunction), in contradiction to ( ∗ ).
Any 
 satisfies the reflection principle Rf ZFC, for if 
 then 
 and therefore 
 for all n. Hence 
, which clearly implies 
.
Now we interpret the modal operator □ no longer as provable in ZFC, which
is equivalent to valid in all ZFC -models, but rather as valid in particular classes
of ZFC -models. For undefined notions used in the sequel we refer to [Ku]. A
‘model’ is to mean throughout a ZFC-model.
Particularly interesting are transitive models, i.e. models 
, where
the set V is transitive. This is to mean a ∈ b ∈ V ⇒ a ∈ V. In these models, 
coincides with the ordinary ∈ -relation restricted to V (a set in our metatheory
that itself is ZFC). We write V for . Let ρa denote the ordinal rank of the set a,
i.e., the smallest ordinal ρ with 
. To prove the soundness half of Theorem
7.3 we will need
Lemma 7.2.
([JK]) Let V,W be transitive models such that ρV < ρW and let V ⊨ α. Then W ⊨
‘ there is a transitive model U with U ⊨ α ’.6
Let the modal logic Gi result from augmenting G by the axiom
(i) 
.
Gi is complete with respect to all preference ordersg, i.e., g is a finite poset
together with some function 
 such that 
, for all P, Q ∈ g. This implies the finite model property of Gi,
which, as for G, ensures the decidability of Gi. More suitable for our aims is the
characterization of preference orders g by the property (p) P < P′ implies P < Q
or Q < P′, for all P, P′, Q ∈ g, which at once follows from the definition: Let P <
P′, hence πP < πP′. If P ≮ Q, i.e. πP ≮ πQ, then πQ ≤ πP < πP′, so that Q < P′.
The

proof of the converse is Exercise 1. The figure shows a poset g that is not a
preference order (for neither P < Q nor Q < P′). Axiom (i) is easily refuted in g
choosing 
, wq = ∅, and verifying that 
 and 
 (for notice that 
 and 
). Hence, (i) does
not belong to G, so that Gi is a proper extension of G. We mention that in [So]
and in [Boo] a somewhat more complex axiomatization of Gi has been
considered.
Remark on splittings in modal logic. The completeness of Gi with respect to
all preference orders follows also from the fact that Gi is the split logic arising
from splitting the lattice of all extensions of G (see e.g. [Kra]) by the subdirect
irreducible G-algebra belonging to the frame from the previous page.
We define insertions 
 as in 7.5 as usual by 
,
where □ α for the set-theoretic sentence 
 is now to mean ‘α is
valid in all transitive models’. Accordingly, ◊ α = ¬ □ ¬α states ‘α holds in at
least one transitive model’.
Theorem 7.3.
 iff 
 for all insertions ı as defined above.
We prove only the direction ⇒, that is, soundness. The converse is much more
difficult, see [Boo]. As regards the axioms of Gi, since □ p → □ □ p is provable
from the other axioms of G (see 7.4), it suffices to prove (A) 
, (B) 
, (C) 
, for all 
.
(A) is trivial, because the sentences valid in any class of models are closed
under MP. (B) is equivalent to (B′): 
. Here is the proof:
Suppose ◊ ¬α, i.e. there is a transitive model in which ¬α holds. Then there is
also one with minimal rank, V say. We claim V ⊨ □ α. Otherwise V ⊨ ◊ ¬α, and
hence there would be a transitive model U ∈ V with U ⊨ ¬α and ρU < ρV,
contradicting our choice of V. Therefore, V ⊨ □ α ∧ ¬α. Thus, there is a
transitive model in which □ α ∧ ¬α holds, which confirms (B′). Finally, (C) is
verified by contraposition: suppose there are transitive models V, W and
sentences α, β such that

(a) V ⊨ ‘ α holds in all transitive models and there is a transitive model in
which ¬β holds’,
 
(b) W ⊨ ‘ β holds in all transitive models’, (c) W ⊨ ¬α.
 
From these assumptions it follows first of all that ρW < ρV. Indeed, suppose
by (a) that U ∈ V is a transitive model for ¬β. If ρV ≤ ρW then ρU < ρW. Hence,
by Lemma 7.2, W ⊨ ‘there is a transitive model for ¬β’, contradicting (b). Now,
since W ⊨ ¬α by (c) and because of ρW < ρV, in V holds ‘there is some
transitive model for ¬α’ by Lemma 7.2, in contradiction to (a). This proves (C).
Soundness of the substitution rule follows as for G in 7.5. MN is trivially sound,
because if α is provable in ZFC then, of course, α is valid in all transitive
models. Also MP is obvious: If α and α → β hold in any class of models, then
also β.
Another interesting model-theoretic interpretation of □ α is ‘α is valid in all 
’. Here κ runs through all inaccessible cardinal numbers. According to [So],
the adequate modal logic for this interpretation of □ is
More precisely, if there are infinitely many inaccessibles then we have
Theorem 7.4.
⊢ Gj H iff ⊢ZFC H ı for all insertions ı, where □α is to mean ‘α is valid in all 
’, κ running through all inaccessible cardinals.
Gj is also denoted by G.3. This logic is complete with respect to all finite strict
linear orders. These, of course, are also frames for Gi, so that
Gi ⊆ Gj. The figure shows a Gi-frame, also called “the fork,” on which the
additional axiom is easily refuted at its initial point O with 
 and wq =
∅. Hence the fork is not a Gj-frame, and so 
. The completeness of Gj
with respect to finite orders entails the finite model property of Gj and hence its
decidability.
We recommend that the reader carry out the proof of the soundness part of
Theorem 7.4, without consulting the hints to the solutions (Exercise 4). It is
easier than the soundness part of Theorem 7.3 proved above. All one needs to
know besides Lemma 7.2 is that each 
 is a transitive ZFC-model and that 

 or 
, for arbitrary inaccessible cardinals κ≠λ. Maybe the reader can
also find a new and lucid proof of the hard direction of Theorem 7.4: If ⊢ ZFC
H ı for all ı then H holds in all finite strict linear orders, or equivalently, 
.
Exercises
1. Let g be a G-frame with property (p), page 64. Show by induction on the
length of a maximal path in g that g is a preference order.
 
2. Show (using Exercise 1) that axiom (i) for Gi holds in a G-frame g iff g
is a preference order. This is an essential step in proving the
completeness of Gi with respect to all preference orders.
 
3. This exercise is a crucial step in the completeness proof of Gj. Show that
a G-frame g is a frame for Gj, i.e., 
) holds in
g if and only if g is (totally) ordered.
 
4. Verify the soundness part of Theorem 7.4, i.e., 
 for all
insertions ı.
 
References
Bu.
S. R. BUSS (editor), Handbook of Proof Theory, Elsevier 1998.
HB.
D. HILBERT, P. BERNAYS, Grundlagen der Mathematik, I, II, Berlin 1934, 1939, 2⟨{ nd}⟩ ed. Springer,
Vol. I 1968, Vol. II 1970.
Vi2. _________ , An overview of interpretability logic, in Advances in Modal Logic, Vol. 1 (editors
M. KRACHT et al.), CSLI Lecture Notes 87 (1998), 307–359.
Gö2. _________ , Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme
I, Monatshefte f. Math. u. Physik 38 (1931), 173–198, also in [Gö3, Vol. I, 144–195], [Hei, 592–617],
[Dav, 4–38].
WR. A. WHITEHEAD, B. RUSSELL, Principia Mathematica, I–III, Cambridge 1910, 1912, 1913, 2⟨{ nd}⟩ ed.
Cambridge Univ. Press, Vol. I 1925, Vol. II, III 1927.
FS.
H. FRIEDMAN, M. SHEARD, Elementary descent recursion and proof theory, Ann. Pure Appl. Logic 71
(1995), 1–47.
Tak. G. TAKEUTI, Proof Theory, Amsterdam 1975, 2⟨{ nd}⟩ ed. Elsevier 1987.
Be4. _________ , Parameter free induction and reflection, in Computational Logic and Proof Theory,

1
2
3
Lecture Notes in Computer Science 1289, Springer 1997, 103–113.
Si.
W. SIEG, Herbrand analyses, Arch. Math. Logic 30 (1991), 409–441.
Bar.
J. BARWISE (editor), Handbook of Mathematical Logic, North-Holland 1977.
HP.
P. HÁJEK, P. PUDLÁK, Metamathematics of First-Order Arithmetic, Springer 1993.
Boo. G. BOOLOS, The Logic of Provability, Cambridge Univ. Press 1993.
Ra1. W. RAUTENBERG, Klassische und Nichtklassische Aussagenlogik, Vieweg 1979.
CZ.
A. CHAGROV, M. ZAKHARYASHEV, Modal Logic, Clarendon Press 1997.
Ra2. _________ , Modal tableau calculi and interpolation, Journ. Phil. Logic 12 (1983), 403–423.
Be1. L. D. BEKLEMISHEV, On the classification of propositional provability logics, Math. USSR – Izvestiya
35 (1990), 247–275.
Be3. _________ , Bimodal logics for extensions of arithmetical theories, J. Symb. Logic 61 (1996), 91–
124.
Gor. S. N. GORYACHEV, On the interpretability of some extensions of arithmetic, Mathematical Notes 40
(1986), 561–572.
Be2. _________ , Iterated local reflection versus iterated consistency, Ann. Pure Appl. Logic 75 (1995),
25–48.
Ku.
K. KUNEN, Set Theory, An Introduction to Independence Proofs, North-Holland 1980.
So.
R. SOLOVAY, Provability interpretation of modal logic, Israel Journal of Mathematics 25 (1976), 287–
304.
Kra. J. KRAJÍČEK, Bounded Arithmetic, Propositional Logic, and Complexity Theory, Cambridge
Univ. Press 1995.
Footnotes
This is not actually necessary, since in ZFC one can talk directly about finite sequences and hence about 
-formulas (Remark 2 in 6.6), but we do so in order to maintain coherence with the exposition in 6.2.
In [Gö2], Gödel presented a list of 45 definable p.r. functions; the last was 
. Following [WR],
Gödel considered a higher-order arithmetical theory. That Gödel’s theorems also hold in first-order
arithmetic was probably first noticed in [HB].
An equivalent formalization of the prime factorization in PA using the β-function is 

4
5
6
.
This axiom is dispensable; it is provable from the remaining, see e.g. [Boo] or [Ra1].
By such a T we mean that the proof steps of Solovay’s Theorem 5.2 not transgressing PA can be carried
out in T. This does not yet imply the provability of the theorem itself. Which steps are transgressing PA is
described in the following proof.
In transitive models W the sentence in ‘ ’ (which with some encoding can be formulated in ℒ∈) is
absolute, and therefore equivalent to the existence of a transitive model U ∈ W with U ⊨ α.

Wolfgang Rautenberg, Universitext, A Concise Introduction to Mathematical Logic, 3, DOI: 10.1007/978-1-
4419-1221-3, © Springer-Verlag New York 2010
Bibliography
[AGM] S. Abramsky, D. M. Gabbay, T. S. E. Maibaum (editors), Handbook of Computer Science, I–IV,
Oxford Univ. Press, Vol. I, II 1992, Vol. III 1994, Vol. IV 1995.
[Ac]
W. Ackermann, Die Widerspruchsfreiheit der Allgemeinen Mengenlehre, Mathematische Annalen
114 (1937), 305–315.
[CrossRef][MathSciNet]
[Bar]
J. Barwise (editor), Handbook of Mathematical Logic, North-Holland 1977.
[BF]
J. Barwise, S. Feferman (editors), Model-Theoretic Logics, Springer 1985.
[Be1]
L. D. Beklemishev, On the classification of propositional provability logics, Math. USSR –
Izvestiya 35 (1990), 247–275.
[MATH][CrossRef][MathSciNet]
[Be2]
L. D. Beklemishev, Iterated local reflection versus iterated consistency, Ann. Pure Appl. Logic 75
(1995), 25–48.
[MATH][CrossRef][MathSciNet]
[Be3]
L. D. Beklemishev, Bimodal logics for extensions of arithmetical theories, J. Symb. Logic 61
(1996), 91–124.
[MATH][CrossRef][MathSciNet]
[Be4]
L. D. Beklemishev, Parameter free induction and reflection, in Computational Logic and Proof
Theory, Lecture Notes in Computer Science 1289, Springer 1997, 103–113.
[BM]
J. Bell, M. Machover, A Course in Mathematical Logic, North-Holland 1977.
[Ben]
M. Ben-Ari, Mathematical Logic for Computer Science, New York 1993, 2nd ed. Springer 2001.
[BP]
P. Benacerraf, H. Putnam (editors), Philosophy of Mathematics, Selected Readings, Englewood
Cliffs NJ 1964, 2nd ed. Cambridge Univ. Press 1983, reprint 1997.
[BA]
A. Berarducci, P. D'Aquino, Δ0 -complexity of the relation y = Пi≤n F (i), Ann. Pure Appl. Logic
75 (1995), 49–56.
[MATH][CrossRef][MathSciNet]
[Bi]
G. Birkhoff, On the structure of abstract algebras, Proceedings of the Cambridge Philosophical
Society 31 (1935), 433–454.
[CrossRef]
[Boo]
G. Boolos, The Logic of Provability, Cambridge Univ. Press 1993.
[BJ]
G. Boolos, R. Jeffrey, Computability and Logic, 3rd ed. Cambridge Univ. Press 1989.
[BGG] E. BÖrger, E. GrÄdel, Y. Gurevich, The Classical Decision Problem, Springer 1997.

[Bue]
S. Buechler, Essential Stability Theory, Springer 1996.
[Bu]
S. R. Buss (editor), Handbook of Proof Theory, Elsevier 1998.
[Ca]
G. Cantor, Gesammelte Abhandlungen (editor E. Zermelo), Berlin 1932, Springer 1980.
[CZ]
A. Chagrov, M. Zakharyashev, Modal Logic, Clarendon Press 1997.
[CK]
C. C. Chang, H. J. Keisler, Model Theory, Amsterdam 1973, 3rd ed. North-Holland 1990.
[Ch]
A. Church, A note on the Entscheidungsproblem, J. Symb. Logic 1 (1936), 40–41, also in [Dav,
108–109].
[MATH][CrossRef]
[CM]
W. Clocksin, C. Mellish, Programming in PROLOG, 3rd ed. Springer 1987.
[Da]
D. van Dalen, Logic and Structure, Berlin 1980, 4th ed. Springer 2004.
[MATH]
[Dav]
M. Davis (editor), The Undecidable, Raven Press 1965.
[Daw]
J. W. Dawson, Logical Dilemmas, The Life and Work of Kurt GÖdel, A. K. Peters 1997.
[De]
O. Deiser, Axiomatische Mengenlehre, Springer, to appear 2010.
[Do]
K. Doets, From Logic to Logic Programming, MIT Press 1994.
[EFT]
H.-D. Ebbinghaus, J. Flum, W. Thomas, Mathematical Logic, New York 1984, 2nd ed. Springer
1994.
[MATH]
[FF]
A. B. Feferman, S. Feferman, Alfred Tarski, Live and Logic, Cambridge Univ. Press 2004.
[Fe1]
S. Feferman, Arithmetization of metamathematics in a general setting, Fund. Math. 49 (1960), 35–
92.
[MATH][MathSciNet]
[Fe2]
S. Feferman, In the Light of Logic, Oxford Univ. Press 1998.
[Fel1]
W. Felscher, Berechenbarkeit, Springer 1993.
[Fel2]
, Lectures on Mathematical Logic, Vol. 1–3, Gordon & Breach 2000.
[Fi]
M. Fitting, Incompleteness in the Land of Sets, College Publ. 2007.
[Fr]
T. FranzÉn, Gödel's Theorem: An Incomplete Guide to Its Use and Abuse, A. K. Peters 2005.
[Fre]
G. Frege, Begriffsschrift, eine der arithmetischen nachgebildete Formelsprache des reinen
Denkens, Halle 1879, G. Olms Verlag 1971, also in [Hei, 1–82].
[FS]
H. Friedman, M. Sheard, Elementary descent recursion and proof theory, Ann. Pure Appl. Logic
71 (1995), 1–47.
[MATH][CrossRef][MathSciNet]

[Ga]
D. Gabbay, Decidability results in non-classical logic III, Israel Journal of Mathematics 10 (1971),
135–146.
[MATH][CrossRef][MathSciNet]
[GJ]
M. Garey, D. Johnson, Computers and Intractability, A Guide to the Theory of NP-Completeness,
Freeman 1979.
[Ge]
G. Gentzen, The Collected Papers of Gerhard Gentzen (editor M. E. Szabo), North-Holland 1969.
[GÖ1]
K. GÖdel, Die Vollständigkeit der Axiome des logischen Funktionenkalküls, Monatshefte f. Math.
u. Physik 37 (1930), 349–360, also in [GÖ3, Vol. I, 102–123], [Hei, 582–591].
[GÖ2]
K. GÖdel, Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme
I, Monatshefte f. Math. u. Physik 38 (1931), 173–198, also in [GÖ3, Vol. I, 144–195], [Hei, 592–
617], [Dav, 4–38].
[CrossRef]
[GÖ3]
K. GÖdel, Collected Works (editor S. Feferman), Vol. I–V, Oxford Univ. Press, Vol. I 1986, Vol. II
1990, Vol. III 1995, Vol. IV, V 2003.
[Gor]
S. N. Goryachev, On the interpretability of some extensions of arithmetic, Mathematical Notes 40
(1986), 561–572.
[MathSciNet]
[Gr]
G. GrÄtzer, Universal Algebra, New York 1968, 2nd ed. Springer 1979.
[HP]
P. HÁjek, P. PudlÁk, Metamathematics of First-Order Arithmetic, Springer 1993.
[Hei]
J. van Heijenoort (editor), From Frege to GÖdel, Harvard Univ. Press 1967.
[He]
L. Henkin, The completeness of the first-order functional calculus, J. Symb. Logic 14 (1949), 159–
166.
[MATH][CrossRef][MathSciNet]
[Her]
J. Herbrand, Recherches sur la théorie de la démonstration, C. R. Soc. Sci. Lett. Varsovie, Cl. III
(1930), also in [Hei, 525–581].
[HR]
H. Herre, W. Rautenberg, Das Basistheorem und einige Anwendungen in der Modelltheorie, Wiss.
Z. Humboldt-Univ., Math. Nat. R. 19 (1970), 579–583.
[MATH][MathSciNet]
[HeR]
B. Herrmann, W. Rautenberg, Finite replacement and finite Hilbert-style axiomatizability, Zeitsch.
Math. Logik Grundlagen Math. 38 (1982), 327–344.
[CrossRef][MathSciNet]
[HA]
D. Hilbert, W. Ackermann, Grundzüge der theoretischen Logik, Berlin 1928, 6th ed. Springer
1972.
[MATH]
[HB]
D. Hilbert, P. Bernays, Grundlagen der Mathematik, I, II, Berlin 1934, 1939, 2nd ed. Springer, Vol.
I 1968, Vol. II 1970.
[Hi]
P. Hinman, Fundamentals of Mathematical Logic, A. K. Peters 2005.

[Ho]
W. Hodges, Model Theory, Cambridge Univ. Press 1993.
[Hor]
A. Horn, On sentences which are true of direct unions of algebras, J. Symb. Logic 16 (1951), 14–
21.
[MATH][CrossRef][MathSciNet]
[Hu]
T. W. Hungerford, Algebra, Springer 1980.
[Id]
P. Idziak, A characterization of finitely decidable congruence modular varieties, Trans. Amer.
Math. Soc. 349 (1997), 903–934.
[MATH][CrossRef][MathSciNet]
[Ig]
K. Ignatiev, On strong provability predicates and the associated modal logics, J. Symb. Logic 58
(1993), 249–290.
[MATH][CrossRef][MathSciNet]
[JK]
R. Jensen, C. Karp, Primitive recursive set functions, in Axiomatic Set Theory, Vol. I (editor D.
Scott), Proc. Symp. Pure Math. 13, I AMS 1971, 143–167.
[Ka]
R. Kaye, Models of Peano Arithmetic, Clarendon Press 1991.
[Ke]
H. J. Keisler, Logic with the quantifier “there exist uncountably many”, Annals of Mathematical
Logic 1 (1970), 1–93.
[MATH][CrossRef][MathSciNet]
[Kl1]
S. Kleene, Introduction to Metamathematics, Amsterdam 1952, 2nd ed. Wolters-Noordhoff 1988.
[Kl2]
S. Kleene, Mathematical Logic, Wiley & Sons 1967.
[KR]
I. Korec, W. Rautenberg, Model interpretability into trees and applications, Arch. math. Logik 17
(1976), 97–104.
[MATH][CrossRef][MathSciNet]
[Kr]
M. Kracht, Tools and Techniques in Modal Logic, Elsevier 1999.
[Kra]
J. KrajÍČek, Bounded Arithmetic, Propositional Logic, and Complexity Theory, Cambridge Univ.
Press 1995.
[KK]
G. Kreisel, J.-L. Krivine, Elements of Mathematical Logic, North-Holland 1971.
[Ku]
K. Kunen, Set Theory, An Introduction to Independence Proofs, North-Holland 1980.
[Le]
A. Levy, Basic Set Theory, Springer 1979.
[Li]
P. Lindström, On extensions of elementary logic, Theoria 35 (1969), 1–11.
[MATH][CrossRef][MathSciNet]
[Ll]
J. W. Lloyd, Foundations of Logic Programming, Berlin 1984, 2nd ed. Springer 1987.
[MATH]
[Lö]
M. Löb, Solution of a problem of Leon Henkin, J. Symb. Logic 20 (1955), 115–118.
[MATH][CrossRef][MathSciNet]
[MS]
A. Macintyre, H. Simmons, GÖdel's diagonalization technique and related properties of theories,
Colloquium Mathematicum 28 (1973), 165–180.

[MATH][MathSciNet]
[Ma]
A. I. Mal'cev, The Metamathematics of Algebraic Systems, North-Holland 1971.
[Mal]
J. Malitz, Introduction to Mathematical Logic, Springer 1979.
[Mar]
D. Marker, Model Theory, An Introduction, Springer 2002.
[Mat]
Y. Matiyasevich, Hilbert's Tenth Problem, MIT Press 1993.
[MV]
R. McKenzie, M. Valeriote, The Structure of Decidable Locally Finite Varieties, Progress in
Mathematics 79, BirkhÄuser 1989.
[Me]
E. Mendelson, Introduction to Mathematical Logic, Princeton 1964, 4th ed. Chapman & Hall 1997.
[Mo]
D. Monk, Mathematical Logic, Springer 1976.
[Moo]
G. H. Moore, The emergence of first-order logic, in History and Philosophy of Modern
Mathematics (editors W. Aspray, P. Kitcher), University of Minnesota Press 1988, 95–135.
[ML]
G. MÜller, W. Lenski (editors), The Ω-Bibliography of Mathematical Logic, Springer 1987.
[Po]
W. Pohlers, Proof Theory, An Introduction, Lecture Notes in Mathematics 1407, Springer 1989.
[Pz]
B. Poizat, A Course in Model Theory, Springer 2000.
[Pr]
M. Presburger, Über die Vollständigkeit eines gewissen Systems der Arithmetik ganzer Zahlen, in
welchem die Addition als einzige Operation hervortritt, Congrès des Mathématiciens des Pays
Slaves 1 (1930), 92–101.
[RS]
H. Rasiowa, R. Sikorski, The Mathematics of Metamathematics, Warschau 1963, 3rd ed. Polish
Scientific Publ. 1970.
[Ra1]
W. Rautenberg, Klassische und Nichtklassische Aussagenlogik, Vieweg 1979.
[Ra2]
W. Rautenberg, Modal tableau calculi and interpolation, Journ. Phil. Logic 12 (1983), 403–423.
[MATH][CrossRef][MathSciNet]
[Ra3]
W. Rautenberg, A note on completeness and maximality in propositional logic, Reports on
Mathematical Logic 21 (1987), 3–8.
[MATH][MathSciNet]
[Ra4]
W. Rautenberg, Einführung in die mathematische Logik, Wiesbaden 1996, 3rd ed. Vieweg+Teubner
2008.
[Ra5]
W. Rautenberg, Messen und ZÄhlen, Eine einfache Konstruktion der reellen Zahlen, Heldermann
2007.
[RZ]
W. Rautenberg, M. Ziegler, Recursive inseparability in graph theory, Notices Amer. Math. Soc. 22
(1975), A–523.
[Ro1]
A. Robinson, Introduction to Model Theory and to the Metamathematics of Algebra, Amsterdam
1963, 2nd ed. North-Holland 1974.
[MATH]

[Ro2]
A. Robinson, NonStandard Analysis, Amsterdam 1966, 3rd ed. North-Holland 1974.
[MATH]
[Rob]
J. Robinson, A machine-oriented logic based on the resolution principle, Journal of the ACM 12
(1965), 23–41.
[MATH][CrossRef]
[Rog]
H. Rogers, Theory of Recursive Functions and Effective Computability, New York 1967, 2nd ed.
MIT Press 1988.
[MATH]
[Ros]
J. B. Rosser, Extensions of some theorems of GÖdel and Church, J. Symb. Logic 1 (1936), 87–91,
also in [Dav, 230–235].
[MATH][CrossRef]
[Rot]
P. Rothmaler, Introduction to Model Theory, Gordon & Breach 2000.
[Ry]
C. Ryll-Nardzewki, The role of the axiom of induction in elementary arithmetic, Fund. Math. 39
(1952), 239–263.
[MathSciNet]
[Sa]
G. Sacks, Saturated Model Theory, W. A. Benjamin 1972.
[Sam]
G. Sambin, An effective fixed point theorem in intuitionistic diagonalizable algebras, Studia Logica
35 (1976), 345–361.
[MATH][CrossRef][MathSciNet]
[Sc]
U. Schöning, Logic for Computer Scientist, Birkhäuser 1989.
[Se]
A. Selman, Completeness of calculi for axiomatically defined classes of algebras, Algebra
Universalis 2 (1972), 20–32.
[MATH][CrossRef][MathSciNet]
[Sh]
S. Shapiro (editor), The Oxford Handbook of Philosophy of Mathematics and Logic, Oxford Univ.
Press 2005.
[She]
S. Shelah, Classification Theory and the Number of Nonisomorphic Models, Amsterdam 1978, 2nd
ed. North-Holland 1990.
[Shoe]
J. R. Shoenfield, Mathematical Logic, Reading Mass. 1967, A. K. Peters 2001.
[Si]
W. Sieg, Herbrand analyses, Arch. Math. Logic 30 (1991), 409–441.
[MATH][CrossRef][MathSciNet]
[Sm]
P. Smith, An Introduction to Gödel's Theorems, Cambridge Univ. Press 2007.
[Smo]
C. SmoryŃski, Self-reference and Modal Logic, Springer 1984.
[Smu]
R. Smullyan, Theory of Formal Systems, Princeton Univ. Press 1961.
[So]
R. Solovay, Provability interpretation of modal logic, Israel Journal of Mathematics 25 (1976),
287–304.

[MATH][CrossRef][MathSciNet]
[Sz]
W. Szmielew, Elementary properties of abelian groups, Fund. Math. 41 (1954), 203–271.
[MathSciNet]
[Tak]
G. Takeuti, Proof Theory, Amsterdam 1975, 2nd ed. Elsevier 1987.
[Ta1]
A. Tarski, Der Wahrheitsbegriff in den formalisierten Sprachen, Studia Philosophica 1 (1936),
261–405 (first edition in Polish, 1933), also in [Ta4, 152-278].
[Ta2]
A. Tarski, Introduction to Logic and to the Methodology of Deductive Sciences, Oxford 1941, 3rd
ed. Oxford Univ. Press 1965 (first edition in Polish, 1936).
[Ta3]
A. Tarski, A Decision Method for Elementary Algebra and Geometry, Santa Monica 1948,
Berkeley 1951, Paris 1967.
[Ta4]
A. Tarski, Logic, Semantics, Metamathematics (editor J. Corcoran), Oxford 1956, 2nd ed. Hackett
1983.
[TMR] A. Tarski, A. Mostowski, R. M. Robinson, Undecidable Theories, North-Holland 1953.
[TV]
A. Tarski, R. Vaught, Arithmetical extensions and relational systems, Compositio Mathematica 13
(1957), 81–102.
[MathSciNet]
[Tu]
A. Turing, On computable numbers, with an application to the Entscheidungsproblem, Proc.
London Math. Soc., 2nd Ser. 42 (1937), 230–265, also in [Dav, 115–154].
[CrossRef]
[Vi1]
A. Visser, Aspects of Diagonalization and Provability, Dissertation, University of Utrecht 1981.
[Vi2]
A. Visser, An overview of interpretability logic, in Advances in Modal Logic, Vol. 1 (editors M.
Kracht et al.), CSLI Lecture Notes 87 (1998), 307–359.
[Wae]
B. L. van der Waerden, Algebra I, Berlin 1930, 4th ed. Springer 1955.
[Wag]
F. Wagner, Simple Theories, Kluwer Academic Publ. 2000.
[Wa1]
H. Wang, From Mathematics to Philosophy, Routlegde & Kegan Paul 1974.
[Wa2]
H. Wang, Computer, Logic, Philosophy, Kluwer Academic Publ. 1990.
[Wa3]
H. Wang, A Logical Journey, From Gödel to Philosophy, MIT Press 1997.
[WR]
A. Whitehead, B. Russell, Principia Mathematica, I–III, Cambridge 1910, 1912, 1913, 2nd ed.
Cambridge Univ. Press, Vol. I 1925, Vol. II, III 1927.
[Wi]
A. Wilkie, Model completeness results for expansions of the ordered field of real numbers by
restricted Pfaffian functions and the exponential function, Journal Amer. Math. Soc. 9 (1996),
1051–1094.
[MATH][CrossRef][MathSciNet]
[WP]
A. Wilkie, J. Paris, On the scheme of induction for bounded arithmetic formulas, Ann. Pure Appl.

Logic 35 (1987), 261–302.
[MATH][CrossRef][MathSciNet]
[Zi]
M. Ziegler, Model theory of modules, Ann. Pure Appl. Logic 26 (1984), 149–213.
[MATH][CrossRef][MathSciNet]
Index of Terms and Names
A
a.c. (algebraically closed)
∀-formula, ∀-sentence
∀-theory
∀∃-sentence, ∀∃-theory
abelian group
divisible, torsion-free
absorption laws
Ackermann function
Ackermann interpretation
algebra
algebraic
almost all
alphabet
antisymmetric
arithmetical
arithmetical hierarchy
arithmetization (of syntax)
associative
automated theorem proving
automorphism
axiom
of choice
of continuity
of extensionality
of foundation
of infinity
of power set
of replacement
of union

axiom system
logical
of a theory
axiomatizable
finitely, recursively
B
β-function
basis theorem
for formulas
for sentences
Behmann
Birkhoff rules
Boolean algebra
atomless
of sets
Boolean basis
for L in T
for L 0 in T
Boolean combination
Boolean function
dual, self-dual
linear
monotonic
Boolean matrix
Boolean signature
bounded
Brouwer
C
Cantor
cardinal number
cardinality
of a structure
chain
of structures
elementary

of theories
characteristic
Chinese remainder theorem
Church
Church's thesis
clause
definite
positive, negative
closed under MP
closure
deductive
of a formula
of a model in T
closure axioms
cofinite
Cohen
coincidence theorem
collision of variables
collision-free
commutative
compactness theorem
first-order
propositional
compatible
complementation
completeness theorem
Birkhoff's
first-order (Gödel's)
for |∼
for G
for 
g
propositional
Solovay's
completion
inductive
composition
computable
concatenation
arithmetical

congruence
in L
congruence class
conjunction
connective
connex
consequence relation
finitary
local, global
predicate logical
propositional
consistency extension
consistent
constant
constant expansion
constant quantification
continuity schema
continuum hypothesis
contradiction
contraposition
converse implication
coprime
course-of-values recursion
cut
cut rule
D
Δ-elementary class
δ-function
Δ0-formula
Δ0-induction
Davis
decidable
deduction theorem
deductively closed
definable
Δ0-definable
explicitly

implicitly
in a structure
in a theory
Σ1-definable
with parameters
DeJongh
derivability conditions
derivable
derivation
diagram
elementary
universal
direct power
disjunction
exclusive
distributive laws
divisibility
domain
E
∃-formula
simple
Ehrenfeucht game
elementary class
elementary equivalent
elementary type
embedding
elementary
end extension
enumerable
effectively
recursively
equation
Diophantine
equipotent
equivalence
equivalence class
equivalence relation

equivalent
in (or modulo) T
in a structure
logically or semantically
Euclid's lemma
existentially closed
expansion
explicit definition
extension
conservative
definitorial
elementary
finite
immediate
of a language
of a theory
transcendental
F
f-closed
factor structure
falsum
family (of sets)
Fermat's conjecture
Fibonacci sequence
fictional argument
field
algebraically closed
of algebraic numbers
of characteristic 0 or p
ordered
real closed
filter
finitary
finite model property
finitely generated
finiteness theorem
fixed point lemma

formula
arithmetical
Boolean
closed
defining
dual
first-order
open (quantifier-free)
prenex
representing
universal
formula algebra
formula induction
formula recursion
four-color theorem
Frege
Frege's formula
function
bijective
characteristic
identical
injective, surjective
partial
primitive recursive
recursive (= μ-recursive)
function term
functional complete
G
gap
generalization
anterior, posterior
generalized of a formula
generally valid
Gentzen calculus
goal clause
Gödel
Gödel number

of a proof
of a string
Gödel term
Gödelizable
Goldbach's conjecture
graph
k-colorable of an operation
planar, simple
ground instance
ground (or constant) term
group, groupoid
ordered
H
H-resolution
Harrington
Henkin set
Herbrand model
minimal
Herbrand universe
Hilbert
Hilbert calculus
Hilbert's program
homomorphism
canonical
strong
homomorphism theorem
Horn clause
Horn formula
basic
positive, negative
universal
Horn sentence
Horn theory
universal, nontrivial
hyperexponentiation
I

ι-term
I-tuple
idempotent
identity
identity-free (=-free)
immediate predecessor
immediate successor
implication
incompleteness theorem
first
second
inconsistent
independent (of T)
individual variable
induction
on ϕ
on t
<-induction
induction axiom
induction hypothesis
induction schema
induction step
infimum
infinitesimal
instance
integral domain
(relatively) interpretable
interpretation
intersection
invariance theorem
invertible
irreflexive
isomorphism
partial
J
Jeroslow
jump

K
kernel
kernel (of a prenex formula)
Kleene
König's lemma
Kreisel
Kripke frame
for G
Kripke semantics
L
L-formula
L-model
L-structure (= L-structure)
language
arithmetizable
first-order (or elementary)
of equations
second-order
lattice
distributive
of sets
legitimate
Lindenbaum
Lindström's criterion
literal
Löb
Löb's formula
logic program
logical matrix
logically valid
M
μ-operation
bounded
mapping (see function)
Matiyasevich

ϕ-maximal
maximal element
maximally consistent
metainduction
metatheory
model
free
minimal
of a theory
predicate logical
propositional
transitive
model companion
model compatible
model complete
model completion
model interpretable
modus ponens
monotonicity rule
Mostowski
N
n-tuple
negation
neighbor
nonrepresentability lemma
nonstandard analysis
nonstandard model
nonstandard number
normal form
canonical
disjunctive, conjunctive
prenex
Skolem
O
ω-consistent
ω-incomplete

ω-rule
ω-term
object language
operation
essentially n-ary
order
continuous
dense
discrete
linear, partial
ordered pair
ordinal rank
P
p.r. (primitive recursive)
Π1-formula
pair set
pairing function
parameter definable
parenthesis economy
Paris
partial order
irreflexive, reflexive
particularization
anterior, posterior
Peano arithmetic
Peirce's formula
persistent
Polish (prefix) notation
(monic) polynomial
poset
power set
predecessor function
predicate
arithmetical
Diophantine
primitive recursive
recursive

recursively enumerable
preference order
prefix
premise
Presburger
prime field
prime formula
prime model
elementary
prime term
primitive recursion
principle of bivalence
principle of extensionality
product
direct
reduced
programming language
projection
projection function
PROLOG
(formal) proof
propositional variable
modalized
provability logic
for T
provable
provably recursive
Putnam
Q
quantification
bounded
quantifier
quantifier compression
quantifier elimination
quantifier rank
quasi-identity, quasi-variety
query

quotient field
R
r.e. (recursively enumerable)
Rabin
range
rank (of a formula)
recursion equations
recursive definition
reduced formula
reduct
reduct theory
reductio ad absurdum
reflection principle
reflexive
refutable
relation
relativised of a formula
renaming
bound, free
replacement theorem
representability
of Boolean functions
of functions
of predicates
representability theorem
(successful) resolution
resolution calculus
resolution closure
resolution rule
resolution theorem
resolution tree
resolvent
restriction
ring
commutative
ordered
Abraham Robinson

Julia Robinson
Robinson's arithmetic
Rogers
rule
basic
derivable (provable)
Gentzen-style
Hilbert-style
of Horn resolution
sound
rule induction
Russell
Russell's antinomy
S
S-invariant
Σ1-completeness
provable
Σ1-formula
special
Sambin
satisfiability relation
satisfiable
satisfiably equivalent
scope (of a prefix)
segment
initial
terminal
semigroup
free
ordered
regular
semilattice
semiring
ordered
sentence
separator
sequence

sequent
initial
set
countable, uncountable
densely ordered
discretely ordered
finite
ordered
transitive
well-ordered
Sheffer function
signature
algebraic
extralogical
logical
signum function
singleton
Skolem
Skolem function
Skolem's paradox
Skolemization
SLD-resolution
solution
soundness
SP-invariant
Stone's representation theorem
string
atomic
structure
algebraic, relational
subformula
substitution
global
identical
propositional
simple
simultaneous
substitution function
substitution invariance

substitution theorem
substring
substructure
(finitely) generated
elementary
substructure complete
subterm
subtheory
successor function
supremum
symbol
extralogical, logical
of T
symmetric
system (of sets)
T
T-model
Tarski
Tarski fragment
Tarski-Lindenbaum algebra
tautologically equivalent
tautology
term
term algebra
term equivalent
term function
term induction
term model
tertium non datur
theorem
Artin's
Cantor's
Cantor-Bernstein
Dzhaparidze's
Goodstein's
Goryachev's
Herbrand's

Lagrange's
Lindenbaum's
Lindström's
Löb's
Łoś's
Löwenheim-Skolem
Morley's
Rosser's
Shelah's
Steinitz's
Trachtenbrot's
Visser's
theory
(finitely) axiomatizable
arithmetizable
complete
consistent (satisfiable)
countable
decidable
elementary or first-order
equational
essentially undecidable
hereditarily undecidable
inconsistent
inductive
κ-categorical quasi-equational
strongly undecidable
undecidable
universal
transcendental
transitive
(directed) tree
true
truth function
truth functor
truth table
truth value
Turing machine

U
U-resolution
U-resolvent
UH-resolution
ultrafilter
nontrivial
ultrafilter theorem
ultrapower
ultraproduct
undecidable
unifiable
unification algorithm
unifier
generic
union
unique formula reconstruction
unique term concatenation
unique term reconstruction
unit element
universal closure
universal part
universe
urelement
V
valuation
value matrix
variable
free, bound
variety
Vaught
verum
W
w.l.o.g.
word (over A)
word semigroup

Z
ℤ-group
zero-divisor
Zorn's lemma
Index of Symbols
ℕ, ℤ, ℚ, ℝ
ℕ+, ℚ+, ℝ+
∪, ∩
⊆, ⊂
∅, PM
∪S, ∩S
M × N
f(a), fa, a f
f :M → N
x → t(x)
dom f, ran f
id M
M I
(a i )i2208;I
(a 1,…, a n)
graph f
⇔,⇒,&,̄
:=, :⇔
B n
∧ , ∨,¬

ℱ, PV
→,↔, , ⊤, ⊥
PN, RPN
Sf α
wϕ
ℱ n , a ( n )
α ≡ β
DNF, CNF
w ⊧ α, ⊧ α
X ⊧ α, X ⊧ Y
⊢
C+, C-
MP
|~
r A , f A , c A 43
⊑ ̄
T F , T R
charp
2
≃ ̄
lh(ξ)
a/2248;, A/2248;
∏ i∈I A i
I
var
∀, =
S L
T (= T L )
var ξ, var t

∃, ∨
≠
L
L ○, L ∈
L =
T L
rk ϕ, qr ϕ
bnd ϕ
free ϕ
L ○, L k , Var k
ϕ(x1, . ϕ,x 1 …, x n )
ι(iota)
M = (A, w)
r M , f M , c M
t A,w , t M
M ⊧ ϕ
A ⊧ ϕ[w]
A ⊧ ϕ[w]
⊧ ϕ
α ≡ β
A ⊧ ϕ, A ⊧ X
X ⊧ ϕ
ϕ g , X g 64
T G , T = G 65

 ⊧ ϕ
A ⊧ ϕ
 67
ϕ A
⊤ ⊥
A ≡ B
≡ A , ≡ K 75
Q (∀ or ∃)
PNF
|(divides)
X⊧ g ϕ
T, MdT
Taut
T + α, T + S 82
≡ T , ≈ T
ThA, ThK
K ⊨ α
L[r], ϕ rd 85
SNF
⊢
mon, fin
⊢ T α
X ¢ T α
ACF, ACF p
N, S
L ar
≤, <
PA
IS

IA
M ~ N 111
ZFC, ZF
AE, AS, AU
AP, AR
{a, b}, {c}
(a, b)
AI, AF, AC
#x03C9;
V α, V ω
∣∼
Λ, Λ1–Λ10
MQ
Tautfin
Γ⊢ B γ
L II
Q, L Q
Pd
F, FX, FT
T k
F k , F k X
GI(X)
C U , C T
□
K ⊨ H
RR, ⊢ RR
Rc
HR, ⊢ HR
P, N
HR(P,N)

V P , ω P , ρ P
K σ
P, ∶-
GI(K)
sum
UR, ⊢ UR
UHR, ⊢ UHR
U ω R, U ω HR
A A , B A
DA
A ≼ B
D el A
|M|
ℵ0, ℵ1, 2ℵ0 174
CH
DO
L, R
DO00, …
<X> , ≡ X
SO, SO00, …
Γ k (A, B)
A ∼ k B
A ≡ k B
T ∀
T J
RCF
ZGE, ZG
≈ F
a/F, w/F
F n , F

h[g 1, … , g m ]
P[g 1, … , g m ]
Oc, Op, Oµ
f = Op(g, h)
x P
K n c ,  , δ
σ, max
prim, p n
rem(a, d)
b i a
℘(a, b), t n
a 1, …, a n 
GN, ℓ
(a) k , (a) last
#s
ξ, ϕ , t
ν
s, W
bew T , bwb T
T prim , L prim
Q
N

Δ1
⊥(coprime)
IΔ0
β, beta
⌜ϕ⌝, ⌜t⌝, ⌜Φ⌝
bew T , bwb T
prov
α P , X P
T Δ, B Δ
CA
TF
ZFCfin (FST)
Sfin, Sfnd
□(x)
□α, ◊α
Con T
D0–D3
∂, d 0, …
D1*, D2*
□[ϕ]
PA⊥
D4, D4○
T n , T ω
□ n α
ℱ□
□,◊ n
MN
G, ⊢G
P⊩H
⊨G H

G≡G H
G n
GS
1bwb PA
①, ➊
GD
Rf T
ρa
Gi
Gj

