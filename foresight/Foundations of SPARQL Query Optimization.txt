Albert-Ludwigs-Universit¨at Freiburg
Technische Fakult¨at, Institut f¨ur Informatik
Foundations of SPARQL Query Optimization
Dissertation zur Erlangung des Doktorgrades (Dr. rer. nat.)
von
Michael Schmidt
geboren am 11.10.1980 in Neunkirchen
Dekan
:
Prof. Dr. Hans Zappe
Referenten
:
Prof. Dr. Georg Lausen
Prof. Dr. Christoph Koch
Datum der Promotion
:
22.12.2009


Freiburg im Breisgau, 7. Januar 2010
Michael Schmidt


Zusammenfassung
Die SPARQL Anfragesprache wurde vom W3C als Standardsprache zur Ex-
traktion von Daten aus RDF Datenbanken vorgeschlagen, einem Datenfor-
mat das speziell zur maschinenlesbaren Repr¨asentation von Informationen
im Semantischen Web entwickelt wurde. Die vorliegende Arbeit besch¨aftigt
sich mit unterschiedlichen Fragestellungen, die im direkten Zusammenhang
mit der eﬃzienten Auswertung von SPARQL Anfragen stehen. Ein erster
wichtiger Beitrag der Arbeit ist eine vollst¨andige Komplexit¨atsanalyse f¨ur
alle Fragmente der SPARQL Anfragesprache, welche Operatorkonstellationen
aufzeigt, die f¨ur die Komplexit¨at der Auswertung hauptverantwortlich sind.
Ein zentrales Ergebnis in diesem Zusammenhang ist, dass schon der SPARQL
Operator Optional allein, der zur optionalen Extraktion von Komponenten
aus RDF Graphen genutzt werden kann, f¨ur die PSpace-Vollst¨andigkeit der
Anfragesprache verantwortlich ist. Motiviert durch die Komplexit¨atsergeb-
nisse wird im Folgenden die algebraische Optimierung von SPARQL Anfra-
gen untersucht. Als zentrales Hilfsmittel in dieser Analyse entwickeln wir die
Konzepte der m¨oglichen und sicheren Variablen in SPARQL-Anfragen, welche
obere und untere Grenzen f¨ur die in SPARQL Ergebnis-Mappings gebunden-
den Variablen festlegen und somit der Besonderheit ungebundener Variablen
in SPARQL Mappings gerecht werden. Diese Konzepte erm¨oglichen es, ¨Aquiv-
alenzen ¨uber SPARQL Algebra Ausdr¨ucken pr¨azise und kompakt zu spezi-
ﬁzieren und mit ihrer Hilfe entwickeln wir eine Vielzahl von Umformungsregeln
¨uber SPARQL Algebra, einerseits mit dem Ziel etablierte Optimierungstech-
niken aus der Relationalen Algebra, wie z.B. Filter und Projection Push-
ing, in den Kontext von SPARQL zu ¨ubertragen, andererseits um SPARQL-
speziﬁsche Transformations-Schemata zu unterst¨utzen. Unser Ansatz zur alge-
braischen Optimierung wird schließlich durch einen Ansatz zur semantischen
Optimierung von SPARQL Anfragen erg¨anzt, der auf dem klassischen Chase-
Algorithmus zur semantischen Optimierung von Konjunktiven Anfragen auf-
baut. Unsere theoretischen Ergebnisse bez¨uglich algebraischer und semantis-
cher Optimierung sind von direkter praktischer Bedeutung, da sie die Grund-
lage zur Entwicklung von kostenbasierten Optimierungsans¨atzen f¨ur SPARQL
bilden. Abschließend stellen wir einen sprachspeziﬁschen Benchmark f¨ur die
SPARQL Anfragesprache, genannt SP2Bench, vor, der es erm¨oglicht die Per-
formanz sowie St¨arken und Schw¨achen von SPARQL Interpretern in einem
umfassenden und anwendungsunabh¨angigen Szenario zu ermitteln.


Abstract
We study fundamental aspects related to the eﬃcient evaluation of SPARQL,
a prominent query language for the RDF data format that has been developed
for the encoding of machine-readable information in the Semantic Web. Our
key contributions include (i) a complete complexity analysis for all operator
fragments of the SPARQL query language, which identiﬁes operator constel-
lations that make query evaluation hard and – as a central result – shows that
the SPARQL Optional operator alone, which allows for the optional selection
of components in RDF graphs, is responsible for the PSpace-completeness of
the SPARQL evaluation problem; (ii) the novel concepts of possible and cer-
tain variables in SPARQL queries, which constitute upper and lower bounds
for variables that might be bound in SPARQL result mappings, account for
the speciﬁcs of the SPARQL query language, and allow to state equivalences
over SPARQL expressions in a precise and compact way; (iii) a comprehensive
analysis of equivalences over SPARQL algebra, including both the investiga-
tion of rewriting rules like ﬁlter and projection pushing that are well-known
from relational algebra optimization as well as SPARQL-speciﬁc rewriting
schemes; (iv) an approach to the semantic optimization of SPARQL queries,
built on top of the classical chase algorithm; (v) a language-speciﬁc benchmark
suite for SPARQL, called SP2Bench, which allows to assess the performance of
SPARQL implementations in a comprehensive, application-independent set-
ting. Although theoretical in nature, our results on algebraic and semantic
query optimization for SPARQL are of immediate practical interest and facil-
itate the development of cost-based SPARQL optimization schemes.


Acknowledgments
First of all, I wish to thank professor Georg Lausen for the opportunity to
write my thesis at Freiburg University. He gave me the freedom I needed and
contributed to this thesis with many valuable ideas and helpful feedback. In
equal parts, my thanks goes to professor Christoph Koch, my former super-
visor at Saarland University and second referee of the thesis. He aroused my
interest in databases, always had an open ear for me, and introduced me into
the world of science. I also am grateful to the DFG and the Graduiertenkol-
leg Mathematical Logic and Applications, which supported me during the last
years. Further, I wish to thank professor Andreas Podelski and professor Bern-
hard Nebel for their willingness to serve as examiners in my defense.
A special thanks goes to all my colleagues from Freiburg University and former
colleagues from Saarland University, namely Liaquat Ali, Lyublena Antova,
Wei Fang, Christina Fries, Harald Hiss, Matthias Ihle, Norbert K¨uchlin, Elisa-
beth Lott, Stefanie Scherzinger, Kai Simon, Dan Olteanu, Florian Schmedding,
Philip Sorst, Martin Weber, and notably my two work roommates Thomas
Hornung und Michael Meier for the memorable time I spent at work. Be-
yond many interesting discussions and fruitful cooperation, they permanently
oﬀered me a pleasant and motivating working environment.
I am also grateful to all external people who spent their time in proof-reading
the thesis, namely Christoph Pinkel, Stefan Schmidt, and Markus Thiele. Their
feedback and proposals sustainably improved the quality of the thesis.
Last but not least, I wish to thank my mother Martina Schmidt, my father
Gerhard Schmidt, the rest of my family, my girl-friend Marie Leinen, and all
my personal friends with all my heart. They accompanied me for many years
and oﬀered me mental and personal support at all times. In the end, they
gave me the motivation for all the time I spent in my work and in this thesis.
Michael Schmidt


Contents
1. Introduction
1
1.1. Outline and Contributions . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.1.
Complexity of SPARQL Evaluation . . . . . . . . . . . . . . .
7
1.1.2.
Algebraic Optimization of SPARQL Queries . . . . . . . . . .
7
1.1.3.
Constraints for RDF and Semantic SPARQL Optimization . .
8
1.1.4.
Benchmarking of SPARQL Engines . . . . . . . . . . . . . . .
9
1.2. Structure of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3. Summary of Publications . . . . . . . . . . . . . . . . . . . . . . . . .
11
2. Preliminaries
13
2.1. Notational and Mathematical Conventions . . . . . . . . . . . . . . .
17
2.2. The RDF Data Format . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.1.
Predeﬁned RDF and RDFS Vocabulary . . . . . . . . . . . . .
20
2.2.2.
RDF(S) Semantics
. . . . . . . . . . . . . . . . . . . . . . . .
22
2.2.3.
Serialization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.3. The SPARQL Query Language
. . . . . . . . . . . . . . . . . . . . .
25
2.3.1.
An Abstract Syntax for SPARQL . . . . . . . . . . . . . . . .
26
2.3.2.
A Set-based Semantics for SPARQL . . . . . . . . . . . . . . .
27
2.3.3.
From Set to Bag Semantics
. . . . . . . . . . . . . . . . . . .
32
2.3.4.
Oﬃcial W3C SPARQL Syntax and Semantics . . . . . . . . .
35
2.3.5.
Limitations of SPARQL
. . . . . . . . . . . . . . . . . . . . .
38
2.4. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3. Complexity of SPARQL Evaluation
43
3.1. Preliminaries: Complexity Theory . . . . . . . . . . . . . . . . . . . .
46
3.1.1.
The Polynomial Hierarchy . . . . . . . . . . . . . . . . . . . .
46
3.1.2.
Complete Problems . . . . . . . . . . . . . . . . . . . . . . . .
46
3.2. Complexity of SPARQL
. . . . . . . . . . . . . . . . . . . . . . . . .
47
3.2.1.
Set vs. Bag Semantics
. . . . . . . . . . . . . . . . . . . . . .
48
3.2.2.
Complexity of OPTIONAL-free Expressions
. . . . . . . . .
49
3.2.3.
Complexity of Expression Classes Including OPTIONAL
. .
51
3.2.4.
The Source of Complexity . . . . . . . . . . . . . . . . . . . .
61
3.2.5.
From Expressions to Queries . . . . . . . . . . . . . . . . . . .
62
3.2.6.
Summary of Results
. . . . . . . . . . . . . . . . . . . . . . .
64
I

3.3. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.4. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4. Algebraic SPARQL Query Optimization
69
4.1. Possible and Certain Variables . . . . . . . . . . . . . . . . . . . . . .
73
4.2. Optimization Rules for Set Algebra . . . . . . . . . . . . . . . . . . .
75
4.2.1.
The Incompatibility Property . . . . . . . . . . . . . . . . . .
76
4.2.2.
Idempotence and Inverse . . . . . . . . . . . . . . . . . . . . .
79
4.2.3.
Associativity, Commutativity, and Distributivity . . . . . . . .
80
4.2.4.
Projection Pushing . . . . . . . . . . . . . . . . . . . . . . . .
82
4.2.5.
Filter Decomposition, Elimination, and Pushing . . . . . . . .
85
4.2.6.
Rewriting Closed World Negation . . . . . . . . . . . . . . . .
90
4.3. From Set to Bag Algebra . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.3.1.
The Incompatibility Property for SPARQL Bag Algebra
. . .
94
4.3.2.
Optimization Rules for Bag Algebra . . . . . . . . . . . . . . .
97
4.4. Summary of Results and Practical Implications
. . . . . . . . . . . . 101
4.4.1.
SPARQL ASK Queries . . . . . . . . . . . . . . . . . . . . . . 102
4.4.2.
SPARQL DISTINCT and REDUCED Queries . . . . . . . . . 103
4.5. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
4.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5. Constraints for RDF and Semantic SPARQL Query Optimization
111
5.1. Preliminaries: First-order Logic, Relational Databases, Constraints,
and Chase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5.1.1.
First-order Logic
. . . . . . . . . . . . . . . . . . . . . . . . . 114
5.1.2.
Relational Databases and Conjunctive Queries . . . . . . . . . 116
5.1.3.
Relational Constraints . . . . . . . . . . . . . . . . . . . . . . 117
5.1.4.
The Chase Algorithm . . . . . . . . . . . . . . . . . . . . . . . 119
5.2. Constraints for RDF . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
5.2.1.
Equality-generating Dependencies . . . . . . . . . . . . . . . . 123
5.2.2.
Tuple-generating Dependencies
. . . . . . . . . . . . . . . . . 124
5.2.3.
Disjunctive Embedded Dependencies . . . . . . . . . . . . . . 126
5.3. SPARQL as a Constraint Language . . . . . . . . . . . . . . . . . . . 127
5.4. Semantic Query Optimization for SPARQL . . . . . . . . . . . . . . . 133
5.4.1.
A Motivating Scenario . . . . . . . . . . . . . . . . . . . . . . 134
5.4.2.
Chase-based Optimization of SPARQL . . . . . . . . . . . . . 135
5.4.3.
Optimizing AND-only Blocks
. . . . . . . . . . . . . . . . . . 136
5.4.4.
SPARQL-speciﬁc Optimization
. . . . . . . . . . . . . . . . . 139
5.5. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
II

6. SP2Bench: A SPARQL Performance Benchmark
147
6.1. Benchmark Design Decisions . . . . . . . . . . . . . . . . . . . . . . . 150
6.2. The DBLP Data Set
. . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.2.1.
Structure of Document Classes
. . . . . . . . . . . . . . . . . 153
6.2.2.
Repeated Attributes
. . . . . . . . . . . . . . . . . . . . . . . 154
6.2.3.
Development of Document Classes over Time
. . . . . . . . . 157
6.2.4.
Authors and Editors . . . . . . . . . . . . . . . . . . . . . . . 158
6.2.5.
Citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.3. The SP2Bench Data Generator
. . . . . . . . . . . . . . . . . . . . . 161
6.3.1.
An RDF Scheme for DBLP
. . . . . . . . . . . . . . . . . . . 161
6.3.2.
Data Generator Implementation . . . . . . . . . . . . . . . . . 164
6.3.3.
Data Generator Evaluation
. . . . . . . . . . . . . . . . . . . 165
6.4. The SP2Bench Queries . . . . . . . . . . . . . . . . . . . . . . . . . . 166
6.4.1.
RDF Characteristics of Interest . . . . . . . . . . . . . . . . . 166
6.4.2.
SPARQL Characteristics of Interest . . . . . . . . . . . . . . . 168
6.4.3.
Discussion of Benchmark Queries . . . . . . . . . . . . . . . . 169
6.5. Benchmark Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
6.6. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
6.7. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
7. Conclusion and Outlook
179
Bibliography
180
A. Survey of Namespaces
193
B. Proofs of Complexity Results
195
B.1. Proof of Theorem 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
C. Proofs of Algebraic Optimization Results
203
C.1. Proof of Lemma 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
C.2. Proof of the Equivalences in Figure 4.3 . . . . . . . . . . . . . . . . . 204
C.3. Proof of the Equivalences in Figure 4.4 . . . . . . . . . . . . . . . . . 206
C.4. Proof of Lemma 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
C.5. Proof of Proposition 4.4
. . . . . . . . . . . . . . . . . . . . . . . . . 209
C.6. Proof of Lemma 4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
C.7. Proof of Lemma 4.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
C.8. Proof of Lemma 4.10 . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
C.9. Proof of Lemma 4.11 . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
C.10.Proof of Lemma 4.13 . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
III

IV

List of Figures
1.1. Semantic Web layer cake . . . . . . . . . . . . . . . . . . . . . . . . .
2
2.1. Relational instance storing the RDF database from the Introduction .
14
2.2. RDF graph representation of the database from Figure 2.1 . . . . . .
15
2.3. RDF graph pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.4. (a) RDF sequence; (b) RDF list . . . . . . . . . . . . . . . . . . . . .
22
2.5. Selected RDFS inference rules . . . . . . . . . . . . . . . . . . . . . .
23
3.1. Summary of complexity results
. . . . . . . . . . . . . . . . . . . . .
65
4.1. Algebraic equivalences: idempotence and inverse . . . . . . . . . . . .
79
4.2. Algebraic equivalences: associativity, commutativity, and distributivity 81
4.3. Algebraic equivalences: projection pushing . . . . . . . . . . . . . . .
83
4.4. Algebraic equivalences: ﬁlter manipulation . . . . . . . . . . . . . . .
87
5.1. Constraint templates: equality-generating dependencies . . . . . . . . 124
5.2. Constraint templates: tuple-generating dependencies . . . . . . . . . . 125
5.3. Constraint templates: disjunctive embedded dependencies . . . . . . . 126
6.1. Extract of the DBLP DTD . . . . . . . . . . . . . . . . . . . . . . . . 153
6.2. Distribution of citations for documents having at least one citation
. 154
6.3. Development of author (a) expected value and (b) statistical spread
for publications having at least one author . . . . . . . . . . . . . . . 156
6.4. (a) Expected number of authors for publications having at least one
author; (b) Development of documents over time . . . . . . . . . . . . 157
6.5. Approximation functions for document class counts . . . . . . . . . . 158
6.6. Powerlaw distributions in DBLP: (a) Number of publications per au-
thor over time; (b) Distribution of citations among publications
. . . 159
6.7. Translation of DBLP attributes into RDF properties
. . . . . . . . . 162
6.8. Sample RDF database
. . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.9. Data generation algorithm . . . . . . . . . . . . . . . . . . . . . . . . 164
A.1. Survey of namespaces used in the thesis . . . . . . . . . . . . . . . . . 193
B.1. Expression tree and associated complexity . . . . . . . . . . . . . . . 200
V

VI

List of Tables
4.1. Survey of algebraic equivalences . . . . . . . . . . . . . . . . . . . . . 110
6.1. Probability distribution for selected attributes . . . . . . . . . . . . . 153
6.2. Performance of document generation
. . . . . . . . . . . . . . . . . . 165
6.3. Characteristics of generated documents . . . . . . . . . . . . . . . . . 166
6.4. Selected properties of the SP2Bench benchmark queries . . . . . . . . 167
6.5. Number of query results (Q1–Q11) and results of ASK queries (Q12abc)
on documents containing up to 25 million RDF triples
. . . . . . . . 169
VII

VIII

Chapter 1.
Introduction
Jen:
“Ever heard about the Semantic Web?”
Roy:
“Yes, but that has nothing to do with databases, has it? By the
way, these Semantic Web guys have their own community, so
I guess there is nothing we could do for them...”
Moss: “Well, Semantic Web means machine-readable information, so
there is deﬁnitely some data. And I’m pretty sure these guys
appreciate expertise from the database community!”
In May 2001, Tim Berners-Lee described his vision of a “new form of Web con-
tent that is meaningful to computers” and would “unleash a revolution of new
possibilities” in an article called The Semantic Web [BLHL01]. Central to the idea
of the Semantic Web is the encoding of information in a machine-readable format,
in coexistence with human-readable HTML pages which are almost impossible to
“understand” and interpret by computers. The vision was that the semantic anno-
tation of Web resources would allow computers to automatically reason about the
underlying data and enable them to make reliable and logically founded conclusions.
At a global scale, the Semantic Web thus can be understood as a large knowledge
base that links information from diﬀerent sources together, eases the access to in-
formation, and ultimately improves search and knowledge discovery in the Internet.
In their eﬀort to push the vision of the Semantic Web, the World Wide Web Con-
sortium (W3C) founded working groups to systematically work out and standardize
its technical foundations [sww]. It soon became clear that the realization of the Se-
mantic Web is an ambitious project that has touching points with numerous areas of
Computer Science. One important subgoal, for instance, is the design of languages
that oﬀer logically founded reasoning mechanisms. This topic is closely related to
description logics [BCM+03], which have been studied in the ﬁeld of Artiﬁcial In-
telligence for many years. Another major challenge, and the one we are going to
deal with in the following, is the storage and manipulation of data in the Semantic
Web. Related problems have – although in the context of diﬀerent data formats –
been extensively studied in database research. Tackling the issue of eﬃcient data
processing in the Semantic Web, in this thesis we investigate the challenges that
arise in the context of the standard Semantic Web data format RDF [rdfa] and,
1

Chapter 1. Introduction
Figure 1.1.: Semantic Web layer cake (taken from [W3Ca]).
in particular, the SPARQL query language for RDF [spac], with the goal to bring
together classical database research and the novel ideas behind the Semantic Web.
The diagram in Figure 1.1, known as the Semantic Web layer cake, summa-
rizes the current state of the W3C eﬀorts.1 At the very bottom of the layer cake
are the concepts of Uniform Resource Identiﬁers (URIs) [w3cc; w3cd] and Inter-
nationalized Resource Identiﬁers (IRIs) [w3cb], which provide ways to construct
names for globally unique physical or logical resources. Abstracting from the de-
tails, URIs are ASCII strings consisting of a namespace and an identiﬁer part.2 For
instance, the URI http://my.namespace.com#Article1 is composed of namespace
http://my.namespace.com# and identiﬁer Article1. It may be used as a globally
unique identiﬁer for a speciﬁc article. For readability reasons, we shall use shortened
versions of URIs, so-called preﬁxed URIs, e.g. writing myns:Article1 for the above
URI, where myns is a shortcut for the namespace http://my.namespace.com#.
The second layer in the Semantic Web cake, built on top of the ﬁrst layer, con-
tains the Resource Description Framework (RDF) [rdfa], which can be used for data
encoding and exchange. Problems like storage, manipulation, and access to RDF
1There are diﬀerent proposals for the architecture of the Semantic Web and diﬀerent versions of
the layer cake, see [GdMB08] for a discussion. We use the latest W3C proposal from [W3Ca].
2IRIs are generalized, internationalized versions of URIs, which diﬀer only in technical details. In
the remainder of this thesis, we will restrict our discussion to URIs.
2

data are closely related to the ﬁeld of databases in nature and therefore will be sub-
ject to investigation in this thesis. Conceptionally, RDF databases are collections of
so-called triples of knowledge, where each knowledge triple in the database is of the
form (subject, predicate, object) and models the binary relation predicate between
the subject and the object. While structurally homogeneous, the triple format oﬀers
great ﬂexibility and allows to establish statements about and connections between
resources (which are encoded as URIs). Consider for instance the RDF database
D := {(myns:Article1, rdf:type, myns:Article),
(myns:Article1, dc:title, “The Semantic Web”),
(myns:Article1, myns:journal, myns:Journal1),
(myns:Article2, rdf:type, myns:Article),
(myns:Article2, dc:title, “WWW: Past, Present, and Future”),
(myns:Article2, dcterms:issued, “1996”),
(myns:Journal1, rdf:type, myns:Journal),
(myns:Journal1, dc:title, “Scientiﬁc American”),
(myns:Person1, rdf:type, foaf:Person),
(myns:Person1, foaf:name, “Tim Berners-Lee”),
(myns:Article1, dc:creator, myns:Person1),
(myns:Article2, dc:creator, myns:Person1)},
which contains twelve RDF triples in total. To give an informal description, there
are two articles (typed using myns:Article) in the database, one titled “The Se-
mantic Web”, the other one titled “WWW: Past, Present, and Future”. The articles
are encoded as URIs myns:Article1 and myns:Article2, while their titles are rep-
resented as simple string values, so-called literals. The ﬁrst article appeared in a
journal represented by URI myns:Journal1 that is called “Scientiﬁc American”.
For the second article, the journal is not speciﬁed, but instead the year “1996” in
which the article has been issued is given. Finally, the last four triples encode that
there is a person (encoded by URI Person1) with name “Tim Berners-Lee” that
has written (dc:creator) the two articles myns:Article1 and myns:Article2.
The ﬁrst interesting thing to observe here is that the RDF database integrates
vocabulary from diﬀerent namespaces, i.e. the standard RDF namespace rdf, a user-
deﬁned namespace myns, as well as the namespaces foaf, dc, and dcterms. The
standard namespace rdf provides some basic vocabulary with predeﬁned semantics,
such as rdf:type used for typing URIs. Next, foaf is a public namespace that
provides domain-speciﬁc vocabulary designed to describe persons in a uniform way.
Similar in idea, the namespaces dc and dcterms provide predeﬁned vocabularies
for describing bibliographic entities. The freedom to mix vocabulary from diﬀerent
namespaces accounts for the global nature of the RDF data format. On the one hand,
this allows to connect entities from diﬀerent RDF databases, on the other hand it
makes it possible to develop standardized, domain-speciﬁc vocabulary collections
that can be utilized by developers when creating databases for the respective domain.
3

Chapter 1. Introduction
A second interesting aspect of RDF that is illustrated in the above examples is the
ﬂexibility of the data format: although both myns:Article1 and myns:Article2 are
of the same type myns:Article, they are structurally diﬀerent: one of them comes
with a title and a journal speciﬁcation, while for the other one the title and the
year of publication are provided. In fact, in RDF there is no a-priori restriction
on the structure of entities, which makes the data format an excellent candidate
for describing semi-structured and unstructured data. In this regard, RDF sharply
contrasts with the relational data model, which relies on a ﬁxed schema.
The RDF standard [rdfa] taken alone comprises only syntax speciﬁcations and a
minimalistic collection of predeﬁned vocabulary. It is extended by the RDF Schema
speciﬁcation (RDFS) [rdfc], settled in the third layer of the Semantic Web cake.
RDFS provides additional, general-purpose vocabulary with predeﬁned semantics.
For instance, the URIs rdfs:domain and rdfs:range can be used for domain and
range assignments, while rdfs:subClassOf and rdfs:subPropertyOf express sub-
class and subproperty relationships.3 Making the intended meaning of this vocabu-
lary explicit, the RDFS speciﬁcation is complemented by a formal semantics [rdfc],
which implements a lightweight reasoning mechanism. Assume for instance that
we add triple (myns:Article,rdfs:subClassOf,myns:Publication) to database D
from before, stating that each article also is a publication. The RDFS semantics
then implies two fresh triples, i.e. (myns:Article1,rdf:type,myns:Publication)
and (myns:Article2,rdf:type,myns:Publication), which are implicit in D.
There are three technologies that coexist with RDFS in the third layer: the Web
Ontology Language (OWL) [owl], the Rule Interchange Format (RIF) [rifa; rifb],
and the SPARQL query language for RDF [spac]. First, OWL is a data descrip-
tion language which, in the style of description logics [BCM+03], allows to set up
user-deﬁned vocabularies with custom semantics, making it possible to implement
extended (and possibly domain-speciﬁc) reasoning mechanisms beyond the prede-
ﬁned semantics of the RDFS vocabulary. Second, RIF is an ongoing W3C eﬀort to
standardize a core rule language to exchange rules between diﬀerent rule systems.
Third, the SPARQL query language oﬀers support to extract data from RDF(S)
and OWL databases. SPARQL will be the central topic in the course of this thesis.
The basic construct of SPARQL queries are so-called triple patterns, which – when
evaluating the query against some RDF database – are matched against the RDF
triples in the input database. As an example, consider the following SPARQL query.
SELECT ? a r t i c l e
WHERE {
? a r t i c l e
r d f : type myns : A r t i c l e
.
? a r t i c l e
dcterms : i s s u e d
? year }
The query contains the two triple patterns ?article rdf:type myns:Article
3rdfs is the default shortcut for namespace http://www.w3.org/2000/01/rdf-schema#.
4

and ?article dcterms:issued ?year; structurally, triple patterns are RDF triples
with variables in some positions (we distinguish variables by a question mark preﬁx).
During query evaluation, these patterns are matched against the triples in the input
database and variables inside the patterns are bound to the respective components
of matching triples. For instance, the evaluation of the ﬁrst pattern against sample
database D from before has two solutions, namely (a1) the mapping that binds
variable ?article to myns:Article1 and (a2) the mapping that binds ?article to
myns:Article2. Similarly, the evaluation of the second triple pattern yields a single
solution mapping, in which (b) variable ?article is bound to myns:Article2 and
?year is bound to the literal “1996”. Next, we can observe that in our sample query
the two patterns are connected through operator “.”, which can be understood as a
join operation. This operator merges exactly those mappings that agree on all shared
variables, so-called compatible mappings. It is easy to see that in our example the
mappings (a1) and (b) are incompatible, because they disagree on variable ?article,
while the mappings (a2) and (b) agree on the single shared variable ?article and
therefore are compatible. Consequently, the evaluation of the join operation yields
a single result mapping, obtained by merging the mappings (a2) and (b), i.e. we
obtain the mapping in which ?article is bound to myns:Article2 and ?year to
“1996” (mapping (a2) is a “subset” of mapping (b), so merging these mappings
gives us a mapping identical to (b)). In the ﬁnal evaluation step, the Select clause
in the query projects variable ?article, so the evaluation result of the query on
document D is the mapping that binds variable ?article to myns:Article2. To
give an informal description of the semantics, the query extracts all articles for which
the year of publication (i.e., predicate dcterms:issued) is present in the database.
In addition to the triple patterns and operator “.” sketched in the running exam-
ple, SPARQL provides the operators Filter, Optional, and Union, which can
be used to compose more expressive queries. Before discussing SPARQL in more
detail, though, we shortly sketch the remaining layers of the Semantic Web cake
from Figure 1.1, to complete the big picture. The main challenge in the fourth layer
is to deﬁne a unifying logic that integrates the reasoning mechanisms of underlying
layers into one logical framework and allows to create proofs of deductions that can
be veriﬁed by users and software agents. As a classical example [KM01], such proof
mechanisms could be used to automatically derive user access rights to Web pages
based on RDF data that is associated with the page. As also shown in Figure 1.1,
the technologies in layers one to four are complemented by encryption technology
(such as digital signatures, which allow to verify the origin of information).
The ﬁfth layer of the Semantic Web then covers the issue of trust [Siz07; Har09],
pursuing the goal to establish mechanisms that build and propagate trust at global
scale. We refer the interested reader to [OH08] for a discussion of research challenges
in this context. Finally, on top of the technologies and mechanisms in layers one
through ﬁve, user interfaces and applications can be built that rely on the content
of the Semantic Web and exploit technologies from underlying layers.
5

Chapter 1. Introduction
1.1. Outline and Contributions
Having introduced the global context and the basic ideas behind data management
in the Semantic Web, we now summarize the work that will be presented in this
thesis and highlight the major contributions. As should be clear from the previ-
ous discussion, our focus will be on the RDF data format and the SPARQL query
language. We tackle these technologies from a database perspective and work out
parallels between SPARQL and classical query languages like SQL, but also dis-
cuss particularities of the RDF data format and the SPARQL query language. In
summary, we investigate four major aspects, all of which are closely related to the
eﬃcient processing of SPARQL queries over RDF databases:
1. The complexity of SPARQL evaluation.
2. Algebraic optimization of SPARQL queries.
3. Constraints for RDF, SPARQL as a constraint language, and semantic SPARQL
query optimization in the presence of constraints for RDF.
4. Benchmarking of SPARQL engines.
Before motivating these topics, we will give some more background on the SPARQL
query language. As discussed earlier, the central constructs in SPARQL are (i) triple
patterns of the form (subject, predicate, object), which may contain variables in their
positions, and (ii) the operators “.” (denoted as And in the following), Filter,
Union, and Optional. In addition, there is a Select clause that allows to project
a subset of the variables used in the query. The semantics of SPARQL evaluation now
proceeds as follows. First, it maps all the operators occurring in the SPARQL query
to algebraic operators, i.e. transforms the query into a SPARQL algebra expression.
This algebra expression is then evaluated on the RDF input database.
The algebraic operators that are deﬁned in SPARQL resemble the algebraic oper-
ators deﬁned in relational algebra; in particular, operator And is mapped to an al-
gebraic join, Filter is mapped to an algebraic selection operator, Union is mapped
to a union operator, Optional is mapped to a left outer join (which allows for the
optional padding of information), and Select is mapped to a projection operator.
As opposed to the operators in relational algebra, which are deﬁned on top relations
with ﬁxed schema, the algebraic SPARQL operators are deﬁned over so called map-
ping sets, obtained when evaluating triple patterns (cf. the running example before).
In contrast to the ﬁxed schema in relational algebra, the “schema” of mappings in
SPARQL algebra is loose in the sense that such mappings may bind an arbitrary set
of variables. This means that in the general case we cannot give guarantees about
which variables are bound or unbound in mappings that are obtained during query
evaluation. In fact, this turns out to be a signiﬁcant diﬀerence which – although
relational and SPARQL algebra deﬁne operators that are similar in idea – implies
some fundamental conceptional diﬀerences between the two algebras. With these
considerations in mind, we are now ready to discuss the contributions of the thesis.
6

1.1. Outline and Contributions
1.1.1. Complexity of SPARQL Evaluation
The ﬁrst central topic in this thesis is a thorough complexity analysis for the
SPARQL query language. We argue that the study of the complexity of a lan-
guage is beneﬁcial in several respects. First, it deepens the understanding of the
individual operators and their interrelations. Second, it allows to separate subsets
of the language that can be evaluated eﬃciently from more complex (and therefore
typically more expressive) subsets of the language, which may be of interest when
building applications that use (fragments of) the SPARQL query language. Last but
not least, our complexity analysis relates the SPARQL query language and speciﬁc
fragments to traditional query languages, which allows to carry over results and
optimization techniques developed in the context of the traditional language.
As usual in the study of query languages, we take the (combined) complexity of
the Evaluation problem as a yardstick for our investigation: given a candidate
solution mapping µ, a query Q, and a data set D as input, we are interested in the
complexity of checking whether µ is contained in the result of evaluating Q on D.
Previous investigations of SPARQL complexity in [PAG06a] have shown that full
SPARQL is PSpace-complete, which is bad news from a complexity point of view.
The study in [PAG06a], though, leaves several questions unanswered. To give only
one example, it is not clear which operator constellation is ultimately responsible for
the PSpace-hardness of the language. We therefore re-investigate the complexity
of SPARQL and complement previous investigations by important new ﬁndings:
Contribution 1 As a key result of our complexity study, we show that the main
source of complexity in SPARQL is operator Optional, which taken alone al-
ready makes the Evaluation problem for SPARQL PSpace-complete (in combined
complexity). This result considerably reﬁnes previous investigations from [PAG06a],
where it was shown that the combination of the three operators Optional, And,
and Union makes the evaluation problem for SPARQL PSpace-complete.
In addition to the previous ﬁnding, we present complexity results for fragments of
SPARQL without the complex Optional operator. Our investigation is complete in
the sense that we derive tight complexity bounds for all possible operator combina-
tions, showing that the complexity of SPARQL query evaluation is either PSpace-
complete (i.e., for all fragments involving operator Optional), NP-complete (i.e.,
for Optional-free fragments where operator And co-occurs with Union or projec-
tion), or is in PTime (for the remaining fragments). Among other things, this shows
that adding (removing) the Filter operator to (from) a fragment of the SPARQL
query language in no case aﬀects the complexity of query evaluation.
2
1.1.2. Algebraic Optimization of SPARQL Queries
Having established the theoretical background, we turn towards an investigation
of algebraic SPARQL query optimization. Motivated by the success of algebraic
7

Chapter 1. Introduction
rewriting rules in the context of relational algebra (see e.g. [Hal75; SC75; Tod75]),
we investigate algebraic rewriting rules for SPARQL algebra. The goal of this study
is twofold. Firstly, we are interested in whether (and how) established optimization
techniques proposed for relational algebra, such as ﬁlter and projection pushing,
carry over to SPARQL algebra. Secondly, going beyond the adaption of existing
techniques, we study SPARQL-speciﬁc optimization schemes. It turns out that, given
the discrepancies between relational algebra and SPARQL algebra discussed earlier
in this section, the adaption of rewriting techniques from relational algebra is a
non-trivial task and requires new concepts and techniques to properly cope with the
loose schema in result mappings. Our major results are the following.
Contribution 2 As a general approach to deal with the loose structure of result
mappings, we develop the concepts of possible and certain variables. Possible and
certain variables are derived from the input query at compile time and constitute
upper and lower bounds for variables that appear in result mappings, independently
from the input document. In this line, they give guarantees about variables that are
deﬁnitely bound and variables that are deﬁnitely not bound in result mappings, thus
imposing structural constraints on mapping sets. Ultimately, these concepts allow
us to state equivalences over SPARQL algebra in a clean and compact way.
Building upon the notions of possible and certain variables, we then summarize
existing and develop new equivalences over SPARQL algebra. In total, we present
about forty equivalences, of which almost three-fourths are new. When interpreted
as rewriting rules, these equivalences allow to implement valuable rewriting tech-
niques that are known from the relational context in SPARQL algebra, in particular
ﬁlter and projection pushing. In addition, we study general-purpose rewriting rules
and SPARQL-speciﬁc optimization schemes, like the rewriting of queries involving
negation which – in SPARQL algebra expressions – is typically encountered in form
of a characteristic constellation of the left outer join and the selection operator.
We ﬁnally investigate the diﬀerences between bag and set semantics in the con-
text of SPARQL algebra equivalences. While previous theoretical investigations of
SPARQL typically build on the set-based SPARQL semantics proposed in [PAG06a],
the oﬃcial W3C SPARQL speciﬁcation [spac] recommends a bag semantics. We show
that almost all rewriting rules that we propose in our study hold under both set and
bag semantics and therefore are of immediate practical relevance.
2
1.1.3. Constraints for RDF and Semantic SPARQL Optimization
Integrity constraints such as keys, foreign keys, or join dependencies, have been an
important topic in classical database research from the beginning [Cod70; Cod71;
Cod74; HM75; Che76; Cod79]. They impose structural restrictions that must hold
on every database instance, thus restricting the state space of the database. It is well-
known that such constraints are vital to schema design, normalization, and lastly
8

1.1. Outline and Contributions
help to avoid insert, update, and deletion anomalies in relational databases [Cod71;
Cod74; Che76; AHV]. Beyond that, they are valuable input to query optimizers and
may help to ﬁnd more eﬃcient evaluation plans [Kin81; BV84; CGM90]. In response
to their central role in relational databases, we investigate constraints for RDF and
study the role of the SPARQL query language in such a setting. Our major ﬁndings
and results in this context can be summarized as follows.
Contribution 3 We propose a collection of integrity constraints for the RDF data
format, including constraint types like functional and inclusion dependencies, or
cardinality constraints, which are well-known from relational databases and XML.
In addition, we discuss SPARQL-speciﬁc constraints that capture the semantics of
the RDFS vocabulary. Our formalization in ﬁrst-order logic shows that many natural
RDF(S) constraints fall into constraint classes that have previously been studied in
the context of relational databases and XML [Fag77; Fag82; BV84; DT01; DT05].
Furthermore, we investigate the capabilities of SPARQL as a constraint language.
As a major result, we show that SPARQL – with only minor extensions – is an
excellent candidate for dealing with constraints, i.e. can be used to check if RDF
constraints that are speciﬁed in ﬁrst-order logic hold over some input database.
This result motivates extensions of SPARQL by constructs to specify user-deﬁned
constraints, e.g. in the style of Create Assertion statements in SQL.
From a practical point of view, we complement the study of algebraic query op-
timization with an approach to semantic SPARQL query optimization. The idea
behind semantic query optimization is, given a query and a set of integrity con-
straints as input, to ﬁnd more eﬃcient queries that are equivalent on every database
instance that satisﬁes the integrity constraints. We present a system that allows to
ﬁnd minimal equivalent rewritings of SPARQL And-only queries (or, alternatively,
can be applied to optimize And-only blocks in more complex queries) and propose
a rule-based approach for optimizing more complex SPARQL queries.
2
1.1.4. Benchmarking of SPARQL Engines
As a ﬁnal contribution, we propose a benchmark for the SPARQL query language.
The central motivation for this work was the observation that in the recent years
a variety of optimization approaches for SPARQL have been made, but a compre-
hensive, SPARQL-speciﬁc benchmark platform was missing, which made it almost
impossible to compare the performance of SPARQL optimization schemes and im-
plementations. Addressing this issue, our fourth major contribution is the following.
Contribution 4 We present SP2Bench, a publicly available benchmark suite de-
signed to test the performance of SPARQL engines. Unlike other benchmarks that
have been proposed in the context of the Semantic Web (such as [GPH05; AMMH;
BS09]), SP2Bench is not motivated by a single use-case, but instead is highly
9

Chapter 1. Introduction
SPARQL-speciﬁc and covers a variety of challenges that engines may face when
processing RDF(S) data with SPARQL. This way, it allows to assess the perfor-
mance of implementations in a comprehensive and application-independent setting.
SP2Bench comes with a data generator that allows to create arbitrarily large
bibliographic databases in RDF format. The design of the data generator relies on
an in-depth study of the well-known DBLP database [Ley] and, in response, the
generated data mirrors vital social-world relationships encountered in real-world
DBLP data. The data generator is complemented by a set of 17 benchmark queries,
speciﬁcally designed to test characteristic SPARQL operator constellations and RDF
access patterns over the generated documents. We discuss design decisions, data
generator implementation, and the speciﬁc challenges of the benchmark queries. 2
1.2. Structure of the Thesis
We start with globally relevant preliminaries in Chapter 2, including mathematical
conventions and a formalization of RDF(S) and SPARQL. Next, Chapters 3 to 6 con-
tain the main content and all the contributions of this thesis. The ordering of these
chapters sequentially follows the list of contributions discussed in Section 1.1: Chap-
ter 3 contains the complexity analysis for the SPARQL query language, Chapter 4
presents our results on algebraic SPARQL optimization, Chapter 5 treats the subject
of constraints for RDF and semantic SPARQL query optimization, and Chapter 6
covers the SP2Bench SPARQL performance benchmark. We wrap up with a short
discussion of future work and some concluding remarks in Chapter 7.
How to Read This Thesis. The preliminaries in Chapter 2 establish technical
background and mathematical notation that are relevant for all subsequent chapters
of the thesis. In addition to this global preliminaries section, the individual chapters
may contain their own, local preliminaries, where we introduce notation and concepts
that are relevant for the respective chapter only. With some minor exceptions, the
chapters are self-contained and do not signiﬁcantly depend on each other. Whenever
we use notation or concepts in some chapter that were not introduced in the global
or its local preliminaries section, we will explicitly point to the position where they
were deﬁned. Therefore, the reader who is familiar with the global preliminaries may
study the chapters in the thesis in random order. We recommend, however, to read
the chapters sequently, to follow the logical ordering of the topics. In particular, the
complexity results right at the beginning in Chapter 3 establish a deep understand-
ing for the expressiveness of the operators and facilitate the understanding of the
algebraic and semantic rewriting approaches in Chapters 4 and 5. Furthermore, the
semantic optimization approach for SPARQL presented in Chapter 5 builds upon
ideas and proofs discussed before in Chapter 4. Finally, the SPARQL benchmark
in Chapter 6 takes common optimization strategies for SPARQL into account and
therefore picks up several ideas that were discussed in the two preceding chapters.
10

1.3. Summary of Publications
Structure of Chapters. Each chapter starts with an introduction that motivates
its speciﬁc content, makes the reader familiar with the topic, and surveys its contri-
butions in more detail. The introduction is followed by an optional, chapter-speciﬁc
preliminaries section, the content (which typically splits up into several sections),
and a chapter-speciﬁc discussion of related work as well as a short conclusion.
1.3. Summary of Publications
Parts of the work that is discussed in the course of this thesis have been published
at major database venues and as technical reports. In the following we summarize
publications that are related to this thesis and sketch their relations to this work.
Most of the author’s publications have been done in the area of SPARQL pro-
cessing and benchmarking [LMS08; SHK+08; SHLP08; SML08; SHLP09; SHM+09].
In [LMS08] we discussed the prospects of integrity constraints for RDFS. We pro-
posed a set of constraints for RDF(S) and showed that these constraints can be
both modeled with user-deﬁned RDF(S) vocabulary and checked with the SPARQL
query language. Moreover, we motivated constraint-based optimization of SPARQL
by example. Revisiting the latter idea, in the technical report [SML08] we worked
out a schematic approach to constraint-based SPARQL query optimization. Many
of the above-mentioned ideas and results have found entrance in Chapter 5, where
we study constraints for RDF(S) and semantic optimization of SPARQL.
In addition to the semantic query optimization approach, in [SML08], we pre-
sented a complexity analysis for SPARQL and discussed algebraic SPARQL query
optimization. An extended version of the complexity analysis can be found in Chap-
ter 3. Similarly, the study of algebraic query optimization in Chapter 4 extends the
results on algebraic SPARQL optimization that were presented in [SML08].
The discussion of the SP2Bench SPARQL benchmark in Chapter 6 relates to our
work presented in [SHK+08; SHLP09; SHM+09] (and the technical report [SHLP08]).
The latter chapter essentially follows the benchmark descriptions from [SHLP09;
SHM+09]. In addition to the material presented in this thesis, the interested reader
may ﬁnd preliminary SP2Bench benchmark results for selected SPARQL engines
and state-of-the-art SPARQL evaluation approaches in [SHK+08; SHLP09].
Finally, we published work concerning the classical chase algorithm (see [MSL09d;
MSL09a] and the associated technical reports [MSL09c; MSL09b]).4 Tackling the is-
sue of the possible non-termination of the chase, the latter publications present novel
suﬃcient termination for the chase algorithm, which strictly generalize previously
known termination conditions. Although we will not discuss these termination con-
ditions in the course of this thesis, we want to point out that they are beneﬁcial
in the context of our semantic query optimization approach for SPARQL, as they
4This work will be published in the thesis of Michael Meier and therefore is not discussed here.
11

Chapter 1. Introduction
extend the applicability of our semantic optimization scheme (which builds upon
the chase procedure). We will resume this discussion later in Chapter 5.
12

Chapter 2.
Preliminaries
Jen:
“Sounds like the Semantic Web is huge...”
Roy:
“Yes, and the W3C specs include lots of boring details...”
Moss: “You’re both right. I agree that RDF is a nice data format, but
do we really need a query language for it? Why don’t we just
use SQL queries to extract data from RDF graphs?”
Roy:
“That’s a reasonable question. So let’s ﬁrst motivate SPARQL
and then deﬁne a compact fragment of the language!”
As shortly discussed in the Introduction, RDF databases are collections of triples
of the form (subject,predicate,object). Due to their homogeneous structure, they are
often represented as labeled directed graphs, where each triple deﬁnes an edge from
the subject node to the object node under label predicate. Figure 2.2 exemplarily
plots the graph representation of the RDF database from the Introduction, where
we indicate URI-type subject and object nodes by ellipses and distinguish string
literals by quotation marks. As we will see later, predicate positions of triples (and
hence the edges in the graph) always carry URIs. To give an example, the edge from
URI myns:Person1 to literal “Tim Berners-Lee” is labeled with URI foaf:name,
so it represents the triple (myns:Person1,foaf:name,“Tim Berners-Lee”). Given
this close connection between RDF databases and labeled directed graphs, in the
following we shall use the terms RDF database and RDF graph interchangeably.
RDF as a graph-structured data model diﬀers conceptionally from other data
models, such as the relational model [Cod70] or tree-structured XML data [w3ce].
As an immediate consequence, traditional database query languages like SQL [CB74]
(for the relational model), or XQuery [xqu] and XPath [xpa] (for XML) are not di-
rectly applicable in the context of RDF data. One straightforward approach that
allows to fall back on, for instance, the SQL query language for RDF data ex-
traction is the mapping of RDF databases into the relational model. An obvious
implementation of this approach would be the creation of a single relational table
Triples(subject,predicate,object) [MACP02; AMMH07] that stores all RDF
triples of the input RDF database. Following this scheme, the RDF graph from
Figure 2.2 could be mapped to the relational instance shown in Figure 2.1.
13

Chapter 2. Preliminaries
Triples
subject
predicate
object
myns:Article1
rdf:type
myns:Article
myns:Article1
dc:title
“The Semantic Web”
myns:Article1
myns:journal
myns:Journal1
myns:Article2
rdf:type
myns:Article
myns:Article2
dc:title
“WWW: Past, Present, and Future”
myns:Article2
dcterms:issued
“1996”
myns:Journal1
rdf:type
myns:Journal
myns:Journal1
dc:title
“Scientiﬁc American”
myns:Person1
rdf:type
foaf:Person
myns:Person1
foaf:name
“Tim Berners-Lee”
myns:Article1
dc:creator
myns:Person1
myns:Article2
dc:creator
myns:Person1
Figure 2.1.: Relational instance storing the RDF database from the Introduction.
This mapping now facilitates data extraction and manipulation using SQL. As-
sume for instance that we want to extract all articles written by “Tim Berners-Lee”,
including the respective years of publication (and only articles for which the year of
publication is given). This request can be expressed by the following SQL query.
SELECT T1 . s u b j e c t AS a r t i c l e ,
T2 . o b j e c t
AS year
FROM T r i p l e s T1 ,
T r i p l e s T2 ,
T r i p l e s T3 ,
T r i p l e s T4
WHERE T1 . p r e d i c a t e =’ r d f : type ’ AND T1 . o b j e c t =’myns : A r t i c l e ’
AND T2 . p r e d i c a t e =’dcterms : issued ’
AND T3 . p r e d i c a t e =’dc : creator ’
AND T4 . p r e d i c a t e =’ f o a f : name ’ AND T4 . o b j e c t =’”Tim Berners−Lee ” ’
AND T1 . s u b j e c t=T2 . s u b j e c t AND T2 . s u b j e c t=T3 . s u b j e c t
AND T3 . o b j e c t=T4 . s u b j e c t
We observe that the query accesses table Triples four times, through T1-T4.
According to the restrictions in the Where-clause, (i) T1 matches all RDF triples of
type myns:Article, (ii) T2 matches triples with predicate dcterms:issued, (iii) T3
matches all URIs that have been created (dc:creator) by someone, and (iv) T4
matches URIs with name (foaf:name) “Tim Berners-Lee”. T1-T3 are then joined
on the subject column, enforcing that the article URIs of the respective tuples are
the same; in addition, there is a join T3.object=T4.subject, which asserts that
the URI of the person in the object position of T3 coincides with the URI of “Tim-
Berners Lee” from T4. When evaluated against the Triples table from Figure 2.1,
the query gives the desired result, namely one tuple (myns:Article2,“1996”).
Concerning the SQL version of the query, one might argue that the query is some-
what “unintuitive” to read. Moreover, we observe that already this simple request
14

myns:Article1
myns:Journal1
myns:Journal
myns:Article
‘The Semantic Web”
“The Scientiﬁc American”
myns:Article2
myns:Person1
foaf:Person
“WWW: Past, Present, and Future”
“1996”
“Tim Berners-Lee”
rdf:type
rdf:type
myns:journal
rdf:type
dc:title
dc:title
dc:creator
dc:creator
dc:title
dcterms:issued
rdf:type
foaf:name
Figure 2.2.: RDF graph representation of the database from Figure 2.1.
involves three self-joins over the Triples table. As discussed in [AMMH07; SHK+08],
such self-joins occur frequently when specifying SQL queries over relational triple
tables for RDF data and it has been observed that they impose severe challenges
to the query evaluator [AMMH07; SHK+08; SGK+08], in particular for large RDF
databases, which often contain millions or even billions of RDF triples.1 Similar prob-
lems have been identiﬁed for more sophisticated relational RDF storage schemes,
such as Vertical Partitioning proposed in [TCK05; AMMH07] (cf. the experimental
study of RDF data management approaches in [SGK+08; SHK+08]).
Generalizing from the previous example, the main problem of using traditional
query languages for RDF data processing is that they do not ﬁt the graph nature
of the RDF data model, which justiﬁes the need for a speciﬁc RDF query language.
In particular, it would be desirable to have a query language that allows to specify
graph patterns that in some sense mirror the structure of the RDF input graph.
Addressing this demand, over the last years diﬀerent graph query languages
(e.g, [SOO99; HS08]) and speciﬁc RDF query languages, such as RQL [KACP02],
SeRQL [ope], and TRIPLE [SD01] have been proposed (see [HBEV04] for a compar-
ison of diﬀerent RDF query languages). In its eﬀort to standardize the most common
and useful constructs of these graph languages, the W3C worked out the SPARQL
Protocol and RDF Query Language [spac], called SPARQL for short.
In response to the desiderata discussed above, SPARQL comes with a power-
ful graph matching facility. The request from before, for instance, can be natu-
rally expressed by the graph pattern in Figure 2.3, where variables are preﬁxed by
1To give a concrete example, the RDF version of the U.S. Census database [Tau] contains more
than one billion RDF triples. Other large-scale RDF repositories are listed at [lin].
15

Chapter 2. Preliminaries
myns:Article
?article
?year
?person
“Tim Berners-Lee”
rdf:type
dcterms:issued
dc:creator
foaf:name
Figure 2.3.: RDF graph pattern.
question marks. The idea is that, during query
evaluation, this graph pattern is embedded
into the RDF input graph, thereby binding
variables in the graph pattern to matching
components of the input graph. Coming back
to our example, the pattern in Figure 2.3
matches all entities of type myns:Article
written by “Tim Berners-Lee” and includ-
ing the year of publications. When evalu-
ated against the RDF graph in Figure 2.2,
the only possible embedding is to bind vari-
able ?article to myns:Article2, ?person to
myns:Person1, and ?year to literal “1996”.
Hence, the evaluation of this graph pattern
yields the same result as the previous SQL
query over the Triples table, extended by a binding for variable ?person.
The SPARQL query language now provides a syntax to express such graph pat-
terns in textual form and to project the desired variables. The pattern in Figure 2.3
for instance, can be expressed by a conjunction of four so-called basic graph patterns,
one for each edge in the pattern. The resulting SPARQL query looks as follows.
SELECT ? a r t i c l e
? year
WHERE {
? a r t i c l e
r d f : type myns : A r t i c l e
.
? a r t i c l e
dcterms : i s s u e d
? year
.
? a r t i c l e
dc : c r e a t o r
? person
.
? person
f o a f : name ”Tim Berners−Lee” }
The four basic SPARQL graph patterns ?article rdf:type myns:Article, . . . ,
?person foaf:name "Tim Berners-Lee" are connected through the (standard) con-
nection method “.”, together forming the graph pattern depicted in Figure 2.3.
Note that variables shared between patterns collapse to a single node, as it is the
case for variable ?article (used in the ﬁrst three basic graph patterns) and vari-
able ?person (used in the last two patterns). Hence, from an algebraic point of view
the connection method “.” can be understood as a join operation, which enforces
that shared variables are bound to the same node. Finally, the Select expression
projects the desired variables ?article and ?year. Arguably, the SPARQL query
above is notationally simpler and easier to understand than its SQL counterpart.
In addition to the features illustrated in our example, the SPARQL query language
provides a set of operators, namely Filter, Union, and Optional, which can be
used to compose more expressive queries. To give a short description, the Filter
operator allows to enforce additional conditions that must hold for results (in the
style of the selection operator in relational algebra), such as the equality of two
16

2.1. Notational and Mathematical Conventions
variables. Next, operator Union computes the set-theoretical union of the evaluation
result of two graph patterns. Finally, Optional allows for the optional selection
of components in graph patterns. Assume for example that we specify the triple
pattern ?article dcterms:issued ?year in the query above as optional. Then
the resulting query would also extract the entity myns:Article1, for which the year
speciﬁcation is missing in the database (and variable ?year would remain unbound).
Having motivated the need for a speciﬁc RDF query language, we will now for-
mally introduce the RDF data format and the SPARQL query language. We focus
on the formalization of RDF and SPARQL core fragments, leaving out technical and
implementation-speciﬁc details, such as data types in RDF and the associated type
system for SPARQL. Still, our formalization covers the most important features of
the oﬃcial W3C RDF(S) [rdfa; rdfc; rdfd] and SPARQL [spac] speciﬁcations and
provides a clean semantics for SPARQL query evaluation. It forms the basis for the
subsequent investigation of SPARQL complexity and optimization in Chapters 3-5.
Structure. We survey mathematical and notational conventions in Section 2.1. In
the following, Section 2.2 introduces the RDF data format, including a deﬁnition of a
formal model for RDF, as well as a discussion of the predeﬁned RDF(S) vocabulary
and RDF(S) reasoning, which will play an important role in our discussion of seman-
tic query optimization for SPARQL in Chapter 5. Next, in Section 2.3 we establish a
formal model for the SPARQL query language, comprising the most important op-
erators and concepts of the language. We provide two (slightly diﬀerent) semantics
for our SPARQL fragment, namely a set-based semantics and a bag semantics (also
called multi-set semantics), and relate them to the oﬃcial W3C SPARQL Recom-
mendation [spac]. The preliminaries chapter concludes with a discussion of related
work concerning RDF and SPARQL formalizations in Section 2.4.
2.1. Notational and Mathematical Conventions
Environments and Fonts. We use standard environments for deﬁnitions, exam-
ples, equations, propositions, lemmas, theorems, corollaries, ﬁgures, and tables. For
the ease of access, all environments are numbered relative to the chapter in which
they have been deﬁned, according to the numbering scheme <chapter id>.<number>,
where <number> is a separate, increasing ID for each environment type.
When introducing important terms and deﬁnitions, we always use italic font. In
contrast, bold font is used for emphasizing important terms in textual descrip-
tions or, alternatively, to indicate the beginning of a new logical paragraph (see for
instance the paragraph “Mathematical Conventions.” on the next page).
SPARQL and SQL operators or keywords appearing in free text are distinguished
by capitalized font (e.g., And, Select); full SQL or SPARQL queries are enclosed
in separate boxes and can be identiﬁed by typewriter font on gray-shadowed back-
ground (see e.g. the example query at the beginning of this chapter).
17

Chapter 2. Preliminaries
Mathematical Conventions. We assume that the set of natural numbers N
does not include element 0 and put N0 := N ∪{0}. We use the notation i ∈[n] as a
shortcut for i ∈{1, . . . , n}. As usual, |S| stands for the cardinality of set S.
Occasionally, we shall write iﬀfor if and only if (or, alternatively, exactly if ). Fur-
ther, we use the symbol “:=” for deﬁning and introducing assignments, the equality
symbol “=” for deductive computation, and “≡” to express logical equivalence.
2.2. The RDF Data Format
The Resource Description Framework has been developed as part of the W3C Seman-
tic Web initiative. As stated in the W3C Primer [rdfb], it is “particularly intended
for representing metadata about Web resources”, but also to “represent information
about things that can be identiﬁed on the Web, even when they cannot be directly
retrieved from the Web”. With these goals in mind, RDF provides mechanisms to
describe arbitrary physical or logical resources in a machine-readable way.
In the following, we give a compact formalization of RDF, closely following the no-
tation introduced in [PAG06a]. We start with the deﬁnition of three disjoint inﬁnite
sets, namely the set of URIs U, the set of blank nodes B, and the set of literals L.
They form the basis for describing data using RDF and can be understood as follows.
• The set U of URIs [w3cc; w3cd] comprises an inﬁnite number of strings,
where each URI identiﬁes a logical or physical resource. Technically, URIs
are ASCII strings that consist of a namespace and an identiﬁer part. For in-
stance, the URI http://my.namespace.com#Article1 is composed of names-
pace http://my.namespace.com# and identiﬁer Article1. It may represent
an article that has been published in some conference. For readability, we typi-
cally denote URIs in preﬁx notation, e.g. writing myns:Article1 for the above
URI where myns is a shortcut for the namespace http://my.namespace.com.
A list of the namespace preﬁxes used in this thesis can be found in Appendix A.
• Elements from B, so-called blank nodes, can be seen as distinct anonymous
resources, i.e. physical or logical entities that are not assigned a ﬁxed URI.
Blank nodes may be used to identify entities for which the URI is unknown,
or in cases where the entity is not prominent enough to be assigned a ﬁxed URI.
For instance, blank nodes are typically used as parent nodes to a grouping of
data. With this scope, they have an existential character, stating that a certain
entity or grouping exists without explicitly ﬁxing the URI. By convention, we
distinguish blank nodes from URIs by the namespace preﬁx “_”, e.g. we use
strings like _:b1 or _:grouping1 to denote blank node identiﬁers.
• The set L constitutes literals, such as strings, integers, or boolean values.
As we shall see later, literals are used to describe properties of URIs and
blank nodes. The RDF speciﬁcation [rdfa] distinguishes between plain literals,
18

2.2. The RDF Data Format
such as “23”, and typed literals such as “23”^^xsd:integer.2 We will mainly
ignore datatypes, as they raise only implementation-speciﬁc issues. Literals are
distinguished from URIs and blank nodes by quotation marks and italic font.
We abbreviate the union of sets B, L, and U by concatenating their identiﬁers,
e.g. writing BLU as a shortcut for B ∪L ∪U. Furthermore, when talking about
resources in the context of RDF, we always refer to elements of the set BU.
Formalization. The central idea of the RDF data format is to describe resources
through triples (s,p,o), where each triple provides the value o of a speciﬁc property p
for resource s. This value could be either a literal, e.g. when providing the name or
age of a person, or a resources again, which allows to establish connections between
entities, such as the friendship relation between persons. Following this idea, we
deﬁne RDF triples and RDF databases over the sets U, B, and L as follows.
Deﬁnition 2.1 (RDF Triple) An RDF triple is a triple of the form
(s, p, o) ∈BU × U × BLU
We call the ﬁrst component s of an RDF triple its subject, the second component p
its predicate, and the third component o its object. We shall refer to the URI in
predicate position as property.2
Deﬁnition 2.2 (RDF Database) An RDF database, also called RDF graph or
RDF document, is a ﬁnite set of RDF triples. We write dom(D) to denote the subset
of elements from BLU that appear in D.
2
It is worth mentioning that, from a logical point of view, RDF databases can
be understood as sets of binary relations, where each distinct property p in the
database (i.e., each URI occurring in predicate position) implicitly deﬁnes a relation
that contains the (subject,object)-pairs that stand in relation p. We illustrate our
formalization of RDF and the latter observation by means of a small example.
Example 2.1 The sample graph in Figure 2.2 represents the RDF database D de-
ﬁned in the Introduction. In this graph representation, each edge s
p→o stands
for a triple (s, p, o). As a triple with a literal in object position consider for in-
stance (myns:Article2,dcterms:issued,“1996”). Triples with resources in object
position, such as (myns:Article2,dc:creator,myns:Person1), encode relationships
between resources. The graph does not contain blank nodes. When switching to the
binary relation view of the RDF database, we identify the six relations dc:title,
myns:journal, rdf:type, foaf:name, dcterms:issued, and dc:creator. Rela-
tion rdf:type for instance stores all type relations that hold between RDF re-
sources, namely (myns:Article1,myns:Article), (myns:Article2,myns:Article),
(myns:Person1,foaf:Person), and (myns:Journal1,myns:Journal).
2
2The xsd namespace http://www.w3.org/2001/XMLSchema# provides a set of predeﬁned base
types like xsd:integer, xsd:string, or xsd:boolean.
19

Chapter 2. Preliminaries
2.2.1. Predeﬁned RDF and RDFS Vocabulary
The W3C speciﬁcations [rdfa; rdfe; rdfc] introduce two standard namespaces: the
RDF namespace http://www.w3.org/1999/02/22-rdf-syntax-ns# (preﬁx rdf)
and the RDF Schema namespace http://www.w3.org/2000/01/rdf-schema# (pre-
ﬁx rdfs). These namespaces comprise a set of URIs with predeﬁned meaning, called
standard vocabulary in the following. The standard vocabulary supports basic RDF
modeling tasks, such as typing, expressing subclass relationships, or creating bags
or sequences of resources. From a logical point of view, the vocabulary splits up into
so-called classes (i.e., URIs that are used for typing resources) and properties (i.e.,
URIs that are used as properties). Complementarily to the concept of classes, we
will use the term object to denote URIs that are instances of some class.3
We summarize the most important standard vocabulary in the following listing.
• The predeﬁned URI rdf:type can be used for typing entities. We illustrated
the use of this property before in our running example (cf. Figure 2.2).
• The two classes rdfs:Class and rdf:Property can be used to assign a logical
type to URIs. For instance, the triple (myns:Article,rdf:type,rdfs:Class)
states that URI myns:Article will be utilized as a class. Similarly, the RDF
triple (dc:creator,rdf:type,rdf:Property) indicates that URI dc:creator
is used as a property. By convention, we distinguish classes from properties by
capitalizing the ﬁrst letter of the identiﬁer part (cf. Article vs. creator).
• The URIs rdfs:domain and rdfs:range can be used to specify the domain
and range of properties. As an extension of our running example, we might in-
troduce the two RDF triples (dc:creator,rdfs:domain,myns:Article) and
(dc:creator,rdfs:range,foaf:Person), to indicate that dc:creator asso-
ciates myns:Article-typed objects with foaf:Person-typed objects.
• rdfs:subClassOf and rdfs:subPropertyOf are used to describe subclass and
subproperty relationships between classes and properties, respectively. The
triple (myns:Article,rdfs:subClassOf,myns:Publication) for instance en-
codes that myns:Article is a subclass of myns:Publication. Similarly, we
could deﬁne the triple (dc:creator,rdfs:subPropertyOf,myns:relatedTo),
stating that property dc:creator specializes property myns:relatedTo.
• The rdf namespace provides vocabulary for creating so-called containers.
There exist three predeﬁned container classes in the rdf namespace, namely
rdf:Bag for modeling sets, rdf:Seq for modeling ordered lists or sequences,
and rdf:Alt for modeling alternatives. In addition, there is an inﬁnite set of
so-called container membership properties rdf:_1, rdf:_2, rdf:_3, . . . , used
to enumerate the members of a container object. Figure 2.4(a) exemplarily
3Although we use the terms class, object, and property to facilitate the subsequent discussion,
strictly speaking there is no separation between them in RDF. For instance, a URI might
represent both a class and a property at the same time.
20

2.2. The RDF Data Format
shows an RDF sequence containing the ﬁrst three prime numbers. The se-
quence itself is modeled as a blank node _:primes of type rdf:Seq. The three
prime numbers “2”, “3”, and “5” are associated using the ﬁrst three container
membership properties, respectively. Note that RDF containers are “open”,
i.e. there is no vocabulary to explicitly ﬁx the number of container objects.
• Complementarily to the container classes, RDF provides so-called collections,
which diﬀer from containers in that they are closed. Collections are represented
as lists of type rdf:List, where properties rdf:first and rdf:rest deﬁne
the head and tail of the list and URI rdf:nil can be used to close the list.
Figure 2.4(b) depicts the RDF collection version of the prime number scenario
from Figure 2.4(a). The blank node _:primes1to3 represents the full collection
and is typed accordingly with rdf:List. We observe that the ﬁrst collection
member, prime number “2”, is associated using property rdf:first. The re-
maining members are stored in the separate rdf:List object _:primes2to3,
which can be understood as the tail of the list and is connected through prop-
erty rdf:rest. This second list refers to the second member, prime number
“3”, and links to a third sublist, identiﬁed through _:primes3to3. The third
list then links to element “5” and is closed using the predeﬁned URI rdf:nil.
• Further, the RDFS standard comprises a set of properties with predeﬁned
meaning. In particular, (i) rdfs:seeAlso allows to link to background in-
formation of a certain resource, (ii) rdfs:isDefinedBy is a subproperty of
rdfs:seeAlso and allows to refer to an external deﬁnition4 of a resource,
(iii) rdfs:label is used to assign a (typically human-readable) label to a
resource, and (iv) rdfs:comment is a reserved keyword for adding comments.
• Finally, the RDF standard comprises so-called reiﬁcation vocabulary, which
allows to make statements about triples. To give an example, using the reiﬁca-
tion vocabulary one could provide background information such as the person
who created a triple or the date when a triple was inserted into the database.
Reiﬁcation is not of particular interest in this thesis, so we will not go into
more detail here, but instead refer the interested reader to [rdfc; rdfd].
Taken together, all these predeﬁned URIs form a valuable basis for creating RDF
documents. The rdfs vocabulary, which allows to express relationships between
entities (such as rdfs:subClassOf, rdfs:subPropertyOf) and to ﬁx property do-
mains and ranges (using rdfs:domain, rdfs:range), is particularly useful to develop
domain-speciﬁc vocabulary collections and ontologies (we implicitly used ontologies
like FOAF [foa] and Dublin Core [dub] in the RDF database from Figure 2.2). The
utilization of such predeﬁned vocabulary collections facilitates RDF database design
and increases the interoperability between RDF repositories in the Semantic Web.
4The exact kind and format of this deﬁnition is not further speciﬁed by the RDFS standard.
21

Chapter 2. Preliminaries
rdf:Seq
:primes
“2”
“3”
“5”
rdf:type
rdf :1 rdf :2
rdf :3
rdf:List
rdf:nil
:primes1to3
:primes2to3
:primes3to3
“2”
“3”
“5”
rdf:ﬁrst
rdf:ﬁrst
rdf:ﬁrst
rdf:type
rdf:type
rdf:type
rdf:rest
rdf:type
rdf:rest
rdf:rest
Figure 2.4.: (a) RDF sequence; (b) RDF list.
2.2.2. RDF(S) Semantics
In the previous section we introduced the RDF and RDFS standard vocabulary and
described its meaning informally. Now, we turn towards a formal deﬁnition of its
semantics. We start our discussion with a motivating example:
Example 2.2 Let us consider the RDF database D′ below.
D′ := { (myns:Article1,rdf:type,myns:ScientificArticle),
(myns:Article1,dc:creator,myns:Person1),
(myns:ScientificArticle,rdf:type,rdfs:Class),
(myns:Article,rdf:type,rdfs:Class),
(myns:Publication,rdf:type,rdfs:Class),
(myns:ScientificArticle,rdfs:subClassOf,myns:Article),
(myns:Article,rdfs:subClassOf,myns:Publication),
(dc:creator,rdfs:range,foaf:Person) }
The database deﬁnes a resource myns:Article1 of type myns:ScientificArticle,
which has been created by an entity referred to as myns:Person1 (yet the type of
myns:Person1 is not speciﬁed). Further, some background schema knowledge is
modeled. In particular, we know that myns:ScientificArticle is a subclass of
myns:Article, which itself is a subclass of myns:Publication again. Finally, the
range of property dc:creator is ﬁxed to objects of type foaf:Person.
Let us now interpret the URIs in D′ as real-world entities and discuss their se-
mantics informally. We may derive a couple of new facts here. For instance, it is rea-
sonable to assume that relation myns:subClassOf is transitive, which implies a new
triple (myns:ScientificArticle,rdfs:subClassOf,myns:Publication). Given the
resulting class hierarchy, the myns:ScientificArticle-typed object myns:Article1
should also be understood as an article (i.e., myns:Article) and – going one step
further in the hierarchy – a publication (i.e., myns:Publication). This could be
expressed by two additional triples (myns:Article1,rdfs:type,myns:Article) and
22

2.2. The RDF Data Format
(I) Subclass
(a, type, Class)
(a, sc, a)
(a, sc, b)
(b, sc, c)
(a, sc, c)
(a, sc, b)
(x, type, a)
(x, type, b)
(II) Subproperty
(a, type, Property)
(a, sp, a)
(a, sp, b)
(b, sp, c)
(a, sp, c)
(a, sp, b)
(x, a, y)
(x, b, y)
(III) Typing
(a, domain, c)
(x, a, y)
(x, type, c)
(a, range, c)
(x, a, y)
(y, type, c)
Figure 2.5.: Selected RDFS inference rules.
(myns:Article1,rdfs:type,myns:Publication), respectively. As another example,
the range speciﬁcation foaf:Person for dc:creator implies that myns:Person1,
which occurs as object in a triple with predicate dc:creator, is of type foaf:Person.
This could be expressed by the triple (myns:Person1,rdf:type,myns:Person). 2
In fact, the oﬃcial RDF(S) semantics deﬁnition [rdfd] introduces a reasoning
mechanism that allows to automatically derive new facts from RDF(S) knowledge
bases, in the style of the previous example. The formal deﬁnition of the semantics
that is proposed by the W3C is model theoretic, building on the notion of so-called
interpretations. Informally speaking, the interpretation of an RDF(S) database con-
tains all the triples that are logically implied according to the semantics.
In [GHM04] it was shown that the model theoretic approach proposed by the
W3C can be equally expressed by a set of inference rules. From a practical perspec-
tive, these rules are more useful than the model theoretic deﬁnition: they can be
successively applied (until a ﬁxpoint is reached) to compute the logically implied
database, whereas the model theoretic semantics is not constructive by nature. Fig-
ure 2.5 provides a subset of the RDFS reasoning rules introduced in [GHM04]. We
use the shortcuts sc for rdfs:subClassOf, sp for rdfs:subPropertyOf and omit
the namespace preﬁxes for rdf:type, rdfs:domain, rdfs:range, rdfs:Class, and
rdfs:Property, simply writing type, domain, Class, and Property instead.
The rules in group (I) deﬁne the semantics of property rdfs:subClassOf. As
usual, we understand the upper part of the inference rule as premise and the lower
part as consequence. According to the ﬁrst two rules in group (I), the subproperty
relationship is reﬂexive and transitive, respectively. Finally, the third rule transfers
subclass reasoning to the object level: whenever an object x is of some type a and
it is known that class a is a subclass of b, then it follows that x also is of type b.
Next, group (II) covers the semantics of rdfs:subPropertyOf, which is quite
similar in idea to the semantics of property rdfs:subClassOf. In particular, sub-
property relationships also are reﬂexive and transitive, as stated by the ﬁrst and
23

Chapter 2. Preliminaries
second rule in this group, respectively. Akin to the last rule in group (I), the last
rule in group (II) transfers the transitivity of the relationship to the object level:
under the premise that a is a subproperty of b and there is a resource x that stands
in relation a with some y, it follows that resource x stands in relation b with y.
Finally, the two remaining rules in group (III) treat the semantics of proper-
ties rdfs:domain and rdfs:range. They allow to derive the respective types for
entities that are used in subject and object positions of triples having property a
whenever the RDF database contains a domain or range speciﬁcation for a.
Example 2.3 When applying the inference rules from Figure 2.5 to database D′
from Example 2.2 until a ﬁxpoint is reached, we obtain the database
D′′:= D′ ∪{(myns:ScientificArticle,rdfs:subClassOf,myns:ScientificArticle),
(myns:Article,rdfs:subClassOf,myns:Article),
(myns:Publication,rdfs:subClassOf,myns:Publication),
(myns:ScientificArticle,rdfs:subClassOf,myns:Publication),
(myns:Article1,rdf:type,myns:Article),
(myns:Article1,rdf:type,myns:Publication),
(myns:Person1,rdf:type,myns:Person) }.
We can observe that the implied database D′′ includes all the triples that have
been informally derived in the previous discussion in Example 2.2.
2
It should be noted that the list of inference rules shown in Figure 2.5 is not
complete. In particular, deduction rules concerning the semantics of blank nodes
and rules for RDF-internal constructs, such as containers, are not included. These
rules, however, are not important for the remainder of this thesis, so we refer the
interested reader to [rdfd; GHM04] for a complete discussion of RDF(S) inference.
We wrap up with the remark that the SPARQL semantics that will be presented in
the following section – just like the oﬃcial W3C SPARQL speciﬁcation – disregards
the issue of RDFS reasoning. This means that SPARQL operates on the RDF graph
as is, without inferring new triples. Whenever reasoning is desired, it is assumed
to be carried out by a separate, underlying layer. This decision, which keeps the
SPARQL query language independent from the reasoning process, brings several
advantages: it results in a clean and compact semantics for SPARQL that does
not interfere with reasoning rules, makes the SPARQL query language resistant
to possible changes in the RDF(S) reasoning process, and allows to use SPARQL
without modiﬁcations on top of other reasoning mechanisms, such as OWL [owl].
The RDF(S) semantics therefore will not play a role in our investigation of SPARQL
complexity and optimization, yet when discussing semantic optimization of SPARQL
queries in Chapter 5 we will come back to the issue of RDF(S) inference.
24

2.3. The SPARQL Query Language
2.2.3. Serialization
There have been diﬀerent proposals on how to serialize RDF(S) data. Probably the
most prominent serialization format is RDF/XML [rdfe], which allows to encode
RDF databases as XML trees. The basic idea behind RDF/XML is to split the RDF
graph into small, tree-structured chunks, which are then described in XML with
the help of predeﬁned tags and attributes. The RDF/XML format was primarily
designed to be processed by computers and we will not use it in this thesis.
In addition to RDF/XML, several triple-oriented serialization formats have been
proposed. Arguably the simplest of them is N-Triples [ntr], which merely deﬁnes
a syntax for the basic elements of RDF (i.e., URIs, blank nodes, and literals) and
allows to encode an RDF database by listing its triples one by one. More advanced
triple-based serialization formats for RDF are Turtle [tur] and Notation 3 [not]
(also known as N3). Both languages extend the N-Triples format, e.g. by providing
syntactic constructs that allow to encode RDF triples and collections of triples (such
as sets of triples sharing the same subject) in a more compact way. We conclude with
the remark that the sample RDF databases used in this thesis are rather small, so
there is no urgent need to introduce simplifying notations; rather, we will fall back
on our mathematical model of RDF databases (cf. Deﬁnition 2.2) and either describe
them as sets of triples or visualize them using the graph representation.
2.3. The SPARQL Query Language
We now turn towards SPARQL and introduce a formal syntax (Section 2.3.1) and
semantics (Sections 2.3.2 and 2.3.3) for a fragment of the query language. This
fragment covers all SPARQL operators found in the oﬃcial W3C Recommenda-
tion [spac], but abstracts from technical and implementation-speciﬁc details, such
as data types and the typing system, which allows us to investigate the complexity
and optimization of SPARQL using an expressive core fragment of the language.
In our formalization, we will introduce two alternative semantics for SPARQL
evaluation, namely a set semantics and a bag semantics (also referred to as multi-
set semantics). The set semantics is simpler and therefore will form the basis for
our discussion of SPARQL complexity in Chapter 3 (yet all complexity results carry
over to the bag semantics), while – in the course of our study of algebraic SPARQL
optimization in Chapter 4 – we will study set and bag semantics in parallel, to work
out practically relevant diﬀerences. As a ﬁnal step in our introduction to SPARQL
we will then relate our formalization to the oﬃcial W3C SPARQL Recommendation
(Section 2.3.4) and discuss limitations of the language (Section 2.3.5).
We point out that our formalization of SPARQL in Sections 2.3.1 and 2.3.2 was
inspired by previous investigations of SPARQL (cf. [PAG06a; AG08a]); the bag
semantics in Section 2.3.3 closely follows ideas found in the oﬃcial W3C Recom-
25

Chapter 2. Preliminaries
mendation [spac] and the bag semantics proposed in [PAG06b]. A historical note on
the SPARQL formalization process can be found in the related work, Section 2.4.
2.3.1. An Abstract Syntax for SPARQL
Let V be a set of variables disjoint from BLU. As a notational convenience, we dis-
tinguish variables by a leading question mark symbol, e.g. strings like ?x and ?name
denote variable names. We start with an abstract syntax for ﬁlter conditions.
Deﬁnition 2.3 (Filter Condition) Let ?x, ?y ∈V be variables and c, d ∈LU.
We deﬁne ﬁlter conditions inductively as follows.
• The expressions ?x = c, ?x =?y, and c = d are ﬁlter conditions.
• The expression bound(?x) (abbreviated as bnd(?x)) is a ﬁlter condition.
• If R1 and R2 are ﬁlter conditions,
then ¬R1, R1 ∧R2, and R1 ∨R2 are ﬁlter conditions.
2
By vars(R) we denote the set of variables occurring in ﬁlter expression R. Using
the previous deﬁnition, we are ready to introduce an abstract syntax for expressions
(where we abbreviate the operator Optional as Opt):
Deﬁnition 2.4 (SPARQL Expression) A SPARQL expression is an expression
that is built inductively according to the following rules.
• A so-called triple pattern t ∈UV × UV × LUV is an expression.
• If Q is an expression and R is a ﬁlter condition,
then Q Filter R is an expression.
• If Q1, Q2 are expressions,
then Q1 Union Q2, Q1 Opt Q2, and Q1 And Q2 are expressions.
2
Note that we do not allow for blank nodes in triple patterns, so SPARQL expres-
sions (and also SPARQL queries, which will be deﬁned subsequently) by deﬁnition
never contain blank nodes, yet the oﬃcial W3C SPARQL Recommendation [spac]
allows for blank nodes in triple patterns. We will further elaborate on this issue Sec-
tion 2.3.4 when relating our SPARQL fragment to the W3C proposal for SPARQL.
The oﬃcial W3C Recommendation deﬁnes four diﬀerent types of queries, namely
Select, Ask, Construct, and Describe queries. In the formalization we will
restrict ourselves on SPARQL Select and Ask queries. While Select queries ex-
tract the set of all result mappings, Ask queries are boolean queries that return
true if there are some results, and false otherwise.5 We next deﬁne the syntax of
5Unlike the oﬃcial W3C SPARQL Recommendation, which proposes “yes” and “no” as answers
to Ask queries, we use the standard boolean predicates “true” and “false” in our formalization.
26

2.3. The SPARQL Query Language
Select and Ask queries (their semantics will be ﬁxed in Section 2.3.2). The re-
maining two query forms, Construct and Describe, will be informally discussed
later in Section 2.3.4; they are not of particular interest in this thesis.
Deﬁnition 2.5 (SPARQL SELECT Query) Let Q be a SPARQL expression
and let S ⊂V be a ﬁnite set of variables. A SPARQL Select query is an ex-
pression of the form SelectS(Q).
2
Deﬁnition 2.6 (SPARQL ASK Query) Let Q be a SPARQL expression. We
call an expression of the form Ask(Q) SPARQL Ask query.
2
In the remainder of the thesis we will mostly deal with SPARQL Select queries.
Therefore, we usually denote them as SPARQL queries, or simply queries. As a no-
tational simpliﬁcation, we omit braces for the variable set appearing in the subscript
of the Select operator, e.g. writing Select?x,?y(Q) instead of Select{?x,?y}(Q).
Further, in the context of abstract syntax SPARQL expressions and queries, we
typeset URIs in italic font and often omit the preﬁxes of URIs.
Example 2.4 The query
Q1 := Select?name,?email(
(((?person, name, ?name) And (?person, age, ?age))
Filter (?age = “30”))
Opt (?person, email, ?email))
is a valid SPARQL Select query, where name, age, and email denote URIs. 2
2.3.2. A Set-based Semantics for SPARQL
Having established an abstract syntax for SPARQL, we now provide a set-based
semantics for its evaluation. We start with the deﬁnition of so-called mappings, which
are used to express variable-to-graph bindings within in the evaluation process:
Deﬁnition 2.7 (Mapping and Mapping Universe)
A mapping is a partial
function µ : V →BLU from a subset of variables V to RDF terms BLU. The
domain of a mapping µ, written dom(µ), is deﬁned as the subset of V for which µ
is deﬁned. By M we denote the universe of all mappings.
2
We next deﬁne the central notion of compatibility between mappings. Informally
speaking, two mappings are compatible if they do not contain contradicting variable
bindings, i.e. if shared variables always map to the same value in both mappings:
27

Chapter 2. Preliminaries
Deﬁnition 2.8 (Compatibility of Mappings) Given two mappings µ1, µ2, we
say µ1 is compatible with µ2 iﬀµ1(?x) = µ2(?x) for all ?x ∈dom(µ1) ∩dom(µ2). We
write µ1 ∼µ2 if µ1 and µ2 are compatible, and µ1 ̸∼µ2 otherwise.
2
We overload function vars (introduced before for ﬁlter expressions) and denote by
vars(t) all variables in triple pattern t. Further, we write µ(t) to denote the triple
pattern obtained when replacing all variables ?x ∈dom(µ) ∩vars(t) in t by µ(?x).
Example 2.5 Let
µ1 := {?person 7→P1, ?name 7→“Pete”},
µ2 := {?person 7→P2, ?name 7→“John”},
µ3 := {?person 7→P1, ?email 7→“pete@tld.com”}
be three mappings. Then it holds that dom(µ1) = dom(µ2) = {?person, ?name}
and dom(µ3) = {?person, ?email}. Further, µ1 ∼µ3, but µ1 ̸∼µ2 and µ2 ̸∼µ3 (they
disagree on variable ?person). Given triple pattern t1 := (?person, name, ?name) we
have vars(t1) = {?person, ?name} and, for instance, µ1(t1) = (P1, name, “Pete”). 2
The next prerequisite for our deﬁnition of SPARQL semantics is the notion of
satisfaction of a ﬁlter condition with respect to some mapping:6
Deﬁnition 2.9 (Filter Semantics) Given a mapping µ, ﬁlter conditions R, R1,
R2, variables ?x, ?y, and c, d ∈LU, we say that µ satisﬁes R, written as µ |= R, if
and only if one of the following conditions holds.
• R is of the form bnd(?x) and ?x ∈dom(µ).
• R is of the form c = d and it holds that c and d are equal.
• R is of the form ?x = c, ?x ∈dom(µ), and µ(?x) = c.
• R is of the form ?x = ?y, {?x, ?y} ⊆dom(µ), and µ(?x) = µ(?y).
• R is of the form ¬R1 and it is not the case that µ |= R1.
• R is of the form R1 ∨R2 and µ |= R1 or µ |= R2.
• R is of the form R1 ∧R2 and µ |= R1 and µ |= R2.
2
The solution of evaluating a SPARQL expression or query on some document D
is described by a set of mappings, where each single mapping represents a possi-
ble answer in form of a binding of query variables to the subset of elements from
set BLU that occur in D. Following the approach proposed in [PAG06a], we deﬁne
the semantics of our SPARQL fragment using a compact algebra over such mapping
sets. We introduce the required algebraic operators in the subsequent deﬁnition.
6The W3C proposes a three-valued semantics for ﬁlters (true, false, plus error). Here, we adopt
the two-valued approach (using only true and false) from [PAG09]. We want to emphasize that
all results stated in the thesis also hold under the three-valued semantics.
28

2.3. The SPARQL Query Language
Deﬁnition 2.10 (SPARQL Set Algebra) Let Ω, Ωl, Ωr be mapping sets, R de-
note a ﬁlter condition, and S ⊂V be a ﬁnite set of variables. We deﬁne the algebraic
operations join 1, union ∪, minus \, left outer join
1, projection π, and selection σ
(also called ﬁlter) over mapping sets as follows.
Ωl 1 Ωr := {µl ∪µr | µl ∈Ωl, µr ∈Ωr : µl ∼µr}
Ωl ∪Ωr
:= {µ | µ ∈Ωl or µ ∈Ωr}
Ωl \ Ωr
:= {µl ∈Ωl | for all µr ∈Ωr : µl ̸∼µr}
Ωl
1 Ωr := (Ωl 1 Ωr) ∪(Ωl \ Ωr)
πS(Ω)
:= {µ1 | ∃µ2 : µ1 ∪µ2 ∈Ω∧dom(µ1) ⊆S ∧dom(µ2) ∩S = ∅}
σR(Ω)
:= {µ ∈Ω| µ |= R}
We refer to the algebra deﬁned by the above operations as SPARQL set algebra.2
We shall illustrate and discuss the algebraic operations using the mapping sets
Ω1 := {{?person 7→P1, ?name 7→“Joe”}, {?person 7→P2, ?name 7→“John”}},
Ω2 := {{?person 7→P1, ?email 7→“joe@tld.com”}}.
Operator 1 is a join operation that combines compatible mappings from two
mapping sets. To show the eﬀect of the join operation let us consider the SPARQL
set algebra expression Ω1 1 Ω2. According to the semantics of the join operation,
the result is the union of the ﬁrst mapping from Ω1 and the (only) mapping in Ω2,
because these mappings are compatible; the second mapping in Ω1 is incompatible
with every mapping in Ω2 and hence does not generate a result. Consequently, we
have Ω1 1 Ω2 = {{?person 7→P1, ?name 7→“Joe”, ?email 7→“joe@tld.com”}}.
The union operator ∪returns the mapping set that contains all mappings from
the ﬁrst or the second set, i.e. Ω1 ∪Ω2 = {{?person 7→P1, ?name 7→“Joe”},
{?person 7→P2, ?name 7→“John”}, {?person 7→P1, ?email 7→“joe@tld.com”}}.
Next, the minus operator \ retains all mappings from the left side set for which
no compatible mappings in the right side set exists. For our example sets Ω1, Ω2,
we observe that for the mapping {?person 7→P2, ?name 7→“John”} there is no
compatible mapping in Ω2, whereas the ﬁrst mapping in Ω1 is compatible with the
only mapping in Ω2. Hence, we have Ω1\Ω2 = {{?person 7→P2, ?name 7→“John”}}.
Probably the most interesting operator is the left outer join
1, which is deﬁned as
Ωl
1 Ωr := (Ωl 1 Ωr)∪(Ωl\Ωr). The idea is as follows. Given a mapping µ from Ωl,
we distinguish two cases: (i) if there are compatible mappings in Ωr, then the left
side of the union Ωl 1 Ωr applies and µ is joined with all compatible mappings
from Ωr; otherwise, (ii) if there is no compatible mapping in Ωr, then µ itself is part
of the result, according to the right side of the union. To give an example, we have
29

Chapter 2. Preliminaries
Ω1
1 Ω2 = (Ω1 1 Ω2) ∪(Ω1 \ Ω2)
= {{?person 7→P1, ?name 7→“Joe”, ?email 7→“joe@tld.com”}} ∪
{{?person 7→P2, ?name 7→“John”}}
= {{?person 7→P1, ?name 7→“Joe”, ?email 7→“joe@tld.com”},
{?person 7→P2, ?name 7→“John”}}.
As can be seen, the left outer join operation allows additional (compatible) in-
formation to be added if it exists (e.g., mapping {?person 7→P1, ?name 7→“Joe”}
from Ω1 is extended by a binding for variable ?email), but does not discard mappings
for which no such additional information is present (which is the case for mapping
{?person 7→P2, ?name 7→“John”} in our example). As we will see soon, the left
outer join operation will be utilized to deﬁne the semantics of operator Opt.
We conclude our discussion of the algebraic operators with the projection and
selection operators π and σ. Operator π is a straightforward projection on a set of
variables, e.g. the expression π?name(Ω1) = {{?name 7→“Joe”}, {?name 7→“John”}}
restricts all mappings in Ω1 to variable ?name. Finally, operator σ implements a
ﬁlter in the style of relational algebra selection. For instance, the algebra expression
σ?name=“Joe”(Ω1) selects all mappings from Ω1 in which variable ?name is bound to
value “Joe”, thus we have σ?name=“Joe”(Ω1) = {{?person 7→P1, ?name 7→“Joe”}}.
Having explained the algebraic operators, we are now ready to deﬁne the semantics
of SPARQL expressions, Select queries, and Ask queries (cf. Deﬁnitions 2.4, 2.5,
and 2.6, respectively). We follow the compositional, set-based approach proposed
in [PAG06a] and deﬁne a function J.KD that maps an abstract syntax SPARQL
expression or query into an algebra expression, which is then evaluated according to
the semantics of SPARQL set algebra introduced before in Deﬁnition 2.10:
Deﬁnition 2.11 (SPARQL Semantics) Let t be a triple pattern, D be an RDF
database, Q1, Q2 denote SPARQL expressions, R be a ﬁlter condition, and S ⊂V
be a ﬁnite set of variables. We deﬁne the SPARQL semantics inductively as follows.
JtKD
:= {µ | dom(µ) = vars(t) and µ(t) ∈D}
JQ1 And Q2KD
:= JQ1KD 1 JQ2KD
JQ1 Opt Q2KD
:= JQ1KD
1 JQ2KD
JQ1 Union Q2KD := JQ1KD ∪JQ2KD
JQ1 Filter RKD := σR(JQ1KD)
JSelectS(Q1)KD := πS(JQ1KD)
JAsk(Q1)KD
:= ¬(∅= JQ1KD)
2
We illustrate the evaluation of SPARQL queries in the following example.
30

2.3. The SPARQL Query Language
Example 2.6 Consider SPARQL query Q1 from Example 2.4, which retrieves the
names of all 30-year-old persons and, optionally (i.e., if speciﬁed), their email ad-
dress. We discuss the evaluation of Q1 on the RDF database
D := {(P1, name, “Joe”), (P1, age, “30”), (P1, email, “joe@tld.com”)),
(P2, name, “John”), (P2, age, “29”), (P2, email, “john@tld.com”),
(P3, name, “Pete”), (P3, age, “30”)}.
In a ﬁrst step, we apply the semantics J.KD to expression Q1, which transforms
operators And, Opt, Select, and Filter into their algebraic counterparts:
JQ1KD = π?name,?email(
σ?age=“30”(J(?person, name, ?name)KD 1 J(?person, age, ?age)KD)
1 J(?person, email, ?email)KD).
Next, we evaluate the basic triple patterns of this algebra expression, using the
topmost rule from Deﬁnition 2.11. We obtain the following intermediate results.
J(?person, name, ?name)KD = {{?person 7→P1, ?name 7→“Joe”},
{?person 7→P2, ?name 7→“John”},
{?person 7→P3, ?name 7→“Pete”}}
J(?person, age, ?age)KD
= {{?person 7→P1, ?age 7→“30”},
{?person 7→P2, ?age 7→“29”},
{?person 7→P3, ?age 7→“30”}}
J(?person, email, ?email)KD = {{?person 7→P1, ?email 7→“joe@tld.com”},
{?person 7→P2, ?email 7→“john@tld.com”}}
We are now ready to evaluate the algebra expression JQ1KD from above bottom-up:
Ω1 := J(?person, name, ?name)KD 1 J(?person, age, ?age)KD
= {{?person 7→P1, ?name 7→“Joe”, ?age 7→“30”},
{?person 7→P2, ?name 7→“John”, ?age 7→“29”},
{?person 7→P3, ?name 7→“Pete”, ?age 7→“30”}},
Ω2 := σ?age=“30”(Ω1)
= {{?person 7→P1, ?name 7→“Joe”, ?age 7→“30”},
{?person 7→P3, ?name 7→“Pete”, ?age 7→“30”}},
Ω3 := Ω2
1 J(?person, email, ?email)KD
= (Ω2 1 J(?person, email, ?email)KD) ∪(Ω2 \ J(?person, email, ?email)KD)
= {{?person 7→P1, ?name 7→“Joe”, ?age 7→“30”, ?email 7→“joe@tld.com”}}
∪{{?person 7→P3, ?name 7→“Pete”, ?age 7→“30”}}
= {{?person 7→P1, ?name 7→“Joe”, ?age 7→“30”, ?email 7→“joe@tld.com”},
{?person 7→P3, ?name 7→“Pete”, ?age 7→“30”}},
Ω:= π?name,?email(Ω3)
= {{?name 7→“Joe”, ?email 7→“joe@tld.com”}, {?name 7→“Pete”}}.
31

Chapter 2. Preliminaries
Observe that variable ?email is not bound in the second mapping of the ﬁnal
result Ω, which shows that query variables are not necessarily bound in result map-
pings. Such unbound variables are crucial for our discussion of SPARQL complexity
and algebraic optimization. We will come back to this observation in later chapters.2
In the remainder of this thesis, we will always fully parenthesize expressions,
except for the case of And- and Union-expressions; as we will see in Chapter 4, the
latter operators are associative, for instance Q1 And (Q2 And Q3) is equivalent to
(Q1 And Q2) And Q3, so we shall write Q1 And Q2 And Q3 in this case.
2.3.3. From Set to Bag Semantics
Given the set semantics deﬁned in the previous subsection, we now deﬁne an alterna-
tive bag semantics for SPARQL. It diﬀers in that identical mappings might appear
multiple times in (intermediate) query results. We point out that the bag semantics
deﬁned in this subsection corresponds to the approach taken by the W3C [spac].7
The central idea of the subsequent bag semantics deﬁnition is to switch from sets
of mappings to multi-sets of mappings. Therefore, in the remainder of this thesis we
will use the terms multi-set semantics and bag semantics interchangeably. We start
our discussion with a formal deﬁnition of mapping multi-sets:
Deﬁnition 2.12 (Mapping Multi-set) A mapping multi-set is a tuple (Ω,m),
where Ω⊂M is a mapping set and m : M 7→N0 is a total function such that
m(µ) ≥1 for all µ ∈Ωand m(µ) = 0 for all µ ̸∈Ω. Given µ ∈Ω, we refer to m(µ)
as the multiplicity of µ in Ω, saying that µ occurs m(µ) times in Ω.
2
We note that, for a mapping multi-set (Ω, m), the associated function m implicitly
deﬁnes Ω(i.e. Ωcontains exactly those mappings µ for which m(µ) ≥1 holds), so
strictly speaking the speciﬁcation of Ωis redundant. The reason for making Ωexplicit
is to clarify the connection between sets and multi-sets and to facilitate subsequent
deﬁnitions and proofs. We illustrate Deﬁnition 2.12 in the following example.
Example 2.7 Let µ1 := {?x 7→a} and µ2 := {?y 7→b} be mappings. Then (Ω,m)
with Ω:= {µ1, µ2} and m(µ1) := 2, m(µ2) := 1, and m(µ) := 0 for all µ ∈M \ Ωis
a mapping multi-set, in which µ1 occurs twice and µ2 a single time.
2
Whenever the multiplicity equals to one for all mappings that are contained in
some mapping multi-set, it can be understood as a simple mapping set:
7As discussed in [AG08a], there are some minor diﬀerences on how the W3C maps SPARQL syntax
into SPARQL algebra. These diﬀerences, however, do not compromise the expressiveness. We
also emphasize that the core evaluation phase (i.e. the result computation for a ﬁxed SPARQL
bag algebra expression over some document) is identical to the W3C approach.
32

2.3. The SPARQL Query Language
Deﬁnition 2.13 (Equivalence of Mapping Sets and Multi-sets) Let Ωbe a
mapping set and (Ω+, m+) a mapping multi-set. We say that Ωequals (Ω+, m+),
written as Ω∼= (Ω+, m+), iﬀit holds that Ω= Ω+ and m+(µ) = 1 for all µ ∈Ω+.2
The bag semantics is now deﬁned using adapted versions of the algebraic opera-
tions from Deﬁnition 2.10, modiﬁed to operate on top of multi-sets and to take the
multiplicity of the set elements into account. Implementing this idea, we overload
the algebraic operations from Deﬁnition 2.10 as follows.
Deﬁnition 2.14 (SPARQL Bag Algebra) Let M := (Ω, m), Ml := (Ωl, ml),
Mr := (Ωr, mr) be mapping multi-sets, R denote a ﬁlter condition, and S ⊂V
be a ﬁnite set of variables. We deﬁne the operations join 1, union ∪, minus \, left
outer join
1, projection π, and selection σ over mapping multi-sets:
Ml 1 Mr := (Ω′, m′), where
Ω′ := {µl ∪µr | µl ∈Ωl, µr ∈Ωr : µl ∼µr} and for all µ ∈M we set
m′(µ) := P
(µl,µr)∈{(µ∗
l ,µ∗r)∈Ωl×Ωr|µ∗
l ∪µ∗r=µ}(ml(µl) ∗mr(µr)).
Ml ∪Mr := (Ω′, m′), where
Ω′ := {µ | µ ∈Ωl or µ ∈Ωr} and for all µ ∈M we set
m′(µ) := ml(µ) + mr(µ).
Ml \ Mr := (Ω′, m′), where
Ω′ := {µl ∈Ωl | for all µr ∈Ωr : µl ̸∼µr}, and for all µ ∈M we set
m′(µ) :=
 ml(µ)
if µ ∈Ω′,
0
otherwise.
Ml
1 Mr := (Ml 1 Mr) ∪(Ml \ Mr)
πS(M) := (Ω′, m′), where
Ω′ := {µ1 | ∃µ2 : µ1 ∪µ2 ∈Ω∧dom(µ1) ⊆S ∧dom(µ2) ∩S = ∅}
and for all µ ∈M we set m′(µ) := P
µ+∈{µ∗
+∈Ω|πS({µ∗
+})={µ}} m(µ+).
σR(M) := (Ω′, m′), where
Ω′ := {µ ∈Ω| µ |= R} and for all µ ∈M we set
m′(µ) :=
 m(µ)
if µ ∈Ω′,
0
otherwise.
We refer to the above algebra as SPARQL bag algebra.
2
The above deﬁnition exactly corresponds to Deﬁnition 2.10 w.r.t. the mappings
that are contained in the result set (i.e., the deﬁnition of Ω′ in each rule mirrors the
deﬁnition of SPARQL set algebra); it diﬀers, however, in that it additionally ﬁxes
the multiplicities for generated set members (cf. function m′ in each rule). To give an
example, consider the computation of m′ in the deﬁnition of the 1 operator. For each
µ ∈M we deﬁne its multiplicity by summing up the multiplicities of all (compatible)
33

Chapter 2. Preliminaries
decompositions µl ∈Ωl, µr ∈Ωr that generate µ when merged together, thereby
taking their multiplicities in Ωl and Ωr into account. Observe that, if µ ̸∈Ω′ then
there exists no such decomposition and consequently m′(µ) is zero by deﬁnition.
As an interesting observation, for the minus operation Ml\Mr we set the resulting
multiplicity m′(µ) either to ml(µ) (i.e., we take over the multiplicity ml(µ) of the
mapping µ in Ml if there is no compatible mapping in Ωr) or set it to 0 (whenever
there is at least one compatible mapping in Ωr). We emphasize that this is exactly
the strategy proposed by the W3C [spac]. This strategy, however, contrasts with
the standard deﬁnition of the diﬀerence operator of relational algebra under bag
semantics, which forms the basis for SQL implementations. There, the diﬀerence
between two relational algebra expressions, say S −R, is computed by subtracting,
for each tuple appearing in S, its multiplicities in S and R (see e.g. [GMUW00]).
More precisely, if tuple t appears m times in S and n times in R, then t is contained
in the result of S −R if m > n and its multiplicity in S −R is deﬁned as m −n.
We ﬁnally point out that Deﬁnition 2.14 is correct in the sense that the alge-
braic operations always return multi-sets that are valid according to Deﬁnition 2.12,
i.e. whenever an operator generates a multi-set (Ω+, m+) then m+(µ) ≥1 for all
µ ∈Ω+ and m+(µ) = 0 for all µ ∈M \ Ω+. This property can be easily proven by
a case-by-case discussion of the algebraic operations. We omit the formal proof.
Based on the bag algebra for SPARQL, we can now formally deﬁne the bag se-
mantics, similar in style to the set semantics introduced in Deﬁnition 2.11:
Deﬁnition 2.15 (SPARQL Bag Semantics) Let t be a triple pattern, D be an
RDF database, Q1, Q2 denote SPARQL expressions, R a ﬁlter condition, and S ⊂V
be a ﬁnite set of variables. We deﬁne the bag semantics inductively as follows.
JtK+
D := (Ω, m), where Ω:= {µ | dom(µ) = vars(t) and µ(t) ∈D}
and m(µ) := 1 for all µ ∈Ω, m(µ) := 0 otherwise.
JQ1 And Q2K+
D
:= JQ1K+
D 1 JQ2K+
D
JQ1 Opt Q2K+
D
:= JQ1K+
D
1 JQ2K+
D
JQ1 Union Q2K+
D := JQ1K+
D ∪JQ2K+
D
JQ1 Filter RK+
D := σR(JQ1K+
D)
JSelectS(Q1)K+
D := πS(JQ1K+
D)
JAsk(Q1)K+
D
:= ¬(∅∼= JQ1K+
D)
2
The deﬁnition is identical to Deﬁnition 2.11, except for the case of triple pattern
evaluation. In particular, we represent the result of evaluating a triple pattern as a
multi-set (instead of a set), where we associate multiplicity 1 to each result map-
ping. Hence, when evaluating a SPARQL expression bottom-up using bag semantics,
algebraic operations will always be interpreted as multi-set operations.
34

2.3. The SPARQL Query Language
The next deﬁnition allows us to compare the results obtained from evaluating
SPARQL expressions and queries with the diﬀerent semantics.
Deﬁnition 2.16 (Coincidence of Semantics) Let Q be a SPARQL expression
or query and D be an RDF document. We say that the set and bag semantics
coincide for Q on D iﬀJQKD ∼= JQK+
D.
2
In general, the two semantics do not coincide, as witnessed by the example below.
Example 2.8 Consider the SPARQL expression Q := (?x, c, c) Union (c, c, ?x)
and document D := {(c, c, c)}. We observe that JQKD = {µ} with µ := {?x 7→c} and
JQK+
D = ({µ}, m) with m(µ) := 2, so the semantics do not coincide for Q on D. 2
2.3.4. Oﬃcial W3C SPARQL Syntax and Semantics
The abstract syntax model and the corresponding semantics for SPARQL presented
in Sections 2.3.1-2.3.3 form the basis for our theoretical investigations, such as the
SPARQL complexity analysis in Chapter 3 and algebraic optimization rules in Chap-
ter 4. Yet, when discussing more practical aspects of the SPARQL query language
such as the SP2Bench benchmark in Chapter 6, we will fall back on the oﬃcial
SPARQL syntax and semantics. In this section we therefore relate our formal model
to the oﬃcial W3C speciﬁcation and discuss both similarities and diﬀerences.
Syntax. We start with a survey of syntactic diﬀerences between our formal
model and the W3C speciﬁcation. Let us open the discussion with the oﬃcial W3C
SPARQL syntax for the (abstract syntax) query from Example 2.4:
SELECT ?name ? email
WHERE {
? person name ?name .
? person
age ? age
FILTER (? age =”30”)
OPTIONAL { ? person
email ? email } }
First note that operator And is abbreviated as “.”. Further, the keyword Op-
tional is used in place of the shortcut Opt. The naming of operators Select,
Union (not shown in the query), and Filter remains unchanged. As a minor syn-
tactic diﬀerence, triple patterns are not parenthesized and the body of the query is
enclosed into a separate block, deﬁned by the keyword Where. It is worth men-
tioning that, in analogy to our deﬁnition of queries (cf. Deﬁnition 2.5), the Select-
operator appears always (and only) at the top-level; in particular, the SPARQL
speciﬁcation, just like our formalization, does not allow for nested subqueries.
A major conceptual diﬀerence between both syntaxes is that operators in the of-
ﬁcial W3C syntax are not denoted in the style of mathematical binary operations,
35

Chapter 2. Preliminaries
but using a more declarative syntax. Accounting for this diﬀerence, the W3C speci-
ﬁcation provides a set of parsing and grouping rules, deﬁning how to transform the
syntax expression into an algebra expression. The W3C algebra itself is similar to
the SPARQL bag algebra introduced in Deﬁnition 2.14.8 We do not formally restate
the complete W3C syntax and semantics here, but instead will explain the meaning
of queries informally whenever using W3C syntax, to rule out ambiguities.
Set vs. Bag Semantics. We presented two diﬀerent semantics, namely the set-
based version J.KD and the bag semantics J.K+
D The W3C SPARQL speciﬁcation
follows the second approach, thus is close to the bag semantics introduced in Def-
inition 2.15. As we shall see in Chapter 3, the issue of bag vs. set semantics does
not play a role from a complexity point of view. In our study of SPARQL algebra
optimization in Chapter 4, however, we will study set and bag semantics separately
and highlight diﬀerences that are of immediate practical relevance.
Solution Modiﬁers. Our SPARQL formalization focuses on a core fragment
of the SPARQL query language, i.e. it implements only a subset of the features
deﬁned in the oﬃcial W3C SPARQL speciﬁcation. One construct that is included
in the oﬃcial recommendation but was not considered in our formalization are the
so-called solution modiﬁers, which can be used to manipulate the extracted result
set. We informally describe the existing solution modiﬁers in the listing below.
• The Distinct modiﬁer ﬁlters away duplicate mappings in the extracted re-
sult set. Note that this modiﬁer makes only sense in combination with bag
semantics, because under set semantics no duplicate solutions exist at all. We
will formalize and investigate the Distinct modiﬁer later in Section 4.4.2.
• While Distinct eliminates duplicate solutions, the Reduced modiﬁer oﬀers
ﬂexibility to the query optimizer in that it permits to eliminate duplicates.
The decision on whether to eliminate duplicates or not thus can be made on a
case-by-case basis, e.g. by comparing the estimated costs for both alternatives.
Like Distinct, the Reduced modiﬁer will be investigated in Section 4.4.2.
• Order By allows to sort the extracted result according to some variables,
either in descending or ascending order. The sort order can be ﬁxed using the
keywords Desc and Asc (where Asc is used as default sort order).
• The solution modiﬁers Limit and Offset can be used to ﬁx the number
of results that are returned and the ﬁrst result mapping that should be out-
put, respectively. They are particularly useful when combined with modiﬁer
Order By. As an example query that uses such a modiﬁer constellation, con-
sider Q11 in Section 6.4.3. The body of the query extracts electronic editions
of publications. According to the solution modiﬁers, the result is sorted by
variable ?ee in ascending lexicographical order. Modiﬁer “Limit 10” enforces
that only (up to) 10 results are returned, while “Offset 50” asserts that the
output contains electronic editions starting from the 51th result element.
8Similar means there are only insigniﬁcant diﬀerences, e.g. concerning the typing system.
36

2.3. The SPARQL Query Language
Note that solution modiﬁers are always applied after the result has been com-
puted, i.e. their evaluation requires a postprocessing of the query evaluation result.
Hence, they are not part of the algebraic evaluation process, but rather constitute
operations that are carried out separately from the core evaluation phase.9 We in-
tentionally omit these modiﬁers in our formalization, whose goal was to identify a
compact fragment that covers the basic, core evaluation process of SPARQL. When
discussing algebraic SPARQL query optimization later in this thesis, we will come
back to the Distinct and Reduced query forms (cf. Section 4.4.2).
Query Forms. We mentioned before in Section 2.3.1 that the oﬃcial SPARQL
standard introduces four diﬀerent types of queries, so-called query forms. In the
following listing, we discuss these four query forms informally.
• Select queries compute all possible variable mappings; they constitute the de
facto standard query form and have been formally introduced in Deﬁnition 2.5.
• SPARQL Ask queries are boolean queries that return yes/no (in our formal-
ization true/false, respectively). We introduced them in Deﬁnition 2.6.
• The Describe query form extracts additional information related to the result
mappings and returns a description in form of a new RDF graph. The SPARQL
speciﬁcation provides only a vague deﬁnition for the semantics of Describe
queries, stating that “the Describe form takes each of the resources identiﬁed
in a solution [...] and assembles a single RDF graph by taking a ’description’
which can come from any information available including the target RDF
dataset. The description is determined by the query service.” [spac]
• SPARQL Construct queries transform the result mapping into a new RDF
graph, according to rules that can be speciﬁed in the Construct clause.
Like for the solution modiﬁers, we observe that the query form implementation for
Describe and Construct is independent from the core evaluation process, but
again requires a postprocessing of the extracted result. Therefore, these two query
forms are not integrated in our SPARQL formalization. Instead, we focus on the
Select query form, which can be seen as the natural counterpart of standard SQL
queries. In some cases, we will also investigate Ask queries, which are particularly
interesting from an optimization perspective of view (see e.g. Section 4.4.1).
Additional Remarks. There are some more implementation-speciﬁc features
that should be listed for completeness. First, our deﬁnition of ﬁlter conditions (see
Deﬁnition 2.3) does not cover the full W3C standard. The latter comprises a broader
set of functions, such as isURI(?x), isLiteral(?x), or isBlank(?x) (used to check if
variable ?x is bound to elements of the respective set), and supports all the standard
relational operators (i.e., <, ≤, =, ̸=, ≥, >). Complementarily, the W3C semantics
includes a type system which is built on top of RDF datatypes (cf. Section 2.2)
9Though, in practical scenarios it could make sense to integrate them into the core evaluation
process, to improve system performance.
37

Chapter 2. Preliminaries
and comes into play when evaluating relational operators, e.g. operator “<” is in-
terpreted as numerical comparison when comparing integer-typed literals, but im-
plements a lexicographical comparison for string values. Arguably, such issues are
implementation-speciﬁc and therefore were abstracted away in our formalization.
Another diﬀerence is that the oﬃcial SPARQL speciﬁcation uses an extended
version of triple patterns (cf. Deﬁnition 2.4), where blank nodes may be used in
the subject and object position of triples. In contrast to URIs and literals, these
blank nodes are not interpreted as identifying nodes that match same-named blank
nodes in the RDF graph, but instead can be understood as variables that match
arbitrary nodes and whose bindings do not appear in the ﬁnal result. Abstracting
from the details (i.e., scoping issues), they can simply be replaced by variables that
are projected away in the Select clause, without changing the semantics of the
query. In particular, they do not add expressiveness, so we decided to ignore them
in our formalization, to keep the semantics as simple as possible.
Another interesting feature of SPARQL is the possibility to query multiple RDF
graphs at a time. This feature accounts for the global nature of the RDF data
format, which was designed to link information from diﬀerent sources together. In
particular, SPARQL provides keywords to specify the default graph and a set of
so-called named graphs to be accessed during query evaluation. In the body of
the SPARQL query, one can associate graph patterns with RDF graphs, which
ultimately allows to combine information from diﬀerent graphs.
Finally, we should note that SPARQL comes with a protocol, which allows to
convey SPARQL queries from a client to a SPARQL query processor. This protocol
empowers the intended use of SPARQL as a query language in the Semantic Web.
We refer the interested reader to [spab] for the oﬃcial protocol speciﬁcation.
2.3.5. Limitations of SPARQL
Given that SPARQL is a comparably young technology, the current W3C speciﬁca-
tion [spac] still has a couple of limitations, which become obvious when comparing
SPARQL to established query languages such as SQL or XQuery. The following list
surveys important features and constructs that are (to date) missing in SPARQL.
• Aggregation: The current speciﬁcation does not support aggregation func-
tions, such as summing up numeric values, counting, or average computation.
• Updates: While the SPARQL standard supports data extraction from RDF
graphs, constructs for inserting new triples into RDF graphs and manipulating
existing graphs (in the style of SQL Insert and Update clauses) are missing.
• Path Expressions: SPARQL does not support the speciﬁcation of (con-
strained) path expressions, e.g. using a single SPARQL query it is impossi-
ble to compute the transitive closure of a graph or to extract all nodes that
38

2.4. Related Work
are reachable from a ﬁxed node. This deﬁciency has repeatedly been iden-
tiﬁed in previous publications and diﬀerent proposals for incorporating path
expressions into the language have been made [KJ07; PAG08; ABE09]. The
interested reader will ﬁnd a short discussion of these approaches in Section 2.4.
• Views: In traditional query languages like SQL, logical views over the data
play an important role. They are crucial to both database design and access
management. SPARQL currently does not support the speciﬁcation of logical
views over the data; note, however, that materialized views over the data can
be extracted from the input graph using the Construct query form.
• Support for Constraints: Mechanism to assert and check for integrity con-
straints in the RDF database are not covered in the current SPARQL speciﬁ-
cation. In SQL, such integrity constraints are implicitly derived from primary
and foreign key speciﬁcations established in the schema design phase. Beyond
that, it is possible to enforce user-deﬁned constraints using the Create As-
sertion statement. Tackling the issue of integrity constraints, we will inves-
tigate general capabilities of SPARQL as a constraint language in Section 5.3.
We conclude with the remark that the W3C SPARQL working group is currently
working on a new version of SPARQL [spaa] (there is also a Wiki page that contains
proposals for future SPARQL extensions10). Based on these documents, it can be
expected that at least some of the above features will be part of the coming release.
2.4. Related Work
Formalization of RDF(S) and Semantics. The reasoning mechanisms imple-
mented in RDF(S) have their foundations in early logic-based languages for object-
oriented data, such as F-logic [KLW95] and description logics [BCM+03]. An early
model theoretic formalization of RDF(S) semantics was provided in [Mar04]. Going
one step further, foundations and advanced aspects of RDFS databases were studied
in [GHM04]. The latter work covers issues like RDFS reasoning and presents a set of
inference rules that implement the core of the model theoretic W3C RDF semantics
deﬁnition [rdfd]. In addition, it studies problems like complexity of entailment (i.e.,
the question if an RDF(S) graph logically implies another graph), normal forms for
RDF(S), and query answering on top of implied RDF(S) databases. Similar in style,
a datalog-style query language for RDF, called RDFLog, was proposed in [BFL+08].
In [MPG07] a minimal deductive system for RDFS is presented. The authors
identify a fragment of RDFS, called ρdf and show that entailment can be decided
eﬃciently this fragment. Although RDFS reasoning is not a central topic in this
thesis, this work gives valuable insights into the RDFS reasoning process.
10See http://esw.w3.org/topic/SPARQL/Extensions.
39

Chapter 2. Preliminaries
Formalization of SPARQL. In the early W3C Working Drafts for SPARQL,
the semantics of SPARQL evaluation was primarily deﬁned through test cases and
declaration of the expected query results.11 At that time, several proposals to deﬁn-
ing its semantics were made by the research community. One notable line of research
in this context is the speciﬁcation of mappings from SPARQL to relational algebra or
SQL, which implicitly ﬁx the semantics of SPARQL [Cyg05; CLJF06]. An early (yet
unﬁnished) approach to an algebraic formalization of SPARQL was given in [FT05].
In 2006 then, the publication [PAG06a] (which was further reﬁned in [PAG06b]) pre-
sented two diﬀerent set-based semantics for a core fragment of the SPARQL query
language, both building upon a compact algebra over mapping sets. The ﬁrst se-
mantics is compositional and diﬀers from the second, operational semantics in some
exceptional cases (such as the issue of evaluating nested Opt expressions). Inspired
by the latter work, the W3C started to work out a formal semantics for the full
SPARQL query language. On March 26, 2007, it released an improved version of
previous Working Drafts, which included a formal SPARQL semantics that closely
followed the compositional approach presented in [PAG06a; PAG06b].12 As a dif-
ference to the set-based semantics in [PAG06a], though, the W3C proposed a bag
semantics. This bag semantics was carried over throughout the standardization pro-
cess and has found entrance in the current W3C SPARQL Recommendation [spac].
Note that, as indicated in Section 2.3, our formalization of SPARQL also follows
the compositional approach from [PAG06a]. Consequently, the bag semantics that
we formalized in Deﬁnition 2.15 mirrors the current W3C semantics.
Extensions of SPARQL. We recently studied the perspectives of SPARQL as a
constraint language in [LMS08]. The work shows that SPARQL – with some minor
extensions – can be used to express a large class of constraints and to extract con-
straints from RDF graphs when these are speciﬁed using a predeﬁned vocabulary for
encoding constraints. We will review and extend some of these ideas in Section 5.3.
Aggregation functions for SPARQL were proposed in [PSS07]. The latter work
deﬁnes an extension of SPARQL, called SPARQL++, which embeds standard ag-
gregate functions in Construct and Filter clauses. The motivation for this ex-
tension was to express schema mappings through SPARQL Construct queries. It
is also worth mentioning that some existing SPARQL engines, e.g. ARQ [arq] and
Virtuoso [Bla07], already have implemented their own strategies to aggregation.
Path expressions for SPARQL have been identiﬁed as an important feature in
several research contributions [KJ07; PAG08; ABE09]. The idea that is common
to all these approaches is to extend SPARQL by constructs that allow to express
relations between nodes that go beyond what can be expressed by simple basic graph
patterns, such as e.g. transitively connected nodes. It is natural to assume that
11See for instance the W3C Working Draft for the SPARQL Query Language from October 4,
2006 at http://www.w3.org/TR/2006/WD-rdf-sparql-query-20061004/.
12See http://www.w3.org/TR/2007/WD-rdf-sparql-query-20070326/.
40

2.4. Related Work
querying for (constrained) paths is an important feature in the context of a graph-
structured data model like RDF. The approach in [KJ07] uses so-called regular path
patterns, akin to regular expressions, to express complex path relationships between
nodes in RDF graphs. These regular path patterns are used to extend SPARQL to
a dialect called SPARQLeR. In [PAG08] a SPARQL extension called nSPARQL is
proposed, driven by the idea of navigating through the RDF graph using a set of
predeﬁned axes, very much in the style of the XPath axes for navigating through
XML documents. Another reasonable approach is the PSPARQL [ABE09] query
language. It relies on an extended version of RDF, called PRDF, where graph edges
(i.e., predicates in RDF triples) may carry regular expression patterns as labels. The
PSPARQL query language is then deﬁned over such PRDF patterns.
41

Chapter 2. Preliminaries
42

Chapter 3.
Complexity of SPARQL Evaluation
Jen:
“How to get started, guys?”
Roy:
“Well, ﬁrst we should try to understand the basics of SPARQL.”
Jen:
“Oh, I’m afraid I know what you’re talking about...”
Moss: “... the complexity of SPARQL evaluation, right?”
Roy:
“Yes, exactly. Don’t panic, I’m pretty sure it’s gonna be fun!”
In this chapter we study the complexity of SPARQL query evaluation, with the
goal to establish a deep understanding of the operators, their complexity, their inter-
action, and their expressiveness. As is customary in the context of query languages,
we take the decision version of the Evaluation problem as a yardstick: given a
mapping µ, an RDF database D, and a SPARQL expression or query Q as input,
we are interested in the complexity of deciding whether µ is contained in the result
of evaluating Q on D. We study this problem for diﬀerent fragments of the SPARQL
query language, to separate operators and operator constellations that can be eval-
uated eﬃciently from more complex (and hence, more expressive) constellations.
In the problem statement above, we left the semantics used for our study of the
Evaluation problem unspeciﬁed. In fact, we are interested in the complexity of
SPARQL query evaluation for both the set semantics introduced in Section 2.3.2 and
the bag semantics from Section 2.3.3. As an important result, however, we will show
in Section 3.2.1 that the semantics do not diﬀer w.r.t. their evaluation complexity.
This observation allows us to restrict our discussion to the (arguably simpler) set
semantics, while all complexity results immediately carry over to the bag semantics
and, more importantly, also apply to the oﬃcial W3C SPARQL semantics.
It should be noted that, according to our deﬁnition of the Evaluation problem,
we investigate the combined complexity of SPARQL evaluation, where both the
database and the query are part of the input. The study of combined complexity
is particularly useful to understand the basic expressive power of the operators and
their interrelations, which is in line with our goal to investigate general capabilities
and expressiveness of SPARQL and fragments of the query language. We refer the
interested reader to [PAG06a] for results on so-called data complexity [Var82], which
diﬀers in that the size of the query is ﬁxed (i.e. the query is not part of the input).
43

Chapter 3. Complexity of SPARQL Evaluation
Coming back to the motivation for the work in this chapter, we argue that a
comprehensive complexity analysis is important for several reasons. As argued be-
fore, one important aspect is that the investigation of diﬀerent SPARQL fragments
separates subsets of the language that can be evaluated eﬃciently from complex frag-
ments. The knowledge of the evaluation complexity may be of immediate practical
interest when building applications that use (fragments of) SPARQL. For instance,
upper complexity bounds implicitly restrict the expressiveness and show that cer-
tain problems or task cannot be solved with the fragment under consideration. The
evaluation problem for SPARQL expressions built using only operators And, Fil-
ter, and triple patterns, for example, is known to be in PTime (cf. [PAG06a]),
so under the common assumption that PTime ̸= NP one cannot encode NP-hard
problems using only these two operators; yet, as we will show later in Section 3.2.5,
when extending the latter fragment by projection the evaluation problem becomes
NP-complete and one gains the power to model NP-hard problems.
Last but not least, our complexity study relates the SPARQL query language and
speciﬁc fragments to traditional query languages. To give a concrete example, there
has been a corpus of research on conjunctive queries (see [AHV] for a survey of
important results) and it is well-known that the evaluation problem for such queries
is NP-complete [CM77]. In our complexity study, we will identify SPARQL operator
combinations for which the evaluation problem falls into the same complexity class,
so the problem of evaluating these queries and the problem of conjunctive query
evaluation are mutually reducible. This close connection makes it possible to trans-
fer known results and optimization techniques from the area of conjunctive query
optimization into the context of SPARQL. For instance, we will exploit translations
from NP-complete SPARQL fragments to conjunctive queries in Chapter 5, which
allows us to exploit the chase procedure [MMS79; BV84], originally designed for
conjunctive query rewriting under data dependencies, in the context of SPARQL.
We conclude our introduction with the remark that the analysis of SPARQL
complexity is not new: the preliminary investigation of the combined complexity of
SPARQL in [PAG06a] shows that the evaluation problem for full SPARQL expres-
sions (i.e., the fragment introduced in Deﬁnition 2.4) is PSpace-complete. Further,
[PAG06a] provides selected results for fragments of the language with lower com-
plexity (we will summarize them in Section 3.2). As a consequent enhancement of
this initial analysis, we systematically explore the complexity of all expression and
query fragments, where fragment means a class of expressions or queries that can be
built using a ﬁxed subset of the SPARQL operators. Our exhaustive study gives us
valuable insights that go beyond the initial ﬁndings presented in [PAG06a]. One cen-
tral result is that the Evaluation problem for operator Opt alone (i.e., SPARQL
expressions built using only Opt and triple patterns) is already PSpace-hard. We
further show that this high complexity is caused by an unlimited nesting of Opt
expressions. In practice, however, it is reasonable to assume that the nesting depth
is ﬁxed, and for this case we prove lower complexity bounds. Still, as a key insight,
44

operator Opt is by far the most complicated construct in SPARQL. This obser-
vation suggests that special care in query optimization should be taken in queries
containing operator Opt and will serve as a guideline for our study of SPARQL
optimization and benchmarking in subsequent chapters of this thesis.
The main contributions and ﬁndings of this section can be summarized as follows.
• We show that the complexity of the Evaluation problem does not change
when switching from set to bag semantics and vice versa. This allows us to
focus on the simpler set semantics, while all results immediately carry over to
the bag semantics and hence to the oﬃcial W3C SPARQL semantics.
• The main source of complexity in Opt-free expression fragments is the com-
bination of operators And and Union. More precisely, these two operator
combinations make the Evaluation problem NP-complete. The Filter op-
erator in no case aﬀects the complexity of the Evaluation problem.
• In [PAG06a] it was shown that the evaluation problem for SPARQL expressions
(i.e. expressions built using And, Union, Opt, Filter, and triple patterns)
is PSpace-complete. We considerably reﬁne this analysis and show that all
fragments involving operator Opt are PSpace-complete. In particular, this
result already holds for queries built using only operator Opt, which reveals
that Opt is by far the most complex operator in the SPARQL query language.
• Motivated by the previous ﬁnding, we present a syntactic restriction for ex-
pressions involving operator Opt that lowers the complexity of the evaluation
problem: when ﬁxing the nesting depth of Opt subexpressions, we obtain tight
complexity bounds in the polynomial hierarchy. Spoken the other way around,
this result clariﬁes that only the unrestricted nesting of Opt expressions is
responsible for the PSpace-completeness of the query language.
• Going one step further, we also study the complexity of SPARQL queries,
obtained from expressions by adding top-level projection in the form of a
Select-clause (cf. Deﬁnition 2.5). We show that projection does not increase
the evaluation complexity for NP- and PSpace-complete fragments (hence,
the complexity of fragments including Opt remains the same), but raises the
complexity from PTime to NP for And-only expressions. The latter result
reveals the close connection between And-only queries and conjunctive queries.
Structure. We will shortly revisit relevant concepts and deﬁnitions from com-
plexity theory in Section 3.1, before presenting an elaborate complexity analysis for
the SPARQL query language in Section 3.2. First, in Section 3.2.1, we show that
the evaluation problem has the same complexity for both set and bag semantics.
Subsequently, we investigate the complexity of Opt-free expression fragments (Sec-
tion 3.2.2), expression fragments involving operator Opt (Sections 3.2.3 and 3.2.4),
and SPARQL queries (Section 3.2.5). For the convenience of the reader, we sum-
marize all gathered complexity results in Section 3.2.6. We ﬁnally wrap up with a
discussion of related work in Section 3.3 and a short conclusion in Section 3.4.
45

Chapter 3. Complexity of SPARQL Evaluation
3.1. Preliminaries: Complexity Theory
Before starting our discussion of SPARQL complexity we introduce some background
and conventions from complexity theory. As usual, we denote by PTime the com-
plexity class comprising decision problems that can be decided by a deterministic
Turing Machine (TM) in polynomial time, by NP the class of problems that can be
decided by a non-deterministic TM in polynomial time, and by PSpace the class
of problems that can be decided by a deterministic TM within polynomial space
bounds. It is common knowledge that the inclusion hierarchy
PTime ⊆NP ⊆PSpace
(3.1)
holds, and it is usually conjectured that both inclusion relations are strict.
3.1.1. The Polynomial Hierarchy
Given a complexity class C we denote by coC the set of decision problems whose
complement can be decided by a TM in class C. Given complexity classes C1 and
C2, the class CC2
1
captures all problems that can be decided by a TM M1 in class C1
enhanced by an oracle machine M2 for solving problems in C2. Informally speaking,
M1 may consult M2 to obtain a yes/no-answer for a problem in C2 in a single step.
We refer the interested reader to [AB07] for a formal discussion of oracle machines.
Given the previous notation, we are now in the position to deﬁne the polynomial
hierarchy, a sequence of complexity classes that was introduced in [Sto76]:
Deﬁnition 3.1 (Polynomial Hierarchy [Sto76])
The polynomial hierarchy is
the sequence of classes ΣP
i and ΠP
i for i ∈N0, inductively deﬁned as
ΣP
0 = ΠP
0 :=PTime, ΣP
n+1:=NPΣP
n , and ΠP
n+1:=coNPΣP
n .
2
Note that, by deﬁnition, ΣP
i = coΠP
i holds. Further, it is known that ΣP
i ⊆ΠP
i+1
and ΠP
i ⊆ΣP
i+1. In addition, the following two inclusion hierarchies are known.
PTime = ΣP
0 ⊆NP = ΣP
1 ⊆ΣP
2 ⊆ΣP
3 ⊆· · · ⊆PSpace
PTime = ΠP
0 ⊆coNP = ΠP
1 ⊆ΠP
2 ⊆ΠP
3 ⊆· · · ⊆PSpace
(3.2)
3.1.2. Complete Problems
We consider completeness only with respect to polynomial-time many-one reduc-
tions. QBF, the tautology test for quantiﬁed boolean formulas, is known to be
PSpace-complete [AB07]. Variants of QBF with restricted quantiﬁer alternation
46

3.2. Complexity of SPARQL
are complete for classes ΠP
i
or ΣP
i , depending on the number i of quantiﬁer al-
ternations and the question whether the ﬁrst quantiﬁer is ∀or ∃, respectively
(cf. [Pap94; AB07]). Finally, the NP-completeness of the SetCover problem and
the 3Sat problem is folklore (see e.g. [Pap94] for a study of these two problems).
We will provide a precise deﬁnition of all problems before using them in reductions.
3.2. Complexity of SPARQL
We introduce the SPARQL operator shortcuts A := And, F := Filter, O := Opt,
and U := Union. For notational convenience, we denote the class of SPARQL ex-
pressions that can be constructed using a set of operators (plus triple patterns) by
concatenating the respective operator shortcuts. To give an example, the class AU
comprises all SPARQL expressions that can be constructed using only operators
And, Union, and triple patterns. By E we denote the full class of SPARQL expres-
sions (according to Deﬁnition 2.4), i.e. we deﬁne E := AFOU. In the remainder of
this chapter, we will use the terms class and fragment interchangeably.
In the subsequent complexity study, we follow the approach from [PAG06a] and
take the complexity of the Evaluation problem as a reference:
Evaluation: given a mapping µ, a document D, and a SPARQL expression
or a SPARQL query Q as input: is µ ∈JQKD?
The following theorem summarizes all previous results on the combined complex-
ity of SPARQL fragments established in [PAG06a], rephrased in our notation. We
refer the interested reader to the original work for the proofs of these results.
Theorem 3.1 (see [PAG06a]) The Evaluation problem is
1. in PTime for class AF (membership in PTime for A and F follows directly),
2. NP-complete for class AFU, and
3. PSpace-complete for classes AOU and E.
2
The theorem (and hence, the study in [PAG06a]) leaves several questions about
the complexity of SPARQL unanswered. First, all results in Theorem 3.1 have been
established for the set semantics J.KD, so it is not immediately clear if these results
carry over to the oﬃcial W3C SPARQL speciﬁcation, which uses a bag semantics.
Second, concerning Theorem 3.1(2) it is an open question if the NP-completeness
stems from the combination of all three operators And, Filter, Union or if it is
already obtained when combining either And or Filter with Union. Third, the
theorem does not identify the minimal operator constellation that causes PSpace-
hardness, i.e. there are no results for subclasses of AOU that include operator Opt.
47

Chapter 3. Complexity of SPARQL Evaluation
Finally, the results are restricted to SPARQL expressions and do not cover the com-
plexity of queries, which have an additional Select operator on top. In the oﬃcial
W3C speciﬁcation, however, the Select-clause is mandatory, so these fragments
are of immediate practical interest. We will now systematically explore these issues.
3.2.1. Set vs. Bag Semantics
The deﬁnition of the Evaluation problem in Section 3.2 relies on set semantics for
query evaluation. Our ﬁrst task is to show that all complexity results obtained for
set semantics immediately carry over to bag semantics. The corresponding problem
for bag semantics, denoted by Evaluation+, can be phrased as follows.
Evaluation+: given a mapping µ, document D, and SPARQL expression or
SPARQL query Q as input: let JQK+
D := (Ω+, m+), is µ ∈Ω+?1
Note that we do not explicitly enforce that m+(µ) ≥1, because this condition
is implicitly given by our deﬁnition of multi-sets (cf. Deﬁnition 2.12), i.e. when-
ever a mapping appears in Ω+, then its multiplicity is at least one. The following
lemma shows that the bag semantics diﬀers from the set semantics at most in the
multiplicity associated to each mapping (which might be greater than one).
Lemma 3.1 Let Q be a SPARQL query or expression, D be an RDF database, and
µ be a mapping. Let Ω:= JQKD and (Ω+, m+) := JQK+
D. Then µ ∈Ω⇔µ ∈Ω+. 2
Proof of Lemma 3.1
We prove the lemma by induction on the structure of Q. To simplify the notation,
we shall write µ ∈JQK+
D if and only if µ ∈Ω+ for JQK+
D := (Ω+, m+). Intuitively,
this short notation is justiﬁed by the property that m+(µ) ≥1 for each µ ∈Ω+. It is
crucial to note that the SPARQL bag algebra operators introduced in Deﬁnition 2.10
maintain this property, i.e. whenever an algebraic operation generates a mapping µ,
then the multiplicity that is associated with µ is at least one.
The induction hypothesis for the proof of Lemma 3.1 is µ ∈JQKD ⇔µ ∈JQK+
D.
For the basic case, let us assume that Q := t is a triple pattern. Let Ω:= JtKD and
(Ω+, m+) := JtK+
D be the results obtained when evaluating Q on D using set and bag
semantics, respectively. From Deﬁnitions 2.10 and 2.14 it follows immediately that
Ω= Ω+, so it trivially holds that µ ∈JQKD ⇔µ ∈JQK+
D, which completes the basic
case. We therefore may assume that the hypothesis holds for every expression.
Coming to the induction step, we distinguish ﬁve cases. (1) Let Q := P1 And P2.
⇒: Let µ ∈JP1 And P2KD = JP1KD 1 JP2KD. Then, by deﬁnition of operator 1,
there are µ1 ∈JP1KD, µ2 ∈JP2KD s.t. µ1 ∼µ2 and µ1 ∪µ2 = µ. By application of the
1An alternative version of the evaluation problem under bag semantics encountered in literature
is to ask whether µ ∈Ωand m(µ) = c for some c. Here, we disregard the multiplicity of µ.
48

3.2. Complexity of SPARQL
induction hypothesis, we have that µ1 ∈JP1K+
D and µ2 ∈JP2K+
D, and consequently
µ = µ1∪µ2 ∈JP1K+
D 1 JP2K+
D = JP1 And P2K+
D. Direction “⇐” is analogical. We omit
the proof for case (2) Q := P1 Union P2, which is similar to case (1). Next, (3) let
Q := P1 Opt P2. We exemplarily discuss direction “⇒”, the opposite direction
is similar. Let µ ∈JP1 Opt P2KD = (JP1KD 1 JP2KD) ∪(JP1KD \ JP2KD). Then µ
is generated (i) by the subexpression JP1KD 1 JP2KD or (ii) by JP1KD \ JP2KD. The
argumentation for (i) is identical to case (1), i.e. we can show that µ is then generated
by JP1 Opt P2K+
D = (JP1K+
D 1 JP2K+
D) ∪(JP1K+
D \ JP2K+
D), namely by the left side of
the union. For case (ii), we argue that µ ∈JP1KD \ JP2KD implies µ ∈JP1K+
D \ JP2K+
D.
So let us assume that µ ∈JP1KD \JP2KD. Then µ ∈JP1KD and there is no compatible
mapping µ′ ∼µ in JP2KD. We have µ ∈JP1K+
D by induction hypothesis. Assume
for the sake of contradiction that there is a compatible mapping µ′ ∼µ in JP2K+
D.
Then, again by induction hypothesis, we have that µ′ ∈JP2KD, which contradicts to
the assumption that there is no compatible mapping to µ in JP2KD. This completes
case (3). Finally, cases (4) Q := SelectS(P) and (5) Q := P Filter R are easily
obtained by application of the induction hypothesis.2
It follows as a corollary from the lemma above that the set and bag semantics do
not diﬀer w.r.t. the complexity of the SPARQL Evaluation problem:
Corollary 3.1 Let µ be a mapping, D be an RDF document, and Q be a SPARQL
expression or query. Then Evaluation(µ, D, Q) ⇔Evaluation+(µ, D, Q).
2
This result allows us to use the simpler set semantics for our study of SPARQL
complexity, while all results immediately carry over to bag semantics. We point out
that the W3C SPARQL Recommendation relies on a bag semantics very close to ours
in Deﬁnition 2.15 (cf. the discussion in Section 2.3.4) and therefore all complexity
results that we derive in this chapter also apply to the oﬃcial W3C semantics.
3.2.2. Complexity of OPTIONAL-free Expressions
Our ﬁrst goal is to establish a precise characterization of the Union operator. As
also noted in [PAG06a], the design of this operator was subject to controversy in the
SPARQL working group.2 In response, we aim to understand the operator and its re-
lation to others beyond the known NP-completeness result for class AFU. The next
theorem gives the results for all Opt-free fragments not covered by Theorem 3.1.
Theorem 3.2 The Evaluation problem is
1. in PTime for classes U and FU, and
2. NP-complete for class AU.
2
2For further details on this issue see the discussion of disjunction in Section 6.1 in the W3C
mailing archive http://www.w3.org/TR/2005/WD-rdf-sparql-query-20050217/.
49

Chapter 3. Complexity of SPARQL Evaluation
Proof of Theorem 3.2
Theorem 3.2(1): We provide a PTime-algorithm that solves the Evaluation
problem for fragment FU. It is deﬁned inductively on the structure of the input
expression P and returns true if µ ∈JPKD, false otherwise. We distinguish three
cases. (a) If P := t is a triple pattern, we return true if and only if µ ∈JtKD.
(b) If P := P1 Union P2 we (recursively) check if µ ∈JP1KD ∨µ ∈JP2KD holds.
(c) If P := P1 Filter R for some ﬁlter condition R, we return true if and only if
µ ∈JP1KD ∧R |= µ. It is easy to see that the above algorithm runs in polynomial
time. Its correctness follows from the deﬁnition of the algebraic operators ∪and σ.
Theorem 3.2(2): In order to prove that the Evaluation problem for fragment AU
is NP-complete we have to show NP-membership and NP-hardness.
(Membership) Let P be a SPARQL expression composed of operators And, Union,
and triple patterns, D a document, and µ a mapping. We provide an NP-algorithm
that returns true if µ ∈JPKD, and false otherwise. Our algorithm is deﬁned on the
structure of P: (a) if P := t is a triple pattern then return true if µ ∈JtKD, false
otherwise; (b) if P := P1 Union P2, return the truth value of µ ∈JP1KD∨µ ∈JP2KD;
ﬁnally, (c) if P := P1 And P2, then guess a decomposition µ = µ1 ∪µ2 and return
the truth value of µ1 ∈JP1KD ∧µ2 ∈JP2KD. The correctness of the algorithm follows
from the deﬁnition of the algebraic operators 1 and ∪. It is easy to see that it can
be implemented by a non-deterministic TM that runs in polynomial time.
(Hardness) We reduce the SetCover problem to the Evaluation problem for
SPARQL. SetCover is known to be NP-complete, so the reduction gives us the
desired hardness result. The SetCover problem is deﬁned as follows.
SetCover: Let U := {u1, . . . , uk} be a universe, S1, . . . Sn ⊆U be sets over U,
and let l be positive integer: is there a set I ⊆{1, . . . , n} of size
|I|≤l such that S
i∈I Si = U?
We use the ﬁxed database D := {(c, c, c)} for our encoding and represent each set
Si := {x1, x2, . . . , xm} by a SPARQL expression of the form
PSi := (c, c, ?X1) And . . . And (c, c, ?Xm).
Next, we deﬁne the expression PS := PS1 Union . . . Union PSn as an encoding
for the set S = {S1, . . . , Sn} of all Si. Finally we deﬁne the SPARQL expression
P := PS And . . . And PS
|
{z
}
PS appears exactly l times
.
The intuition of the encoding is as follows. PS encodes all subsets Si. A set element,
say x, is represented by the presence of a binding from variable ?X to value c. The
encoding of P allows us to “merge” (at most) l arbitrary sets Si. It is straightforward
50

3.2. Complexity of SPARQL
to show that SetCover is true iﬀµ := {?U1 7→c, . . . , ?Uk 7→c} ∈JPKD, i.e. if the
complete universe U can be obtained by merging these sets.2
Theorems 3.1 and 3.2 taken together clarify that the source of complexity in
Opt-free fragments is the combination of And and Union. In particular, adding
or removing Filter expressions in no case increases or decreases the complexity.
3.2.3. Complexity of Expression Classes Including OPTIONAL
We next investigate the complexity of operator Opt and its interaction with other
operators. The PSpace-completeness results for classes AOU and E := AFOU
in Theorem 3.1 give only partial answers to these questions. The following theorem
reﬁnes the above results, showing that already class AO is PSpace-complete:
Theorem 3.3 Evaluation is PSpace-complete for class AO.
2
Proof of Theorem 3.3
We reduce QBF, a prototypical PSpace-complete problem, to the SPARQL
Evaluation problem for class AO. The hardness part of the proof is in parts in-
spired by the proof of Theorem 3.1(3), which has been formally proven in [PAG06a]:
there, QBF was encoded using operators And, Opt, and Union. Here, we en-
code the problem using only And and Opt, which turns out to be considerably
harder. More precisely, our contribution is to show how to encode a quantiﬁer-
free boolean formula using And and Opt, while the encoding of the surrounding
quantiﬁer sequence remains the same. Membership in PSpace, and hence PSpace-
completeness, then follows directly from the PSpace-membership of the fragment
AOU ⊃AO (cf. Theorem 3.1(3)). Formally, QBF is deﬁned as follows.3
QBF: given a quantiﬁed boolean formula ϕ := ∀x1∃y1∀x2∃y2 . . . ∀xm∃ymψ
as input, where ψ is a quantiﬁer-free formula in conjunctive normal
form (CNF): is the formula ϕ valid?
Let us start the discussion with a quantiﬁed boolean formula
ϕ := ∀x1∃y1∀x2∃y2 . . . ∀xm∃ymψ
and assume that the inner formula ψ of the quantiﬁed formula is in conjunctive
normal form, i.e. ψ := C1∧· · ·∧Cn where the Ci (i ∈[n]) are disjunctions of literals4.
By Vψ we denote the set of (boolean) variables in ψ and by VCi the set of variables
in clause Ci. For our encoding, we use the polynomial-size database
3Like the proof in [PAG06a], we assume that the inner formula of the quantiﬁed formula is in CNF.
It is known that also this variant of the QBF problem is PSpace-complete.
4A literal is either a boolean variable x or a negated boolean variable ¬x.
51

Chapter 3. Complexity of SPARQL Evaluation
D := {(a, false, 0), (a, true, 1), (a, tv, 0), (a, tv, 1)} ∪
{(a, vari, v) | v ∈VCi} ∪{(a, v, v) | v ∈Vψ},
where the second and the third part of the union set up triples for the variables
in each Ci and Vψ, respectively. For instance, if VC1 = Vψ = {x}, the second and the
third part of the union would generate the triples (a, var1, x) and (a, x, x), respec-
tively, where x is understood as a URI representing the boolean variable x.
For each clause Ci := v1 ∨· · ·∨vj ∨¬vj+1 ∨· · ·∨¬vk, where v1, . . . , vj are positive
and vj+1, . . . , vk are negated variables, we deﬁne a separate SPARQL expression
PCi := (. . . ((. . . ((a, vari, ?vari)
Opt ((a, v1, ?vari) And (a, true, ?V1)))
. . .
Opt ((a, vj, ?vari) And (a, true, ?Vj)))
Opt ((a, vj+1, ?vari) And (a, false, ?Vj+1)))
. . .
Opt ((a, vk, ?vari) And (a, false, ?Vk))),
where v1, . . . , vk stand for the URIs that are associated with the respective vari-
ables according to D. We then encode formula ψ as Pψ := PC1 And . . . And PCn.
It is straightforward to verify that ψ is satisﬁable iﬀthere is a mapping µ ∈JPψKD.
Even more, each mapping µ ∈JPψKD represents a set of truth assignments, where
each assignment ρµ is obtained as follows: for each vi ∈Vψ we set ρµ(vi) := µ(?Vi)
if ?Vi ∈dom(µ), or deﬁne either ρµ(vi) := 0 or ρµ(vi) := 1 if ?Vi ̸∈dom(µ); vice
versa, for each truth assignment ρ that satisﬁes ψ there is µ ∈JPψKD that deﬁnes
ρ according to the construction rule for ρµ above. Note that the deﬁnition of ρµ
accounts for the fact that some ?Vi may be unbound in µ; in such a case, the value
of the variable is not relevant to obtain a satisfying truth assignment and we can
randomly choose a value for the corresponding boolean variable vi.
Given Pψ, we can encode the quantiﬁer-sequence using a series of nested Opt
statements as shown in [PAG06a]. To make the proof self-contained, we shortly sum-
marize this construction. We use SPARQL variables ?X1, . . . , ?Xm and ?Y1, . . . , ?Ym
to represent variables x1, . . . xm and y1, . . . , ym, respectively. In addition, we use
fresh variables ?A0, . . .?Am, ?B0, . . .?Bm, and operators And, Opt to encode the
quantiﬁer sequence ∀x1∃y1 . . . ∀xm∃ym. For each i ∈[m] we deﬁne Pi and Qi as
Pi := ((a, tv, ?X1) And . . . And (a, tv, ?Xi) And
(a, tv, ?Y1) And . . . And (a, tv, ?Yi−1) And
(a, false, ?Ai−1) And (a, true, ?Ai)),
Qi := ((a, tv, ?X1) And . . . And (a, tv, ?Xi) And
(a, tv, ?Y1) And . . . And (a, tv, ?Yi) And
(a, false, ?Bi−1) And (a, true, ?Bi)).
52

3.2. Complexity of SPARQL
Using these expressions, we encode the quantiﬁed boolean formula ϕ as
Pϕ := (a, true, ?B0) Opt (P1 Opt (Q1
Opt (P2 Opt (Q2
. . .
Opt (Pm Opt (Qm And Pψ)) . . . )))).
It can be shown that µ := {?B0 7→1} ∈JPϕKD if and only if ϕ is valid, which
completes the reduction. We do not restate this technical part of the proof here, but
refer the interested reader to the proof of Theorem 3 in [PAG06a] for details.2
To clarify the previous encoding of QBF, let us shortly sketch a small example
that illustrates the construction for a ﬁxed quantiﬁed boolean formula.
Example 3.1 We show how to encode the quantiﬁed boolean formula
ϕ := ∀x1∃y1(x1 ⇔y1)
= ∀x1∃y1((x1 ∨¬y1) ∧(¬x1 ∨y1)),
where ψ := ((x1 ∨¬y1) ∧(¬x1 ∨y1)) is in CNF. First, observe that formula ϕ is
a tautology. The variables in ψ are Vψ := {x1, y1}; further, we have C1 := x1 ∨¬y1,
C2 := ¬x1 ∨y1, and VC1 = VC2 := {x1, y1}. Strictly following the construction
described in the proof of Theorem 3.3, we set up the database
D := {(a, false, 0), (a, true, 1), (a, tv, 0), (a, tv, 1),
(a, var1, x1), (a, var1, y1), (a, var2, x1), (a, var2, y1),
(a, x1, x1), (a, y1, y1)},
where x1, y1 are URIs. We next deﬁne the expression Pψ := PC1 And PC2 with
PC1 := ((a, var1, ?var1) Opt ((a, x1, ?var1) And (a, true, ?X1)))
Opt ((a, y1, ?var1) And (a, false, ?Y1)),
PC2 := ((a, var2, ?var2) Opt ((a, y1, ?var2) And (a, true, ?Y1)))
Opt ((a, x1, ?var2) And (a, false, ?X1)).
When evaluating these expressions we obtain
JPC1KD = ({{?var1 7→x1}, {?var1 7→y1}}
1 {{?var1 7→x1, ?X1 7→1}})
1 {{?var1 7→y1, ?Y1 7→0}}
= {{?var1 7→x1, ?X1 7→1}, {?var1 7→y1, ?Y1 7→0}},
JPC2KD = {{?var2 7→x1}, {?var2 7→y1}}
1 {{?var2 7→y1, ?Y1 7→1}}
1 {{?var2 7→x1, ?X1 7→0}}
= {{?var2 7→x1, ?X1 7→0}, {?var2 7→y1, ?Y1 7→1}},
JPψKD
= JPC1 And PC2KD
= {{?var1 7→x1, ?var2 7→y1, ?X1 7→1, ?Y1 7→1},
{?var1 7→y1, ?var2 7→x1, ?X1 7→0, ?Y1 7→0}}.
53

Chapter 3. Complexity of SPARQL Evaluation
We observe that the mappings in JPC1KD, JPC2KD, and JPψKD reﬂect the satisfying
truth assignments for the respective boolean formulas C1, C2, and ψ. For instance,
formula ψ is true iﬀeither both x1 and y1 are true or both are false. This is reﬂected
by the two result mappings of JPψKD: the ﬁrst mapping binds the corresponding
SPARQL variables ?X1, ?Y1 to 1 (true) and the second one maps both to 0 (false).
The ﬁnal step of the encoding is to set up the expressions P1, Q1, and Pϕ:
P1 := ((a, tv, ?X1) And (a, false, ?A0) And (a, true, ?A1))
Q1 := ((a, tv, ?X1) And (a, tv, ?Y1) And (a, false, ?B0) And (a, true, ?B1))
Pϕ := (a, true, ?B0) Opt (P1 Opt (Q1 And Pψ))
It is easily shown that µ := {?B0 7→1} ∈JPϕKD, which conﬁrms that ψ is valid.2
Note that, in contrast to the PSpace-hardness proofs for AOU, and E in [PAG06a]
(cf. Theorem 3.1(3) above), the database used in the previous reduction from QBF
to fragment AO is not ﬁxed, but depends on the input formula. Therefore, it is an
open question whether the PSpace-hardness result for AO carries over to expression
complexity (i.e., the evaluation complexity when ﬁxing the database).
Having shown that the Evaluation problem for class AO is PSpace-complete,
another interesting question that arises is whether we can ﬁnd tight complexity
bounds for the Opt-only expression fragment O. The following theorem subsumes
the previous one and constitutes one of the main results in this chapter.
Theorem 3.4 Evaluation is PSpace-complete for class O.
2
Given that the Evaluation problem for fragments A, F, and U is in PTime
(cf. Theorems 3.1 and 3.2), this ﬁnding clariﬁes that Opt is by far the most compli-
cated operator in the language. The intuition behind this result is that operator
1,
the algebraic counterpart of Opt, is deﬁned using operators 1, ∪, and \; the mix of
these operations (and in particular the operator \, which allows to encode negation)
compensates for missing And and Union operators at syntax level.
In the remainder of this subsection, we will sketch the technically involved proof
for Theorem 3.4. Adapting the idea from the proof of Theorem 3.3, we present a
reduction from QBF to the Evaluation problem for SPARQL queries, where the
main challenge is to encode the quantiﬁed boolean formula using only operator Opt.
Rather than starting from scratch, our strategy is to take the proof of Theorem 3.3
as a starting point and to replace all And expressions by Opt-only constructions.
As we will see later, most of the And operators in the encoding can simply be
replaced by Opt without changing the semantics. However, for the innermost And
expressions in the encoding of Pϕ it turns out that the situation is not that easy. We
therefore start with a lemma that will later help us to solve this situation elegantly.
54

3.2. Complexity of SPARQL
Lemma 3.2 Let
• Q, Q1, Q2, . . . , Qn (n ≥2) be SPARQL expressions,
• S denote the set of all variables appearing in Q, Q1, Q2, . . . , Qn,
• D := {(a, false, 0), (a, true, 1), (a, tv, 0), (a, tv, 1)} ∪D′ be an RDF database
such that dom(D′) ∩{true, false} = ∅,
• ?V2, ?V3, . . . , ?Vn be a set of n −1 variables distinct from the variables in S.
Further, we deﬁne the expressions
Vi := (a, true, ?Vi),
V i := (a, false, ?Vi),
Q′ :=((. . . ((Q Opt V2) Opt V3) . . . ) Opt Vn), and
Q′′ :=((. . . ((Q1 Opt (Q2 Opt V2))
Opt (Q3 Opt V3))
. . . )
Opt (Qn Opt Vn)).
The following claims hold.
(1) JQ′KD = {µ ∪{?V2 7→1, . . . , ?Vn 7→1} | µ ∈JQKD}
(2) JQ′ Opt (Q1 And Q2 And . . . And Qn)KD
= JQ′ Opt ((. . . ((Q′′ Opt V 2) Opt V 3) . . . ) Opt V n)KD
2
Informally speaking, claim (2) of the lemma provides a mechanism to rewrite an
And expression that is encapsulated in the right side of an Opt expression by means
of an Opt expression. It is important to realize that there is a restriction imposed
on the left side expression Q′, i.e. Q′ is obtained from Q by extending each result
mapping in JQKD by {?V2 7→1, . . . , ?Vn 7→1}, as stated in claim (1). Before proving
the lemma, let us illustrate the construction by means of a small example:
Example 3.2 Consider database D := {(a, false, 0), (a, true, 1), (a, tv, 0), (a, tv, 1)}
and the three SPARQL expressions
Q := (a, tv, ?x)
, thus JQKD= {{?x 7→0}, {?x 7→1}},
Q1 := (a, tv, ?y)
, thus JQ1KD= {{?y 7→0}, {?y 7→1}},
Q2 := (a, true, ?y) , thus JQ2KD= {{?y 7→1}}.
Concerning claim (1) of Lemma 3.2, we observe that
JQ′KD = JQ Opt V2KD
= JQ Opt (a, true, ?V2)KD
= {{?x 7→0, ?V2 7→1}, {?x 7→1, ?V2 7→1}},
55

Chapter 3. Complexity of SPARQL Evaluation
so JQ′KD diﬀers from JQKD only in that each mapping contains an additional
binding ?V2 7→1. As for claim (2) of the lemma, we observe that the left expression
JQ′ Opt (Q1 And Q2)KD
= JQ′KD
1 {{?y 7→1}}
= {{?x 7→0, ?y 7→1, ?V2 7→1}, {?x 7→1, ?y 7→1, ?V2 7→1}}
yields the same result as the right side expression
JQ′ Opt ((Q1 Opt (Q2 Opt V2)) Opt V2)KD
= JQ′KD
1 ((JQ1KD
1 (JQ2KD
1 JV2KD))
1 JV2KD)
(1)
= JQ′KD
1 ((JQ1KD
1 {{?y 7→1, ?V2 7→1}})
1 JV2KD)
(2)
= JQ′KD
1 ({{?y 7→0}, {?y 7→1, ?V2 7→1}}
1 JV2KD)
(3)
= JQ′KD
1 {{?y 7→0, ?V2 7→0}, {?y 7→1, ?V2 7→1}}
(4a)
= {{?x 7→0, ?V2 7→1}, {?x 7→1, ?V2 7→1}}
1 {{?y 7→0, ?V2 7→0}, {?y 7→1, ?V2 7→1}}
(4b)
= {{?x 7→0, ?y 7→1, ?V2 7→1}, {?x 7→1, ?y 7→1, ?V2 7→1}}.
The right side expression simulates the inner And expression from the left side
using a series of Opt expressions. The idea of the construction is as follows. In
step (1) we extend each mapping in JQ2KD by an additional binding ?V2 7→1. Now
recall that Ω1
1 Ω2 := (Ω1 1 Ω2) ∪(Ω1 \ Ω2). When computing the left outer
join between JQ1KD and the mapping set from step (1) in step (2), the binding
?V2 7→1 will be carried over to mappings that result from the 1 part of the left
outer join (cf. mapping {?y 7→1, ?V2 7→1}), but does not appear in mappings that
are generated from the \ part of the left outer join (cf. mapping {?y 7→0}). Next, in
step (3) we extend all mappings from the prior set for which ?V2 is not bound by a
binding ?V2 7→0. This extension aﬀects only the mapping obtained from the \ part,
while the mapping from the 1 part is left unchanged. In the ﬁnal steps (4a) and (4b),
the bindings ?V2 7→1 in each µ ∈JQ′KD serve as ﬁlters, which reject all mappings
that come from the \ part. Thus, only those mappings that have been created by
the 1 part are retained. Hence, the construction simulates the behavior of the And
expression (the syntactic counterparts of operator 1) using Opt operators.
2
Proof of Lemma 3.2
Lemma 3.2(1): First, we observe that all ?Vi are unbound in each µ ∈JQKD,
because by assumption the ?Vi are fresh variables that do not appear in Q. Next,
given that dom(D′) does not contain the URI true it follows that no triple in D′
matches the triple pattern Vi := (a, true, ?Vi), so we have that JViKD = {{?Vi 7→1}}.
Hence, in JQ′KD each mapping µ ∈JQKD is successively extended by the (compatible)
mappings {?V2 7→1}, . . . , {?Vn 7→1}, which implies that the claim holds.
56

3.2. Complexity of SPARQL
Lemma 3.2(2): We study the evaluation of the right side expression and argue that
it yields exactly the same result as the left side expression. Rather than working out
all technical details, we try to give the intuition behind the equivalence. We start the
discussion with the right side subexpression Q′′. First observe that the result of eval-
uating Qi Opt Vi corresponds to the result of Qi, except that each result mapping
is extended by ?Vi 7→1. We use the abbreviation QVi
i
:= Qi Opt Vi, which allows
us to compactly denote Q′′ by ((. . . ((Q1 Opt QV2
2 ) Opt QV3
3 ) Opt . . . ) Opt QVn
n ).
By application of semantics and some simple algebraic laws, such as distributivity
of 1 over ∪(cf. Chapter 4), we can bring JQ′′KD into the form
JQ′′KD = J((. . . ((Q1 Opt QV2
2 ) Opt QV3
3 ) Opt . . . ) Opt QVn
n )KD
= . . .
= JQ1 And QV2
2 And QV3
3 And . . . And QVn
n KD ∪PD,
where we call the left subexpression of the union join part and PD at the right
side is an algebra expression (over database D) with the following property: for each
mapping µ ∈PD there is at least one ?Vi (2 ≤i ≤n) s.t. ?Vi ̸∈dom(µ). We observe
that, in contrast, for each mapping µ that is generated by the join part, we have
that dom(µ) ⊇{?V2, . . . , ?Vn} and, even more, µ(?Vi) = 1, for 2 ≤i ≤n. Hence, for
all these mappings it holds that µ(?V2) = µ(?V3) = · · · = µ(?Vn) = 1.
To clarify the previous claims by example, let us consider the case n = 3 (the
argumentation for this case naturally generalizes to larger n). We then have
J(Q1 Opt QV2
2 ) Opt QV3
3 KD
= (JQ1KD
1 JQV2
2 KD)
1 JQV3
3 KD
= ((JQ1KD 1 JQV2
2 KD) ∪(JQ1KD \ JQV2
2 KD))
1 JQV3
3 KD
(∗1)
= ((JQ1KD 1 JQV2
2 KD)
1 JQV3
3 KD) ∪((JQ1KD \ JQV2
2 KD)
1 JQV3
3 KD)
= (((JQ1KD 1 JQV2
2 KD) 1 JQV3
3 KD) ∪((JQ1KD 1 JQV2
2 KD) \ JQV3
3 KD)) ∪
(((JQ1KD \ JQV2
2 KD) 1 JQV3
3 KD) ∪((JQ1KD \ JQV2
2 KD) \ JQV3
3 KD))
(∗2)
= (JQ1KD 1 JQV2
2 KD 1 JQV3
3 KD) ∪((JQ1KD 1 JQV2
2 KD) \ JQV3
3 KD) ∪
((JQ1KD \ JQV2
2 KD) 1 JQV3
3 KD) ∪((JQ1KD \ JQV2
2 KD) \ JQV3
3 KD)
= JQ1 And QV2
2 And QV3
3 KD ∪(((JQ1KD 1 JQV2
2 KD) \ JQV3
3 KD) ∪
((JQ1KD \ JQV2
2 KD) 1 JQV3
3 KD) ∪((JQ1KD \ JQV2
2 KD) \ JQV3
3 KD))
= JQ1 And QV2
2 And QV3
3 KD ∪PD, where
PD:=((JQ1KD 1 JQV2
2 KD) \ JQV3
3 KD) ∪((JQ1KD \ JQV2
2 KD) 1 JQV3
3 KD) ∪
((JQ1KD \ JQV2
2 KD) \ JQV3
3 KD).
Step (∗1) is justiﬁed by the right distributivity of the left outer join over union
and step (∗2) follows from the associativity of union and join (we refer the reader
to the rules from Figure 4.2 in Section 4.2.3 for details). Now observe that each
57

Chapter 3. Complexity of SPARQL Evaluation
mapping generated by PD is generated by (exactly) one of the three subexpressions
P1\ := ((JQ1KD 1 JQV2
2 KD) \ JQV3
3 KD), P\1 := ((JQ1KD \ JQV2
2 KD) 1 JQV3
3 KD), or by
P\\ := ((JQ1KD \ JQV2
2 KD) \ JQV3
3 KD). It is easy to see that in mappings generated
by P1\ variable ?V3 is not bound, in mappings generated by P\1 variable ?V2 is not
bound, and in mappings generated by P\\ neither ?V2 nor ?V3 is bound. Hence, PD
cannot generate a mapping in which both ?V2 and ?V3 are bound. In contrast, both
variable ?V2 and ?V3 are bound in the join part JQ1 And QV2
2 And QV3
3 KD.
Let us now go one step further and consider the larger right side subexpression
P ′ := ((. . . ((Q′′ Opt V 2) Opt V 3) Opt . . . ) Opt V n).
It is easily veriﬁed that, when evaluating expression P ′, we obtain exactly the
mappings from JQ′′KD, but each mapping µ ∈JQ′′KD is extended by ?Vi 7→0 for all
variables ?Vi ̸∈dom(µ) with 2 ≤i ≤n. As argued before, all mappings in the join
part of Q′′ are complete in the sense that all ?Vi are bound to 1, so these mappings
are not modiﬁed. The remaining mappings (i.e. those originating from PD) will be
extended by bindings ?Vi 7→0 for at least one ?Vi. The resulting situation can be
summarized as follows: ﬁrst, for each µ ∈JP ′KD we have dom(µ) ⊇{?V2, . . . , ?Vn};
second, for those µ ∈JP ′KD that evolve from the join part of JQ′′KD we have that
µ(?V2) = · · · = µ(?Vn) = 1; third, for those µ ∈JP ′KD that evolve from the subex-
pression PD (i.e., not from the join part) there is i ∈{2, . . . , n} such that µ(?Vi) = 0.
Going one step further, we ﬁnally consider the whole right side expression, namely
JQ′ Opt P ′KD. From claim (1) of the lemma we know that each mapping in JQ′KD
maps all ?Vi to 1. Hence, when computing JQ′ Opt P ′KD = JQ′KD
1 JP ′KD, the
bindings ?Vi 7→1 for all i ∈{2, . . . , n} in every µ ∈JQ′KD assert that the mappings
in JQ′KD are pairwise incompatible with those mapping from JP ′KD that bind one
or more ?Vi to 0. As discussed before, the condition that at least one ?Vi maps to 0
holds for exactly those mappings that originate from PD, so all mappings originating
from PD do not contribute to the result of JQ′ Opt P ′KD. Hence, it holds that
JQ′ Opt P ′KD = JQ′KD
1 JP ′KD
= JQ′KD
1 JQ1 And QV2
2 And QV3
3 And . . . And QVn
n KD
= JQ′ Opt (Q1 And QV2
2 And QV3
3 And . . . And QVn
n )KD.
Even more, we know from claim (1) of the lemma that all ?Vi are bound to 1
for each µ ∈JQ′KD. It follows that we can replace QVi
i
:= Qi Opt Vi by Qi in P ′,
without changing the semantics of expression JQ′ Opt P ′KD:
JQ′ Opt P ′KD = JQ′ Opt (Q1 And QV2
2 And QV3
3 And . . . And QVn
n )KD
= JQ′ Opt (Q1 And Q2 And Q3 And . . . And Qn)KD
The ﬁnal step in our transformation corresponds exactly to the left side expression
of the original claim (2), which completes the proof.2
58

3.2. Complexity of SPARQL
Proof of Theorem 3.4
Having established Lemma 3.2 we are now in the position to prove PSpace-
completeness for fragment O. As before in the proof of Theorem 3.3, it suﬃces
to show hardness. Following the idea discussed before, we show that each And
expression in the proof of Theorem 3.3 can be replaced by a construction using only
Opt expressions. Let us again start with a quantiﬁed boolean formula
ϕ := ∀x1∃y1∀x2∃y2 . . . ∀xm∃ymψ,
where ψ is a quantiﬁer-free formula in conjunctive normal form, i.e. ψ is a con-
junction of clauses ψ := C1 ∧· · · ∧Cn where the Ci (i ∈[n]), are disjunctions of
literals. As before, by Vψ we denote the set of variables inside ψ, by VCi the variables
in clause Ci (either in positive of negative form), and we deﬁne the database
D := {(a, tv, 0), (a, tv, 1), (a, false, 0), (a, true, 1)} ∪
{(a, vari, v) | v ∈VCi} ∪{(a, v, v) | v ∈Vψ}.
The ﬁrst modiﬁcation of the proof for class AO concerns the encoding of clauses
Ci := v1∨· · ·∨vj∨¬vj+1∨· · ·∨¬vk. In the prior encoding we used both And and Opt
operators to encode such clauses. It is easy to see that we can simply replace each
And operator there by Opt without changing semantics. The reason is that, for all
subexpressions P1 Opt P2 in the encoding of PCi, we have vars(P1) ∩vars(P2) = ∅
and JP2KD ̸= ∅. More precisely, each And expression in the encoding PCi is of
the form (a, vj, ?vari) And (a, false, ?Vj) (or (a, vj, ?vari) And (a, true, ?Vj)), so the
right side pattern generates one result mapping {?Vj 7→0} (or {?Vj 7→1}), which is
compatible with the single mapping {?vari 7→vj} obtained when evaluating the left
pattern. Clearly, in this case the left join is identical to the join. When replacing all
And operators by Opt, we obtain the Opt-only encoding P Opt
Ci
for clauses Ci:
P Opt
Ci
:= (. . . ((. . . ((a, vari, ?vari)
Opt ((a, v1, ?vari) Opt (a, true, ?V1)))
. . .
Opt ((a, vj, ?vari) Opt (a, true, ?Vj)))
Opt ((a, vj+1, ?vari) Opt (a, false, ?Vj+1)))
. . .
Opt ((a, vk, ?vari) Opt (a, false, ?Vk))).
This encoding gives us a preliminary encoding P ′
ψ for formula ψ (as a replacement
for Pψ from the proof for Theorem 3.3), deﬁned as P ′
ψ := P Opt
C1
And . . . And P Opt
Cn ;
we will tackle the replacement of the remaining And expressions in P ′
ψ later. Let
us next consider the Pi and Qi used for simulating the quantiﬁer alternation. With
a similar argumentation as before, we can replace each occurrence of operator And
by Opt without changing the semantics. This modiﬁcation results in the equivalent
Opt-only encodings P Opt
i
(for Pi) and QOpt
i
(for Qi), i ∈[m], deﬁned as
59

Chapter 3. Complexity of SPARQL Evaluation
P Opt
i
:= ((a, tv, ?X1) Opt . . . Opt (a, tv, ?Xi) Opt
(a, tv, ?Y1) Opt . . . Opt (a, tv, ?Yi−1) Opt
(a, false, ?Ai−1) Opt (a, true, ?Ai)),
QOpt
i
:= ((a, tv, ?X1) Opt . . . Opt (a, tv, ?Xi) Opt
(a, tv, ?Y1) Opt . . . Opt (a, tv, ?Yi) Opt
(a, false, ?Bi−1) Opt (a, true, ?Bi)).
Let us shortly summarize what we have achieved so far. Given the modiﬁcations
presented before, our preliminary encoding P ′
ϕ for ϕ is
P ′
ϕ := (a, true, ?B0) Opt (P Opt
1
Opt (QOpt
1
. . .
Opt (P Opt
m−1 Opt (QOpt
m−1
Opt P∗)) . . . )), where
P∗:= P Opt
m
Opt (QOpt
m
And P ′
ψ)
= P Opt
m
Opt (QOpt
m
And P Opt
C1
And . . . And P Opt
Cn ).
Expression P∗is the only subexpression of P ′
ϕ that still contains And opera-
tors (where QOpt
m , P Opt
C1 , . . . , P Opt
Cn
are Opt-only expressions). We now exploit the
rewriting from Lemma 3.2(2) and replace P∗by the O expression P Opt
∗
deﬁned as
P Opt
∗
:= Q′ Opt ((. . . ((Q′′ Opt V 2) Opt V 3) Opt . . . ) Opt V n+1)), where
Q′ := ((. . . ((P Opt
m
Opt V2) Opt V3) . . . ) Opt Vn+1),
Q′′ := ((. . . ((QOpt
m
Opt (P Opt
C1
Opt V2))
Opt (P Opt
C2
Opt V3))
. . .
Opt (P Opt
Cn
Opt Vn+1))),
Vi := (a, true, ?Vi), V i := (a, false, ?Vi),
and the ?Vi (i ∈{2, . . . , n + 1}) are fresh variables.
Let P Opt
ϕ
denote the expression obtained from P ′
ϕ by replacing the subexpression
P∗by P Opt
∗
. First observe that P Opt
ϕ
is an O expression. From Lemma 3.2(2) it
follows that JP Opt
∗
KD equals to JQ′ Opt (QOpt
m
And P Opt
Ci
. . . And P Opt
Cn )KD, where
the evaluation result JQ′KD is obtained from JP Opt
m
KD by extending each µ ∈JP Opt
m
KD
with bindings ?V2 7→1, . . . , ?Vn+1 7→1, according to Lemma 3.2(1). Consequently,
the result obtained when evaluating P Opt
∗
is identical to JP∗KD except for the addi-
tional bindings for (the fresh) variables ?V2, . . . , ?Vn+1. It is straightforward to verify
that these bindings do not harm the overall construction, i.e. it is straightforward
to show that {?B0 7→1} ∈JP Opt
ϕ
KD iﬀϕ is valid.2
We conclude this subsection with a corollary that follows from Theorems 3.1
and 3.4 and makes the complexity study of the expression fragments complete:
Corollary 3.2 The Evaluation problem for every expression fragment involving
operator Opt is PSpace-complete.
2
60

3.2. Complexity of SPARQL
3.2.4. The Source of Complexity
Given the high complexity of operator Opt, an interesting question is whether we
can ﬁnd natural syntactic conditions that lower the complexity of fragments that
involve this operator. A closer investigation reveals that the proofs of Theorems 3.3
and 3.4 both rely on a nesting of Opt expression, which increases with the num-
ber of quantiﬁer alternations in the quantiﬁed boolean formula that is used in the
reduction of QBF to the SPARQL Evaluation problem. It turns out that, when
restricting the nesting depth of Opt expressions, better complexity bounds in the
polynomial hierarchy can be derived (assuming that the complexity classes in the
polynomial hierarchy are strictly contained in PSpace, a widely accepted conjecture
in complexity theory). We start with the notion of Opt-rank, which is a measure for
the nesting depth of Opt expressions and will be used to formalize our restriction:
Deﬁnition 3.2 (Opt-rank) The nesting depth of Opt expressions in expression Q,
called Opt-rank rank(Q), is deﬁned inductively on the structure of Q as
rank(t)
:= 0
rank(Q1 Filter R) := rank(Q1)
rank(Q1 And Q2)
:= max(rank(Q1),rank(Q2))
rank(Q1 Union Q2) := max(rank(Q1),rank(Q2))
rank(Q1 Opt Q2)
:= max(rank(Q1),rank(Q2)) + 1,
where function max(n1,n2) returns the maximum of n1 and n2.
2
Let F be an expression fragment. By F≤n we denote the class of expressions
Q ∈F with rank(Q) ≤n. When ﬁxing the Opt-rank of E expressions, the SPARQL
Evaluation problem falls into some ﬁxed class of the polynomial hierarchy:
Theorem 3.5 For every n ∈N0, the Evaluation problem is ΣP
n+1-complete for
the SPARQL fragment E≤n.
2
Observe that, according to the theorem, Evaluation for class E≤0 is complete
for ΣP
1 =NP, which is identical to the result for Opt-free expressions (i.e., class AFU)
stated in Theorem 3.1. With increasing nesting-depth of Opt expressions we climb
up the polynomial hierarchy. This is reminiscent of the QBF problem for quantiﬁed
boolean formulas with restricted quantiﬁer alternation, where the number of quan-
tiﬁer alternations ﬁxes the complexity class in the polynomial hierarchy. In fact, the
hardness part of the proof (see Appendix B.1) makes these similarities explicit.
61

Chapter 3. Complexity of SPARQL Evaluation
3.2.5. From Expressions to Queries
We ﬁnally turn towards a discussion of SPARQL queries, i.e. fragments involving
top-level projection in the form of a Select operator (see Deﬁnition 2.5). We extend
the notation for classes as follows. Let F be an expression fragment. We denote by
F π the class of queries of the form SelectS(Q), where S ⊂V is a ﬁnite set of
variables and Q ∈F is an expression. The next lemma shows that we obtain (top-
level) projection for free in all fragments that are at least NP-complete.
Lemma 3.3 Let C be a complexity class and F a class of expressions. If Evalua-
tion is C-complete for F and C ⊇NP then Evaluation is C-complete for F π.2
Proof of Lemma 3.3
Let F be a fragment for which the Evaluation problem is C-complete, where C
is a complexity class such that C ⊇NP. We argue that, for a query Q ∈F π,
document D, and mapping µ, testing if µ ∈JQKD is contained in C (C-hardness
follows trivially from C-completeness of fragment F). By deﬁnition, each query
in F π is of the form Q := SelectS(Q′), where S ⊂V is a ﬁnite set of variables
and Q′ ∈F. According to the semantics of Select, we have that µ ∈JQKD iﬀthere
is a mapping µ′ ⊇µ in JQ′KD such that πS({µ′}) = {µ}. We observe that the domain
of candidate mappings µ′ is bounded by the set of variables in Q′ and dom(D).
Hence, we can ﬁrst guess a mapping µ′ ⊇µ (recall that we are at least in NP) and
subsequently check if πS({µ′}) = {µ} (in polynomial time) and µ′ ∈JQ′KD (using a
C-algorithm, by assumption). Clearly, this algorithm falls into class C.2
We naturally extend the Opt-rank from Deﬁnition 3.2 from SPARQL expressions
to queries and deﬁne rank(SelectS(Q)) := rank(Q). Combining the observation in
Lemma 3.3 with previous ﬁndings we obtain several new complexity results:
Corollary 3.3 The SPARQL Evaluation problem is
1. PSpace-complete for all query fragments involving operator Opt,
2. ΣP
n+1-complete for the SPARQL fragment Eπ
≤n (for n ∈N0), and
3. NP-complete for fragment AUπ.
2
Proof of Corollary 3.3
Corollary 3.3(1): Follows immediately from Lemma 3.3 and Corollary 3.2.
Corollary 3.3(2): Follows immediately from Lemma 3.3 and Theorem 3.5.
Corollary 3.3(3): Follows immediately from Lemma 3.3 and Theorem 3.2(2).2
It is still an open question whether top-level projection increases the complexity of
expression fragments that are in PTime. The following theorem clariﬁes this issue.
62

3.2. Complexity of SPARQL
Theorem 3.6 The Evaluation problem is
1. in PTime for classes FUπ, Fπ, Uπ, and
2. NP-complete for classes Aπ and AFπ.
2
Proof of Theorem 3.6
Theorem 3.6(1): We prove membership in PTime for fragment FUπ, which di-
rectly implies PTime-membership for Fπ and Uπ. Let D be an RDF database, µ
be a mapping, and Q := SelectS(Q′) be an FUπ expression. We show that there
is a PTime-algorithm that checks if µ ∈JQKD. Let t1, . . . , tn be all triple patterns
occurring in Q. Our strategy is as follows: we process triple pattern by triple pat-
tern and check for each µ′ ∈JtiKD if the following two conditions hold: (1) all ﬁlter
conditions that are deﬁned on top of ti in Q′ satisfy µ′ and (2) πS({µ′}) = {µ}. We
return true if there is a mapping that satisﬁes both conditions, false otherwise.
The idea behind this algorithm is that condition (1) implies that µ′ ∈JQ′KD, while
condition (2) asserts that the top-level projection generates mapping µ from µ′. It is
straightforward to show that µ ∈JQKD if and only if there is some i ∈[n] such that
JtiKD contains a mapping µ′ that satisﬁes both conditions, and clearly our algorithm
(which checks all candidates) would ﬁnd such a mapping, if it exists. The number of
triple patterns is linear to the size of the query and the number of mappings in each
JtiKD is linear to the size of D (where each mapping is of bounded size); further,
conditions (1) and (2) can be checked in PTime, so the algorithm is in PTime.
Theorem 3.6(2): First, we show that Evaluation for AFπ-queries is contained
in NP (membership for Aπ queries then follows). By deﬁnition, each query in AF π
is of the form Q := SelectS(Q′), where S ⊂V is a ﬁnite set of variables and Q′ is
an AF expression. We ﬁx a document D and a mapping µ. To prove membership,
we follow the approach taken in the proof of Lemma 3.3 and eliminate the Select-
clause. More precisely, we guess a mapping µ′ ⊇µ s.t. πS({µ′}) = µ and check
if µ′ ∈JQ′KD (see the proof of Lemma 3.3 for more details). The size of the mapping
to be guessed is bounded, and it is easy to see that the resulting algorithm is in NP.
To prove NP-hardness for Aπ and AFπ we reduce 3Sat, a prototypical NP-
complete problem, to the Evaluation problem for class Aπ. The subsequent proof
was inspired by the reduction of 3Sat to the evaluation problem for conjunctive
queries in [BEE+07]. It nicely illustrates the relation between And-only queries and
conjunctive queries. We start with a formal deﬁnition of the 3Sat problem.
3SAT: given a boolean formula ψ := C1 ∧· · · ∧Cn in conjunctive normal form
as input, where each clause Ci is a disjunction of exactly three literals:
is the formula ψ satisﬁable?
Let ψ := C1 ∧· · · ∧Cn be a boolean formula in CNF, where each Ci is of the form
Ci := li1 ∨li2 ∨li3 and the lij are literals. For our encoding we use the ﬁxed database
63

Chapter 3. Complexity of SPARQL Evaluation
D := {(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0),
(1, 0, 1), (1, 1, 0), (1, 1, 1), (0, c, 1), (1, c, 0)},
where we assume that 0 and 1 are URIs. Further let Vψ = {x1, . . . xm} denote the
set of variables occurring in formula ψ. We deﬁne the And-only expression
P ′ := (L∗
11, L∗
12, L∗
13) And . . . And (L∗
n1, L∗
n2, L∗
n3)
And (?X1, c, ?X1) And . . . And (?Xm, c, ?Xm)
And (0, c, ?A),
where L∗
ij :=?Xk if lij = xk, and L∗
ij :=?Xk if lij = ¬xk.
Finally, deﬁne P := Select?A(P ′). It is easily veriﬁed that JP ′KD computes all
satisfying truth assignments for ψ, extended by an additional binding ?A 7→1.
Consequently, formula ψ is satisﬁable if and only if µ := {?A 7→1} ∈JPKD.2
3.2.6. Summary of Results
We summarize (and slightly extend) the results established throughout Sections 3.2.2
to 3.2.5 in Figure 3.1. We point out that all fragments that fall into the classes NP,
ΣP
i , and PSpace also are complete for the respective complexity class.
First, according to Corollary 3.2, all expressions including operator Opt fall into
PSpace if the nesting depth is not explicitly ﬁxed. In addition, as stated in Corol-
lary 3.3(1), also the corresponding query classes are PSpace-complete.
When ﬁxing the nesting depth of classes containing operator Opt, we obtain
fragments that fall into the polynomial hierarchy. As indicated in the survey, each
class ΣP
n+1 contains the fragment E≤n (cf. Theorem 3.5) and the corresponding query
fragment Eπ
≤n (cf. Corollary 3.3(2)). Note that class ΣP
1 equals to NP and that
fragment AFOU≤0 equals to AFU, which is contained in ΣP
1 = NP.
In addition to the fragments E≤n and Eπ
≤n, each ΣP
n+1 also contains the fragment
AFO≤n and the respective query fragment AFOπ
≤n. These results were not explicitly
stated before, but follow from the proof of Theorem 3.5 in Appendix B.1, which does
not use the Union operator for the encoding of the given quantiﬁed boolean formula.
Going downwards in the complexity hierarchy, we next identify several fragments
that are NP-complete. The survey indicates that there are exactly two constellations
that are responsible for NP-hardness, namely either the combination of And and
Union or the combination of And with projection. The associated results were
established in Theorems 3.1(2), 3.2(2), 3.6(2), and Corollary 3.3(3).
Finally, Figure 3.1 lists all fragments that have been shown to be in PTime.
The latter class contains the single-operator expression fragments A, F, U and
the classes obtained when combining either operator And or operator Union with
Filter expressions (cf. Theorems 3.1(1) and 3.2(1)). In addition, the lightweight
query classes Fπ, Uπ, and FUπ fall into PTime, according to Theorem 3.6(1).
64

3.3. Related Work
O, AO, FO, . . . , AFOU,
Oπ, AOπ, FOπ, . . . , AFOUπ
ΣP
3
AFO≤2, AFOπ
≤2, E≤2, Eπ
≤2
ΣP
2
AFO≤1, AFOπ
≤1, E≤1, Eπ
≤1
AU,
AFU,
Aπ,
AFπ,
AUπ,
AFUπ
A, F, U, AF, FU,
Fπ, Uπ, FUπ
Figure 3.1.: Summary of complexity results.
3.3. Related Work
Complexity of Query Languages. The study of complexity, expressiveness, and
relationships between database query languages has been an important topic ever
since the invention of the relational model by Codd [Cod69; Cod70]. Few years
after its proposal, Codd himself introduced a predicate calculus called relational
calculus [Cod72], which can be understood as a dialect of ﬁrst-order logic5 over
the relational data model. Following Codd’s argumentation, the relational calculus
should ﬁx a lower bound for the expressive power of database query languages,
thus serving as a yardstick when designing new query languages for the relational
model. He coined the notion of relational completeness to identify query languages
that are at least as expressive as the relational calculus and showed that relational
algebra (RA), which was introduced before in [Cod70], is relationally complete, a
5We introduce ﬁrst-order logic in Chapter 5 and will therefore not go into more detail here.
65

Chapter 3. Complexity of SPARQL Evaluation
result that is widely known as Codd’s Theorem in the literature. Due to the close
connection between the relational calculus and ﬁrst-order logic, it essentially shows
that relational algebra has the same expressiveness as ﬁrst-order logic.
In the years after these early results, lots of research eﬀort was spent in investi-
gating the complexity and expressiveness of relational algebra, its fragments, and
possible extensions. A simple yet expressive fragment of relational algebra are so-
called conjunctive queries, a subclass of RA comprising the (relational) selection,
projection, and join operator. In [CM77] it was shown that query evaluation is NP-
complete for conjunctive queries and a near-optimal algorithm for implementing
such queries was developed. One property that makes conjunctive queries partic-
ularly attractive is that the containment problem, and hence also the equivalence
problem, is decidable (in fact, it can easily be reduced to the problem of conjunctive
query answering). Going one step further, [JK82] studied the problem of conjunctive
query containment under data dependencies and showed that this problem is still
decidable for large, practically relevant classes of dependencies. These results are
of particular interest in this thesis: as a practical application, we will later show in
Chapter 5 that the minimization and optimization problem for SPARQL And-only
queries under data dependencies can be solved by falling back on the containment
and optimization problem for conjunctive queries under data dependencies.
In 1979, Aho and Ullman revisited the notion of relational completeness and
highlighted important classes of queries that cannot be expressed in relational al-
gebra [AU79]. Among others, they formally proved that relational algebra is not
expressive enough to compute the transitive closure of binary relations. For such
queries, more expressive query languages are required, which go beyond ﬁrst-order
logic and have their logical foundations in higher-order logics. Investigating the rela-
tions between relational algebra, fragments, and extensions, a classiﬁcation of query
languages using the so-called ﬁxpoint query hierarchy was worked out in [CH80].
Another paper that inﬂuenced subsequent studies of query language complexity
was [Var82]. The work proposes two novel complexity measures for query languages:
data complexity considers the complexity of a query language as a function of the
size of the database, whereas expression complexity denotes the study of the evalu-
ation complexity as a function of the size of the query. These measures complement
the study of combined complexity used in previous investigations, in which neither
database nor query are ﬁxed. As an important result, a comparative study of ex-
isting query languages reveals that the expression complexity of query languages is
typically one exponential higher than their associated data complexity.
In addition to relational algebra, other query languages and data models have
been proposed and theoretically investigated. One important line of research that
goes into this direction is the study of datalog [MW88], a rule-based query language
for deductive databases, which syntactically and semantically resembles the Prolog
programming language [SS86]. To date, a variety of results have been established, re-
lating datalog and possible extensions (e.g., [EGM97]) or fragments of the language
66

3.3. Related Work
to relational algebra, both in terms of expressiveness and complexity. A detailed
discussion of all these results is beyond the scope of this summary. We refer the
interested reader to [AHV] for a survey of important results and to [BEE+07] for
a comprehensive introduction to rule-based query languages. Another notable line
of related research is the theoretical investigation of query languages for the tree-
structured XML data format, such as XQuery and XPath, e.g. in [GKP03; GKPS05].
Complexity of RDF Query Languages. Early complexity results on query
answering on RDF(S) databases have been presented in [GHM04]. Amongst oth-
ers, the authors investigate a datalog-style, rule-based query language for RDF(S)
graphs. In contrast to our investigation of SPARQL complexity, the focus of the lat-
ter work is on query containment and answering on the implied database. While the
query language is rather simple compared to SPARQL, additional challenges arise
due to the fact that query answering must take RDF(S) inferencing into account.
The ﬁrst complexity analysis for the SPARQL query language has been presented
in [PAG06a]. Beyond the results on combined complexity that we summarized in
Theorem 3.1, the authors show that the data complexity version of the Evaluation
problem is LogSpace-complete. The reﬁned journal version [PAG09] presents some
more interesting results. One ﬁnding is that so-called well-designed graph patterns,
a practically relevant class of SPARQL expressions deﬁned by a syntactic restric-
tion, are coNP-complete (although such patterns might contain nested Opt expres-
sions). As a further result in [PAG09], the authors prove that the SPARQL Eval-
uation problem is PSpace-complete for class AFO, which is subsumed by our
result that already fragment O is PSpace-complete.6 Related to [PAG06a; PAG09]
is also the complexity study for a navigational, path-based extension of SPARQL,
called nSPARQL, presented in [PAG08] (cf. the discussion in Section 2.4).
In addition to the investigation of SPARQL complexity, there has been work con-
cerning the expressiveness of the language. Early mapping schemes for SPARQL
into relational algebra and SQL [Cyg05; CLJF06] indicate a close connection be-
tween SPARQL and relational algebra in terms of expressiveness. In [Pol07] then, a
translation of SPARQL queries into a datalog fragment that is known to be equally
expressive as relational algebra was presented. This translation makes the close con-
nection between SPARQL and rule-based languages explicit and shows that RA is
at least as expressive as SPARQL. Tackling the opposite direction, it was recently
shown in [AG08a] that SPARQL is relationally complete, by providing a translation
of the above-mentioned datalog fragment into SPARQL. As argued in [AG08a], the
results from [Pol07] and [AG08a] taken together imply that SPARQL has exactly
the same expressive power as relational algebra.
6Note that we already published this result informally several months earlier in [SML08].
67

Chapter 3. Complexity of SPARQL Evaluation
3.4. Conclusion
In this chapter we presented a comprehensive complexity analysis for SPARQL and
fragments of the language. While all results have been proven in the context of the
set semantics introduced in Deﬁnition 2.11, we showed that – from a complexity
point of view – the bag semantics from Deﬁnition 2.15 does not diﬀer, so all results
immediately carry over to the oﬃcial W3C SPARQL Recommendation.
As a key result, our investigation of SPARQL complexity reveals that operator
Opt alone is responsible for the high complexity (i.e., PSpace-completeness) of the
query language. The theorem upgrades the claim from [PAG06a] that “the main
source of complexity in SPARQL comes from the combination of Union and Opt
operators”, by showing that Union (and And) are not necessary to obtain PSpace-
hardness. The high complexity is caused by the possibility of an unlimited nesting
of Opt-expressions and arises from the fact that the deﬁnition of the Opt operator
implicitly involves negation: on the one hand, it joins mappings from the left side
expression with compatible mappings in the right side, on the other hand, it also
retains all mappings from the left side mapping set for which no compatible mapping
in the right mapping set exists. When ﬁxing the nesting depth of Opt-expressions,
we implicitly limit the nesting depth of such negation encodings and in that case
better complexity bounds in the polynomial hierarchy can be derived (the classes in
this hierarchy are conjectured to be strictly contained in PSpace).
Beyond this central result, we presented a complete study for all SPARQL expres-
sion and query fragments, in particular showing that, in Opt-free fragments, it is
the combination of operators And and Union or And and Select that makes the
Evaluation problem for SPARQL hard (in this case, NP-complete). Such frag-
ments are quite related to conjunctive queries, whose evaluation complexity falls
into the same class. Later, in Chapter 5, we will use a translation of SPARQL And
queries to conjunctive queries, which allows us to apply algorithms and results that
have been developed for this important and well-studied query class.
Ultimately, the complexity results that have been developed in this chapter char-
acterize the operators, their interrelation, and give hints on the expressive power
of the query language. The high complexity of Opt, for instance, suggests that in
query optimization special care should be taken in optimizing Opt expressions. We
will address these needs in the subsequent study of SPARQL query optimization.
68

Chapter 4.
Algebraic SPARQL Query
Optimization
Roy:
“What about optimization of SPARQL?”
Jen:
“I would be interested in its relation to relational algebra,
maybe we can transfer established optimization strategies.”
Moss: “Sounds good, but we should not forget to address the
particularities of RDF and SPARQL.”
Roy:
“Great, let’s see what we can do there!”
Query optimization has been a central topic in database research from the be-
ginning and up to the present a variety of query optimization techniques for the
relational context have been proposed, including (but not limited to) algebraic
rewriting (e.g. [Hal75; SC75; Tod75]), statistics and cost-based query optimization
(e.g. [ABC+76; MD88]), indices for fast data access (e.g. [BM72]), and semantic
query optimization (e.g. [Kin81]). Standard optimization techniques like indexing
and cost-based rewriting have found entrance in virtually every major relational
database system and it is well-known that such techniques may reduce query eval-
uation time by orders of magnitude, ultimately making database systems utilizable
in practical scenarios (see e.g. the experiments presented in [Hal75]).
One fundamental prerequisite to query optimization in the relational context is
the study of equivalences over relational algebra (RA) expressions [Hal75; SC75;
Tod75; GLR97]. When interpreted as rewriting rules and enhanced by a cost es-
timation function (or, alternatively, adequate heuristics), these equivalences allow
to transform RA expressions into structurally diﬀerent, but equivalent expressions
that can be evaluated more eﬃciently than the original expressions. Established op-
timization techniques like ﬁlter pushing or join reordering, for instance, lastly rely
on the knowledge of equivalences over RA expressions (cf. [Hal75; SC75; Tod75]).
Given the central role of query optimization in the relational context, it is natural
to assume that the development of eﬃcient evaluation approaches for SPARQL
queries is an important step towards the realization of the Semantic Web, where
engines must be able to deal eﬃciently with RDF repositories containing millions
or even billions of RDF triples (see e.g. [BC07; Tau; lin] for existing large-scale
69

Chapter 4. Algebraic SPARQL Query Optimization
RDF databases). The complexity results for SPARQL evaluation established in the
previous chapter indicate that the eﬃcient processing of SPARQL queries is a non-
trivial task and – given that years of research have been spent in the investigation
of SQL and relational algebra optimization – seems to be an ambitious goal.
Addressing the issue of SPARQL optimization, over the last years various pro-
posals for the eﬃcient evaluation of SPARQL have been made. These approaches
comprise a wide range of optimization techniques, including normal forms [PAG06a],
graph pattern reordering based on selectivity estimations [NW08; SSB+08; NW09]
(similar to relational join reordering), query graph models for SPARQL [HH07],
RISC-style query processing [NW08], and semantic SPARQL optimization [SKCT05;
LMS08]. In addition, there has been a corpus of research on specialized indices [HD05;
GGL07; FB08] and storage schemes [ACKP01; BKvH02; HG03; TCK05; AMMH07;
WKB08] for RDF, with the aim to provide eﬃcient data access paths. Another no-
table line of research is the translation of SPARQL queries and RDF data into estab-
lished data models like SQL [Cyg05; CDES05; CLJF06] or datalog [Pol07; AG08a],
thus facilitating SPARQL evaluation with traditional engines, to exploit optimiza-
tion techniques implemented in existing SQL (respectively, datalog) systems.
One interesting observation is that the majority of the “native” optimization
proposals for SPARQL (i.e. those that do not rely on a mapping into the relational
context or datalog) have a strong focus on optimizing SPARQL And-only queries
and mostly disregard the optimization of queries involving operators like Union,
Filter, or Opt (cf. [HD05; GGL07; FB08; NW08; SSB+08; WKB08; NW09]). The
eﬃcient evaluation of And-only queries (or And-connected blocks inside queries) is
undoubtedly an important task in SPARQL evaluation, so the latter approaches form
valuable groundwork for SPARQL optimizers. Still, a comprehensive optimization
framework should also address the optimization of more involved SPARQL queries.
To give evidence for this claim, the experimental studies in [SHK+08; SHLP09] reveal
severe performance bottlenecks when evaluating more complex SPARQL queries, in
particular queries involving the complex Opt operator, for both existing SPARQL
engines and state-of-the-art mapping schemes from SPARQL into SQL.
We argue that – like in relational algebra, where the study of algebraic rewriting
rules has facilitated the development of diverse optimization techniques – a study
of SPARQL algebra (SA) rewritings will improve the understanding of the algebraic
operators and alleviate the development of comprehensive optimization approaches
for SPARQL. To date, however, surprisingly few fundamental work has been done in
the context of SPARQL algebra (we will resume some initial results from [PAG06a;
AG08a] in the course of this chapter). Based on all these considerations, we believe
that a schematic investigation of SPARQL algebra is long overdue. Therefore, in this
chapter we present an elaborate study of SA equivalences, covering all its operators
and their interrelations. When interpreted as rewriting rules, these equivalences form
the theoretical foundations for transferring established RA optimization techniques,
such as projection and ﬁlter pushing, into the context of SPARQL optimization.
70

Going beyond the adaption of existing techniques, we also tackle SPARQL-speciﬁc
issues, such as the simpliﬁcation of expressions involving negation, which – when
translating SPARQL expressions or queries into SA according to the semantics –
manifests into a characteristic combination of operators
1 and σ. Ultimately, our
results improve the understanding of SPARQL algebra and lay the foundations for
the development of advanced and comprehensive SPARQL optimization approaches.
Akin to the operators deﬁned in RA, SPARQL algebra comprises operations such
as join, union, left outer join, minus, projection, and selection, so at ﬁrst glance there
are many parallels between SA and RA. In fact, the study in [AG08a] reveals that
SA and RA have exactly the same expressive power. In spite of all these similarities,
existing mappings of SPARQL into relational algebra [Cyg05] or SQL [CLJF06] indi-
cate that a semantics-preserving translation of SPARQL into the relational context
is far from being trivial. This shows that, although both algebras provide similar
operators, there are still fundamental diﬀerences between both. One of the most
striking discrepancies, as also argued in [PAG06a], is that joins in RA are rejecting
over null values, but in SA, where the schema is loose in the sense that mappings
may bind an arbitrary set of variables, joins over unbound variables (essentially the
counterpart of RA null values) are accepting. Let us exemplify the issue of unbound
variables in SPARQL mappings by means of the candidate equivalence
σ?x=c(A 1 B)
?≡σ?x=c(A) 1 B,
(4.1)
where A and B are SPARQL algebra expressions. Whether or not this equivalence
holds, depends on the variables that appear in expressions A and B. Let us discuss
the most general case and assume that variable ?x appears in both A and B. If we
could guarantee that variable ?x is bound in every result mapping obtained when
evaluating expression A, then it would follow that the equivalence holds: informally
speaking, in the latter case the join of A with B does not modify bindings for
variable ?x, so it does not matter whether the selection is applied early, i.e. on
top of A, or late, on top of A 1 B. The problem, however, is that the appearance
of ?x in A does not imply that ?x also is bound in each result mapping obtained
when evaluating A. Consider for example the RDF database D := {(c, c, c)} and
the algebra expressions A := J(c, c, ?y)KD Opt J(d, d, ?x)KD and B := J(c, c, ?x)KD,
where ?x appears in both the left and right side expression. It is easily veriﬁed
that JAKD = {{?y 7→c}} and JBKD = {{?x 7→c}}, so the left side of Equation (4.1)
evaluates to {{?x 7→c, ?y 7→c}}, while the right side evaluates to ∅. Although ?x
appears in A, it is unbound in JAKD, which causes the equivalence to fail.
Addressing the issue of unbound variables in result mappings and their importance
for the study of SPARQL algebra equivalences, we propose the concepts of certain
and possible variables: the certain variables of a SPARQL algebra expression are an
underestimation for the set of variables that are bound in every result mapping
(independently from the RDF input document), while the associated set of possible
71

Chapter 4. Algebraic SPARQL Query Optimization
variables constitutes an overestimation for the variables that might be bound in
result mappings. Taken together, these two concepts capture the rationale behind
unbound variables in SPARQL mappings and allow us to state equivalences in a
compact and precise way. Coming back to the equivalence in Equation (4.1), for
instance, we will show in Section 4.2.5 that the equivalence holds whenever ?x is a
certain variables of expression A or ?x is not a possible variable of expression B.
We ﬁnally want to note that we investigate both the theoretically motivated
SPARQL set algebra from Deﬁnition 2.10 and the SPARQL bag algebra from Def-
inition 2.14, which reﬂects the approach that is proposed by the W3C [spac]. Our
investigations reveal that there are indeed diﬀerences between set and bag seman-
tics w.r.t. the rewriting rules that they exhibit and, in response, we both highlight
equivalences for which the two semantics diﬀer and identify a large fragment of
SPARQL algebra for which both semantics coincide, allowing engines to fall back
on the simpler set semantics for a broad class of SPARQL algebra expressions.
We summarize the major contributions of this chapter as follows.
• We develop the concepts of certain and possible variables, which account for
the issue of unbound variables in SPARQL result mappings and allow us to
state equivalences over SPARQL algebra in a clean and precise way.
• We both summarize existent and develop new equivalences over SPARQL al-
gebra. In total, we discuss about forty rewriting rules for SA (of which almost
three-fourths are new), covering all algebraic operators and their interaction.
• In our study of algebraic rewriting, we systematically address established
rewriting strategies for the relational model, such as ﬁlter and projection
pushing. This allows us to transfer established optimization strategies from
relational algebra into the context of SPARQL query evaluation.
• Beyond the investigation of rewriting rules that have proven useful in the
relational context, we develop equivalences that address SPARQL-speciﬁc op-
timization tasks. For instance, we present a rewriting scheme for queries in-
volving negation, typically encountered in SPARQL algebra expressions as a
characteristic combination of operators
1, σ, and ﬁlter predicate bnd.
• We study interrelations between SPARQL set and bag algebra. Our analysis
reveals that most (but not all) of the equivalences hold under both set and bag
semantics. We also discuss the implications of our results for SPARQL engines
that – following the oﬃcial W3C proposal – implement the bag semantics.
Structure. We start with the concepts of certain and possible variables in Sec-
tion 4.1. The subsequent study of algebraic rewriting divides into the study of equiv-
alences for the SPARQL set algebra from Deﬁnition 2.10 (Section 4.2) and the
investigation of rewriting rules for the SPARQL bag algebra from Deﬁnition 2.14
(Section 4.3). We summarize our results and discuss their implications for engines
that implement the W3C SPARQL semantics in Section 4.4. The chapter ends with
a discussion of related work in Section 4.5 and a short conclusion in Section 4.6.
72

4.1. Possible and Certain Variables
4.1. Possible and Certain Variables
Before starting our investigation of SPARQL algebra, we introduce two functions
to statically classify the variables that appear in some ﬁxed SPARQL algebra ex-
pression A. The ﬁrst one, function pVars(A), gives an upper bound for the so-called
possible variables of A, i.e. provides an overestimation for the set of variables that
might be bound in result mappings. The second one, function cVars(A), estimates
the certain variables of A and ﬁxes a lower bound for variables that are bound in
every result mapping obtained when evaluating A. Note that both functions are
independent from the input document. We ﬁrst illustrate the concepts by example:
Example 4.1 Consider the SPARQL set algebra expression
A := J(a, a, ?u)KD 1 (J(a, ?v, ?w)KD ∪J(a, ?v, ?x)KD).
It is easy to see that, according to the semantics from Deﬁnition 2.10, when
evaluating expression A on some document D, variable ?u will be bound in every
result mapping; the same observation holds for ?v, which is contained in both triple
patterns of the union subexpression. Next, we observe that each result mapping
binds either ?w or ?x. Consequently, ?u and ?v are categorized as both possible and
certain variables of A, while ?w and ?x are possible but not certain variables.
2
The concepts of certain and possible variables account for the speciﬁcs of the
SPARQL query language, in which – caused by the operators union ∪, left outer
join
1, and projection π – variables occurring in the expression may be unbound in
(some) result mappings. As discussed in the introduction of this chapter, they will
take a central role in our subsequent investigation of SPARQL algebra: the applica-
bility of many rewritings ultimately depends on these two estimations, in particular
the rules concerning projection pushing in Section 4.2.4 and ﬁlter manipulation in
Section 4.2.5. We start with the estimation function for possible variables:
Deﬁnition 4.1 (Function pVars)
Let A be a set algebra expression, S ⊂V a
ﬁnite set of variables and R be a ﬁlter condition. We deﬁne function pVars(A), which
extracts so-called possible variables, inductively on the structure of A:
pVars(JtKD)
:= vars(t)
pVars(A1 1 A2) := pVars(A1) ∪pVars(A2)
pVars(A1 ∪A2)
:= pVars(A1) ∪pVars(A2)
pVars(A1 \ A2)
:= pVars(A1)
pVars(A1
1 A2) := pVars(A1) ∪pVars(A2)
pVars(πS(A1))
:= pVars(A1) ∩S
pVars(σR(A1))
:= pVars(A1)
2
73

Chapter 4. Algebraic SPARQL Query Optimization
The following proposition shows that possible variables always constitute a super-
set of the variables that appear in result mappings:
Proposition 4.1 Let A be a SPARQL set algebra expression and let ΩA denote
the mapping set obtained when evaluating expression A on any RDF document D.
Then for all µ ∈ΩA : ?x ∈dom(µ) →?x ∈pVars(A).
2
The proof of the proposition works by induction on the structure of SPARQL
algebra expressions, the application of the deﬁnition of the algebraic operators, and
the deﬁnition of function pVars(A) above. We omit the technical details.
Complementarily to function pVars(A), we propose an underestimation for the
certain variables of an algebra expression, implemented through function cVars(A):
Deﬁnition 4.2 (Function cVars) Let A be a SPARQL set algebra expression,
S ⊂V a set of variables, and R a ﬁlter condition. We deﬁne function cVars(A),
which extracts so-called certain variables, inductively on the structure of A:
cVars(JtKD)
:= vars(t)
cVars(A1 1 A2)
:= cVars(A1) ∪cVars(A2)
cVars(A1 ∪A2)
:= cVars(A1) ∩cVars(A2)
cVars(A1 \ A2)
:= cVars(A1)
cVars(A1
1 A2) := cVars(A1)
cVars(πS(A1))
:= cVars(A1) ∩S
cVars(σR(A1))
:= cVars(A1)
2
The deﬁnition of cVars(A) diﬀers from pVars(A) in two cases. First, for union
expressions only those variables are certain that are certain for both subexpression.
Second, for left outer join expressions, no guarantees for the right side variables can
be made. The key property of certain variables can be formalized as follows.
Proposition 4.2 Let A be a SPARQL set algebra expression and let ΩA denote
the mapping set obtained when evaluating expression A on any RDF document D.
It holds that ?x ∈cVars(A) →∀µ ∈ΩA : ?x ∈dom(µ).
2
Again, the proof of the proposition is straightforward and we omit the details. We
next provide an example that illustrates the deﬁnitions of the two functions:
Example 4.2 Consider the SPARQL set algebra expression
A := π?u,?v,?x,?y(((J(?u, a, ?v)KD 1 J(?u, a, ?w)KD)
1 J(?u, a, ?x)KD) ∪
σ?x=1(J(?u, ?x, ?y)KD)).
It is easily veriﬁed that pVars(A) = {?u, ?v, ?x, ?y} and cVars(A) = {?u}.
2
74

4.2. Optimization Rules for Set Algebra
We conclude with the remark that possible (certain) variables are indeed only
upper (lower) bounds for variables that might be (are always) bound in result
mappings. For instance, we could further improve the deﬁnitions of pVars(A) and
cVars(A) when catching some special cases of ﬁlter expressions. To be concrete,
consider the two algebra expressions B := σbnd(?y)(J(a, b, ?x)KD
1 J(a, c, ?y)KD) and
B¬ := σ¬bnd(?y)(J(a, b, ?x)KD
1 J(a, c, ?y)KD). It is easy to see that ?y is a certain
variable for B in the sense of Proposition 4.2 (i.e. it will be bound in every result
mapping), but we observe that ?y ̸∈cVars(B). Analogously, ?y does never appear in
result mappings when evaluating expression B¬, but ?y ∈pVars(B¬). It is straight-
forward to incorporate such conditions into Deﬁnitions 4.1 and 4.2. However, the
analysis of such special cases does not bring further insights and we decided to ignore
them in the interest of a compact deﬁnition for the two functions.
From Set to Bag Algebra. In Deﬁnitions 4.1 and 4.2 we introduced functions
pVars(A) and cVars(A) over SPARQL set algebra expressions (cf. Deﬁnition 2.10).
The deﬁnitions of the two functions can be easily adapted for bag algebra expressions
(cf. Deﬁnition 2.14), by simply replacing the ﬁrst rule in Deﬁnitions 4.1 and 4.2
by pVars(JtK+
D) := vars(t) and cVars(JtK+
D) := vars(t), respectively. Therefore, we
shall overload the two functions and use them for both SPARQL set algebra and
SPARQL bag algebra expressions in the remainder of this chapter. It is easy to see
that, according to Lemma 3.1, Propositions 4.1 and 4.2 naturally carry over from set
to bag algebra: for each SPARQL bag algebra expressions A+ it holds that (i) the
possible variables of A+ are a superset of the variables that might appear in result
mappings (more precisely, in the mapping set component of the result mapping
multi-set) and (ii) certain variables of A+ are bound in every result mapping.
Example 4.3 Consider the SPARQL bag algebra expression
A+ := π?u,?v,?x,?y(((J(?u, a, ?v)K+
D 1 J(?u, a, ?w)K+
D)
1 J(?u, a, ?x)K+
D) ∪
σ?x=1(J(?u, ?x, ?y)K+
D)).
We can observe that A+ is structurally identical to the set algebra expression A
from Example 4.2, so the possible and certain variables are exactly the same as
for A, namely pVars(A+) = {?u, ?v, ?x, ?y} and cVars(A+) = {?u}.
2
4.2. Optimization Rules for Set Algebra
We start with the discussion of algebraic equivalences for SPARQL set algebra, cov-
ering all the algebraic operators introduced in Deﬁnition 2.10. In query optimization,
such equivalences are typically interpreted as rewriting rules and therefore we shall
use the terms equivalence and (rewriting) rule interchangeably in the following.
75

Chapter 4. Algebraic SPARQL Query Optimization
In the interest of a complete survey, we will include equivalences that have been
stated before in [PAG06a].1 To be concrete, most of the equivalences from Figures 4.1
and 4.2, equivalences (FDecompI), (FDecompII), and (FUPush) from Figure 4.4, as
well as equivalence (MJ) stated in Proposition 4.4 are borrowed from [PAG06a]
and listed for completeness only. Furthermore, ( ^
JIdem) in Figure 4.1, (FJPush) in
Figure 4.4, and (f
LJ) in Lemma 4.4 are generalizations of Lemma (2), Lemma 1(2),
and Lemma 3(3) in [PAG06a], respectively. These generalizations rely on the novel
notion of incompatibility property and the fragment eA (which will be introduced in
Section 4.2.1) and extend the applicability of the original rules. We conclude with
the remark that in total almost three-fourths of the rules presented in this section
are new. The subsequent discussion will focus on these newly-discovered rules.
In our investigation of rewriting rules for the set semantics we study two fragments
of SPARQL set algebra. The ﬁrst fragment of interest is the full class of SPARQL
set algebra expressions. We call this fragment A and deﬁne it as follows.
Deﬁnition 4.3 (Fragment A) The fragment A denotes the full class of SPARQL
set algebra expressions, i.e. expression built using operators ∪, 1, \,
1, π, σ, and
(bracket-enclosed) triple patterns of the form JtKD.
2
We understand fragment A as a set of purely syntactic entities. Yet, according
to the SPARQL set semantics in Deﬁnition 2.11, each expression A ∈A implicitly
deﬁnes a mapping set (given that document D is ﬁxed). Therefore, when document D
is known from the context, we will sometimes refer to the mapping set obtained by
application of the semantics as the result of evaluating A on D and, abusing notation,
shall write µ ∈A for a mapping that is contained in this result.
4.2.1. The Incompatibility Property
In addition to the full fragment of SPARQL set algebra expressions A we introduce a
subfragment of A that has a special property, called incompatibility property. As we
shall see, expressions that satisfy the incompatibility property exhibit some rewriting
rules that do not hold in the general case and therefore will be of particular interest.
Deﬁnition 4.4 (Incompatibility Property for Set Algebra) A SPARQL set
algebra expression A has the incompatibility property if, for every document D and
each two distinct mappings µ1 ̸= µ2 contained in the result of evaluating A on D, it
holds that µ1 ̸∼µ2.
2
1Most equivalences in [PAG06a] were established at the syntactic level, while we study optimiza-
tion at the algebraic level. Still, according to Deﬁnition 2.11 there is essentially a one-to-one
correspondence between abstract syntax operators and algebraic operators.
76

4.2. Optimization Rules for Set Algebra
We next deﬁne a large fragment eA ⊂A of SPARQL set algebra, which comprises
only expressions that satisfy the incompatibility property. The fragment is deﬁned
inductively on the structure of expressions and can be checked eﬃciently:
Deﬁnition 4.5 (Fragment eA) We deﬁne the fragment eA ⊂A inductively on the
structure of A expressions. An expression eA ∈A is an eA expression if
• eA := JtKD is a triple pattern,
• eA := f
A1 1 f
A2, where f
A1 and f
A2 are eA expressions,
• eA := f
A1 \ f
A2, where f
A1 is an eA expression and f
A2 ∈A,
• eA := f
A1
1 f
A2, where f
A1 and f
A2 are eA expressions,
• eA := σR(f
A1), where R is a ﬁlter condition and f
A1 ∈eA,
• eA := πS(f
A1), where S is a set of variables, f
A1 ∈eA, and
S ⊇pVars(f
A1) or S ⊆cV ars(f
A1), or
• eA := f
A1 ∪f
A2, where f
A1 f
A2 are eA expressions and
pVars(f
A1)=cVars(f
A1)=pVars(f
A2)=cVars(f
A2).
2
Lemma 4.1 Every expression eA ∈eA has the incompatibility property.
2
Proof of Lemma 4.1
We prove the lemma by induction on the structure of eA expressions, thereby
exploiting the structural constraints imposed by Deﬁnition 4.5. The basic case is
eA := JtKD. By semantics (see Deﬁnition 2.11), all mappings in the result then bind
exactly the same set of variables, and consequently the values of each two distinct
mappings must diﬀer in at least one variable, which makes them incompatible. We
assume that every eA ∈eA has the incompatibility property and distinguish six cases.
(1) Consider an expression eA := f
A1 1 f
A2. By Deﬁnition 4.5, both f
A1, f
A2 are eA
expressions and by induction hypothesis both have the incompatibility property. We
observe that each mapping µ ∈eA is of the form µ = µ1 ∪µ2 with µ1 ∈f
A1, µ2 ∈f
A2,
and µ1 ∼µ2 (by semantics of 1). We ﬁx µ and show that each mapping µ′ ∈eA
that is distinct from µ is incompatible. Any distinct mapping µ′ ∈eA is of the form
µ′
1 ∪µ′
2 with µ′
1 ∈f
A1, µ′
2 ∈f
A2, and it holds that µ′
1 is diﬀerent from µ1 or that µ′
2
is diﬀerent from µ2 (because µ is distinct from µ′). Let us w.l.o.g. assume that µ′
1 is
diﬀerent from µ1. We know that f
A1 ∈eA, so it holds that µ1 is incompatible with µ′
1.
It follows that µ = µ1∪µ2 is incompatible with µ′ = µ′
1∪µ′
2, since µ1 and µ′
1 disagree
in the value of at least one variable. (2) Let eA := f
A1 \ f
A2 where f
A1 ∈eA, so each
two distinct mappings in f
A1 are pairwise incompatible by induction hypothesis. By
semantics of \, eA is a subset of f
A1, so the incompatibility property trivially holds
for eA. (3) Let eA := f
A1
1 f
A2, where both f
A1 and f
A2 are eA expressions. We rewrite
the left outer join according to its semantics: eA = f
A1
1 f
A2 = (f
A1 1 f
A2)∪(f
A1\ f
A2).
77

Chapter 4. Algebraic SPARQL Query Optimization
Following the argumentation in cases (1) and (2), the incompatibility property holds
for both subexpressions eA1 := f
A1 1 f
A2 and eA\ := f
A1 \ f
A2, so it suﬃces to show
that the mappings in eA1 are pairwise incompatible to those in eA\. First note that
eA\ is a subset of f
A1. Further, by semantics each mapping µ ∈eA1 is of the form
µ = µ1 ∪µ2, where µ1 ∈f
A1, µ2 ∈f
A2, and µ1 ∼µ2. Applying the induction
hypothesis, we conclude that each mapping in f
A1 and hence each mapping µ′
1 ∈eA\
is (3a) either incompatible with µ1 or (3b) identical to µ1. (3a) If µ′
1 is incompatible
with µ1, then it follows that µ′
1 is incompatible with µ1 ∪µ2 = µ and we are
done. (3b) Let µ1 = µ′
1. By assumption, mapping µ2 (which is generated by f
A2) is
compatible with µ1 = µ′
1. We conclude that f
A1 \ f
A2 does not generate µ′
1, which
is a contradiction (i.e., assumption (3b) was invalid). (4) Let eA := σR(f
A1), where
f
A1 ∈eA. By semantics of σ, eA is a subset of f
A1, so the property trivially follows
by application of the induction hypothesis (5) Let eA := πS(f
A1), where f
A1 ∈eA and
by Deﬁnition 4.5 it holds that (5a) S ⊇pVars(f
A1) or (5b) S ⊆cVars(f
A1). (5a)
If S ⊇pVars(f
A1) then, according to Proposition 4.1, the projection maintains all
variables that might occur in result mappings, so eA is equivalent to f
A1. The claim
then follows by induction hypothesis. Concerning case (5b) S ⊆cVars(f
A1) it follows
from Proposition 4.2 that each result mapping produced by expression eA binds all
variables in S ⊆cVars(f
A1), and consequently all result mappings bind exactly the
same set of variables. Recalling that we assume set semantics, we conclude that two
distinct mappings must diﬀer in the value of at least one variable, which makes them
incompatible. (6) Let eA := f
A1∪f
A2, where f
A1, f
A2 are eA expressions and it holds that
pVars(f
A1) = cVars(f
A1) = pVars(f
A2) = cVars(f
A2). From Propositions 4.1 and 4.2 it
follows that each two mappings that are generated by f
A1 ∪f
A2 bind exactly the same
set of variables. Analogical to the argumentation in case (5b), two distinct mappings
then disagree in the value of at least one variable, which makes them incompatible.2
We want to note that the property of mappings in the result being pairwise in-
compatible has been used before in the proof of Lemma 2 in [PAG06a]. There, it
was shown at syntax level that Union-free SPARQL expressions always exhibit the
incompatibility property.2 We emphasize that Lemma 4.1 above strictly generalizes
this case, i.e. it is easily shown that algebra expressions derived from Union-free
expressions are always eA expressions. More precisely, algebra expressions built us-
ing only operators 1, \,
1, and σ (i.e., those that may appear when translating
Union-free SPARQL expressions into algebra) are a proper subclass of eA, so the
incompatibility property always holds for such expressions. The following example
illustrates that expressions involving the remaining two operators, namely projec-
tion π and union ∪, do not exhibit the incompatibility property in the general case.
2The property was not called “incompatibility property” there and was only used for proving a
single equivalence. We will apply it in diﬀerent contexts, which justiﬁes its prominent role.
78

4.2. Optimization Rules for Set Algebra
I. Algebraic Equivalences: Idempotence and Inverse
A ∪A
≡A
(UIdem)
eA 1 eA ≡eA
( ^
JIdem)
eA
1 eA ≡eA
( ^
LIdem)
A \ A
≡∅
(Inv)
Figure 4.1.: Algebraic equivalences: idempotence and inverse. Expression A stands
for an A expression and eA stands for an eA expression.
Example 4.4 Let D := {(0, f, 0), (1, t, 1), (a, tv, 0), (a, tv, 1)} be an RDF database.
When evaluating the two algebra expressions
A1 := J(0, f, ?x)KD ∪J(1, t, ?y)KD and
A2 := π?x,?y((J(a, tv, ?z)KD
1 J(?z, f, ?x)KD)
1 J(?z, t, ?y)KD)
on D we obtain the mapping set Ω= {{?x 7→0}, {?y 7→1}} for both. The two
mappings in Ωare compatible. Note that neither A1 nor A2 are eA expressions. 2
4.2.2. Idempotence and Inverse
We start our investigation of SPARQL set algebra equivalences with some very basic
rules that hold with respect to idempotence and inverse algebraic laws in Figure 4.1,
where A stands for an A expression and eA represents an eA expression. Following
common notation, we write A ≡B if SPARQL algebra expression A is equivalent
to B on every document D. As a notational convention, we distinguish equivalences
that speciﬁcally hold for fragment eA by a tilde symbol, e.g. writing ( ^
JIdem) for the
idempotence of the join operator over expressions in class eA.
The rules (UIdem) and (Inv) in Figure 4.1 hold for the whole fragment A. They
follow trivially from the deﬁnition of the algebraic operators ∪and \. More interest-
ing are rules ( ^
JIdem) and ( ^
LIdem), established for fragment eA. We next present
the proofs for these rules; observe that both exploit the incompatibility property.
Proof of Equivalences ( ^
JIdem) and ( ^
LIdem) from Figure 4.1
( ^
JIdem). Let eA be an eA expression. We show that both directions of the equiva-
lence hold. ⇒: Consider a mapping µ ∈eA 1 eA. Then µ = µ1 ∪µ2 where µ1, µ2 ∈eA
and µ1 ∼µ2. From Lemma 4.1 we know that each eA expression has the incompat-
ibility property, so each pair of distinct mappings in eA is incompatible. It follows
that µ1 = µ2 and, consequently, µ1 ∪µ2 = µ1, which is generated by eA, and hence
by the right side expression. ⇐: Consider a mapping µ ∈eA. Choose µ for both the
left and right expression in eA 1 eA. By assumption, µ ∪µ = µ is contained in the
left side expression of the equation, which completes the proof.
79

Chapter 4. Algebraic SPARQL Query Optimization
( ^
LIdem). Let eA ∈eA. The following rewriting proves the equivalence.
eA
1 eA = ( eA 1 eA) ∪( eA \ eA)
[semantics]
= ( eA 1 eA) ∪∅
[(Inv)]
=
eA 1 eA
[semantics]
=
eA
[( ^
JIdem)]2
The observation that both proofs exploit the incompatibility property suggests
(yet does not logically imply) that equivalences ( ^
JIdem) and ( ^
LIdem) do not hold
for fragments that do not satisfy the incompatibility property. The following propo-
sition clariﬁes this issue, showing that the two equivalences do generally not hold
for expressions involving operators ∪and π (i.e., when relaxing the restrictions for
operators π and ∪imposed on eA expressions in the last two bullets of Deﬁnition 4.5).
Proposition 4.3 The two equivalences ( ^
JIdem) and ( ^
LIdem) do not hold for frag-
ments involving operator ∪and/or π in the general case.
2
Proof of Proposition 4.3
We argue that the RDF database D and the SPARQL algebra expressions A1, A2
from Example 4.4 constitute counterexamples for both operator ∪and π. First note
that A1 contains operator ∪and A2 contains operator π (and recall that neither A1
nor A2 are eA expressions). We now show that, for A1 and A2, neither rule ( ^
JIdem)
nor rule ( ^
LIdem) does hold. As discussed in Example 4.4 the result of evaluating
A1 and A2 on D is Ω:= {{?x 7→0}, {?y 7→1}}. It is easy to verify that A1 1 A1 =
A1
1 A1 = A2 1 A2 = A2
1 A2 = {{?x 7→0}, {?y 7→1}, {?x 7→0, ?y 7→1}}.
Obviously, this result diﬀers from Ω, which proves Proposition 4.3.2
4.2.3. Associativity, Commutativity, and Distributivity
We survey associativity, commutativity, and distributivity in Figure 4.2. The rules
speak for themselves and do not require further explanation. One outstanding ques-
tion is whether the rules for the algebraic laws listed in Figure 4.2 are complete
w.r.t. possible operator combinations. The following lemma answers this question:
Lemma 4.2 Let O1 := {1, \,
1} and O2 := O1 ∪{∪} be sets of operators.
1. Associativity and commutativity do not hold for operators \ and
1.
2. Neither \ nor
1 are left-distributive over ∪.
3. Let o1 ∈O1, o2 ∈O2, and o1 ̸= o2. Then operator o2 is neither left- nor
right-distributive over operator o1.
2
80

4.2. Optimization Rules for Set Algebra
II. Algebraic Equivalences: Associativity
(A1 ∪A2) ∪A3
≡A1 ∪(A2 ∪A3)
(UAss)
(A1 1 A2) 1 A3 ≡A1 1 (A2 1 A3)
(JAss)
III. Algebraic Equivalences: Commutativity
A1 ∪A2
≡A2 ∪A1
(UComm)
A1 1 A2 ≡A2 1 A1
(JComm)
IV. Algebraic Equivalences: Distributivity
(A1 ∪A2) 1 A3
≡(A1 1 A3) ∪(A2 1 A3)
(JUDistR)
A1 1 (A2 ∪A3) ≡(A1 1 A2) ∪(A1 1 A3)
(JUDistL)
(A1 ∪A2) \ A3
≡(A1 \ A3) ∪(A2 \ A3)
(MUDistR)
(A1 ∪A2)
1 A3 ≡(A1
1 A3) ∪(A2
1 A3)
(LUDistR)
Figure 4.2.: Algebraic equivalences: associativity, commutativity, and distributitvity.
Expressions A1, A2, A3 stand for A expressions.
The proof in Appendix C.1 is by an exhaustive listing of counterexamples. It is
easily veriﬁed that the lemma rules out associativity, commutativity, and distributiv-
ity for all operator combinations diﬀerent from those listed in Figure 4.2. Amongst
others, the lemma shows that
1 is not left-distributive over ∪. This implies that
Proposition 1(3) from [PAG06a], stating that operator Opt is left-distributive over
Union, is actually wrong. We prove this claim in the following example.
Example 4.5 We show that the SPARQL expression equivalence
A1 Opt (A2 Union A3) ≡(A1 Opt A2) Union (A1 Opt A3)
stated in Proposition 1(3) in [PAG06a] does not hold in the general case. As a
counterexample, we choose the database D := {(0, c, 1)} and set A1 := (0, c, ?x),
A2 := (?x, c, 1), and A3 := (0, c, ?y). It is easily veriﬁed that
JA1 Opt (A2 Union A3)KD = {{?x 7→1, ?y 7→1}}, but
J(A1 Opt A2) Union (A1 Opt A3)KD = {{?x 7→1}, {?x 7→1, ?y 7→1}}.
Obviously, the left and right side evaluation results diﬀer.
2
Remark 4.1 The union normal form proposed in Section 2.3 in [PAG06a] builds
upon this invalid equivalence, so this ﬁnding calls the existence of such a normal
form into question. In response to our observation, the authors published a corrected
version of the proof in the journal version [PAG09], proposing a rewriting scheme to
bring expressions into union normal form without using the invalid equivalence. 2
81

Chapter 4. Algebraic SPARQL Query Optimization
4.2.4. Projection Pushing
We will next introduce a set of rules that allow us to manipulate expressions in-
volving the algebraic projection operator π, with focus on pushing down projection
operators in the operator tree. We start with the observation that, according to
the deﬁnition of SPARQL queries (Deﬁnition 2.5), projection initially occurs only
at the top-level, in form of a Select clause. Hence, when translating queries into
algebra, there is exactly one projection operator π and this operator stands on top
of the whole algebra expression (the same observation holds for the W3C SPARQL
Recommendation [spac]). Arguably, a good optimization scheme should include the
possibility to choose among evaluation plans where projection is applied at diﬀerent
positions in the operator tree. It is well-known from SQL optimizers that early pro-
jection may serve as a ﬁlter that can signiﬁcantly decrease the size of intermediate
results, thus speeding up subsequent computations. We expect similar beneﬁts for
SPARQL query processing and underline this claim by the following example.
Example 4.6 Consider the algebra expression B1 below, which selects URIs and
names of persons that know at least one other person:
B1 := π?person,?name(J(?person, name, ?name)KD 1 J(?person, knows, ?person2)KD).
When evaluating this query strictly according to the semantics, we would ﬁrst
compute the join between the result of the two inner triple patterns and after-
wards project for variables ?person and ?name. Let us assume that the number of
knows relationships is very high (assume, for instance, a social networking site),
so the second triple pattern would extract a large number of mappings, and many
of them may share the value for variable ?person (i.e., whenever a ﬁxed person
knows more than one person). In that case, it would be desirable to project for
?person in the evaluation result of the right pattern before joining with the left
pattern, to reduce the cost of the subsequent join. Such an early projection is of
particular beneﬁt when the triples containing predicate knows are sorted on disk
by the subject: then the projection can be carried out in linear time (even on-the-
ﬂy while extracting the bindings). In fact, state-of-the-art RDF processing systems
like the Hexastore [WKB08] system or the Vertical Partitioning scheme proposed
in [AMMH07] support sorted extraction of data, so this situation is not unrealistic.
As we will see later, SPARQL algebra expression B1 is equivalent to the expression
Bopt
1
:= J(?person, name, ?name)KD 1 π?person(J(?person, knows, ?person2)KD),
which – under the assumptions discussed before – constitutes a more eﬃcient query
evaluation plan. The algebraic equivalences that we will present in the following can
be used to transform the two expressions B1 and Bopt
1
into each other.
2
82

4.2. Optimization Rules for Set Algebra
V. Algebraic Equivalences: Projection Pushing
πpVars(A)∪S(A) ≡A
(PBaseI)
πS(A)
≡πS∩pVars(A)(A)
(PBaseII)
We deﬁne the shortcuts S′ := S ∪(pVars(A1) ∩pVars(A2))
and S′′ := pVars(A1) ∩pVars(A2). The following equivalences hold.
πS(A1 ∪A2)
≡πS(A1) ∪πS(A2)
(PUPush)
πS(A1 1 A2)
≡πS(πS′(A1) 1 πS′(A2))
(PJPush)
πS(A1 \ A2)
≡πS(πS′(A1) \ πS′′(A2))
(PMPush)
πS(A1
1 A2) ≡πS(πS′(A1)
1 πS′(A2))
(PLPush)
πS(σR(A))
≡πS(σR(πS∪vars(R)(A)))
(PFPush)
πS1(πS2(A))
≡πS1∩S2(A)
(PMerge)
Figure 4.3.: Algebraic equivalences: projection pushing. A, A1, A2 stand for A ex-
pressions and S, S1, S2 ⊂V denote sets of variables.
Figure 4.3 presents equivalences to simplify, eliminate, and push down (or, alter-
natively, pull out) projection operators. We will present the proof for rule (PJPush)
later in this section and refer the reader to Appendix C.2 for the remaining proofs.
The ﬁrst two equivalences in Figure 4.3, (PBaseI) and (PBaseII), establish general-
purpose rewriting rules for projection expressions. Firstly, (PBaseI) shows that,
when projecting a variable set that contains all possible variables (and possibly
some additional variables S), the projection can simply be dropped. The second
equivalence, (PBaseII), complements (PBaseI) by showing that all variables in S
that do not belong to the possible variables of A can be dropped when projecting S.
Intuitively, one might argue that these two equivalences are irrelevant for query
rewriting in practice, given that users typically do not write queries that project
variables that cannot be bound in the inner expression. We therefore want to em-
phasize that the beneﬁt of these two equivalences stems from a combination with
the remaining equivalences, which may introduce such redundant variables within
the rewriting process. We will clarify this observation later in Example 4.7.
The remaining six rules in Figure 4.3 address the issue of pushing down projection
expressions. The simplest rule among them is (PUPush). It shows that, in case of
the union operator ∪, projection can be independently applied to the two subex-
pressions instead of the whole expression. Equivalence (PJPush) for join expressions
is more involved. It relies on the observation that, when pushing projections inside
join subexpressions, we must keep variables that may occur in both subexpressions,
because such variables may cause incompatibility between mappings and aﬀect the
result of the inner join, thus changing the semantics of the whole expression. To
this end, we deﬁne S′ := S ∪(pVars(A1) ∩pVars(A2)) as an extension of S and
83

Chapter 4. Algebraic SPARQL Query Optimization
project the variables in S′ in the two subexpressions. Note that, due to the fact that
S′ ⊇S, we cannot eliminate the topmost projection in the general case. Yet, as we
will show in Example 4.7, a subsequent application of rules (PBaseI) and (PBaseII)
may help to eliminate the topmost projection in cases where it became redundant.
The subsequent proof of rule (PJPush) further clariﬁes the above considerations.
Proof of Equivalence (PJPush) from Figure 4.3
⇒: We show (Claim1) that, for each mapping µ that is generated by the left
side subexpression A1 1 A2, there is a mapping µ′ generated by the right side
subexpression πS′(A1) 1 πS′(A2) s.t. for all ?x ∈S either ?x ∈dom(µ) ∩dom(µ′)
and µ(?x) = µ′(?x) holds or ?x is unbound in both µ and µ′. It is easy to see that, if
this claim holds, then the right side generates all mappings that are generated by the
left side: the mapping that is generated by the left side expression of the equation is
obtained from µ when projecting variables in S, and this mapping is also generated
by the right side expression when projecting S in µ′. So let us consider a mapping
µ ∈A1 1 A2. By the semantics of 1, mapping µ is of the form µ := µ1 ∪µ2,
where µ1 ∈A1, µ2 ∈A2, and µ1 ∼µ2 holds. We observe that, on the right side,
πS′(A1) then generates a mapping µ′
1 ⊆µ1, obtained from µ1 ∈A1 by projecting S′;
similarly πS′(A2) generates a mapping µ′
2 ⊆µ2, obtained from µ2 by projecting S′.
Then µ′
1 (µ′
2) agrees with µ1 (µ2) on variables in S (where “agrees” means that
they either map the variable to the same value or the variable is unbound in both
mappings), because S′ ⊇S holds and therefore no variables in S are projected away
when computing πS′(A1) and πS′(A2). It is easy to see that µ1 ∼µ2 implies µ′
1 ∼µ′
2,
so the right side expression µS′(A1) 1 µS′(A2) generates µ′ := µ′
1 ∪µ′
2. From the
observation that µ1 (µ2) agrees with mapping µ′
1 (µ′
2) on all variables in S it follows
that µ′ agrees with µ on all variables in S and we conclude that (Claim1) holds.
⇐: We show (Claim2) that, for each mapping µ′ that is generated by the right side
subexpression πS′(A1) 1 πS′(A2) there is a mapping µ ∈A1 1 A2 such that for all
?x ∈S either ?x ∈dom(µ) ∩dom(µ′) and µ(?x) = µ′(?x) holds or ?x is unbound in
both µ and µ′. Analogously to the other direction, it then follows immediately that
all mappings generated by the right side also are generated by the left side of the
equation. So let us consider a mapping µ′ ∈πS′(A1) 1 πS′(A2). Then µ′ is of the form
µ′ := µ′
1 ∪µ′
2, where µ′
1 ∈πS′(A1), µ′
2 ∈πS′(A2), and µ′
1 ∼µ′
2 holds. Assume that µ′
1
is obtained from mapping µ1 ∈A1 by projecting S′, and similarly assume that µ′
2 is
obtained from µ2 ∈A2 by projecting S′. We distinguish two cases: (a) if µ1 and µ2 are
compatible, then µ := µ1 ∪µ2 is the desired mapping that agrees with µ′ := µ′
1 ∪µ′
2
on variables in S, because µ1 ⊇µ′
1 and µ2 ⊇µ′
2 holds. Otherwise, (b) if µ1 and µ2
are incompatible this means there is a variable ?x ∈dom(µ1) ∩dom(µ2) such that
µ1(?x) ̸= µ2(?x). From Proposition 4.1 we know that ?x ∈pVars(A1) ∩pVars(A2),
which implies that ?x ∈S′ ⊇pVars(A1) ∩pVars(A2). Hence, ?x is bound in µ′
1 and
in µ′
2 and it follows that µ′
1(?x) ̸= µ′
2(?x), which contradicts the assumption that
µ′
1 ∼µ′
2 (i.e., assumption (b) was invalid). This completes the proof.2
84

4.2. Optimization Rules for Set Algebra
Having discussed rule (PJPush), we resume the discussion of the equivalences from
Figure 4.3. Equivalences (PMPush) and (PLPush) rely on similar ideas as (PJPush)
and cover projection pushing for minus and left outer join expressions. Finally,
(PFPush) covers projection pushing into ﬁlter expressions and (PMerge) shows that
nested projection expressions can always be merged into a single projection. We
conclude with a small example, which shall also serve as a proof of concept:
Example 4.7 We use the rules from Figure 4.3 to transform the algebra expression
B1 from Example 4.6 into its optimized version Bopt
1 . We start with application of
rule (PJPush), to push projection down inside the join expression and obtain
B′
1 := π?person,?name( π?person,?name(J(?person, name, ?name)KD) 1
π?person,?name(J(?person, knows, ?person2)KD))
We observe that, according to equivalence (PBaseI), the left inner projection
expression can simply be dropped. Further, by application of rule (PBaseII) we can
drop ?name from the second inner projection. This gives us the expression
B′′
1 := π?person,?name( J(?person, name, ?name)KD 1
π?person(J(?person, knows, ?person2)KD))
Finally, we again apply (PBaseI) to eliminate the outer projection, which gives
us the optimized version Bopt
1
from Example 4.6. We point out that the application
of rules (PBaseI) and (PBaseII) helped us to eliminate redundant ﬁlters introduced
during the rewriting process and to reﬁne the second ﬁlter expression.
2
4.2.5. Filter Decomposition, Elimination, and Pushing
Figure 4.4 presents equivalences to decompose, eliminate, and rearrange (parts of)
ﬁlter conditions. When interpreted as rewriting rules, they form the basis for trans-
ferring established relational algebra ﬁlter pushing techniques into the context of
SPARQL. We emphasize that these rules are more than simple translations of exist-
ing relational algebra equivalences: ﬁrstly, they rely on the speciﬁcs of SPARQL and
build upon the concepts of possible and certain variables from Section 4.1; secondly,
they address speciﬁcs of SPARQL algebra, such as ﬁlter predicate bnd (cf. equiv-
alences (FBndI)-(FBndIV)). In the following, we will discuss all rules and sketch
their possible interplay. Missing proofs can be found in Appendix C.3.
The ﬁrst three equivalences in group VI cover decomposition and reordering of
ﬁlter conditions. More precisely, (FDecompI) and (FDecompII) allow to decompose
ﬁlter conditions that are connected using the boolean connectives ∧and ∨, while
(FReord) shows that the application order of nested ﬁlters does not matter (this
follows from equivalence (FDecompI) and the commutativity of ∧). When combined,
85

Chapter 4. Algebraic SPARQL Query Optimization
they allow to split complex ﬁlter conditions, which may be useful to push individual
components down in the operator tree using the ﬁlter pushing rules in group VII.
The subsequent four equivalences (FBndI)-(FBndIV) are SPARQL-speciﬁc and
address the issue of ﬁlter predicate bnd. They cover cases where predicate bnd is
used in either a redundant or contradicting way. In some sense, these rules reﬂect the
intuition behind the concepts of possible and certain variables. To give an example,
precondition ?x ∈cVars(A1) in rule (BndI) implies that ?x is bound in each result
mapping (by Proposition 4.2), so the ﬁlter is redundant and can be dropped.
Finally, the rules in group VII cover the issue of ﬁlter pushing. While the two
equivalences (FUPush) and (FMPush) hold independently from the variables that
are used in the ﬁlter conditions, the equivalences (FJPush) and (FLPush) heavily
rely on the notions of safe and certain variables. According to these rules, the ﬁlter
can be pushed inside the ﬁrst component of a join A1 1 A2 (respectively, left outer
join A1
1 A2) if each variable that is used inside the ﬁlter expression is a certain
variable of A1 (i.e., deﬁnitely bound in left side mappings) or is not a possible vari-
able of A2 (i.e., deﬁnitely not bound in right side mappings). Informally speaking,
these conditions guarantee that the join (respectively, left outer join) does not aﬀect
the validity of the ﬁlter condition. In the generarl case, the equivalences do not hold
if this precondition is violated, as illustrated by the next example:
Example 4.8 Consider the algebra expressions A1 := J(?x, c, c)KD
1 J(?x, d, ?y)KD,
A2 := J(?y, c, c)KD, and the expressions
A1
l := σ?y=c(A1 1 A2)
vs. A1
r := σ?y=c(A1) 1 A2,
A
1
l := σ?y=c(A1
1 A2) vs. A
1
r := σ?y=c(A1)
1 A2.
First observe that ?y ̸∈cVars(A1) and ?y ∈pVars(A2), so neither (FJPush) nor
(FLPush) are applicable. In particular, A1
l ≡A1
r and A
1
l ≡A
1
r
do not follow. We
ﬁx the database D := {(c, c, c)}. It is easily veriﬁed that both A1
l and A
1
l
evaluate
to {{?x 7→c, ?y 7→c}} on D, whereas A1
r and A
1
r both evaluate to ∅. This shows
that the equivalences generally do not hold if the precondition is violated.
2
We further clarify the idea of the precondition in the proof for rule (FJPush):
Proof of Equivalence (FJPush) from Figure 4.4
⇒: Let µ ∈σR(A1 1 A2). By semantics, µ |= R and we know that µ is of the
form µ = µ1 ∪µ2 with µ1 ∈A1, µ2 ∈A2, and µ1 ∼µ2. Further, by assumption each
variable ?x ∈vars(R) is (i) contained in cVars(A1) or (ii) not contained in pVars(A2)
(or both). It suﬃces to show that (Claim1) µ1 |= R holds, because this implies that
the right side generates µ. Let us, for the sake of contradiction, assume that µ1 ̸|= R.
Now consider the semantics of ﬁlter expressions in Deﬁnition 2.9. and recall that
µ1 ⊆µ. Given that µ |= R, it is clear that µ1 does not satisfy R if and only if there
is one or more ?x ∈vars(R) such that ?x ∈dom(µ), ?x ̸∈dom(µ1) and ?x causes
86

4.2. Optimization Rules for Set Algebra
VI. Algebraic Equivalences: Filter Decomposition and Elimination
σR1∧R2(A)
≡σR1(σR2(A))
(FDecompI)
σR1∨R2(A)
≡σR1(A) ∪σR2(A)
(FDecompII)
σR1(σR2(A))
≡σR2(σR1(A))
(FReord)
σbnd(?x)(A)
≡A, if ?x ∈cVars(A)
(FBndI)
σbnd(?x)(A)
≡∅, if ?x ̸∈pVars(A)
(FBndII)
σ¬bnd(?x)(A)
≡∅, if ?x ∈cVars(A)
(FBndIII)
σ¬bnd(?x)(A)
≡A, if ?x ̸∈pVars(A)
(FBndIV)
VII. Algebraic Equivalences: Filter Pushing
σR(A1 ∪A2)
≡σR(A1) ∪σR(A2)
(FUPush)
σR(A1 \ A2)
≡σR(A1) \ A2
(FMPush)
If for all ?x ∈vars(R) : ?x ∈cVars(A1) ∨?x ̸∈pVars(A2), then
σR(A1 1 A2)
≡σR(A1) 1 A2
(FJPush)
σR(A1
1 A2)
≡σR(A1)
1 A2
(FLPush)
Figure 4.4.: Algebraic equivalences: ﬁlter manipulation. A, A1, A2 stand for A ex-
pressions; S ⊂V is a set of variables; R, R1, R2 denote ﬁlter conditions.
the ﬁlter to evaluate to false. We now exploit the constraints (i) and (ii) that are
imposed on the variables in vars(R): if variable ?x satisﬁes constraint (i), then it
follows from Proposition 4.2 that ?x ∈dom(µ1), which is a contradiction; otherwise,
if ?x satisﬁes constraint (ii) we know from Proposition 4.1 that ?x ̸∈dom(µ2). Given
that ?x ∈dom(µ), this implies that ?x must be contained in dom(µ1), which again
gives us a contradiction. We conclude that µ1 |= R and therefore (Claim1) holds.
Direction “⇐” can be shown with an analogical argumentation.2
We conclude our discussion of equivalences for ﬁlter manipulation with two rules
that can be used to make implicit joins and selections in ﬁlter expressions explicit:
Lemma 4.3 Let A be a SPARQL set algebra expression built using only operators
1, ∪, and triple patterns of the form JtKD. Further let ?x, ?y ∈cVars(A). By A ?y
?x
we denote the expression obtained from A by replacing all occurrences of ?x in A
by ?y; similarly, A c
?x is obtained from A by replacing ?x by a URI or a literal c. The
following two equivalences hold.
(FElimI)
πS\{?x}(σ?x=?y(A)) ≡πS\{?x}(A ?y
?x)
(FElimII)
πS\{?x}(σ?x=c(A)) ≡πS\{?x}(A c
?x)
2
87

Chapter 4. Algebraic SPARQL Query Optimization
The two equivalences (FElimI) and (FElimII) allow to eliminate atomic ﬁlter
conditions of the form ?x =?y and ?x = c, by replacing all occurrences of ?x in the
inner expression by ?y and c, respectively. The proof of the lemma is technically
involved and can be found in Appendix C.4. Note that in both equivalences the
ﬁlter expression must be embedded in a projection expressions that projects the
variable set S \{?x}, i.e. not including variable ?x that is to be replaced (otherwise,
variable ?x may be bound in left side mappings but not in right side mappings).
Given our complete rewriting framework, this is not a major restriction: using the
projection pushing rules from Figure 4.4, we can always push projections down on
top of ﬁlter expressions and afterwards check if rule (FElimI) or (FElimII) applies.
As an important precondition for the equivalence, expression A must be contained in
a restricted fragment of set algebra, composed of operators 1, ∪, and triple patterns.
A second precondition is that ?x ∈cVars(A) and ?y ∈cVars(A) hold. The following
example illustrates the rationale behind all these preconditions.
Example 4.9 Consider the expression-document pairs
A1 := J(a, b, ?x)KD1 1 (J(?z, b, ?y)KD1 \ J(?z, d, ?x)KD1), D1 := {(a, b, c), (a, d, e)},
A2 := J(a, a, ?x)KD2 1 J(a, a, ?y)KD2 1 σbnd(?y)(J(a, a, ?xKD2), D2 := {(a, a, a)},
A3 := J(a, a, ?x)KD3 ∪J(?x, b, ?y)KD3, D3 := {(a, a, a)},
A4 := J(a, a, ?y)KD4 ∪J(?x, b, ?y)KD4, D4 := {(a, a, a)}.
Let S := {?y}. It is easily veriﬁed that, for each i ∈[4], the results of evalu-
ating πS\{?x}(σ?x=?y(Ai)) and πS\{?x}(Ai
?y
?x) on document Di diﬀer. Expression A1
constitutes a counterexample for rule (FElimI) for fragments involving operator \,
while A2 gives a counterexample for fragments involving operator σ. Concerning
A3 and A4 we observe that ?y ̸∈cVars(A3) and ?x ̸∈cVars(A4), respectively. This
shows that the preconditions ?y ∈cVars(A) and ?x ∈cVars(A) are necessary.
2
We conclude our discussion of ﬁlter manipulation with an elaborate example that
illustrates both the application of the ﬁlter manipulation rules from Figure 4.4 and
their possible interplay with the previous rules from Figures 4.1, 4.2, and 4.3:
Example 4.10 Consider the SPARQL algebra expression
π?p,?e(σ?sn̸=“Smith”∧?gn=“Pete”(
(J(?p, givenname, ?gn)KD 1 J(?p, surname, ?sn)KD 1 J(?p, rdf:type, Person)KD)
1 J(?p, email, ?e)KD))
which extracts all persons (?p) with givenname (?gn) “Pete”, surname (?sn) dif-
ferent from “Smith”, and optionally their email (?e). We transform this expression,
pursuing the goal to push the ﬁlter condition down as far as possible. To keep the
example short, we only sketch the strategy and skip several intermediate results.
88

4.2. Optimization Rules for Set Algebra
First, we apply rule (FLPush) to push the ﬁlter condition inside the left subex-
pression of the left outer join and afterwards split the ﬁlter condition into its compo-
nents using equivalence (FDecompI). We then push the ﬁlter component ?gn=“Pete”
down, to the top of triple pattern J(?p, givenname, ?gn)KD, using rule (FJPush). This
series of rewriting steps gives us the following algebra expression.
π?p,?e (σ?sn̸=“Smith”(
σ?gn=“Pete”(J(?p, givenname, ?gn)KD)
1 J(?p, surname, ?sn)KD 1 J(?p, rdf:type, Person)KD
)
1 J(?p, email, ?e)KD)
We next apply rules from Figure 4.3, to push down the top-level projection to the
top of the ﬁlter σ?gn=“Pete”, then eliminate the ﬁlter using (FElimII) (by replacing
variable ?gn with “Pete”), and reapply the rules from Figure 4.3 in inverse direction,
to pull out projection subexpressions again. This gives us the expression below.
π?p,?e (σ?sn̸=“Smith”(
J(?p, givenname, “Pete”)KD
1 J(?p, surname, ?sn)KD 1 J(?p, rdf:type, Person)KD
)
1 J(?p, email, ?e)KD).
In the ﬁnal step, we reorder the triple patterns in the join sequence using equiva-
lences (JAss) and (JComm) and push the ﬁlter expression σ?sn̸=“Smith” down using
rule (FJPush). We obtain the following SPARQL set algebra expression.
π?p,?e(
( σ?sn̸=“Smith”(J(?p, surname, ?sn)KD)
1 J(?p, givenname, “Pete”)KD 1 J(?p, rdf:type, Person)KD
)
1 J(?p, email, ?e)KD)
We may assume that this expression can be evaluated more eﬃciently than the
original expression, because both ﬁlters are applied early. The ﬁlter σ?gn=“Pete” is
even implicitly evaluated when extracting data for the respective triple pattern. 2
The example illustrates that the rewriting rules provided so far establish a power-
ful framework for ﬁnding alternative evaluation plans. It should be clear that further
techniques like heuristics, statistics about the data, knowledge about data access
paths, and cost estimation functions are necessary to implement an eﬃcient and
comprehensive optimizer on top of these rules, just like it is the case in the context
of relational algebra (cf. [Hal75; SC75; Tod75; GLR97]). With our focus on alge-
braic rewritings and the goal to establish the foundations for advanced optimization
approaches, the study of such techniques is beyond the scope of this work.
89

Chapter 4. Algebraic SPARQL Query Optimization
4.2.6. Rewriting Closed World Negation
We conclude the discussion of SPARQL set algebra optimization with a thorough
investigation of operator \. First recall that an expression A1 \ A2 retains exactly
those mappings from A1 for which no compatible mapping in A2 exists (cf. Deﬁni-
tion 2.10), so the minus operator essentially implements closed-world negation. It
is crucial for our discussion here that, in contrast to the other algebraic operations,
operator \ has no direct counterpart at the syntactic level, but – in SPARQL syntax
– is only implicit by the semantics of operator Opt (i.e., Opt is mapped into
1 and
the deﬁnition of
1 relies on operator \). As argued before in [AG08a], the lack of
a syntactic counterpart complicates the encoding of queries involving negation and,
as we will show in the following, poses speciﬁc challenges to the query optimizer.
We also stress that, as discussed in Section 3.2.3, it is mainly the operator \ that
is responsible for the high complexity of the (syntactic) operator Opt. Therefore, at
the algebraic level special care should be taken in optimizing expressions involving \.
We start our discussion with the observation from [AG08a] that operator \ can
always be encoded at the syntactic level using a combination of operators Opt,
Filter, and (the negated) ﬁlter predicate bnd.3 We illustrate the idea by example:
Example 4.11 The SPARQL expression Q1 and the corresponding algebra expres-
sion C1 below (obtained by application of the set semantics from Deﬁnition 2.11
to Q1) select all persons for which no name is speciﬁed in the data set.
Q1 := ((?p, rdf:type, Person) Opt
((?p, rdf:type, Person) And (?p, name, ?n))) Filter (¬bnd(?n))
C1 := σ¬bnd(?n)(J(?p, rdf:type, Person)KD
1
(J(?p, rdf:type, Person)KD 1 J(?p, name, ?n)KD))
2
From an optimization point of view it would be desirable to have a clean transla-
tion of the operator constellation in query Q1 using only operator \, but the seman-
tics maps Q1 into C1, which involves a comparably complex construction using opera-
tors σ,
1, 1, and predicate bnd (thus using operator \ implicitly, according to the se-
mantics of
1). This translation seems overly complicated and in fact better transla-
tions exist for a large class of practical queries, using only \, without operators σ,
1,
1, and predicate bnd. The expression C1 from Example 4.11, for instance, is equiv-
alent to the simpler expression Copt
1
:= J(?p, rdf:type, Person)KD \ J(?p, name, ?n)KD.
In the following we present equivalences that allow us to simplify expressions like C1,
making such “simulated” negation – an artifact of the missing negation operator at
syntax level – explicit. We start with a proposition that states some equivalences
over expressions involving the minus and the left outer join operator.
3The SPARQL fragment used in the proof in [AG08a] slightly diﬀers from our fragment and it
is an open problem if negation can always be encoded using our SPARQL fragment (cf. Def-
initions 2.5 and 2.4). Nevertheless, the encoding proposed in [AG08a] works in most practical
cases. We will come back to this discussion in Section 5.3.
90

4.2. Optimization Rules for Set Algebra
Proposition 4.4 Let A1, A2 be A expressions and f
A1, f
A2 be eA expressions. The
following equivalences hold.
(MReord)
(A1 \ A2) \ A3 ≡(A1 \ A3) \ A2
(MMUCorr)
(A1 \ A2) \ A3 ≡A1 \ (A2 ∪A3)
(MJ)
A1 \ A2
≡A1 \ (A1 1 A2)
(f
LJ)
f
A1
1 f
A2
≡f
A1
1 (f
A1 1 f
A2)
2
The proof of the proposition can be found in Appendix C.5. Rules (MReord)
and (MMUCorr) are general-purpose rewriting rules, listed for completeness. Most
important in our context is (f
LJ). It allows to eliminate redundant subexpressions
in the right side of
1 expressions (over fragment eA). We provide an example:
Example 4.12 The application of rule (f
LJ) simpliﬁes C1 from Example 4.11 to
C′
1 := σ¬bnd(?n)(J(?p, rdf:type, Person)KD
1 J(?p, name, ?n)KD).
2
It is worth mentioning that equivalence (f
LJ) taken alone only allows to eliminate
right side subexpressions that exactly correspond to the left side
1 subexpression.
When combining (f
LJ) with previous rules, though, we can even eliminate And-
connected subexpressions of the left side expression that occur in the right side:
Example 4.13 Consider the expression C2 below, which selects all persons that
know at least one other person and, optionally, their name.
C2 := (J(?p, rdf:type, Person)KD 1 J(?p, knows, ?p2)KD)
1
(J(?p, rdf:type, Person)KD 1 J(?p, name, ?n)KD)
Rule (f
LJ) cannot be applied directly to remove the pattern J(?p, rdf:type, Person)KD
from the right side of the left join expression, but we can apply the rewriting
C2
( f
LJ)
= (J(?p, rdf:type, Person)KD 1 J(?p, knows, ?p2)KD)
1
((J(?p, rdf:type, Person)KD 1 J(?p, knows, ?p2)KD) 1
(J(?p, rdf:type, Person)KD 1 J(?p, name, ?n)KD)))
(∗)
=
(J(?p, rdf:type, Person)KD 1 J(?p, knows, ?p2)KD)
1
((J(?p, rdf:type, Person)KD 1 J(?p, rdf:type, Person)KD) 1
(J(?p, knows, ?p2)KD 1 J(?p, name, ?n)KD))
( ^
JIdem)
=
(J(?p, rdf:type, Person)KD 1 J(?p, knows, ?p2)KD)
1
(J(?p, rdf:type, Person)KD 1 J(?p, knows, ?p2)KD 1 J(?p, name, ?n)KD)
( f
LJ)
=
(J(?p, rdf:type, Person)KD 1 J(?p, knows, ?p2)KD)
1 J(?p, name, ?n)KD,
where the correctness of step (∗) follows from (JAss) and (JComm).
2
91

Chapter 4. Algebraic SPARQL Query Optimization
The two previous examples show that equivalence (f
LJ) is a useful tool to simplify
subexpressions appearing in the right side of left outer join expressions.
The following lemma completes our rewriting scheme for simulated negation.
Lemma 4.4 Let A1, A2 ∈A and ?x ∈cVars(A2) \ pVars(A1) be a variable. The
following two equivalences hold:
σ¬bnd(?x)(A1
1 A2) ≡A1 \ A2
(FLBndI)
σbnd(?x)(A1
1 A2)
≡A1 1 A2
(FLBndII)
2
Proof of Lemma 4.4
(FLBndI). Let A1, A2 be A expressions and ?x ∈cVars(A2) \ pVars(A1) be a
variable, which implies that ?x ∈cVars(A1 1 A2) and ?x ̸∈pVars(A1 \ A2). We
transform the left side expression into the right side expression:
σ¬bnd(?x)(A1
1 A2)
= σ¬bnd(?x)((A1 1 A2) ∪(A1 \ A2))
[semantics]
= σ¬bnd(?x)(A1 1 A2) ∪σ¬bnd(?x)(A1 \ A2)
[(FUPush)]
= ∅∪σ¬bnd(?x)(A1 \ A2)
[(FBndIII)]
= σ¬bnd(?x)(A1 \ A2)
[semantics]
= A1 \ A2
[(FBndIV)]
(FLBndII). Similar to (FLBndI), see Appendix C.6 for details.2
The relevant rule in our context is (FLBndI). We apply it to the running example:
Example 4.14 The application of the rewriting rule (FLBndI) from Lemma 4.4 to
expression C′
1 from Example 4.12 yields the expression
Copt
1
:= J(?p, rdf:type, Person)KD \ J(?p, name, ?n)KD.
The construction in expression C1 from Example 4.11, involving
1, 1, σ, and
ﬁlter predicate bnd, has been replaced by a simple minus expression.
2
4.3. From Set to Bag Algebra
We now take the leap from SPARQL set algebra to the bag algebra introduced in
Deﬁnition 2.14. The results presented in this section are more interesting from a
practical point of view, because they immediately carry over to the oﬃcial W3C
semantics for SPARQL [spac], which follows the bag algebra approach.
Analogously to our discussion of SPARQL set algebra, we start with the deﬁnition
of the fragment comprising all SPARQL bag algebra expressions, called A+:
92

4.3. From Set to Bag Algebra
Deﬁnition 4.6 (Fragment A+) Fragment A+ denotes the full class of SPARQL
bag algebra expressions, i.e. expression built using operators ∪, 1, \,
1, π, σ, and
(bracket-enclosed) triple patterns of the form JtK+
D.
2
Like Deﬁnition 4.3, the above deﬁnition introduces a class of purely syntactic
entities. It should be clear that, when evaluating an expression A+ ∈A+, this
implicitly involves the SPARQL bag algebra operators introduced in Deﬁnition 2.14,
because all triple patterns are of the form JtK+
D and therefore extract multi-sets (as
opposed to Deﬁnition 4.3, where the triple patterns of the form JtKD extract simple
sets of mappings). Following the conventions from before, if D is known from the
context we shall refer to the mapping multi-set obtained from A+ by application of
the semantics from Deﬁnition 2.14 as the result obtained when evaluating A+ on D.
The ultimate goal of our analysis of bag semantics is to identify those equivalences
from Section 4.2 that carry over from set to bag semantics. To discuss the relations
between the two semantics, we deﬁne a bijective mapping between SPARQL set
algebra and SPARQL bag algebra, which allows us to map expressions from one
algebra into same-structured expressions from the other algebra. This is simply
done by replacing each triple pattern JtKD with JtK+
D when converting from set to
bag algebra and, vice versa, by replacing each pattern JtK+
D with JtKD when converting
from bag to set algebra. Formally, we deﬁne the function s2b (“set-to-bag”) as follows.
Deﬁnition 4.7 (Function s2b) Let A1, A2 ∈A be SPARQL set algebra expres-
sions, S ⊂V a set of variables, and R a ﬁlter condition. We deﬁne the bijective
function s2b : A 7→A+ inductively on the structure of A-expression:
s2b(JtKD)
:= JtK+
D
s2b(A1 1 A2)
:= s2b(A1) 1 s2b(A2)
s2b(A1 ∪A2)
:= s2b(A1) ∪s2b(A2)
s2b(A1 \ A2)
:= s2b(A1) \ s2b(A2)
s2b(A1
1 A2) := s2b(A1)
1 s2b(A2)
s2b(πS(A1))
:= πS(s2b(A1))
s2b(σR(A1))
:= σR(s2b(A1))
2
We shall use the inverse of the function, denoted as s2b−1(A+), to transform
a bag algebra expression A+ ∈A+ into its set algebra counterpart. Intuitively,
the function reﬂects the close connection between the set and bag semantics from
Deﬁnitions 2.11 and 2.15, which diﬀer only in the translation for triple patterns. In
particular, it is easily veriﬁed that, for each SPARQL expression or query Q, it holds
that JQK+
D = s2b(JQKD) and JQKD = s2b−1(JQK+
D) (when interpreting the results of
function J.KD and J.K+
D as SPARQL algebra expressions rather than sets or multi-sets
of mappings). Given this connection, we next transfer Lemma 3.1, which relates the
two semantics to each other, into the context of SPARQL set and bag algebra.
93

Chapter 4. Algebraic SPARQL Query Optimization
Lemma 4.5 The following claims hold.
1. Let A ∈A and D be an RDF document. Let Ωdenote the result of evalu-
ating A on D and let (Ω+, m+) denote the mapping multi-set obtained when
evaluating s2b(A) on D. Then for all µ ∈M : µ ∈Ω⇔µ ∈Ω+.
2. Let A+ ∈A+ and D be an RDF document. Let (Ω+, m+) denote the mapping
multi-set obtained when evaluating A+ on D and let Ωdenote the result of
evaluating s2b−1(A+) on D. Then for all µ ∈M : µ ∈Ω+ ⇔µ ∈Ω.
2
Essentially, the lemma shows that corresponding bag and set algebra expressions
coincide w.r.t. the mappings that they compute and only may diﬀer in that the bag
semantics assigns a multiplicity greater than 1 to some result mappings. The proof
of the lemma is similar in idea to the proof of Lemma 3.1 and therefore omitted.
Finally recall that the concepts of possible and certain variables directly carry over
to bag algebra expressions (cf. Section 4.1), so we shall use functions pVars(A+) and
cVars(A+) to extract possible and certain variables from some SPARQL bag algebra
expression A+ ∈A+. The adaption of the incompatibility property for bag algebra
expressions is not that straightforward and requires some advanced considerations,
to properly cope with multi-sets. We will tackle this task in the following subsection.
4.3.1. The Incompatibility Property for SPARQL Bag Algebra
As before in the case of set algebra, we are interested in bag algebra expressions that
exhibit the incompatibility property. Given that a mapping might appear multiple
times in the result when evaluating a bag algebra expression, we need a reﬁned
deﬁnition of the incompatibility property from Deﬁnition 4.4:
Deﬁnition 4.8 (Incompatibility Property for Bag Algebra) Let A+ be a bag
algebra expression and let (Ω+
D,m+
D) denote the multi-set obtained when evaluating
expression A+ on some document D. Then A+ has the incompatibility property if and
only if for every document D it holds that (i) each two distinct mappings µ1 ̸= µ2
in Ω+
D are incompatible and (ii) m+
D(µ) = 1 for all µ ∈Ω+
D.
2
Informally speaking, as an additional constraint compared to the incompatibility
property for SPARQL set algebra we enforce that (ii) no mapping appears multiple
times in the result multi-set. Intuitively, this additional constraint arises from the
observation that duplicates are always compatible to each other (i.e., µ ∼µ holds
trivially for each mapping µ) and may harm equivalences that – in the context of
set algebra – hold for expressions that exhibit the incompatibility property.
An appealing question is whether the deﬁnition of fragment eA, our syntactic
restriction of A expressions for which the incompatibility property holds (cf. Deﬁni-
tion 4.5 and Lemma 4.1) naturally carries over to bag algebra. Formally, we phrase
94

4.3. From Set to Bag Algebra
this question as follows: does eA ∈eA imply that s2b( eA) has the incompatibility
property (for bag algebra)? The following example shows that this is not the case.
Example 4.15 Consider the two SPARQL set algebra expressions
f
A1 := J(?x, c, 1)KD ∪J(?x, c, 1)KD,
f
A2 := π?x(J(?x, c, ?y)KD),
and the database D := {(c, c, 1), (c, c, 2)}. It is easily veriﬁed that f
A1, f
A2 ∈eA.
The corresponding SPARQL bag algebra expressions are
f
A+
1 := s2b(f
A1) = J(?x, c, 1)K+
D ∪J(?x, c, 1)K+
D,
f
A+
2 := s2b(f
A2) = π?x(J(?x, c, ?y)K+
D).
Both f
A+
1 and f
A+
2 evaluate on D to ({{?x 7→c}}, m) with m({?x 7→c}) := 2, so
neither f
A+
1 nor f
A+
2 has the incompatibility property, as they violate condition (ii) in
Deﬁnition 4.8. Hence, both are counterexamples for the claim that the bag algebra
expression s2b( eA) has the incompatibility property if eA ∈eA holds.
2
The example clariﬁes that we need to adjust the deﬁnition of fragment eA for
SPARQL bag algebra. In particular, expressions f
A+
1 and f
A+
2 from the above exam-
ple indicate that the restrictions for expressions involving operator projection π and
union ∪(the last two bullets in Deﬁnition 4.5) are not strong enough for SPARQL
bag algebra. A closer investigation reveals that we can merely impose syntactic
conditions such that fragments involving these operators still exploit the incom-
patibility property: intuitively, the syntactic restrictions for ∪- and π-expressions
imposed in Deﬁnition 4.5 exploit the observation that in some cases compatible
mappings collapse to a single mapping under set semantics, which is never the case
under bag semantics. Therefore, we exclude these operators in our eﬀorts to identify
a SPARQL bag algebra fragment that exhibits the incompatibility property. This
restriction leads to a compact fragment of SPARQL bag algebra, called f
A+:
Deﬁnition 4.9 (Fragment f
A+) The fragment f
A+ comprises all SPARQL bag al-
gebra expressions built using only operators 1, \,
1, σ, and (bracket-enclosed) triple
patterns of the form JtK+
D.
2
First observe that f
A+ is a subset of fragment eA w.r.t. to our bijective mapping:
Proposition 4.5 It holds that f
A+ ∈f
A+ →s2b−1( f
A+) ∈eA.
2
The proof follows directly from the deﬁnitions of the two fragments. Also note
that the opposite direction does not hold, as witnessed by the eA expressions f
A1
and f
A2 from Example 4.15 (i.e., we observe that s2b(f
A1) ̸∈f
A+ and s2b(f
A2) ̸∈f
A+).
When combining the above proposition with Lemma 4.1 and Lemma 4.5 we obtain:
95

Chapter 4. Algebraic SPARQL Query Optimization
Corollary 4.1 Let f
A+ ∈f
A+, D be an RDF document, and let (Ω+, m+) denote
the result obtained when evaluating f
A+ on D. Then each pair of distinct mappings
in Ω+ is incompatible.
2
Hence, condition (i) of Deﬁnition 4.8 always holds for f
A+ expressions. Thus, to
show that these expressions satisfy the incompatibility property, it remains to show
that (ii) no mapping occurs multiple times. We prove the following lemma.
Lemma 4.6 Let f
A+ ∈f
A+ and D be an RDF document. We denote by (Ω+, m+)
the mapping multi-set obtained when evaluating f
A+ on D and by Ωthe mapping
set obtained when evaluating s2b−1( f
A+) on D. It holds that Ω∼= (Ω+, m+) (where
symbol “∼=” denotes equality between sets and multi-sets, cf. Deﬁnition 2.13).
2
Proof of Lemma 4.6
Given Lemma 4.5, which shows that the two semantics diﬀer at most w.r.t. the
associated multiplicity, it suﬃces to show that bag algebra expressions contained
in f
A+ have multiplicity 1 associated with every mapping. We prove this claim by
induction on the structure of f
A+ expressions. Consider expression f
A+ ∈f
A+. We ﬁx
document D and denote by (Ω, m) the mapping set obtained when evaluating f
A+
on D. Similarly, if A+
1 and A+
2 are subexpressions of A+, we denote their evaluation
results on D by (Ω1, m1) and (Ω2, m2), respectively. The basic case f
A+ := JtK+
D
follows trivially from Deﬁnition 2.14, which ﬁxes m(µ) := 1 for all µ ∈Ω. Following
the structure of fragment f
A+ (cf. Deﬁnition 4.9), we distinguish four cases.
(1) Let f
A+ := f
A+
1 1 f
A+
2 . Then each µ ∈Ωis of the form µ = µ1 ∪µ2 where
µ1 ∈Ω1 and µ2 ∈Ω2 are compatible mappings. By induction hypothesis we know
that m1(µ1) = 1 and m2(µ2) = 1, which implies (by the deﬁnition of m for the case
of join expressions) that m(µ) = 1 + x, where 1 is obtained from m1(µ1) ∗m2(µ2)
and x is some additional multiplicity obtained from mapping pairs (µ′
1, µ′
2) distinct
from (µ1, µ2). Hence, we have to show that there are no mappings distinct from
µ1 and µ2 that generate µ. Let us for the sake of contradiction assume there exist
two mappings µ′
1 ∈Ω1, µ′
2 ∈Ω2 such that µ′
1 ∼µ′
2, µ′
1 ∪µ′
2 = µ and (w.l.o.g)
µ′
1 ̸= µ1. Then µ′
1 must be compatible with µ1, since otherwise it would follow
that µ1 ∪µ2 ̸∼µ′
1 ∪µ′
2. This observation contradicts Corollary 4.1, which states
that each two distinct mappings obtained when evaluating an f
A+ expressions are
incompatible (and clearly A+
1 ∈f
A+). (2) Let f
A+ := f
A+
1 \ f
A+
2 . The claim follows easily
by application of the induction hypothesis and the observation that the multiplicity
of each mapping in Ω⊆Ω1 is either identical to its multiplicity in m1 or equals
to 0. (3) Let f
A+ := f
A+
1
1 f
A+
2 = ( f
A+
1 1 f
A+
2 ) ∪( f
A+
1 \ f
A+
2 ). We ﬁx some µ ∈Ωand
distinguish three case. (3a) If mapping µ is generated by the left side subexpression
A+
1 := f
A+
1 1 f
A+
2 but not by the right side-expression A+
\ := f
A+
1 \ f
A+
2 , then the
96

4.3. From Set to Bag Algebra
claim follows analogously to case (1) of the proof. (3b) If µ is generated by A+
\
but not by A+
1, then the argumentation is similar to case (2) of the proof. Finally,
(3c) assume that µ is generated by both (i) A+
1 and (ii) A+
\ . Then (i) implies that
there are compatible mappings µ1 ∈Ω1, µ2 ∈Ω2 such that µ = µ1 ∪µ2, while
(ii) implies that µ ∈Ω1 and there is no compatible mapping to µ in Ω2. Similar
to the argumentation in case (1) we have that each pair of distinct mappings in Ω1
is incompatible, which – given that both µ1 ⊆µ ∈Ω1 and µ ∈Ω1 – implies that
µ1 = µ. This leads to a contradiction: according to (i), µ2 ∈Ω2 is compatible with
µ1 = µ, but (ii) enforces that there is no mapping in Ω2 compatible to µ. Finally,
case (4) f
A+ := σR( f
A+
1 ) follows by induction hypothesis, analogical to case (2).
2
Note that a similar (weaker) result has been stated in Proposition 1 in [AG08b],
namely that every result mapping obtained when evaluating an expression built
using the (abstract syntax operators) And, Filter, and Opt under bag semantics
has cardinality 1. When combining Corollary 4.1 and Lemma 4.6 above, it follows
that the SPARQL bag algebra fragment f
A+ has the incompatibility property:
Lemma 4.7 Every expression f
A+ in f
A+ has the incompatibility property.
2
4.3.2. Optimization Rules for Bag Algebra
We now systematically revisit the rewriting rules from Section 4.2 and investigate
which of these equivalences are valid for SPARQL bag algebra expressions in place
of set algebra expressions. We introduce the following concept.
Deﬁnition 4.10 (Equivalence Carry-over from Set to Bag Algebra) Given
an equivalence (SomeEq) for the SPARQL set algebra fragment A we say that the
equivalence carries over to SPARQL bag algebra if it holds for all expressions in A+
as well. Similarly, an equivalence ( ^
SomeEq) for fragment eA carries over to SPARQL
bag algebra if it holds for all expressions in f
A+. We denote the resulting SPARQL
bag algebra equivalences by (SomeEq+) and (
^
SomeEq+), respectively.
2
It should be clear from Lemma 4.5 that the only reason why equivalences may not
carry over to bag algebra is that some result mappings diﬀer in their multiplicity;
the set of mappings in the result is always guaranteed to be identical. Hence, to
show that an equivalence does not carry over from SPARQL set to bag algebra
we must provide an instantiation of the equivalence and a ﬁxed document D such
that, when evaluating the left and right side of the equivalence on D, we obtain a
mapping that has diﬀerent multiplicities associated in the left and right side result
mapping multi-set. To clarify this issue, we start with an example that provides an
equivalence that does not carry over to SPARQL bag algebra.
97

Chapter 4. Algebraic SPARQL Query Optimization
Example 4.16 Consider equivalence (UIdem) for SPARQL set algebra from Fig-
ure 4.1. The rule was originally established for fragment A. To show that it does
not carry over to SPARQL bag algebra, we provide an expression A ∈A+ and a
document D such that the left side of the equation (i.e., A) evaluated on D and the
right side (i.e., A ∪A) evaluated on D generate a mapping that appears with diﬀer-
ent multiplicity. Consider expression A := J(c, c, ?x)K+
D, document D := {(c, c, c)},
and observe that A ∈A+ The result of evaluating A on D is ({{?x 7→c}}, m)
with m({?x 7→c}) := 1, whereas A ∪A evaluates to ({{?x 7→c}}, m′) with
m′({?x 7→c}) := 2. Although the mapping sets coincide, the results diﬀer w.r.t. the
multiplicity that is assigned to the result mapping {?x 7→c}.
2
Idempotence and Inverse. We start with the equivalences from Figure 4.1:
Lemma 4.8 Consider the equivalences for SPARQL set algebra from Figure 4.1.
1. (UIdem) does not carry over to SPARQL bag algebra.
2. ( ^
JIdem), ( ^
LIdem), and (Inv) carry over to SPARQL bag algebra.
2
Proof of Lemma 4.8
Lemma 4.8(1): See Example 4.16 for a counterexample.
Lemma 4.8(2): To show that ( ^
JIdem) carries over to SPARQL bag algebra we
have to show that f
A+ 1 f
A+ ≡f
A+ for every expression f
A+ ∈f
A+. From Lemma 4.6
we know that the set and bag semantics coincide for f
A+ expressions and we know
that the equivalence holds under set semantics. From Deﬁnition 4.8 it follows that
f
A+ 1 f
A+ ∈f
A+, so the bag algebra equivalence ( ^
JIdem+) holds. The argumentation
for ( ^
LIdem+) is the same. Finally, equivalence (Inv+) follows easily from Lemma 4.5
and the observation that the equivalence holds under set semantics (the extracted
mapping set is empty, so there cannot be any diﬀerences in the multiplicity).2
Associativity, Commutativity, and Distributivity. Concerning the eight
equivalences from Figure 4.2 we obtain only positive results:
Lemma 4.9 The equivalences (UAss), (JAss), (UComm), (JComm), (JUDistR),
(JUDistL), (MUDistR), (LUDistR) introduced in Figure 4.2 for SPARQL set algebra
carry over to SPARQL bag algebra.
2
As a representative, we provide the proof for rewriting rule (JUDistR) below. The
proofs for the remaining equivalences are provided in Appendix C.7.
Proof of Equivalence (JUDistR+) from Lemma 4.9
We exploit Lemma 4.5, which states that the results of evaluating set and bag
algebra expressions diﬀer at most in the associated multiplicity. Given that (JUD-
istR) holds for SPARQL set algebra, it thus suﬃces to show that, for each mapping
98

4.3. From Set to Bag Algebra
µ that is contained in the left and right side result of the equivalence, the associ-
ated left and right side cardinalities of µ coincide. Let A+
1 , A+
2 , A+
3 ∈A+. We ﬁx
document D and deﬁne A+
l := (A+
1 ∪A+
2 ) 1 A+
3 , A+
r := (A+
1 1 A+
3 ) ∪(A+
2 1 A+
3 ),
A+
1∪2 := A+
1 ∪A+
2 , A+
113 := A+
1 1 A+
3 , and A+
213 := A+
2 1 A+
3 . Given a SPARQL
bag algebra expression A+
i with some index i, we denote by (Ωi, mi) the result of
evaluating A+
i on D. Now let us consider a mapping µ that is contained in both Ωl
and Ωr. Using the semantics from Deﬁnition 2.14 we rewrite ml(µ) step-by-step:
ml(µ) = P
(µ1∪2,µ3)∈{(µ∗
1∪2,µ∗
3)∈Ω1∪2×Ω3|µ∗
1∪2∪µ∗
3=µ}(m1∪2(µ1∪2) ∗m3(µ3))
= P
(µ1∪2,µ3)∈{(µ∗
1∪2,µ∗
3)∈Ω1∪2×Ω3|µ∗
1∪2∪µ∗
3=µ}((m1(µ1∪2) + m2(µ1∪2)) ∗m3(µ3))
= P
(µ1∪2,µ3)∈{(µ∗
1∪2,µ∗
3)∈Ω1∪2×Ω3|µ∗
1∪2∪µ∗
3=µ}(
(m1(µ1∪2) ∗m3(µ3)) + (m2(µ1∪2) ∗m3(µ3)))
(S3)
= P
(µ1∪2,µ3)∈{(µ∗
1∪2,µ∗
3)∈Ω1∪2×Ω3|µ∗
1∪2∪µ∗
3=µ}(m1(µ1∪2) ∗m3(µ3)) +
P
(µ1∪2,µ3)∈{(µ∗
1∪2,µ∗
3)∈Ω1∪2×Ω3|µ∗
1∪2∪µ∗
3=µ}(m2(µ1∪2) ∗m3(µ3))
(∗)
= P
(µ1,µ3)∈{(µ∗
1,µ∗
3)∈Ω1×Ω3|µ∗
1∪µ∗
3=µ}(m1(µ1) ∗m3(µ3)) +
P
(µ2,µ3)∈{(µ∗
2,µ∗
3)∈Ω2×Ω3|µ∗
2∪µ∗
3=µ}(m2(µ2) ∗m3(µ3))
= m113(µ) + m213(µ)
= mr(µ),
where (S3) stands for the application of rule (S3) for sum expression stated in
Proposition C.1 in Appendix C.7 and step (∗) follows by semantics of operator ∪.2
Projection Pushing. All projection pushing rules carry over to bag semantics:
Lemma 4.10 The equivalences (PBaseI), (PBaseII), (PUPush), (PJPush), (PM-
Push), (PLPush), (PFPush), and (PMerge) introduced in Figure 4.3 for SPARQL
set algebra carry over to SPARQL bag algebra.
2
The proofs of the rules are systematic rewritings of sum expressions, similar in
style to the proof of equivalence (JUDistR) discussed previously. The interested
reader will ﬁnd the technical details in Appendix C.8.
Filter Decomposition, Elimination, and Pushing. Concerning the ﬁlter ma-
nipulation rewriting rules from Figure 4.4 we observe the following:
Lemma 4.11 Consider the equivalences for SPARQL set algebra from Figure 4.4.
1. Equivalence (FDecompII) does not carry over to SPARQL bag algebra.
2. (FDecompI), (FReord), (FBndI), (FBndII), (FBndIII), (FBndIV), (FUPush),
(FMPush), (FJPush), and (FLPush) carry over to SPARQL bag algebra.
2
99

Chapter 4. Algebraic SPARQL Query Optimization
Proof of Lemma 4.11
Lemma 4.11(1): As a counterexample for the equivalence consider the document
D := {(c, c, c)} and the bag algebra expressions A+
l := σ(¬?x=a)∨(¬?x=b)(J(c, c, ?x)K+
D)
and A+
r := σ¬?x=a(J(c, c, ?x)K+
D) ∪σ¬?x=b(J(c, c, ?x)K+
D). The result obtained when
evaluating expression Al on D is ({{?x 7→c}}, ml) with ml({?x 7→c}) := 1, but
expression Ar evaluates on D to ({{?x 7→c}}, mr) with mr({?x 7→c}) := 2.
Lemma 4.11(2): We only prove equivalence (FJPush+) here; proof sketches for the
remaining rules can be found in Appendix C.9. Again we exploit that, by Lemma 4.5,
the results of evaluating set and bag SPARQL algebra expressions diﬀer at most
w.r.t. the associated multiplicity, i.e. we show that, for a ﬁxed mapping µ that is
contained in the left and right side of the equivalence, the associated left and right
side cardinalities for the mapping coincide. We ﬁx document D. As in previous
proofs, given a SPARQL bag algebra expression A+
i with some index i we denote
by (Ωi, mi) the mapping multi-set obtained when evaluating A+
i on D.
Let A+
1 , A+
2 ∈A+ and R be a ﬁlter condition such that for all ?x ∈vars(R):
?x ∈cVars(A1)∨?x ̸∈pVars(A2). Put A+
l := σR(A+
1 1 A+
2 ), A+
r := σR(A+
1 ) 1 A+
2 ,
A+
112 := A+
1 1 A+
2 , and A+
σ1 := σR(A+
1 ). Consider a mapping µ that is contained
in the result of evaluating A+
l and A+
r . Clearly it holds that µ |= R and µ ∈Ω112.
Combining these observations with the semantics from Deﬁnition 2.10 we obtain
ml(µ) = m112(µ)
= P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|µ∗
1∪µ∗
2=µ}(m1(µ1) ∗m2(µ2))
(∗)
= P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ωσ1×Ω2|µ∗
1∪µ∗
2=µ}(mσ1(µ1) ∗m2(µ2))
= mr(µ),
where rewriting step (∗) follows from the observation that the precondition for all
?x ∈vars(R): ?x ∈cVars(A1)∨?x ̸∈pVars(A2) implies that for all µ1 ∈Ω1, µ2 ∈Ω2
s.t. µ1 ∪µ2 = µ the mappings µ1 and µ agree on variables in vars(R), i.e. each
?x ∈vars(R) is either bound to the same value in µ1 and µ or unbound in both.2
Finally, we address the rules (FElimI) and (FElimII) from Lemma 4.3. The fol-
lowing lemma shows that these rules are also applicable under bag semantics:
Lemma 4.12 Let A be a SPARQL bag algebra expression composed of operators
1, ∪, and triple patterns of the form JtK+
D. Further let ?x, ?y ∈cVars(A). By A ?y
?x we
denote the expression obtained from A by replacing all occurrences of ?x in A by ?y;
similarly, expression A c
?x is obtained from A by replacing ?x by URI or literal c. The
following two equivalences hold.
πS\{?x}(σ?x=?y(A)) ≡πS\{?x}(A ?y
?x)
(FElimI+)
πS\{?x}(σ?x=c(A)) ≡πS\{?x}(A c
?x)
(FElimII+)
2
100

4.4. Summary of Results and Practical Implications
The proof is an adapted version of the proof of Lemma 4.3, which additionally
takes the multiplicities of the mappings into account. We omit the details.
Rewriting Closed World Negation. Finally, we investigate the equivalences
established in the context of our rewriting scheme for (simulated) negation. Like in
most cases before, our ﬁnding is that all these rules carry over to bag algebra, so
our rewriting scheme remains applicable when switching to SPARQL bag algebra:
Lemma 4.13 Rules (MReord), (MMUCorr), (MJ), (f
LJ) from Proposition 4.4 and
(FLBndI), (FLBndII) from Lemma 4.4 carry over to SPARQL bag algebra.
2
The proofs are straightforward and can be found in Appendix C.10.
4.4. Summary of Results and Practical Implications
We summarize all results established throughout this chapter in Table 4.1 at the
end of the chapter (due to space limitations, we do not repeat equivalences FElimI
and FElimII from Lemma 4.3 and 4.12). The last four columns of the table indicate
for which fragments the respective equivalence is valid. To give an example, the
check mark “✓” in column f
A+ for equivalence LIdem indicates that the equivalence
holds for every expression A ∈f
A+; the minus symbol “−” in column A+ for the
same equivalence shows that the equivalence does not hold in the general case when
choosing some A ∈A+. Clearly, all equivalences that are valid for fragment A (re-
spectively A+) are also valid for the subfragment eA (respectively f
A+). As discussed
previously in Section 4.3.2, most of the equivalences carry over from SPARQL set
to bag algebra, which is implicit in the ﬁgure whenever a check mark in column A
(respectively eA) implies a check mark in the corresponding A+ (respectively f
A+)
column. The only two equivalences that do not carry over to SPARQL bag alge-
bra are UIdem and FDecompII ; interestingly, in both cases it is the union operator
that poses problems. The observation that almost all equivalences are valid under
SPARQL bag algebra is good news from a practical point of view, since all equiv-
alences except for the two outliers can be safely used for query optimization in the
context of the oﬃcial W3C SPARQL Recommendation [spac].
We ﬁnally discuss implications and extensions of previous results for SPARQL
engines that build upon the oﬃcial W3C semantics. Our focus in this study is on
the interplay between the semantics and diﬀerent SPARQL query forms and solution
modiﬁers (cf. the discussion in Section 2.3.4). In particular, we will show that in
many cases SPARQL engines can use the simpler set semantics for query evaluation,
thus avoiding the overhead imposed by the additional multiplicity computation. We
start with a discussion of SPARQL Ask queries in the following subsection.
101

Chapter 4. Algebraic SPARQL Query Optimization
4.4.1. SPARQL ASK Queries
First recall that SPARQL Ask queries (cf. Deﬁnition 2.6) return true if the extracted
mapping set is not empty, and false otherwise. When combining and extending
previous results on algebraic SPARQL query optimization, we obtain the following.
Lemma 4.14 Let Q, Q1, Q2 be SPARQL expressions. The following claims hold.
1. JAsk(Q)KD ⇔JAsk(Q)K+
D
2. JAsk(Q1 Union Q2)KD ⇔JAsk(Q1)KD ∨JAsk(Q2)KD
3. JAsk(Q1 Opt Q2)KD ⇔JAsk(Q1)KD
4. If pVars(JQ1KD) ∩pVars(JQ2KD) = ∅then
JAsk(Q1 And Q2)KD ⇔JAsk(Q1)KD ∧JAsk(Q2)KD.
2
Proof of Lemma 4.14
Lemma 4.14(1): Follows from the semantics of Ask queries and Lemma 3.1
Lemma 4.14(2): Follows from the semantics of Ask queries and the semantics of
the Union operator (cf. Deﬁnitions 2.10 and 2.11).
Lemma 4.14(3): Follows from the semantics of Ask queries and the semantics of
the Opt operator (cf. Deﬁnitions 2.10 and 2.11). In particular, observe that for
1,
the algebraic counterpart of Opt, we have (i) JQ1KD = ∅→JQ1KD
1 JQ2KD = ∅
and (ii) if there is some µ ∈JQ1KD, then there also is some µ′ ∈JQ1KD
1 JQ2KD.
Lemma 4.14(4): Follows from the semantics of Ask queries, the semantics of
operator And, and Proposition 4.1. The crucial observation is that precondition
pVars(JQ1KD)∩pVars(JQ2KD) = ∅together with Proposition 4.1 implies that for each
pair of mappings (µ1, µ2) ∈JQ1KD × JQ2KD it holds that dom(µ1) ⊆pVars(JQ1KD),
dom(µ2) ⊆pVars(JQ2KD), and therefore dom(µ1) ∩dom(µ2) = ∅.2
The ﬁrst claim of the lemma shows that, for Ask queries, it does not matter
whether bag or set semantics is used for evaluating the inner expression. The remain-
ing claims indicate cases where we can individually evaluate subqueries, which may
help to reduce evaluation costs. For instance, to compute JAsk(Q1 Union Q2)KD
we could choose the simpler expression among Q1 and Q2 (e.g., based on a cost
function), evaluate this expression, and check if it evaluates to true. In that case,
we could skip the computation of the second expressions and directly return true.
This rule also could be combined with the union normal form proposed in [PAG06a]
(cf. the discussion in Section 4.2.3): ﬁrst, we transform the expression into a disjunc-
tion of union-free subexpressions and then decompose this disjunction by repeated
application of Lemma 4.14(2) into a sequence of ∨-connected Ask queries.
Interesting is also the rule in Lemma 4.14(3) for operator Opt, which shows that
top-level Opt-expressions can simply be replaced by the left side expression, thus
saving the cost for computing the right side expression. Note, however, that it is
generally not valid to drop Opt-expressions that are encapsulated in the query
(i.e. do not stand at the top-level), as witnessed by the following example.
102

4.4. Summary of Results and Practical Implications
Example 4.17 Consider document D := {(c, c, c)} and the SPARQL expressions
Q1 := ((c, c, ?x) Opt (c, c, ?y)) Filter (?y = c),
Q2 := (c, c, ?x) Filter (?y = c).
We can observe that JAsk(Q1)KD = true, but JAsk(Q2)KD = false.
2
Finally, Lemma 4.14(4) shows that we can individually evaluate And-connected
subqueries that do not contain shared possible variables. The beneﬁt of this rule is
twofold: ﬁrst, it allows to abort the computation if one of these subqueries, say Qi,
contains no result (i.e., if JAsk(Qi)KD = false); second, we avoid the computation of
the join between such subqueries, which can be seen as a cartesian product (because
left and right side mappings never contain shared variables). This may sustainably
reduce the size of intermediate results and speed up query evaluation. The rule may
be particularly useful for Ask queries that check for a conjunction of independent
conditions, expressed in SPARQL as a set of SPARQL expressions Q1, . . . , Qn with
pairwise distinct variables that are interconnected through operator And.
4.4.2. SPARQL DISTINCT and REDUCED Queries
Having discussed Ask queries, we now turn towards an investigation of the solu-
tion modiﬁers Distinct and Reduced, which were informally discussed earlier in
Section 2.3.4. Before presenting our results, we formally deﬁne their semantics:
Deﬁnition 4.11 (SELECT DISTINCT Query) Let Q be a SPARQL expres-
sion and S ⊂V . A SPARQL Select Distinct query is an expression of the form
Select DistinctS(Q). We extend the bag semantics from Deﬁnition 2.15 to Se-
lect Distinct queries as follows. Let (Ω+, m+) := JSelectS(Q)K+
D. We deﬁne
JSelect DistinctS(Q)K+
D := (Ω+, m), where function m is deﬁned as m(µ) := 1 if
m+(µ) ≥1 and m(µ) := 0 otherwise.
2
Note that duplicates do not occur under set semantics, so we deﬁned only the
bag semantics (and we will use the Select Distinct query form only in this
context). While Select Distinct queries eliminate duplicates from the evaluation
result, the Reduced query form gives a greater amount of freedom to the query
optimizer in that it allows to eliminate (an arbitrary number of) duplicates. From a
theoretical point of view, the result of a Reduced query can be understood as a set
of mapping sets, where each mapping set that is contained in the result describes a
possible solution. In practice, though, the query engines does not need to compute
all possible solutions, but can compute and return the solution that best ﬁts its
internal processing strategy. We next formalize Select Reduced queries.
103

Chapter 4. Algebraic SPARQL Query Optimization
Deﬁnition 4.12 (SELECT REDUCED Query) Let Q be a SPARQL expres-
sion and S ⊂V . A SPARQL Select Reduced query is an expression of the form
Select ReducedS(Q). We extend the bag semantics from Deﬁnition 2.15 to Se-
lect Reduced queries as follows. Let (Ω+, m+) := JSelectS(Q)K+
D. The solution
to query Select ReducedS(Q) is deﬁned as the set of mapping sets of the form
(Ω+, m) such that for all µ ∈M it holds that (i) m+(µ) = 0 →m(µ) = 0 and
(ii) m+(µ) > 0 →m(µ) ≥1 ∧m(µ) ≤m+(µ).
2
Note that Select Reduced queries, like Select Distinct queries, are deﬁned
only for SPARQL bag algebra. We illustrate the previous deﬁnitions by example:
Example 4.18 Consider the SPARQL expression Q := (c, c, ?x) Union (c, c, ?x),
query Q1 := Select?x(Q), Distinct query Q2 := Select Distinct?x(Q), Re-
duced query Q3 := Select Reduced?x(Q), and document D := {(c, c, c)}. Then
JQ1K+
D = ({{?x 7→c}}, m1) where for µ ∈M function m1 is deﬁned as
m1(µ) := 2 if µ = {?x 7→c} and m1(µ) := 0 otherwise,
JQ2K+
D = ({{?x 7→c}}, m2) where for µ ∈M function m2 is deﬁned as
m2(µ) := 1 if µ = {?x 7→c} and m2(µ) := 0 otherwise,
JQ3K+
D = {({{?x 7→c}}, m1), ({{?x 7→c}}, m2)}.
2
The observant reader may have noticed that there is a close connection between
Select Distinct queries for SPARQL bag semantics and simple Select queries
for set semantics. The following lemma formalizes this and other connections:
Lemma 4.15 Let Q be a SPARQL expression and S ⊂V . Then
1. JSelectS(Q)KD ∼= JSelect DistinctS(Q)K+
D,
2. JSelect DistinctS(Q)K+
D ∈JSelect ReducedS(Q)K+
D, and
3. There is (Ω, m) ∈JSelect ReducedS(Q)K+
D s.t. JSelectS(Q)KD ∼= (Ω, m).2
Proof of Lemma 4.15.
Lemma 4.15(1): Follows from the deﬁnition of Select Distinct queries and
Lemma 3.1, which shows that bag and set semantics coincide w.r.t. mapping sets.
Lemma 4.15(2): Follows from the deﬁnition of the Select Distinct and Select
Reduced query forms, i.e. it is easily shown that the deﬁnition of function m in
the Select Distinct query form satisﬁes the two conditions (i) and (ii) that are
enforced for function m in the deﬁnition of Select Reduced queries.
Lemma 4.15(3): Follows when combining claims (1) and (2) of the lemma.
2
The lemma has some direct practical implications: it shows that, for both Select
Distinct and Select Reduced queries (posed in the context of bag semantics),
engines can fall back on the simpler set semantics for query evaluation. Hence, for
104

4.5. Related Work
such queries no cardinality computation is required at all and duplicate solutions
can be discarded in each intermediate computation step. We conclude our discussion
of algebraic SPARQL rewriting with a lemma that identiﬁes another large class of
queries that can be safely evaluated using set semantics rather than bag semantics
(we reuse the notation for SPARQL fragments introduced in Section 3.2):
Lemma 4.16 Let Q ∈AFO and S ⊇pVars(JQKD). Then
1. JQKD ∼= JQK+
D and
2. JSelectS(Q)KD ∼= JSelectS(Q)K+
D.
2
Proof of Lemma 4.16
Lemma 4.16(1): Follows from the observation that Q ∈AFO+ →JQK+
D ∈f
A+
(when interpreting JQK+
D as an expression), combined with Lemma 4.6.
Lemma 4.16(2): Follows from Lemma 4.16(1) and the observation that the pro-
jection for S ⊇pVars(JQKD) does not modify the evaluation result.2
Combined with the observation that we can safely use set semantics for Ask,
Select Distinct, and Select Reduced queries, we conclude that set semantics
is applicable in the context of a large and practical class of queries. Ultimately,
engines that rely on SPARQL bag algebra for query evaluation may implement a
separate module for set semantics – with simpler data structures and possibly more
eﬃcient operations – and switch between these modules based on the above results.
4.5. Related Work
Query Optimization in the Relational Context. Query optimization for the
relational context has been a central topic of database research since its beginning.
We refer the interested reader to [JK84; Cha98; AHV] for top-level surveys of op-
timization approaches that have been proposed for relational systems; an in-depth
discussion of all these approaches is beyond the scope of this section. Most related
to our work here are probably [Hal75; SC75; Tod75], which were (to the best of our
knowledge) the ﬁrst works to study and exploit equivalences over relational algebra.
Rewriting approaches for expressions involving the relational left outer join oper-
ator, which can be seen as the relational counterpart of SPARQL operator Opt,
were investigated in [GLR97]. We emphasize that, although all these studies form
valuable groundwork for the investigations presented in this chapter, their results do
not trivially carry over to SPARQL algebra. To give an example, the ﬁlter pushing
rules from Sections 4.2.5 and 4.3.2 were inspired by the well-known ﬁlter pushing
rules for relational algebra proposed in [Hal75; Tod75]; however, the speciﬁcation of
rules over SPARQL algebra is considerably complicated by the issue of bound and
105

Chapter 4. Algebraic SPARQL Query Optimization
unbound variables (whereas in relational algebra the underlying schema is ﬁxed)
and we have seen in Section 4.2.5 that the resulting ﬁlter pushing rules for SPARQL
algebra heavily rely on our novel concepts of certain and possible variables.
SPARQL Query Optimization. Also in the context of the SPARQL query
language a variety of optimization approaches have been proposed. Closely related
to the study in this chapter is the work in [PAG06a; PAG09], which (amongst others)
presents a set of rewriting rules over SPARQL abstract syntax expressions, used to
transform SPARQL expressions into the so-called union normal form, an equivalent
representation of the expression in form of a disjunction of Union-free subqueries.
Although stated at syntax level in [PAG06a], these rules carry immediately over to
SPARQL set algebra. Apart from this goal-driven rewriting scheme (which comprises
only few algebraic equivalences) a closer investigation of [PAG06a] reveals some more
algebraic equivalences that are implicitly used in proofs there. We refer the reader
back to the beginning of Section 4.2, where we listed all equivalences established
in [PAG06a]. To conclude the discussion of the latter work, we want to remark that
the focus of [PAG06a] is not on algebraic SPARQL rewriting, but on a study of two
diﬀerent semantics and normal forms for SPARQL expressions. The initial results on
SPARQL rewriting can be seen as a by-product of the proper contributions. With
the focus on equivalences over SPARQL algebra expressions, the investigation in
this chapter was much more detailed and therefore considerably extends previous
investigations. Going beyond the rewriting of set algebra expressions, we also have
explored the rewriting of bag algebra expressions and syntax expressions evaluated
under bag semantics, which gives our results immediate practical relevance.
Another notable optimization approach for SPARQL is the reordering of graph
patterns based on selectivity estimations proposed in [BKS07; SSB+08]. The rewrit-
ing of SPARQL queries presented there has a strong focus on And-connected blocks
inside the query and relies on the commutativity and associativity of the join oper-
ator (cf. rules (JComm) and (JAss) from Figure 4.4 in our framework). In addition,
in [BKS07] the decomposition and elimination of ﬁlter expressions, similar in style
to rules (FDecompI) from Figure 4.4 and (FElimII) from Lemma 4.3 has been pro-
posed. However, there the idea is only described by example and conditions when
such rewritings are possible (which we provided in this thesis) are missing. In sum-
mary, [BKS07; SSB+08] rely on a small subset of the rules proposed in this chapter
and can be seen as a ﬁrst step towards eﬃcient SPARQL query optimization. With
our results, we lay foundations for advanced approaches beyond And-only queries.
The work in [HH07] proposes a query graph model for SPARQL. In a ﬁrst step,
the query is translated into a graphical model that reﬂects the operators and the
data ﬂow between those operators. The authors propose to manipulate the query
graph model to obtain more eﬃcient query plans using so-called transformation
rules. These rules are exempliﬁed by means of a rule that allows to merge triple
patterns that share a variable into a single operator, which can then be evaluated
more eﬃciently. Unfortunately, the discussion of further transformation rules is not
106

4.5. Related Work
subject of the paper. One may expect that our algebraic rewritings can be transferred
into the context of the query graph model (i.e., can be expressed as transformation
rules), so our algebraic results would be beneﬁcial for suchlike approaches as well.
Next, in [GGK09] rewriting rules for manipulating ﬁlter conditions at abstract
syntax level were proposed, some of which are overlapping with the ﬁlter rewriting
rules presented in this chapter. We therefore emphasize that the latter work has
been published after we made our results available (as technical report) in [SML08].
Another major line of research in the context of SPARQL query optimization
is the investigation of specialized indices for RDF data [HD05; GGL07; WKB08].
In [GGL07], an index of precomputed joins for RDF is proposed; the experimental
results show that the approach does not scale to large RDF data sets. Similar by idea,
[HD05] proposes a combination of indices for the eﬃcient processing of RDF data,
such as keyword indices and quad indices (which, in addition to the three components
of RDF triples, take the RDF document id into account). Further, [WKB08] proposes
diﬀerent combinations of linked lists to provide eﬃcient access to RDF triples and
(as a consequent enhancement of ideas found in [AMMH07]) supports fast merge
joins when computing any kind of join between two triple patterns. One might
expect that, like in the context of other data formats like the relational model or
XML, indices for RDF are an important step towards eﬃcient RDF processing, as
they provide fast data access paths. A comprehensive optimization scheme may use
them complementarily to the algebraic rules presented in this chapter and optimally
would build upon the (estimated) eﬃciency of access paths to the RDF data.
Going into a similar direction as indices, in [CPST03] the labeling of large RDF
Schema descriptions has been proposed, akin to XML labeling schemes for the ef-
ﬁcient XPath [xpa] or XQuery [xqu] processing using native relational database
systems (such as [LM01; Gru02]). The latter work studies the eﬃciency of diﬀerent
labeling schemes in order to accelerate path queries along the RDFS subsumption
hierarchy; with this goal, the approach is not SPARQL-speciﬁc, yet constitutes an
interesting idea that has not been explored before in the context of RDF data.
Much research eﬀort has been spent in processing RDF data with traditional sys-
tems, such as relational DBMSs or datalog engines [TCK05; CDES05; AMMH07;
FB08; NW08; SGK+08; SHK+08; WKB08; Pol07], thus falling back on established
optimization strategies. The ﬁrst line of research into this direction are proposals for
mappings from SPARQL or SPARQL algebra to the relational data model [Cyg05;
CLJF06]. From a more practical point of view, in [CDES05; TCK05] diﬀerent stor-
age schemes for RDF data were discussed, starting from simple triple tables (which
contain all the triples of the RDF graph), to more advanced schemes like parti-
tioning along the predicate values (thus essentially following the binary relation
view on RDF) and clustering of data. The second, predicate partitioning scheme
was reinvestigated in [AMMH07], where it is shown that in such a setting (called
Vertical Partitioning there) eﬃcient merge joins often can be exploited for the eﬃ-
cient RDF data processing using relational DBMS. Subsequent work identiﬁes dif-
107

Chapter 4. Algebraic SPARQL Query Optimization
ferent deﬁciencies of the Vertical Partitioning scheme [SGK+08; SHK+08; WKB08]
and proposes more advanced storage schemes for RDF. In fact, one can observe
that the current trend of RDF data management goes away from using (and cus-
tomizing) existing relational data management systems, to more specialized, native
solutions [NW08; WKB08; NW09]. One reason for this might be that, as shown
in [SHK+08], existing RDF data processing schemes that build upon relational sys-
tems experience severe performance bottlenecks, in particular for complex queries
(such as queries involving negation). This indicates that traditional approaches are
not laid out for the speciﬁc challenges that come along with SPARQL processing
and served as an additional motivation for the investigation in this chapter. For in-
stance, with our rewriting scheme for simulated negation presented in Section 4.2.6,
we tackle the problem of the eﬃcient evaluation of SPARQL negation queries.
Concerning the latter issue, it has been shown in [AG08a] that the algebraic
minus operation can be simulated at the syntax level, essentially using operators
Opt, Filter, and predicate bnd, as discussed in Section 4.2.6. The encoding scheme
presented there was theoretically motivated; our approach to rewriting closed world
negation presented in Section 4.2.6 is practically motivated and goes in the other
direction: we presented a rewriting scheme to transform algebra expressions involving
simulated negation into simple negation expressions using only the minus operator.
4.6. Conclusion
We have presented a large set of rewriting rules for SPARQL algebra, covering alge-
braic laws like idempotence, inverse, commutativity, associativity and distributivity,
advanced optimization techniques like ﬁlter and projection pushing, as well as the
rewriting of queries involving negation. Taken together, our rules establish a pow-
erful framework for manipulating algebra expressions. We emphasize that, although
the rules have been established in the context of SPARQL algebra, most of them
can easily be transferred to SPARQL syntax. In particular, each rule that involves
only operators 1, ∪, σ, and
1 directly carries over to the syntax level and – due
to the close connection between SPARQL syntax and SPARQL algebra stated in
Deﬁnition 2.11 (for set semantics) and Deﬁnition 2.15 (for bag semantics) – can be
expressed at syntax level using the corresponding operators And, Union, Filter,
and Opt, respectively.4 To give an example, rule (FJPush) from Figure 4.4 can be
expressed at syntax level for expressions Q1, Q2 and ﬁlter condition R as
(Q1 And Q2) Filter R ≡(Q1 Filter R) And Q2.
4Care must be taken for equivalences involving the negation operator \, which has no syntactic
counterpart, and projection π, because its syntactic counterpart, operator Select, is always
placed at the top-level of SPARQL queries (cf. Deﬁnition 2.5).
108

4.6. Conclusion
It holds whenever for all ?x ∈vars(R):?x ∈cVars(JQ1K+
D)∨?x ̸∈pVars(JQ2K+
D).
Assuming bag semantics, the correctness follows directly from the semantics in Def-
inition 2.15, which translates the left side of the equation into σR(JQ1K+
D 1 JQ2K+
D),
the right side into σR(JQ1K+
D) 1 JQ2K+
D, and both bag algebra expressions are known
to be equivalent according to rule (FJPush+) if the above precondition holds.
With this observation in mind, most of our algebraic rewriting rules also are
useful for engines that rely on evaluation mechanisms that diﬀer from the algebraic
evaluation approach (whenever they are equivalent, of course). To give a concrete
example, mapping schemes that build upon a translation of SPARQL queries into
the relational context or datalog, such as [AMMH07; Pol07; FB08; NW08; SGK+08;
SHK+08; WKB08]), may ﬁrst transform the SPARQL syntax expression according
to syntactic rules derived from our algebraic rules, then translate and evaluate it.
We again want to emphasize that, although we picked up several ideas from es-
tablished relational algebra optimization approaches, such as ﬁlter and projection
pushing, most equivalences fundamentally rely on characteristics of SPARQL al-
gebra. With the novel concepts of possible and certain variables we introduced a
useful tool to deal with these characteristics. The integration of our rules into an
automated optimization scheme based on heuristics and/or cost estimation func-
tions is left as future work. In our opinion, nothing speaks against the assumption
that SPARQL optimizers will beneﬁt from our rewriting techniques in the same way
SQL optimizers beneﬁt from relational algebra equivalences.
109

Chapter 4. Algebraic SPARQL Query Optimization
Group
Rule
Name
A
eA
A+
g
A+
I
A ∪A ≡A
UIdem
✓
✓
−
−
A 1 A ≡A
JIdem
−
✓
−
✓
A
1 A ≡A
LIdem
−
✓
−
✓
A \ A ≡∅
Inv
✓
✓
✓
✓
II
(A1 ∪A2) ∪A3 ≡A1 ∪(A2 ∪A3)
UAss
✓
✓
✓
✓
(A1 1 A2) 1 A3 ≡A1 1 (A2 1 A3)
JAss
✓
✓
✓
✓
III
A1 ∪A2 ≡A2 ∪A1
UComm
✓
✓
✓
✓
A1 1 A2 ≡A2 1 A1
JComm
✓
✓
✓
✓
IV
(A1 ∪A2) 1 A3 ≡(A1 1 A3) ∪(A2 1 A3)
JUDistR
✓
✓
✓
✓
A1 1 (A2 ∪A3) ≡(A1 1 A2) ∪(A1 1 A3)
JUDistL
✓
✓
✓
✓
(A1 ∪A2) \ A3 ≡(A1 \ A3) ∪(A2 \ A3)
MUDistR
✓
✓
✓
✓
(A1 ∪A2)
1 A3 ≡(A1
1 A3) ∪(A2
1 A3)
LUDistR
✓
✓
✓
✓
V
πpVars(A)∪S(A) ≡A
PBaseI
✓
✓
✓
✓
πS(A) ≡πS∩pVars(A)(A)
PBaseII
✓
✓
✓
✓
Let S′ := S ∪(pVars(A1) ∩pVars(A2)), S′′ := pVars(A1) ∩pVars(A2). Then
πS(A1 ∪A2) ≡πS(A1) ∪πS(A2)
PUPush
✓
✓
✓
✓
πS(A1 1 A2) ≡πS(πS′(A1) 1 πS′(A2))
PJPush
✓
✓
✓
✓
πS(A1 \ A2) ≡πS(πS′(A1) \ πS′′(A2))
PMPush
✓
✓
✓
✓
πS(A1
1 A2) ≡πS(πS′(A1)
1 πS′(A2))
PLPush
✓
✓
✓
✓
πS(σR(A)) ≡πS(σR(πS∪vars(R)(A)))
PFPush
✓
✓
✓
✓
πS1(πS2(A)) ≡πS1∩S2(A)
PMerge
✓
✓
✓
✓
VI
σR1∧R2(A) ≡σR1(σR2(A))
FDecompI
✓
✓
✓
✓
σR1∨R2(A) ≡σR1(A) ∪σR2(A)
FDecompII
✓
✓
−
−
σR1(σR2(A)) ≡σR2(σR1(A))
FReord
✓
✓
✓
✓
σbnd(?x)(A) ≡A, if ?x ∈cVars(A)
FBndI
✓
✓
✓
✓
σbnd(?x)(A) ≡∅, if ?x ̸∈pVars(A)
FBndII
✓
✓
✓
✓
σ¬bnd(?x)(A) ≡∅, if ?x ∈cVars(A)
FBndIII
✓
✓
✓
✓
σ¬bnd(?x)(A) ≡A, if ?x ̸∈pVars(A)
FBndIV
✓
✓
✓
✓
VII
σR(A1 ∪A2) ≡σR(A1) ∪σR(A2)
FUPush
✓
✓
✓
✓
σR(A1 \ A2) ≡σR(A1) \ A2
FMPush
✓
✓
✓
✓
If for all ?x ∈vars(R) it holds that ?x ∈cVars(A1) ∨?x ̸∈pVars(A2), then
σR(A1 1 A2) ≡σR(A1) 1 A2
FJPush
✓
✓
✓
✓
σR(A1
1 A2) ≡σR(A1)
1 A2
FLPush
✓
✓
✓
✓
Prop. 4.4
(A1 \ A2) \ A3 ≡(A1 \ A3) \ A2
MReord
✓
✓
✓
✓
(A1 \ A2) \ A3 ≡A1 \ (A2 ∪A3)
MMUCorr
✓
✓
✓
✓
A1 \ A2 ≡A1 \ (A1 1 A2)
MJ
✓
✓
✓
✓
A1
1 A2 ≡A1
1 (A1 1 A2)
LJ
−
✓
−
✓
Lemma 4.4
Let ?x ∈cVars(A2) \ pVars(A1) be a variable. Then
σ¬bnd(?x)(A1
1 A2) ≡A1 \ A2
FLBndI
✓
✓
✓
✓
σbnd(?x)(A1
1 A2) ≡A1 1 A2
FLBndII
✓
✓
✓
✓
Table 4.1.: Survey of algebraic equivalences: R, R1, R2 are conditions, S, S1, S2 sets
of variables. The four algebra fragment columns A, eA, A+, f
A+ indicate
fragments for which the rules are valid (✓) or invalid (−).
110

Chapter 5.
Constraints for RDF and Semantic
SPARQL Query Optimization
Roy:
“So we’re done with SPARQL query optimization. What’s next?”
Moss: “Hold on a second, we only discussed algebraic optimization.
I would expect that semantic query optimization is particularly
promising in the context of SPARQL and RDFS data.”
Jen:
“Why do you think so?”
Moss: “Well... First, constraints may compensate for the homogeneous
structure of RDF databases. Second, there are already some
implicit constraints by the semantics of RDFS, right?”
Roy:
“Oh, I see... Sounds interesting! Let’s have a look at this!”
The study of integrity constraints such as keys, foreign keys, and functional de-
pendencies (also known as data dependencies in the literature) has received con-
siderable attention in the relational context; to give only few examples, early work
in this area includes [Cod70; Cod71; Cod74; HM75; Che76; Cod79]. Integrity con-
straints ﬁx vital relationships between data entities that must hold on every database
instance and that way restrict the state space of the database. The importance
of such constraints is twofold. First of all, they play a major role in schema de-
sign [Cod71; Cod74; Che76], where diﬀerent types of normals forms and normaliza-
tion techniques based on data dependencies have been developed, with the goal to
avoid insertion, update, and deletion anomalies (cf. [Cod71; Cod74; AHV]). Second,
beyond their importance for schema design and normalization it is well-known that
integrity constraints are valuable input to query optimizers: the knowledge about
relationships between data entities often allows to transform the original query into
queries that are equivalent on every database that satisﬁes the set of integrity con-
straints, but can be evaluated more eﬃciently (see e.g. [Kin81; BV84; CGM90]).
Coming along with their central role, data dependencies have become an inherent
part of relational database systems, where they are implicit in primary and foreign
key speciﬁcations. In addition, the SQL92 standard provides the Create Asser-
tion statement, which allows to add user-deﬁned constraints on demand. Due to
their importance in the relational model, dependencies also have been studied in the
111

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
context of other data formats like XML (see e.g. [FL01; FS03]), OWL [MHS07], and
for deductive databases (see e.g. [CGM90; CGL09]). In this chapter, we propose in-
tegrity constraints for RDF, highlight their relations to the RDFS inferencing rules,
and investigate the role of the SPARQL query language in such a setting. We discuss
three major aspects of RDF constraints, which will be sketched in the following.
Firstly, motivated by dependencies that are commonly encountered in the rela-
tional context and XML, we propose and formalize several types of data depen-
dencies for RDF, such as keys, foreign keys, and cardinality constraints. Comple-
mentarily, we also address the speciﬁc needs of the RDFS data format and propose
constraints like subclass, subproperty, property domain, and property range speci-
ﬁcations. We formalize all these dependencies in ﬁrst-order logic and classify them
as tuple-generating [BV84], equality-generating [BV84], and disjunctive embedded
dependencies [DT05], showing that many practical RDF(S) constraints fall into con-
straints classes that have been studied before in the relational context and XML.
Secondly, we investigate the role of SPARQL as a constraint language. We argue
that a natural way to check if a constraint holds on some RDF document D is to write
a SPARQL Ask query that checks for constraint violations in the input document.
We then investigate whether, given a constraint in form of a ﬁrst-order logic sentence
as input, it is always possible to write a SPARQL Ask that implements this task.
Our investigation reveals that there are some minor issues that make it impossible
to check ﬁrst-order logic sentences with SPARQL in the general case. In response,
we extend SPARQL by so-called constant mappings (which can be understood as
the natural counterparts of constants in relational algebra) to a language which we
call SPARQLC and prove that every ﬁrst-order sentence can be checked when using
this extended version of the query language. The proof of this result makes the
close connection between SPARQLC and ﬁrst-order logic sentences (over a ternary
schema, which stores all the triples of the RDF database) explicit.
Finally, in the third part of the chapter we complement the study of algebraic
SPARQL query optimization from the previous chapter with a comprehensive ap-
proach to constraint-based query optimization, also known as semantic query op-
timization (SQO), for the SPARQL query language. As sketched previously, the
central idea behind SQO is to exploit integrity constraints in order to ﬁnd more ef-
ﬁcient query evaluation plans that yield the same result on every database instance
that satisﬁes the constraints. The data dependencies that are used for the optimiza-
tion process could be provided by the user, be automatically extracted from the
underlying database, or – in case SPARQL is evaluated on top of an RDFS infer-
ence system – may be implicitly given by the semantics of the RDFS vocabulary,
which implies a variety of correlations among entities in the RDF database, such as
subclass or subproperty relationships (cf. the discussion in Section 2.2.2).
The ﬁrst part of our semantic optimization approach for SPARQL is to translate
SPARQL And-blocks inside queries into conjunctive queries over a ternary relation
that is assumed to store all RDF triples. This allows us to optimize the individ-
112

ual (conjunctive) subqueries using the well-known chase algorithm [MMS79; JK82;
BV84], which has been proposed 25+ years ago to check equivalence between con-
junctive queries under a set of integrity constraints. In particular, we fall back on
the so-called Chase & Backchase (C&B) algorithm [DPT06] for query optimization,
which builds upon the standard chase and enumerates all minimal equivalent (under
the given set of constraints) queries. The minimized conjunctive queries can then
easily be translated back into SPARQL, which gives us minimized SPARQL queries.
The second part of our SQO scheme is highly SPARQL-speciﬁc and goes beyond
the optimization of SPARQL And-only queries and And-connected subqueries. We
provide a collection of optimization rules for diﬀerent SPARQL operators that al-
low us to rewrite and optimize queries with more complex operator constellations.
Motivated by the high complexity of operator Opt established in Section 3.2.3, we
put a special focus on optimizing queries that involve Opt expressions. To give an
example, we propose a rewriting rule that allows to replace operator Opt by op-
erator And in cases where the expression inside the Opt clause is implied by the
constraint set. In summary, the main contributions of this chapter are the following.
• We formalize a set of practically relevant constraints for RDF in ﬁrst-order
logic, showing that these constraints fall into existing constraint classes (such
as tuple-generating, equality-generating, or embedded dependencies) that have
been studied extensively in the context of relational databases and XML.
• We investigate the role of SPARQL as a constraint language. Our results show
that, with only minor extensions, the SPARQL query language can be used
to specify and check all RDF constraints that can be expressed by ﬁrst-order
sentences. This result shows that SPARQL is an excellent candidate for dealing
with constraints. From a practical point of view, it motivates the extension of
SPARQL by a query form that allows to enforce user-deﬁned constraints over
RDF databases, akin to SQL Create Assertion statements for RDBMSs.
• We present an SQO scheme for SPARQL that can be used to optimize queries
under a set of constraints over the RDF database. Falling back on the well-
known Chase & Backchase algorithm from [DPT06], we derive guarantees for
cases in which we can ﬁnd minimal equivalent subqueries. Going beyond And-
only queries, we also tackle SPARQL-speciﬁc desiderata, such as the optimiza-
tion of queries that involve operators Filter and Opt.
Structure. We revisit the necessary background from ﬁrst-order logic, relational
databases, constraints, and the chase algorithm in Section 5.1. Next, in Section 5.2
we introduce and classify diﬀerent types of constraints for RDF data. Section 5.3
investigates the role of SPARQL as a constraint language. Our semantic query op-
timization scheme for SPARQL is presented in Section 5.4. We wrap up with a
discussion of related work and a short conclusion in Sections 5.5 and 5.6.
113

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
5.1. Preliminaries: First-order Logic, Relational
Databases, Constraints, and Chase
In the following we introduce the necessary background for the study in the remain-
der of this chapter. We start with preliminaries from ﬁrst-order logic in Section 5.1.1,
then turn towards relational databases and conjunctive queries in Section 5.1.2, pro-
vide background on integrity constraints and their formalization in ﬁrst-order logic
in Section 5.1.3, and introduce the chase algorithm in Section 5.1.4.
5.1.1. First-order Logic
Our introduction to ﬁrst-order logic follows the formalization in [EFT94]. We de-
ﬁne three pairwise disjoint, inﬁnite sets: the set of constants ∆, the set of labeled
nulls ∆null, and the set of variables VR.1 Usually, we use identiﬁers x, y, z to denote
variables in VR, letters c, d for constants from ∆, and n1, n2, . . . to denote elements
from the set ∆null of labeled nulls. We start with the deﬁnition of signatures:
Deﬁnition 5.1 (Relational Signature) A relational signature R is a ﬁnite set of
relational symbols {R1, ..., Rn}, where we assign to each relational symbol Ri ∈R
a number ar(Ri) ∈N, called arity of Ri. When used in the context of relational
databases, we often call a relational signature database schema.
We use the term position to refer to the positions of a relational symbol Ri ∈R.
If ar(Ri) = n, then Ri has n positions, which we denote by R1
i , . . . , Rn
i .
2
Building upon relational signatures, we next introduce the concept of R-formulas:
Deﬁnition 5.2 (R-formula) Let R be a relational signature. An R-formula is an
expression that is obtained by a ﬁnite number of applications of the following rules.
• If t1, t2 ∈∆∪VR, then t1 = t2 is an R-formula.
• If R ∈R and t1, . . . , tar(R) ∈∆∪VR, then R(t1, . . . , tar(R)) is an R-formula.
• If ϕ is an R-formula, then ¬ϕ is an R-formula.
• If ϕ, ψ are R-formulas, then (ϕ ∨ψ) and (ϕ ∧ψ) are R-formulas.
• If ϕ is an R-formula and x ∈VR, then ∃xϕ and ∀xϕ are R-formulas.
We call R-formulas of the form Ri(t1, . . . , tar(Ri)) (i.e., those obtained by applica-
tion of the second bullet) R-atoms, formulas of the form t1 = t2 equality R-formulas,
and formulas of the form ¬t1 = t2 non-equality R-formulas.
2
We next deﬁne free variables in R-formulas and the related notion of sentences:
1We use VR to distinguish this set from the set V of SPARQL variables.
114

5.1. Preliminaries: First-order Logic, Relational Databases, Constraints, and Chase
Deﬁnition 5.3 (Free Variables of R-formulas) For any R-formula ϕ, the set
of free variables, free(ϕ), is inductively deﬁned as follows.
free(t1 = t2)
:= {t1, t2} ∩VR
free(R(t1, ..., tar(R))) := {t1, ..., tar(R)} ∩VR
free(¬ψ)
:= free(ψ)
free((ψ1 ∨ψ2))
:= free(ψ1) ∪free(ψ2)
free((ψ1 ∧ψ2))
:= free(ψ1) ∪free(ψ2)
free(∀xψ)
:= free(ψ)\{x}
free(∃xψ)
:= free(ψ)\{x}
2
Deﬁnition 5.4 (R-sentence) Let ϕ be an R-formula. Then ϕ is called R-sentence,
or simply sentence if R is known from the context, iﬀfree(ϕ) = ∅.
2
Structures, Interpretations, and Satisfaction. Having introduced the syntax
of ﬁrst-order formulas, we now come to a formal deﬁnition of their semantics.
Deﬁnition 5.5 (R-structure) An R-structure is a pair I := (∆∪∆null, a), where
symbol a denotes a function that is deﬁned on R and assigns to every relational
symbol R ∈R a subset of (∆∪∆null)ar(R), i.e. a(R) ⊆(∆∪∆null)ar(R).
2
We introduce some additional notation. Given a relational symbol R ∈R, we
often write RI instead of a(R). Further, by R(t1, . . . , tar(R)) we denote the fact
that (t1, . . . , tar(R)) ∈RI. Given a signature R := {R1, . . . , Rn}, we shall use the pre-
vious notation and denote a structure I := (∆∪∆null, a) as (∆∪∆null, RI
1∪· · ·∪RI
n),
where we represent each RI
i (for 1 ≤i ≤n) as the set of its facts.
Example 5.1 Let R := {S, T} with ar(S) := 1 and ar(T) := 2. Consider the R-
structure I := (∆∪∆null, a) where a(S) := {(a), (b)}, a(T) := {(a, b)}. The structure
contains three facts, namely S(a), S(b), and T(a, b). Using the notation introduced
above, we denote I as (∆∪∆null, {S(a), S(b), T(a, b)}).
2
Deﬁnition 5.6 (Variable Assignment, Interpretation) An assignment of vari-
ables in an R-structure I is a mapping γ : VR →∆∪∆null. An R-interpretation I
is a pair (I, γ), where I is an R-structure and γ an assignment of variables in I.
If γ is an assignment of variables in I, then we denote by γ a
x the assignment that
maps x to a and coincides with γ for all variables diﬀerent from x. Similarly, given
an interpretation I := (I, γ), we write I a
x for the interpretation (I, γ a
x).
2
We are now in the position to deﬁne the semantics of R-formulas. The deﬁnition
is folklore and works by induction on the structure of such formulas:
115

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
Deﬁnition 5.7 (Satisfaction of R-formulas by R-interpretations) Consider
an R-interpretation I := (I, γ). For every x ∈VR we deﬁne I(x) := γ(x) and
for every constant c ∈∆we put I(c) := c. We deﬁne the notion of satisfaction
of R-formulas by R-interpretations by induction on the structure of R-formulas:
• I |= t1 = t2 ⇔I(t1) = I(t2),
• I |= R(t1, ..., tar(R)) ⇔(I(t1), ..., I(tar(R))) ∈RI,
• I |= ¬ϕ ⇔not I |= ϕ,
• I |= (ϕ ∨ψ) ⇔I |= ϕ or I |= ψ,
• I |= (ϕ ∧ψ) ⇔I |= ϕ and I |= ψ,
• I |= ∃xϕ ⇔there is some a ∈∆∪∆null such that I a
x |= ϕ, and
• I |= ∀xϕ ⇔for all a ∈∆∪∆null it holds that I a
x |= ϕ.
For a set Σ of R-formulas we write I |= Σ iﬀfor all ϕ ∈Σ : I |= ϕ.
2
From the coincidence lemma in classical logic [EFT94] we know that if an R-
formula ϕ is an R-sentence, then for every R-structure I and each two assignments
of variables γ1, γ2 in I it holds that
(I, γ1) |= ϕ ⇔(I, γ2) |= ϕ.
(5.1)
This equation implies that for every sentence ϕ and R-structure I there is either
no variable assignment γ such that (I, γ) |= ϕ or it holds that (I, γ) |= ϕ for every
variable assignment γ. If the second case applies, we shall also write I |= ϕ, thus
implicitly stating that (I, γ) |= ϕ for every variable assignment γ.
Notational Conventions and Shortcuts. We deﬁne the common shortcut
(ϕ →ψ) := (¬ϕ ∨ψ) and write t1 ̸= t2 for ¬t1 = t2. Usually, we abbreviate
a sequence of quantiﬁers ∃x1 . . . ∃xnϕ (or ∀x1 . . . ∀xnϕ) as ∃x1, . . . , xnϕ (respec-
tively, ∀x1, . . . , xnϕ). Similarly, if x := (x1, . . . , xn) is a tuple of variables, we often
write ∃xϕ and ∀xϕ in place of ∃x1 . . . ∃xnϕ and ∀x1 . . . ∀xnϕ, respectively.
To improve the readability of R-formulas, we will use brackets liberally. We assume
that ∧and ∨are left-associative and deﬁne the following precedence order: (i) ¬
binds stronger than ∧, ∨; (ii) ∧, ∨bind stronger than ∀, ∃; (iii) ∀, ∃bind stronger
than →. Occasionally, we use brackets redundantly, to facilitate reading.
If a formula ϕ contains at most variables from x, we denote this by writing ϕ(x);
note that this does not mean that all variables occurring in x must appear in ϕ.
5.1.2. Relational Databases and Conjunctive Queries
We introduced the concept of database schema (also called relational signature) in
Deﬁnition 5.1. Complementarily, we now deﬁne the notion of database instances:
116

5.1. Preliminaries: First-order Logic, Relational Databases, Constraints, and Chase
Deﬁnition 5.8 (Database Instance) A database instance I over some database
schema R := {R1, . . . , Rn} is a set of relational atoms with predicate symbols
from R that contain only elements from ∆∪∆null in its positions, i.e. atoms of
the form R(a1, ..., aar(R)) where R ∈R and a1, ..., aar(R) ∈∆∪∆null.
The domain of an instance I, dom(I), is the set of elements from ∆∪∆null that
appear in I, i.e. dom(I) = {a1, ..., aar(R) | R(a1, ..., aar(R)) ∈I}.
2
Each database instance I can be understood as an R-structure (∆∪∆null, I),
where I contains the set of all facts in the structure. Given a sentence ϕ and an
instance I, we therefore shall write I |= ϕ as a shortcut for (∆∪∆null, I) |= ϕ.
Conjunctive Queries. The deﬁnition of conjunctive queries is folklore:
Deﬁnition 5.9 (Conjunctive Query) A conjunctive query (CQ) is an expression
of the form q : ans(x) ←ϕ(x, y), where (a) ϕ(x, y) is a conjunction of R-atoms,
(b) x, y are tuples of variables and constants, and (c) every variable that occurs
in x also occurs in ϕ(x, y). We denote by body(q) the set of R-atoms in ϕ(x, y). The
evaluation result of q on instance I is deﬁned as q(I) := { a | I |= ∃yϕ(a, y) }.
2
5.1.3. Relational Constraints
We use the terms (integrity) constraints and (data) dependencies interchangeably.
Following [Nic78; Deu08], we encode constraints as ﬁrst-order sentences. We next
introduce diﬀerent classes of constraints proposed in previous work [Fag77; Fag82;
BV84; DT01; DT05], highlight their expressiveness, relevance in practice, and their
interrelationships. We start with equality- and tuple-generating dependencies:
Deﬁnition 5.10 (Equality-generating Dependency) Let x, y be tuples of vari-
ables. An equality-generating dependency (EGD) is a ﬁrst-order sentence
ϕ := ∀x(φ(x) →xi = xj)
such that (a) φ(x) is a non-empty conjunction of R-atoms, (b) xi, xj are variables
from x, and (c) xi, xj occur in φ(x). We denote by body(ϕ) the set of R-atoms
in φ(x) and by head(ϕ) the set {xi = xj}.
2
Deﬁnition 5.11 (Tuple-generating Dependency) Let x, y be tuples of vari-
ables. A tuple-generating dependency (TGD) is a ﬁrst-order sentence
ϕ := ∀x(φ(x) →∃yψ(x, y))
such that (a) φ(x) and ψ(x, y) are conjunctions of R-atoms, (b) ψ(x, y) is not
empty, (c) φ(x) is possibly empty, and (d) all variables from x that occur in ψ(x, y)
also occur in φ(x). We denote by body(ϕ) the set of R-atoms in φ(x) and by head(ϕ)
the set of R-atoms in ψ(x, y).
2
117

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
TGDs can be seen as a generalization of join and inclusion dependencies, while
EGDs generalize functional dependencies (the interested reader will ﬁnd more back-
ground on join, inclusion, and functional dependencies in [AHV]). We illustrate
equality-generating and tuple-generating dependencies by a small example:
Example 5.2 Let R := {R, S} with ar(R) := 2 and ar(S) := 2 be a relational
signature. Further consider the EGD ϕ1 and the TGD ϕ2 deﬁned as
ϕ1 := ∀x, y1, y2(R(x, y1) ∧R(x, y2) →y1 = y2),
ϕ2 := ∀x, y(R(x, y) →∃zS(y, z)).
The dependency ϕ1 can be understood as a primary key constraint. Informally
speaking, it states that for each database instance I |= ϕ1 the value in the ﬁrst
position R1 of predicate R uniquely identiﬁes tuples in RI. It is well-known that (in
the relational context) primary keys are special case of functional dependencies.
The TGD ϕ2 represents a foreign key constraint (which is a special case of inclusion
dependencies). It enforces that each value that appears in position R2 of some tuple
tR ∈RI is also encountered in the ﬁrst position of some tuple tS ∈SI.
2
Having introduced EGDs and TGDs, we go one step further and deﬁne the class
of so-called embedded dependencies, originally proposed in [Fag77; Fag82]:
Deﬁnition 5.12 (Embedded Dependency) Let x, y be tuples of variables. An
embedded dependency (ED) is a ﬁrst-order sentence
ϕ := ∀x(φ(x) →∃yψ(x, y))
such that (a) φ(x) is a possibly empty conjunction of R-atoms, (b) ψ(x, y) is a
non-empty conjunction of R-atoms and equality R-formulas, and (c) each variable
from x that occurs in ψ(x, y) also occurs in φ(x).
2
It is easy to see that both EGDs and TGDs are special cases of EDs: EGDs
are obtained from EDs when restricting to a single equality R-formula in the head,
while TGDs are obtained when restricting to a conjunction of R-atoms in the head of
the embedded dependency. Conversely, it is well-known that every set of embedded
dependencies can be expressed as a set of TGDs and EGDs [Fag82].
As argued in previous work such as [Deu08; DPT06], embedded dependencies
comprise most of the natural constraints used in practical database scenarios, such
as keys, join dependencies, inclusion dependencies, and functional dependencies. As
a contribution, we will later discuss natural counterparts of relational constraints
for RDF in Section 5.2, showing that embedded dependencies also capture many
natural constraints that appear in the context of RDF data.
As an extension of EDs, we ﬁnally deﬁne disjunctive embedded dependencies [DT01]
and disjunctive embedded dependencies with non-equality [DT05]:
118

5.1. Preliminaries: First-order Logic, Relational Databases, Constraints, and Chase
Deﬁnition 5.13 (Disjunctive Embedded Dependency) Let x, y be tuples of
variables. A disjunctive embedded dependency (DED) is a ﬁrst-order sentence
ϕ := ∀x(φ(x) →Wl
i=1(∃yiψi(x, yi)))
such that (a) φ(x) is a possibly empty conjunction of R-atoms, (b) ψi(x, yi) is a
non-empty conjunction of R-atoms and equality R-formulas, and (c) each variable
from x that occurs in ψi(x, yi) (for some 1 ≤i ≤l) also occurs in φ(x).
The class of disjunctive embedded dependencies with non-equality (DED̸=) is ob-
tained when we replace condition (b) above by the condition (b̸=) ψi(x, yi) is a non-
empty conjunction of R-atoms, equality R-formulas, and non-equality R-formulas.2
5.1.4. The Chase Algorithm
The chase procedure [MMS79; BV84] is a fundamental algorithm that has been
successfully applied in diﬀerent areas of database research (we will list possible
application scenarios when discussing related work in Section 5.5). Our focus here is
the use of the algorithm for SPARQL query optimization under data dependencies.
To achieve this goal, we exploit known results and properties of the chase algorithm
that have been established in the context of conjunctive query optimization under
data dependencies [ASU79; JK82; DPT06]. We will restrict ourselves to chasing sets
of TGDs and EGDs here (which are known to be as expressive as sets of EDs, as
discussed previously in Section 5.1.3). Note, however, that the chase also has been
successfully applied in the context of DEDs (see e.g. [DT05]), so the techniques
presented in this chapter can easily be extended to this larger class of constraints.
The core idea of the chase is simple: given a set of dependencies and a database
instance as input, the algorithm successively ﬁxes constraint violations in the in-
stance. When using the chase for query optimization under a set of dependencies,
this means that the query – interpreted as database instance – is provided as input,
together with the constraints that are known to hold on every database instance.
One major problem with the chase is that – given an arbitrary set of constraints
– it does not terminate in the general case; even worse, the termination problem is
undecidable in general, even for a ﬁxed instance [DNR08]. We start with a formal
deﬁnition of the chase algorithm and come back to the issue of chase termination
later. As a prerequisite, we give a formal deﬁnition of homomorphisms:
Deﬁnition 5.14 (Homomorphism) A homomorphism from database instance I1
to database instance I2 is a mapping ν : ∆∪∆null →∆∪∆null such that the
following three conditions hold: (1) if c ∈∆, then ν(c) = c; (2) if n ∈∆null, then
ν(n) ∈∆∪∆null; (3) if R(t1, ..., tar(R)) ∈I1, then R(ν(t1), ..., ν(tar(R))) ∈I2.
2
119

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
Using the notion of homomorphism, we are now ready to deﬁne the chase algo-
rithm. We ﬁx a set of TGDs and EGDs Σ and a database instance I.
A tuple-generating dependency ∀xϕ ∈Σ is applicable to I if there is a homomor-
phism ν from body(∀xϕ) to I and ν cannot be extended to a homomorphism ν′ ⊇ν
from head(∀xϕ) to I. In such a case, the chase step I
∀xϕ,ν(x)
−→
J is deﬁned as follows.
We deﬁne a homomorphism ν+ with the following two properties: (a) ν+ agrees
with ν on all universally quantiﬁed variables in ϕ and (b) for every existentially
quantiﬁed variable y in ∀xϕ we choose a “fresh” labeled null ny ∈∆null and deﬁne
ν+(y) := ny. Finally, we set J to I ∪ν+(head(∀xϕ)).
We say that an equality-generating dependency ∀xϕ ∈Σ is applicable to I if there
is a homomorphism ν from body(∀xϕ) to I and ν(xi) ̸= ν(xj). In such a case the
chase step I
∀xϕ,ν(x)
−→
J is deﬁned as follows. We set J to be
• I except that all occurrences of ν(xj) are substituted by ν(xi), if ν(xj) ∈∆null,
• I except that all occurrences of ν(xi) are substituted by ν(xj), if ν(xi) ∈∆null,
• undeﬁned, if both ν(xj) and ν(xi) are constants. In this case we say that the
chase fails.
A chase sequence is an exhaustive application of applicable constraints
I0
ϕ0,a0
−→I1
ϕ1,a1
−→I2
ϕ2,a2
−→. . .,
where we impose no strict order on what constraint must be applied in case sev-
eral constraints are applicable. If the chase sequence is ﬁnite, say Ir being its ﬁnal
element, the chase terminates and its result IΣ
0 is deﬁned as Ir. Although diﬀerent
orders of application of applicable constraints may lead to diﬀerent chase results, it
is common knowledge that two diﬀerent chase orders always lead to homomorphi-
cally equivalent results, if these exist (see e.g. [FKMP05]). Therefore, we write IΣ
for the result of the chase on an instance I under constraint set Σ. It has been
shown in [MMS79; JK82; BV84] that IΣ |= Σ. If a chase step cannot be performed
(e.g., because a homomorphism would have to equate two constants, see bullet three
above) or in case of an inﬁnite chase sequence, the chase result is undeﬁned.
Example 5.3 Consider the database schema R := {E, S} with ar(E) := 2 and
ar(S) := 1, where E stores graph edges and S stores a subset of graph nodes. Put
α1 := ∀x, y(E(x, y) →E(y, x)),
α2 := ∀x(S(x) →∃yE(x, y)),
α3 := ∀x, y(S(x) ∧E(x, y) ∧E(y, x) →x = y),
and Σ := {α1, α2, α3}. One chase sequence for I := {S(a), E(c, d)} with Σ is
120

5.1. Preliminaries: First-order Logic, Relational Databases, Constraints, and Chase
I := {S(a), E(c, d)}
α1,(c,d)
−→{S(a), E(c, d), E(d, c)}
α2,(a)
−→{S(a), E(c, d), E(d, c), E(a, n1)}
α1,(a,n1)
−→
{S(a), E(c, d), E(d, c), E(a, n1), E(n1, a)}
α3,(a,n1)
−→
{S(a), E(c, d), E(d, c), E(a, a)} =: IΣ,
where n1 ∈∆null. Observe that IΣ satisﬁes all constraints from Σ.
2
Chase Termination. Non-termination of the chase can be caused by fresh la-
beled null values that are repeatedly created when ﬁxing constraint violations:
Example 5.4 Consider the database schema from Example 5.3, the tuple-generating
dependency α4 := ∀x, y(E(x, y) →∃zE(y, z)), and the instance I := {E(a, b)}. We
chase instance I with the constraint set Σ′ := {α4}:
I := {E(a, b)}
α4,(a,b)
−→{E(a, b), E(b, n1)}
α4,(b,n1)
−→
{E(a, b), E(b, n1), E(n1, n2)}
α4,(n1,n2)
−→
{E(a, b), E(b, n1), E(n1, n2), E(n2, n3)}
α4,(n2,n3)
−→
. . . ,
where n1, n2, . . . are fresh labeled nulls. The chase does not terminate.
2
Addressing the issue of non-terminating chase sequences, suﬃcient conditions for
the constraint set have been proposed that guarantee termination on every database
instance [FKMP05; DNR08; SML08; MSL09d]. Whenever such conditions apply,
one can safely chase without risking non-termination. One such condition is weak
acyclicity [FKMP05], which asserts that there are no cyclically connected positions
in the constraint set that may introduce fresh labeled null values, by a global study of
relations between the constraints. Although more general termination conditions ex-
ist [DNR08; MSL09d; MSL09a] (we will survey them in Section 5.5), we will restrict
our discussion to weak acyclicity, which is suﬃcient to illustrate the idea behind such
conditions and is strong enough to guarantee termination for all examples that we
will discuss later. In practical scenarios, however, it makes sense to fall back on more
general termination conditions, to extend the applicability of the chase algorithm.
Weak acyclicity builds on the so-called dependency graph introduced in [FKMP05]:
Deﬁnition 5.15 (Dependency Graph [FKMP05]) Let Σ be a set of TGDs and
EGDs. The dependency graph dep(Σ) := (V, E) of Σ is the directed graph deﬁned
as follows. There are two kinds of edges in E. Add them as follows: for every TGD
∀x(φ(x) →∃yψ(x, y)) ∈Σ and for every x in x that occurs in ψ(x, y) and every
occurrence of x in φ(x) in position π1
121

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
• for every occurrence of x in ψ(x, y) in position π2, add an edge π1 →π2 (if it
does not already exist), and
• for every existentially quantiﬁed variable y and for every occurrence of y in a
position π2, add a so-called special edge π1
∗→π2 (if it does not already exist).2
On top of the dependency graph we can easily deﬁne the notion of weak acyclicity:
Deﬁnition 5.16 (Weak Acyclicity [FKMP05]) Let Σ be a set of TGDs and
EGDs. Σ is weakly acyclic iﬀdep(Σ) has no cycles going through a special edge.2
It is known that, for weakly acyclic constraint sets, the chase terminates on every
database instance in polynomial-time data complexity [FKMP05].
Example 5.5 The dependency graphs for the constraint set Σ from Example 5.3
and for the constraint set Σ′ from Example 5.4 are
dep(Σ) := ({E1, E2, S1}, {E1 →E2, E2 →E1, S1 →E1, S1
∗→E2}),
dep(Σ′) := ({E1, E2, S1}, {E2 →E1, E2
∗→E2}).
We observe that dep(Σ) has no cycle going through a special edge, so Σ is weakly
acyclic and the chase with Σ always terminates. In contrast, dep(Σ′) contains the
cycle E2
∗→E2, so no chase termination guarantees for Σ′ can be made (in fact,
Example 5.4 shows a non-terminating chase sequence for Σ′).
2
The Chase for Semantic Query Optimization. In the context of SQO, the
chase takes a CQ q and a set of TGDs and EGDs Σ as input. It interprets the
body of the query, body(q), as database instance and successively ﬁxes constraint
violations in body(q). We denote the output obtained when chasing q with Σ as qΣ.
It is known that body(qΣ) |= Σ and that qΣ is equivalent to q on every instance
D |= Σ. Note that qΣ is undeﬁned whenever the chase fails or does not terminate.
The semantic optimization approach for SPARQL that we will present in Sec-
tion 5.4 relies on the Chase & Backchase (C&B) algorithm for semantic optimiza-
tion of CQs proposed in [DPT06], which builds upon the standard chase procedure.
To give a short description, the C&B algorithm does the following: given a CQ q
and a set of constraints Σ as input, it lists all Σ-equivalent minimal (with respect
to the number of atoms in the body) rewritings of q, up to isomorphism. We do
not describe the C&B algorithm in detail here, but use it as a black-box with the
above-mentioned properties. It is important to note that also the C&B algorithm
does not necessarily terminate, because it uses the standard chase as a subproce-
dure. However, all suﬃcient termination conditions for the chase also apply to the
C&B algorithm. Given a set Σ of EGDs and TGDs and a conjunctive query q, we
denote the output of the C&B algorithm, namely the set of all minimal conjunctive
queries that are Σ-equivalent to q, by cbΣ(q), if it is deﬁned (the output is deﬁned
whenever all chase sequences that are triggered by C&B are deﬁned).
122

5.2. Constraints for RDF
5.2. Constraints for RDF
In the previous section, we formalized dependencies for relational databases as ﬁrst-
order sentences. RDF constraints can be seen as a special case of such sentences:
Deﬁnition 5.17 (RDF Constraint) An RDF constraint is a ﬁrst-order sentence
over signature R := {T} with ar(T) := 3. We deﬁne the satisfaction of an RDF
constraint ϕ as follows. Let D be an RDF document. We deﬁne the R-structure
ID := (dom(D), {T(s, p, o) | (s, p, o) ∈D}). We say that D satisﬁes ϕ, written
D |= ϕ, iﬀID |= ϕ. Similarly, for a set of RDF constraints Σ, D |= Σ iﬀID |= Σ.2
Example 5.6 Consider the RDF constraints
β1 := ∀x1, x2, m( T(x1, rdf:type, Student) ∧T(x2, rdf:type, Student)∧
T(x1, matric, m) ∧T(x2, matric, m) →x1 = x2),
β2 := ∀x(T(x, rdf:type, Student) →∃m T(x, matric, m)).
The EGD β1 can be understood as a key stating that the matriculation number
matric identiﬁes objects of type student; TGD β2 states that each student has at
least one matriculation number associated. Now consider the RDF document
D := {(P1, rdf:type, Student), (P1, matric, “111111”), (P2, rdf:type, Student),
(P3, rdf:type, Student), (P3, matric, “222222”)}.
It is easily veriﬁed that D |= ϕ1, D ̸|= ϕ2, and therefore D ̸|= {ϕ1, ϕ2}.
2
In the remainder of Section 5.2, we propose a collection of constraint templates for
RDF and classify them as TGDs, EGDs, and DED̸=s. The subsequent listing is not
complete in any sense, but pursues the goals (a) to show that constraints like keys
and foreign keys for structured and semi-structured data models or cardinality con-
straints for XML (as found e.g. in XML Schema [xml]), naturally carry over to RDF
and fall into established constraint classes and (b) to present a proper formalization
of diﬀerent constraint types, which can be used in subsequent discussions.
5.2.1. Equality-generating Dependencies
In Figure 5.1 we summarize EGD templates for RDF, where C is to be instantiated
by some RDF class and p, p1, . . . , pn, q are to be replaced by RDF properties. When
instantiated, these templates express diﬀerent types of functional dependencies over
RDF data. First, constraint Key(C, [p1, . . . , pn]) states that properties p1, . . . , pn
form an n-ary key for objects of type C in the sense that they identify objects
of this type. Note that the constraint “applies” only in cases where p1, . . . , pn are
123

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
Name
Formalization in First-order Logic (EGD)
Key(C, [p1, . . . , pn])
∀x1, x2, o1, . . . , on(
T(x1, rdf:type, C) ∧T(x2, rdf:type, C)∧
T(x1, p1, o1) ∧· · · ∧T(x1, pn, on)∧
T(x2, p1, o1) ∧· · · ∧T(x2, pn, on)
→x1 = x2)
FD([p1, . . . , pn], q)
∀x1, x2, o1, . . . , on, o′
1, o′
2(
T(x1, p1, o1) ∧· · · ∧T(x1, pn, on) ∧T(x1, q, o′
1)∧
T(x2, p1, o1) ∧· · · ∧T(x2, pn, on) ∧T(x2, q, o′
2)
→o′
1 = o′
2)
FD•(C, [p1, . . . , pn], q)
∀x1, x2, o1, . . . , on, o′
1, o′
2(
T(x1, rdf:type, C) ∧T(x2, rdf:type, C)∧
T(x1, p1, o1) ∧· · · ∧T(x1, pn, on) ∧T(x1, q, o′
1)∧
T(x2, p1, o1) ∧· · · ∧T(x2, pn, on) ∧T(x2, q, o′
2)
→o′
1 = o′
2)
Func(p)
∀x, o1, o2(T(x, p, o1) ∧T(x, p, o2) →o1 = o2)
Func•(C, p)
∀x, o1, o2(T(x, rdf:type, C) ∧T(x, p, o1) ∧T(x, p, o2) →o1 = o2)
Figure 5.1.: Templates of equality-generating dependencies for RDF, where C stands
for an RDF class and p, p1, . . . , pn, q denote RDF properties.
all bound; we will present a variant of the constraint in Section 5.2.3, called Key∗,
which additionally enforces that p1, . . . , pn are bound (but is not an EGD anymore).
The templates FD([p1, . . . , pn], q) and FD•(C, [p1, . . . , pn], q) express general func-
tional dependencies, stating that a set of properties p1, . . . , pn with their correspond-
ing values identify the value of property q whenever all properties p1, . . . , pn, q are
present. While FD can be understood as a universal functional dependency that
holds for objects of any type (and even untyped objects), template FD• allows to
encode functional dependencies for objects of some ﬁxed type C only.
Finally, the two constraint templates Func(p) and Func•(C, p) express that RDF
property p is functional in the sense that it occurs either zero or one times for each
object (cf. Func(p)) or objects of some ﬁxed type C (cf. Func•(C, p)).
Example 5.7 β1 from Example 5.6 can be encoded as Key(Student,[matric]).
2
5.2.2. Tuple-generating Dependencies
Figure 5.2 presents TGD templates for RDF. FKey(C, [p1, . . . , pn], D, [q1, . . . , qn])
can be understood as an n-ary foreign key constraint for RDF. It states that, for
each object of type C with properties p1, . . . , pn, there is an object of type D with
properties q1, . . . , qn such that each qi (1 ≤i ≤n) maps to the same value as pi.
Next, SubC(C, D) and SubP(p, q) express the subclass and subproperty relation-
ships between classes and properties, respectively. These constraints are motivated
124

5.2. Constraints for RDF
Name
Formalization in First-order Logic (TGD)
FKey(C, [p1, . . . , pn],
D, [q1, . . . , qn])
∀x, o1, . . . , on(
T(x, rdf:type, C) ∧T(x, p1, o1) ∧· · · ∧T(x, pn, on)
→∃y T(y, rdf:type, D) ∧T(y, q1, o1) ∧· · · ∧T(y, qn, on))
SubC(C, D)
∀x(T(x, rdf:type, C) →T(x, rdf:type, D))
SubP(p, q)
∀x, o(T(x, p, o) →T(x, q, o))
PDom(p, C)
∀x, y(T(x, p, y) →T(x, rdf:type, C))
PRan(p, C)
∀x, y(T(x, p, y) →T(y, rdf:type, C))
PChainP([p1, . . . , pn], q)
∀x, o1, . . . , on(
T(x, p1, o1) ∧T(o1, p2, o2) ∧· · · ∧T(on−1, pn, on)
→T(x, q, on))
PChainP•(C, [p1, . . . , pn], q)
∀x, o1, . . . , on(T(x, rdf:type, C)∧
T(x, p1, o1) ∧T(o1, p2, o2) ∧· · · ∧T(on−1, pn, on)
→T(x, q, on))
PChainC([p1, . . . , pn], D)
∀x, o1, . . . , on(
T(x, p1, o1) ∧T(o1, p2, o2) ∧· · · ∧T(on−1, pn, on)
→T(on, rdf:type, D))
PChainC•(C, [p1, . . . , pn], D)
∀x, o1, . . . , on(T(x, rdf:type, C)∧
T(x, p1, o1) ∧T(o1, p2, o2) ∧· · · ∧T(on−1, pn, on)
→T(on, rdf:type, D))
Figure 5.2.: Templates of tuple-generating dependencies for RDF, where C, D stand
for RDF classes and p, p1, . . . , pn, q, q1, . . . , qm denote RDF properties.
by the predeﬁned RDFS properties rdfs:subClassOf and rdfs:subPropertyOf
(cf. the discussion in Section 2.2.2). Note, however, that there is a very basic concep-
tual diﬀerence: the RDFS vocabulary can be seen as a set of axioms that imply sub-
class and subproperty relationships that are not explicit in the RDF database accord-
ing to the semantics of RDFS; in contrast, we understand SubC(C, D), SubP(p, q)
as hard database constraints, which assert that the speciﬁed subclass and subprop-
erty relationships are explicitly contained in the database. Also closely related to
the RDFS vocabulary are the constraints PDom(p, C) and PRan(p, C), which ﬁx
the domain and range of property p to C and can be understood as the constraint
versions of the RDFS properties rdfs:domain and rdfs:range, respectively. We
will resume the discussion about connections between RDFS and constraints later
in Section 5.4.1 when motivating constraint-based query optimization for SPARQL.
Next, Figure 5.2 contains the template constraints PChainP([p1, . . . , pn], q) and
PChainP•(C, [p1, . . . , pn], q). These two constraints can be understood as referential
integrity constraints along a chain of properties p1, . . . , pn, stating that each value
reachable when walking along p1, . . . , pn is linked directly through property q (again,
there is a universal version and a version for objects of some class C). Complementar-
ily, the path constraints PChainC([p1, . . . , pn], D) and PChainC•(C, [p1, . . . , pn], D)
assert that objects reached when walking along properties p1, . . . , pn are of type D.
125

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
Name
Formalization (DEDs/DED̸=s or sets thereof)
We deﬁne someEq([o1, . . . , om]) := W
1≤i≤m
W
i<j≤m oi = oj,
allDist([o1, . . . , om]) := V
1≤i≤m
V
i<j≤m oi ̸= oj.
Min(n, p)
∀x, o(T(x, rdf:type, o) →
∃o1, . . . , onT(x, p, o1) ∧· · · ∧T(x, p, on) ∧allDist([o1, . . . , on]))
Min•(C, n, p)
∀x(T(x, rdf:type, C) →
∃o1, . . . , onT(x, p, o1) ∧· · · ∧T(x, p, on) ∧allDist([o1, . . . , on]))
Max(n, p)
∀x, o, o1, . . . , on+1(T(x, rdf:type, o)∧
T(x, p, o1) ∧· · · ∧T(x, p, on+1) →someEq([o1, . . . , on+1]))
Max•(C, n, p)
∀x, o1, . . . , on+1(T(x, rdf:type, C)∧
T(x, p, o1) ∧· · · ∧T(x, p, on+1) →someEq([o1, . . . , on+1]))
Exact(n, p)
{Min(n, p), Max(n, p)}
Exact•(C, n, p)
{Min•(C, n, p), Max•(C, n, p)}
Total(p)
Exact(1, p)
Total•(C, p)
Exact•(C, 1, p)
PDom∨(p, [C1, . . . , Cn])
∀x, y(T(x, p, y) →T(x, rdf:type, C1) ∨· · · ∨T(x, rdf:type, Cn))
PRan∨(p, [C1, . . . , Cn])
∀x, y(T(x, p, y) →T(y, rdf:type, C1) ∨· · · ∨T(y, rdf:type, Cn))
CProp∨(C, [p1, . . . , pn])
∀x, p, o(T(x, rdf:type, C) ∧T(x, p, o) →p = p1 ∨· · · ∨p = pn)
Key∗(C, [p1, . . . , pn])
{Key(C, [p1, . . . , pn])} ∪Total•(C, p1) ∪· · · ∪Total•(C, pn)
Figure 5.3.: Templates of disjunctive embedded dependencies with non-equality,
where C, C1, . . . , Cn stand for RDF classes and p, p1, . . . , pn, q, q1, . . . , qn
denote RDF properties.
5.2.3. Disjunctive Embedded Dependencies
We conclude our discussion of constraints for RDF with a collection of constraint
templates that can be expressed as (sets of) disjunctive embedded dependencies or
disjunctive embedded dependencies with non-equality. The ﬁrst constraint types that
are listed in Figure 5.3 are the cardinality constraints Min(n, p) and Max(n, p), stat-
ing that for objects of any o type the RDF property p is present minimally or maxi-
mally n times, respectively. As usual, the corresponding constraints Min•(C, n, p)
and Max•(C, n, p) impose the respective restrictions to objects of type C only.
Note that we use the shortcuts someEq([o1, . . . , om]) (encoding that there must be
some i ̸= j such that oi = oj) and allDist([o1, . . . , om] (stating that for all i ̸= j it
holds that oi ̸= oj); the formal deﬁnition of these functions is given in the ﬁgure. As
can be seen, the templates Total(p), Total•(C, p), Exact(n, p), and Exact•(C, n, p) are
easily obtained when combining the diﬀerent versions of min- and max-cardinality
constraints; note that the latter templates are expressed as sets of constraints.
Example 5.8 β2 from Example 5.6 can be encoded as Min•(Student, 1, matric). 2
126

5.3. SPARQL as a Constraint Language
Next, as generalizations of the tuple-generating constraint templates PDom(p, C)
and PRan(p, C), we introduce PDom∨(p, [C1, . . . , Cn]) and PRan∨(p, [C1, . . . , Cn]).
They enforce that URIs or literals appearing in the subject and object position of
property p are typed with at least one of the classes C1, . . . , Cn; observe that, in
line with the philosophy of RDF, we allow for multiple domain assignments. Com-
plementarily to the extended versions of the property domain and range restriction
constraints, the template CProp∨(C, [p1, . . . , pn]) allows to restrict the set of prop-
erties that are used in combination with objects of type C to p1, . . . , pn.
Finally, the constraint Key∗(C, [p1, . . . , pn]) is a variant of the key constraint
Key(C, [p1, . . . , pn]) presented in Figure 5.1. Like the original key constraint, it
enforces that the values of properties p1, . . . , pn identify objects of type C, and
additionally asserts that all these properties are present exactly once.
5.3. SPARQL as a Constraint Language
One interesting question that arises in the context of constraints for RDF data is
to which degree SPARQL can be used to check if constraints hold over the input
RDF database. Before starting, we give a formal notion of constraint checking with
SPARQL. We argue that the natural way to verify if a constraint holds on some
RDF document is using the SPARQL Ask query form introduced in Deﬁnition 2.6:
Deﬁnition 5.18 (Constraint Checking with SPARQL) Let Q be a SPARQL
Ask query and ϕ be an RDF constraint. We say that Q checks ϕ iﬀfor every RDF
document D it holds that JQKD ⇔D ̸|= ϕ.
2
Informally speaking, to check a constraint we have to write a SPARQL Ask
query that detects exactly the constraint violations in the database. If the constraint
holds on some database, then there is no constraint violation in the database and
consequently the Ask query evaluates to false; otherwise, it evaluates to true.
Example 5.9 Let us consider the equality-generating dependency β1 from Exam-
ple 5.6 and the tuple-generating dependency β3 := ∃xT(x, rdf:type, Student). It is
easy to see that the SPARQL query
Qβ1 := Ask(((?x1, rdf:type, Student) And (?x2, rdf:type, Student) And
(?x1, matric, ?m) And (?x2, matric, ?m)) Filter (¬(?x1 =?x2))),
checks constraint β1: obviously, the query returns true if and only if there are two
distinct students in the database that have the same matriculation number.
We further argue that there is no SPARQL query that checks constraint β3. To
see why this is the case, consider the empty RDF document D := ∅. We observe
that D ̸|= β3, so one restriction for the SPARQL check query is that it must return
true on the empty document. It can easily be shown (by induction on the structure
of SPARQL expressions) that such a SPARQL query does not exist.
2
127

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
The example shows that we cannot check TGDs in the general case. It follows
immediately that we generally cannot check the constraint classes ED, DED, and
DED̸=, which generalize the class of TGDs. We next present a simple extension of
SPARQL by constant mappings, called SPARQLC, which can be understood as the
counterpart of constants in relational algebra; we shall see later that this extension
gives SPARQL the expressive power to check arbitrary ﬁrst-order logic sentences.
Given the result from [AG08a] that SPARQL has the same expressiveness as re-
lational algebra and the close connection between relational algebra and ﬁrst-order
logic, one might wonder why such an extension is necessary. The crucial observation
is that, in the proof of the result that relational algebra has the same expressiveness
as SPARQL in [AG08a], the authors use a version of SPARQL that extends our
fragment from Deﬁnition 2.4 by (i) so-called empty graph patterns {} (with seman-
tics J{}KD := {∅}) and by (ii) a Minus operator at syntax level (with semantics
JQ1 Minus Q2KD := JQ1KD \ JQ2KD). We emphasize that the encoding of RDF con-
straints (cf. Theorem 5.1 below) is also possible when using these two extensions.
With SPARQLC we propose another extension of SPARQL that compensates for
both missing empty graph patterns and a missing syntactic Minus operator:2
Deﬁnition 5.19 (SPARQLC) Let ?x ∈V and c ∈LU be a literal or a URI. A
constant triple pattern, or c-pattern for short, is an expression of the form t(?x 7→c).
We extend the set and bag semantics from Deﬁnitions 2.11 and 2.15 as follows:
Jt(?x 7→c)KD := {{?x 7→c}}
Jt(?x 7→c)K+
D := ({{?x 7→c}}, m), where for µ ∈M we deﬁne
m(µ) := 1 if µ = {?x 7→c}, and m(µ) := 0 otherwise.
A SPARQLC expression or query is deﬁned as a SPARQL expression or query in
which c-patterns can be used in place of simple triple patterns.
2
Example 5.10 Consider the SPARQLC query
QC:= Select?p,?status(t(?status 7→“Student”) And
(?p, rdf:type, Person) And (?p, matric, ?m))
which extracts all persons with matriculation number (property matric) from the
database and assigns status “Student” to these persons. Let us assume that the
database D := {(Jil, rdf:type, Person), (Jil, matric, “1234”), (John, rdf:type, Person)}
is given. It is easily veriﬁed that JQCKD = {{?p 7→Jil, ?status 7→“Student”}}.
2
Resuming our discussion of constraint checking with SPARQL, ﬁrst observe that
we can check the constraint β3 from Example 5.9 in SPARQLC:
2In fact, similar extensions have already been used in practice. For instance, the ARQ SPARQL
processor [arq] supports such constant patterns by means of a Let clause, which allows to bind
variables to some ﬁxed value, see http://jena.sourceforge.net/ARQ/assignment.html.
128

5.3. SPARQL as a Constraint Language
Example 5.11 The SPARQLC query
Qβ3 := Ask((t(?x 7→c) Opt (?y, rdf:type, Student)) Filter (¬bnd(?y)))
checks constraint β3 from Example 5.9: if database D contains no object of type
Student then D ̸|= β3 and JQβ3KD = true, otherwise D |= β3 and JQβ3KD = false. 2
As illustrated in the example, the SPARQLC language allows to encode (top-level)
negation in SPARQL queries. We generalize this construction as follows (we use the
concept of possible variables, i.e. function pVars introduced in Section 4.1).
Proposition 5.1 Let Q be a SPARQLC expression and ?x, ?y ∈V \ pVars(JQKD).
Deﬁne Q¬ := (t(?x 7→c) Opt (t(?y 7→c) And Q)) Filter (¬bnd(?y)). Then for
every RDF document D it holds that JAsk(Q)KD ⇔¬JAsk(Q¬)KD.
2
Proof of Proposition 5.1
⇒: (a) Assume that JAsk(Q)KD = true. Then JQKD is not empty. Now consider
what happens when evaluating JQ¬KD. First recall that ?x, ?y ̸∈pVars(JQKD) by
assumption, so it is easy to see that the inner And expression extends each mapping
in JQKD by ?y 7→c, and the Opt expression further extends each mapping by
?x 7→c; consequently, the surrounding ﬁlter discards all mappings, so JQ¬KD = ∅
and JAsk(Q¬)KD = false. If (b) JAsk(Q)KD = false, then JQKD = ∅. It is easily
veriﬁed that, in this case, JQ¬KD = {{?x 7→c}} and therefore JAsk(Q¬)KD = true.
⇐: (a) Assume that JAsk(Q¬)KD = true. Then JQ¬KD is not empty. This implies
that there is a mapping in Jt(?x 7→c) Opt (t(?y 7→c) And Q)KD such that ?y is not
bound. It is easily veriﬁed that ?y is bound in every result mapping whenever JQKD
is not empty, so we conclude that JQKD = ∅and therefore JAsk(Q)KD = false.
(b) If JAsk(Q¬)KD = false, then JQ¬KD = ∅. Hence, it must hold that ?y is bound
in every mapping obtained when evaluating Jt(?x 7→c) Opt (t(?y 7→c) And Q)KD.
Assume for the sake of contradiction that JQKD = ∅. It is easily veriﬁed that, in this
case, JQ¬KD = {{?x 7→c}}, which is a contradiction. We conclude that JQKD ̸= ∅
and therefore it holds that JAsk(Q)KD = true.2
We emphasize that top-level negation cannot be encoded in standard SPARQL,
because we cannot write a SPARQL query that returns true on the empty docu-
ment and therefore we cannot negate the result of a query in the general case. The
main result in this subsection is that the extension from SPARQL to SPARQLC is
suﬃcient to check every ﬁrst-order sentence (over a single, ternary signature). Of
course, this comprises all dependency classes (i.e., EGDs, EDs, DEDs, and DED̸=s)
and consequently all the constraint templates proposed in Section 5.2.
Theorem 5.1 Let ϕ be an RDF constraint. Then there is a SPARQLC query that
checks ϕ.
2
129

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
As a practical implication, the theorem shows that SPARQL (when marginally
extended) is a good candidate for a constraint language. In particular, SPARQLC
can be used for checking if a constraint holds on some database instance and –
phrased more generally – for expressing arbitrary RDF constraints. In practice, it
would make sense to deﬁne an Assert query form for SPARQLC, which – akin to
Create Assertion statements in SQL – allows to specify user-deﬁned constraints
on top of RDF database management systems. Such ﬁxed constraints could then
be used internally to check data integrity and also for SQO. We ﬁnally point out
that, although we deﬁned constraint checking over set semantics in Deﬁnition 5.18,
our results immediately carry over to bag semantics, because the evaluation of Ask
queries is invariant w.r.t. the underlying semantics (see Lemma 4.14(1)). In the
remainder of this subsection we present the proof for Theorem 5.1. It is constructive
and makes the connection between ﬁrst-order logic and SPARQL explicit.
Proof of Theorem 5.1
We show that for each RDF constraint, i.e. each ﬁrst-order sentence ϕ over sig-
nature R := {T} with ar(T) := 3, there is a SPARQLC query Qϕ such that
JAsk(Qϕ)KD = true ⇔D |= ϕ. This is suﬃcient, because the construction from
Proposition 5.1 then gives us a query Q¬
ϕ such that JAsk(Q¬
ϕ)KD = false ⇔D |= ϕ.3
More precisely, we present a SPARQLC encoding of a ﬁrst-order sentence ϕ that
is built using (1) equality R-formulas t1 = t2, (2) R-atoms of the form T(t1, t2, t3),
(3) the negation operator ¬, (4) the conjunction operator ∧, and (5) R-formulas of
the form ¬∃xψ. It is folklore that ψ1 ∨ψ2 can be written as ¬(¬ψ1 ∧¬ψ2) and each
quantiﬁer R-formula can easily be brought into the form (5), i.e. ∀xψ is equivalent
to ¬∃x¬ψ and ∃xψ can be written as ¬(¬∃xψ), so cases (1)-(5) are suﬃcient. Note
that we chose the variant ¬∃xψ because its encoding is simpler than e.g. ∀xψ or ∃xψ.
Having ﬁxed the structural constraints for the input formula ϕ, we now show how
to construct a SPARQLC query Qϕ such that JQϕKD = ∅if and only if ID ̸|= ϕ,
where ID := (dom(D), {T(s, p, o) | (s, p, o) ∈D}). Before presenting this encoding,
we introduce some additional notation and shortcut deﬁnitions.
Let var(ϕ) := {x1, . . . , xn} denote all variables appearing in formula ϕ. We deﬁne
the set S := {?x1, . . . , ?xn} of corresponding SPARQL variables. Further, we intro-
duce the function v : var(ϕ) 7→S that translates each variable occurring in ϕ into
its corresponding SPARQL variable, i.e. we deﬁne v(xi) :=?xi for 1 ≤i ≤n. Further
assume that S¬ is an inﬁnite set of variables disjoint from S, i.e. S∩S¬ = ∅. For each
subexpression ψ of the input formula ϕ we deﬁne an inﬁnite partition S¬
ψ ⊂S¬ such
that, for each pair of distinct subexpressions ψ1 ̸= ψ2 of ϕ it holds that S¬
ψ1∩S¬
ψ2 = ∅.
Based on these variable set partitions, we deﬁne for each subexpression ψ of ϕ its so-
called active domain expression Qψ as follows. Let free(ψ) := {v1, . . . , vk} ⊆var(ϕ)
be the set of free variables in subexpression ψ. Then we deﬁne Qψ as
3This shows that we also could negate the deﬁnition of SPARQL check queries (Deﬁnition 5.18),
i.e. deﬁne a check query as an Ask query that returns true iﬀthe constraint is satisﬁed.
130

5.3. SPARQL as a Constraint Language
Qψ := t(?a 7→c) And
((v(v1), ?a11, ?a12) Union (?a11, v(v1), ?a12) Union (?a11, ?a12, v(v1))) And
((v(v2), ?a21, ?a22) Union (?a21, v(v2), ?a22) Union (?a21, ?a22, v(v2))) And
. . . And
((v(vk), ?ak1, ?ak2) Union (?ak1, v(vk), ?ak2) Union (?ak1, ?ak2, v(vk)))
where ?a, ?a11, ?a12, . . . , ?ak1, ?ak2 are pairwise distinct variables from S¬
ψ. It is cru-
cial to note that the active domain expressions for two distinct subexpressions share
at most variables from S, because ?a and all ?aij are chosen from partitions that
belong to the respective subexpressions. Furthermore, observe that Qϕ := t(?a 7→c),
because ϕ is a sentence and therefore free(ϕ) = ∅. To give the intuition, each Qψ rep-
resents all combinations of binding the free variables v1, . . . , vk in ψ (more precisely,
the corresponding SPARQL variables v(v1), . . . , v(vk)) to elements of the input doc-
ument, where ?a and the ?aij are globally unique dummy variables that are not of
further importance (but were required for the construction).
The remainder of the proof follows a naive evaluation of ﬁrst-order formulas on
ﬁnite structures. With the help of the active domain subexpressions Qψ we generate
all possible bindings for the free variables in a subformula. Note that there is no
need to project away the dummy variables ?aij: we use fresh, distinct variables for
every subformula ψ, so they never aﬀect compatibility between two mappings (and
hence do not inﬂuence the evaluation process); in the end, we are only interested in
the boolean value, so these bindings do not harm the construction.
Coming to the encoding, we deﬁne enc(t) := v(t) if t is a variable and enc(t) := t
if t is a constant (i.e. we simply interpret t as a URI). The idea of the encoding enc(ϕ)
for ϕ is to inductively simulate the formula’s semantics by generating all possible
bindings for the free variables of some (sub)formula using SPARQL. Thereby, we
follow the structural constraints for ϕ ﬁxed in cases (1)-(5) before:
(1) For ψ := t1 = t2 we deﬁne enc(ψ) := Qψ Filter (enc(t1) = enc(t2)).
(2) For ψ := T(t1, t2, t3) we deﬁne enc(ψ) := Qψ And (enc(t1), enc(t2), enc(t3)).
(3) For ψ := ¬ψ1 we deﬁne
enc(ψ) := (Qψ Opt (t(?b 7→c) And enc(ψ1))) Filter (¬bnd(?b)),
where ?b ∈S¬ neither appears in the encoding enc(ψ1) nor in Qψ.
(4) For ψ := (ψ1 ∧ψ2) we deﬁne enc(ψ) := enc(ψ1) And enc(ψ2).
(5) For ψ := ¬∃xψ1 we deﬁne
enc(ψ) := (Qψ Opt (t(?b 7→c) And enc(ψ1))) Filter (¬bnd(?b)),
where ?b ∈S¬ neither appears in the encoding enc(ψ1) nor in Qψ.
Rather than going into detail, we sketch the idea behind the encoding. It satisﬁes
the following property: for each formula ψ with free(ψ) := {v1, . . . , vk} it holds that
(⇒) foreach interpretation I := (ID, γ) such that I |= ψ there exists a mapping
µ ∈Jenc(ϕ)KD such that µ ⊇{?v1 7→γ(v1), . . . , ?vk 7→γ(vk)} and (⇐) foreach
mapping µ ∈Jenc(ϕ)KD it holds that every interpretation (ID,γ) with γ(vi) := µ(?vi)
131

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
for 1 ≤i ≤k satisﬁes ϕ. It is straightforward to verify that these two directions imply
the original claim, i.e. it follows from “⇐” and “⇒” that ID ̸|= ϕ ⇔Jenc(ϕ)KD = ∅.
The two directions can be proven by induction on the structure of formulas.
Concerning the two basic cases (1) and (2) it is easy to see that in their encoding the
active domain expressions generate the universe of all bindings for the free variables,
which is then restricted by application of the ﬁlter (for case (1) ψ := t1 = t2) or
by joining the active domain expression with the respective triple pattern (for case
(2) ψ := T(t1, t2, t3)). We omit the detailed proof of these cases. In the induction
step there are three cases that remain to be shown. The idea of the encoding for
(3) ψ := ¬ψ1 is to subtract from the universe of solutions exactly the solutions of
ψ1, encoded by enc(ψ1). To see how the encoding works, observe that
Jenc(ψ)KD = J(Qψ Opt (t(?b 7→c) And enc(ψ1)) Filter (¬bnd(?b))KD
= σ¬bnd(?n)(JQψKD
1 (Jt(?b 7→c)KD 1 Jenc(ψ1)KD))
(∗1)
= JQψKD \ (Jt(?b 7→c)KD 1 Jenc(ψ1)KD)
(∗2)
= JQψKD \ Jenc(ψ1)KD
where step (∗1) follows by application of equivalence (FLBndI) from Lemma 4.4
presented in Section 4.2.6 and step (∗2) follows from the observation that the single
mapping in Jt(?b 7→c)KD = {{?b 7→c}} is compatible with every mapping in Jψ1KD
(?b does not appear in enc(ψ1)) and the extension of mappings from Jψ1KD by ?b 7→c
does not modify the result. Hence, the encoding implements negation.
To discuss the remaining cases, observe that (4) a conjunction ψ := ψ1 ∧ψ2 is
straightforwardly mapped to a join operation between enc(ψ1) and enc(ψ2). Finally,
the encoding for (5) ψ := ¬∃xϕ is similar to the encoding for the negation; however,
in this case it holds that ?x ̸∈free(ψ), so the active domain expression does not
contain variable ?x anymore, which gives us an implicit projection.2
We conclude with a small example that illustrates the construction.
Example 5.12 Consider the RDF constraint ∃xT(x, a, a), which can be rewritten
as ϕ := ¬(¬∃xT(x, a, a)). First, we set up the active domain expressions for the
subexpressions of ϕ:
QT(x,a,a) := t(?a 7→c) And ((?x, ?a1, ?a2) Union (?a1, ?x, ?a2) Union (?a1, ?a2, ?x)),
Q¬∃xT(x,a,a) := t(?a′ 7→c),
Qϕ := t(?a′′ 7→c).
Next, we encode the formula using these active domain expressions:
enc(T(x, a, a)) := QT(x,a,a) And (?x, a, a),
enc(¬∃xT(x, a, a)) := (Q¬∃xT(x,a,a) Opt (t(?b 7→c) And enc(T(x, a, a)))
Filter (¬bnd(?b)),
enc(ϕ) := (Qϕ Opt (t(?b′ 7→c) And enc(¬∃xT(x, a, a)))) Filter (¬bnd(?b′)).
132

5.4. Semantic Query Optimization for SPARQL
Now consider the RDF database D := {(a, a, a), (b, b, b)} |= ϕ. We observe that
JQT(x,a,a)KD
= {{?x 7→a, ?a 7→c, ?a1 7→a, ?a2 7→a},
{?x 7→b, ?a 7→c, ?a1 7→b, ?a2 7→b}},
JQ¬∃xT(x,a,a)KD = {{?a′ 7→c}},
JQϕKD
= {{?a′′ 7→c}},
Jenc(T(x, a, a))KD
= {{?x 7→a, ?a 7→c, ?a1 7→a, ?a2 7→a}},
Jenc(¬∃T(x, a, a))KD = ∅, and ﬁnally
Jenc(ϕ)KD
= {{?a′′ 7→c}}.
This coincides with the observation that D |= ϕ. It is easily veriﬁed that, when
evaluating enc(ϕ) on e.g. D′ := {(b, b, b)} ̸|= ϕ we obtain the empty result.
2
5.4. Semantic Query Optimization for SPARQL
We now present a scheme for the semantic optimization (SQO) of SPARQL queries.
Generally speaking, the key idea of SQO is as follows. Given a query and a set
of integrity constraints over the database, the goal is to ﬁnd more eﬃcient queries
that are semantically equivalent to the original query for each database instance
that satisﬁes the constraints. The constraints that are given as input may have been
speciﬁed by the user, automatically extracted from the underlying database, or – in
our scenario – may be implicitly given by the semantics of RDFS when SPARQL is
coupled with an RDFS inference system (we will describe the latter setting in more
detail later in Section 5.4.1). Formally, we deﬁne the SQO problem for SPARQL as
follows: given a SPARQL expression or query Q and a set of RDF constraints Σ, we
want to enumerate expressions or queries Q′ (typically with some desired property,
such as minimality or optimality w.r.t. some cost measure) that are equivalent to Q
on every database D such that D |= Σ. If Q is equivalent to Q′ on every D such
that D |= Σ, we say that Q and Q′ are Σ-equivalent and denote this by Q ≡Σ Q′.
We note that constraint-based query optimization in the context of RDFS infer-
ence has been discussed before in [SKCT05]. Our approach is much more general
and supports constraints beyond those implied by the semantics of RDFS, i.e. it also
works on top of user-deﬁned or automatically extracted constraints. In [LMS08], for
instance, we propose to carry over constraints from relational databases, such as
primary and foreign keys, when translating relational data into RDF. Also the lat-
ter may serve as input to our semantic optimization scheme. As another diﬀerence
to [SKCT05], our approach addresses the speciﬁcs of SPARQL, e.g. we also provide
rules for the semantic optimization of queries that involve operator Opt.
133

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
5.4.1. A Motivating Scenario
Before presenting our constraint-based SPARQL optimization scheme, we will sketch
a motivating scenario that illustrates the beneﬁts of semantic query optimization in
the context of SPARQL and RDF. We start with the observation that, as discussed
earlier in Section 2.2.2, the SPARQL standard disregards RDFS reasoning, but eval-
uates queries on the database as is. Consider for instance the RDF database
D := {(knows, rdfs:domain, Person), (knows, rdfs:range, Person),
(P1, knows, P2), (P1, knows, P3)}
The ﬁrst and the second triple in the database state that the domain and range
of property knows is Person. According to the RDFS semantics deﬁnition (cf. Sec-
tion 2.2.2, Figure 2.5), these two triples imply that, for each triple (s, knows, o) in
the database the subject s and the object o are of type Person. Formally speaking,
the triples (knows, rdfs:domain, Person) and (knows, rdfs:range, Person) thus can be
understood as tuple-generating dependencies
ϕd := PDom(knows, Person) = ∀s, o(T(s, knows, o) →T(s, rdf:type, Person)),
ϕr := PRan(knows, Person) = ∀s, o(T(s, knows, o) →T(o, rdf:type, Person)),
which assert that subjects and objects occurring in triples with predicate knows
are typed accordingly with class Person. In this line, the RDFS reasoning process can
be seen as a chase sequence that ﬁxes constraint violations in the database, i.e. for
database D this process would derive the fresh triples t1 := (P1, rdf:type, Person),
t2 := (P2, rdf:type, Person), t3 := (P3, rdf:type, Person). Hence, according to the
semantics of RDFS, database D is equivalent to D′ := D ∪{t1, t2, t3}.
Now let us see what happens when evaluating SPARQL queries on top of the two
RDF databases deﬁned above. Let us exemplarily consider the SPARQL query
Q := Select?p1,?p2((?p1, rdf:type, Person) And (?p2, rdf:type, Person)
And (?p1, knows, ?p2)),
which is supposed to extract all pairs of persons (represented through variables ?p1
and ?p2) that know each other. When evaluated on the original database D, we
observe that JQKD = ∅, because D lacks triples stating that P1, P2, and P3 are
Person-typed objects. In contrast, when evaluating Q on the implied database D′
we get the desired result JQKD′ = {{?p1 7→P1, ?p2 7→P2}, {?p1 7→P1, ?p2 7→P3}}.
At ﬁrst glance, this seems paradoxical and one might argue that the decision to
exclude RDFS inferencing from the SPARQL semantics speciﬁcation was an unfor-
tunate design decision. However, the idea is that SPARQL can easily be coupled with
an RDFS inference engine whenever RDFS reasoning is desired. In such a scenario,
the SPARQL engine would request the implied database from the underlying RDFS
reasoning engine and therefore always operates on top of the implied database.
134

5.4. Semantic Query Optimization for SPARQL
The interesting observation here – which also is one of the major motivations for
our study of constraint-based SPARQL optimization – is that, whenever SPARQL
engines are coupled with RDFS inference engines, then ϕd and ϕr can be understood
as hard database constraints: the RDFS inference mechanism guarantees that these
constraints are always satisﬁed on the implied database. Consequently, the SPARQL
engine can take these constraints as granted and may exploit them for query op-
timization. In fact, it is easy to see that, for each database instance that satisﬁes
constraints ϕd and ϕr, query Q is equivalent to the (arguably simpler) query
Qopt := Select?p1,?p2((?p1, knows, ?p2)),
because ϕr and ϕd imply the existence of the missing tuples (?p1, rdf:type, Person)
and (?p2, rdf:type, Person) in the database. Thus, instead of evaluating the original
query Q, a SPARQL engine that operates on top of an RDFS inference layer could
evaluate the simpler query Qopt. The SQO scheme that we present in the following
would propose Qopt as an alternative when given Q and constraints ϕr, ϕd as input.
In the example above, we interpreted RDF triples containing RDFS vocabulary
as data dependencies. We can even go one step further and interpret the RDFS
reasoning rules (cf. Figure 2.5) as integrity constraints. Consider for instance the
two rules in group (III) of Figure 2.5. These rules can be expressed by the TGDs
ϕtyping1 := ∀a, c, x, y(T(a, rdfs:domain, c) ∧T(x, a, y) →T(x, rdf:type, c)),
ϕtyping2 := ∀a, c, x, y(T(a, rdfs:range, c) ∧T(x, a, y) →T(y, rdf:type, c)),
which – like ϕd and ϕr – imply database D′ when D is given as input. Analogously
to ϕtyping1 and ϕtyping2, we can easily express the remaining rules from Figure 2.5
(and other RDFS rules [rdfd; GHM04]) as ﬁrst-order sentences. All these RDFS con-
straints then can be used as a basis for semantic query optimization by SPARQL
engines that are built on top of an RDFS reasoning layer. This ﬁxed RDFS constraint
base can be extended by user-deﬁned constraints and automatically extracted con-
straints that are known to hold on the (implied) input database, whenever available.
5.4.2. Chase-based Optimization of SPARQL
Having motivated semantic query optimization for SPARQL, we now come to a
detailed discussion of our SQO scheme. The basic idea of our approach is as follows.
Given a SPARQL query and a set of constraints, we ﬁrst translate And-only blocks
(or full And-only queries), into conjunctive queries. In a second step, we then use the
Chase & Backchase (C&B) algorithm [DPT06] described in Section 5.1.4 to minimize
these conjunctive queries and translate the minimized CQs (i.e., the output of the
C&B algorithm) back into SPARQL, which usually gives us more eﬃcient SPARQL
queries. It should be mentioned that the C&B algorithm by default returns the set
of all Σ-equivalent queries that are minimal w.r.t. the number of atoms in the body
135

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
of the query. There are, of course, no guarantees that these queries can be evaluated
more eﬃciently than the original one. We therefore emphasize that, as described
in [DPT06], the C&B algorithm also can be coupled with a cost estimation function
and in that case would return the set of queries that are minimal w.r.t. the estimated
evaluation cost. In the absence of a cost function, though, we restrict ourselves to
the minimality property in the following discussion, but point out that our approach
also works in combination with advanced, implementation-speciﬁc cost measures.
The optimization scheme described above, which is restricted to And-only queries
or And-only subqueries, will be described in more detail in Section 5.4.3. Com-
plementarily, in Section 5.4.4 we discuss SPARQL-speciﬁc rules that allow for the
semantic optimization of complex queries involving operators Filter and Opt.
5.4.3. Optimizing AND-only Blocks
We start with a translation scheme for And-only queries into conjunctive queries.
We reuse the notation introduced in Section 3.2, e.g. writing Aπ for SPARQL queries
of the form SelectS(Q), where S ⊂V and Q is an And-only expression.
Deﬁnition 5.20 Let S ⊂V and Q ∈Aπ be a SPARQL query deﬁned as
Q := SelectS((s1, p1, o1) And . . . And (sn, pn, on)),
We deﬁne the translation cq(Q) := q of SPARQL query Q into CQ q, where
q := ans(s) ←T(s1, p1, o1) ∧· · · ∧T(sn, pn, on),
the tuple s contains exactly the variables from S, and in q we interpret variables
from S as elements from VR and elements from LU as constants from ∆.
Further, we deﬁne cq−1(q) as follows. It takes a CQ in the form of q as input and
returns Q if it is a valid SPARQL query, i.e. if (si, pi, oi) ∈UV × UV × LUV for
all i ∈[n]; in case Q is not a valid SPARQL query, cq−1(q) is undeﬁned.
2
Functions cq and cq−1 are straightforward translations from And-only SPARQL
queries to CQs and back. We illustrate their deﬁnition by example:
Example 5.13 Consider query Q from the running example in Section 5.4.1. Then
cq(Q) = ans(?p1, ?p2) ←T(?p1, knows, ?p2) ∧T(?p1, rdf:type, Person)
∧T(?p2, rdf:type, Person)
and cq−1(cq(Q)) = Q. As another example, cq−1(ans(?x) ←T(“a”, rdf:type, ?x))
is undeﬁned: Select?x((“a”, rdf:type, ?x)) is not valid SPARQL, because literal “a”
appears in subject position.
2
136

5.4. Semantic Query Optimization for SPARQL
The key property of our translation functions is that they are semantics-preserving:
Lemma 5.1 Let q : ans(?x1, . . . , ?xn) ←ϕ(?x1, . . . , ?xn, y) be a conjunctive query
and Qπ ∈A be a SPARQL query s.t. cq(Q) = q and cq−1(q) = Q. For every RDF
document D and associated relational instance I := {T(s, p, o) | (s, p, o) ∈D} it
holds that {?x1 7→c1, . . . , ?xn 7→cn} ∈JQKD ⇔(c1, . . . , cn) ∈q(I).
2
The lemma shows that we can safely transform SPARQL And-only queries into
conjunctive queries, apply equivalence transformations, and translate the resulting
CQ back into SPARQL without changing the semantics. The proof of the lemma is
straightforward and relies on the observation that operator And implements a join
in the style of conjunctive queries, where the atoms in the body are joined together
(observe that all variables in And-only expressions are bound in SPARQL result
mappings, so unbound variables are not an issue here). We omit the details.
Our ﬁrst result is that, when coupled with the C&B algorithm, the forth-and-back
translations cq and cq−1 provide a sound approach to semantic query optimization
for And-only queries whenever the underlying chase algorithm terminates regularly:
Lemma 5.2 Let Q be an Aπ expression, D be an RDF database, and let Σ be a set
of EGDs and TGDs. If cbΣ(cq(Q)) is deﬁned, q ∈cbΣ(cq(Q)), and cq−1(q) is deﬁned,
then cq−1(q) ≡Σ Q.
2
The lemma follows immediately from the correctness of the C&B algorithm and
the correctness of the functions cq and cq−1 stated in Lemma 5.1:
Proof of Lemma 5.2
Let Q′ ∈cq−1(cbΣ(cq(Q))) ∩Aπ. Then cq(Q′) ∈cbΣ(cq(Q)). This directly implies
that cq(Q′) ≡Σ cq(Q) and it follows (by Lemma 5.1) that Q′ ≡Σ Q.2
Lemma 5.2 formalizes the key idea of our SQO scheme: given that the chase result
for the translation cq(Q) of an And-only query Q is deﬁned (which implies that the
chase terminates), we can apply the C&B algorithm to cq(Q) and translate the
resulting minimal queries back into SPARQL, to obtain minimal SPARQL And-
only queries that are Σ-equivalent to Q. The following example clariﬁes the idea.
Example 5.14 Consider queries Q, Qopt, and constraints ϕtyping1, ϕtyping2 from Sec-
tion 5.4.1. Put Σ := {ϕtyping1, ϕtyping2}. First note that Σ is weakly acyclic, so the
chase with Σ always terminates. Moreover, we have that cq(Qopt) ∈cbΣ(cq(Q)). It
follows from Lemma 5.2 that cq−1(cq(Qopt)) = Qopt is Σ-equivalent to Q.
2
We emphasize that our approach is not restricted to And-only queries, but also
works for And-connected blocks inside more complex queries, i.e. we can apply our
scheme to And-connected blocks and replace the original blocks by the minimized
blocks delivered by the C&B algorithm. Given that our translation function cq is
137

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
deﬁned for SPARQL queries (rather than expressions), there are two possibilities
to implement this idea. First, observe that every expression Q ∈A is equivalent to
the Aπ query SelectpVars(JQKD)(Q), so we could simply use the latter query for our
optimization scheme. A second approach is to map the whole SPARQL expression
to SPARQL algebra, apply the projection pushing rules from Figure 4.3 to push
top-level projection down on top of 1-connected blocks, and interpret the inner
projection expressions as And-only queries. Interestingly, this may gives us better
optimization results as the ﬁrst approach, as illustrated in the following example.
Example 5.15 Consider the SPARQL query
Q′ := Select?p(((?p, rdf:type, Person) And (?p, email, ?e)) Filter (¬?p = P1)),
which selects all persons diﬀerent from P1 that have an email address. Assume
that the constraint set Σ′ := {Min•(Person, email, 1)} is given, stating that each ob-
ject of type Person has an email address. Now assume we want to optimize the inner
And block (?p, rdf:type, Person) And (?p, email, ?e). One possibility is to interpret
this block as query Qi := Select?p,?e((?p, rdf:type, Person) And (?p, email, ?e)).
This query, however, is already minimal and the C&B algorithm (when given Σ′
as input) is not able to minimize cq(Qi), because variable ?e appears in the result.
An alternative way is to translate Q′ into SPARQL algebra, which gives us
JQ′KD:= π?p(σ¬?p=P1(J(?p, rdf:type, Person)KD 1 J(?p, email, ?e)KD))
Applying rule (PFPush) from Figure 4.3, we transform this expression into
A′ := π?p(σ¬?p=P1(π?p(J(?p, rdf:type, Person)KD 1 J(?p, email, ?e)KD))).
In the next step we translate the inner projection expression back into the SPARQL
query Q′
i := Select?p((?p, rdf:type, Person) And (?p, email, ?e)) and pass the con-
junctive query cq(Q′
i) and Σ′ to the C&B algorithm. The output of C&B (inter-
preted as SPARQL query) is Select?p((?p, rdf:type, Person)), which is equivalent
to Qmin := (?p, rdf:type, Person). Substituting Qmin into the original query, we get
Q′
opt:= Select?p((?p, rdf:type, Person) Filter (¬?p = P1))
as an optimized version of Q′ (which is equivalent to Q′ on every D |= Σ).
The crucial point here is that Q′
i has an optimized Select clause, which projects
only ?p instead of ?p and ?e. The example shows that our semantic optimization
approach for SPARQL can beneﬁt from the algebraic rewriting rules in Chapter 4.2
Coming back to the discussion of Lemma 5.2, the observant reader may have
noticed that it states only soundness of the SQO scheme for And-only queries.
In fact, one can observe that under certain circumstance the scheme proposed in
Lemma 5.2 is not complete. We demonstrate the problem in the following example.
138

5.4. Semantic Query Optimization for SPARQL
Example 5.16 Consider the SPARQL queries
Q1 := Select?x((?x, a, “l”)),
Q2 := Select?x((?x, a, “l”) And (?x, b, c)),
and Σ := {∀x, y, z(T(x, y, z) →T(z, y, x))}. It holds that Q1 ≡Σ Q2 because the
answer to both Q1 and Q2 is always the empty set on documents that satisfy Σ:
the single constraint in Σ enforces that all RDF documents satisfying Σ have no
literal in object position, because otherwise this literal would also appear in subject
position, which is invalid RDF. On the other hand, cq(Q1) ≡Σ cq(Q2) does not
hold. To see why, consider the relational instance I := {T(a, a, “l”), T(“l”, a, a)},
where a, “l” ∈∆. We observe that I |= Σ, (cq(Q1))(I) = {(a)}, and (cq(Q2))(I) = ∅.
Therefore, our scheme would not detect Σ-equivalence between Q1 and Q2.
2
Arguably, Example 5.16 presents a constructed scenario and it seems reasonable
to assume that such constraints (which in some sense contradict the type restrictions
of RDF) do not occur in practice. We next provide a precondition that guarantees
completeness for virtually all practical scenarios. It relies on the observation that, in
the example above, (cq(Q1))Σ and (cq(Q2))Σ (i.e., the queries obtained when chasing
cq(Q1) and cq(Q2) with Σ, respectively) do not reﬂect valid SPARQL queries. We
can guarantee completeness if we explicitly exclude such cases:
Lemma 5.3 Let D be an RDF database and let Q be an Aπ expression such
that cq−1((cq(Q))Σ) ∈Aπ. If cbΣ(cq(Q)) terminates then for all Q′ ∈Aπ such that
cq−1((cq(Q′))Σ) ∈Aπ we have Q′ ∈cq−1(cbΣ(cq(Q))) ⇔Q′ ≡Σ Q and Q′ minimal.2
Conditions cq−1((cq(Q))Σ) ∈Aπ and cq−1((cq(Q′))Σ) ∈Aπ in the lemma encode
exactly the before-mentioned restrictions that the back-translation of the chase re-
sult for cq(Q) and cq(Q′) with Σ are valid SPARQL (And-only) queries.
Proof of Lemma 5.3
Direction “⇒” follows from Lemma 5.2, so it suﬃces to prove direction “⇐”. So let
us assume that Q′ ≡Σ Q and Q′ is minimal. First observe that cq−1((cq(Q′))Σ) and
cq−1((cq(Q))Σ) are Aπ expressions. From Lemma 5.1 it follows that cq(Q′) ≡Σ cq(Q).
From this observation, the minimality of Q′, and the correctness of the translation
in Lemma 5.1 it follows that cq(Q′) ∈cbΣ(cq(Q)) and Q′ ∈cq−1(cbΣ(cq(Q))).2
5.4.4. SPARQL-speciﬁc Optimization
By now we have established a mechanism that allows us to enumerate equivalent
minimal queries of SPARQL And-only queries or subqueries. Next, we present ex-
tensions that go beyond And-only queries. We open the discussion with rewritings
that can be used to simplify queries involving operator Filter:
139

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
Lemma 5.4 Let Q1, Q2 ∈A, S ⊂V \ {?y} be a set of variables, Σ be a set of
TGDs and EGDs, ?x, ?y ∈pVars(JQ2KD), and D be a document such that D |= Σ.
We write Q2
?x
?y to denote the query obtained from Q2 by replacing each occurrence
of ?y through ?x. The following rules are valid.
(FSI)
If Q2 ≡Σ Q2 Filter (?x =?y), then
SelectS(Q2) ≡SelectS(Q2
?x
?y).
(FSII)
If Q2 ≡Σ Q2 Filter (?x =?y), then
JQ2 Filter (¬(?x =?y))KD = ∅.
(FSIII)
If Q1 ≡Σ SelectpVars(JQ1KD)(Q1 And Q2), then
J(Q1 Opt Q2) Filter(¬bnd(?x))KD = ∅.
2
The intended use of the rewriting rules in Lemma 5.4 is as follows. We utilize
the chase algorithm to check if the precondition holds; whenever this is the case,
then we may exploit the equivalences in the conclusion. More precisely, the pre-
condition Q1 ≡Σ SelectpVars(JQ1KD)(Q1 And Q2) in rule (FSIII) can be checked by
testing if cq(SelectpVars(JQ1KD)(Q1)) and cq(SelectpVars(JQ1KD)(Q1 And Q2)) are Σ-
equivalent (it is folklore that Σ-equivalence between CQs can be tested with the chase
whenever the chase result for both queries is deﬁned). To test the preconditions for
(FSI) and (FSII), we can check if Σ |= body(cq(Q2)) →?x =?y holds, which can
easily be proven to be equivalent to the precondition Q2 ≡Σ Q2 Filter (?x =?y);
thus, we reduce the problem of checking the precondition to the implication problem
for constraints, which has been studied e.g. in [BB79; MMS79; BV84; CV85].
The ﬁrst rule (FSI) in Lemma 5.4 states that, if the constraint set implies equiva-
lence between ?x and ?y (in some And-only query), we can replace each occurrence
of ?y by ?x if ?y if projected away (observe that S ⊂V \ {?y} by assumption). Rule
(FSII) states that, under the same precondition, a ﬁlter for condition ¬(?x =?y)
is never satisﬁed. Finally (FSIII) tackles negation queries discussed before in Sec-
tion 4.2.6. We illustrate the idea behind rule (FSIII) in the following example.
Example 5.17 Consider the SPARQL query
Qf := Select?s,?m(
((?s, rdf:type, Student) Opt (?s, matric, ?m)) Filter (¬bnd(?m))),
which selects all students without matriculation number. Further assume that the
constraint set Σ := {Min•(Student, 1, matric)} is given, which asserts that every
student has a matriculation number. First note that Σ is weakly acyclic, so the
chase with Σ terminates for every instance. Next, we observe that the precondition
of rule (FSIII) from Lemma 5.4 holds, i.e. it is easy to see that (?s, rdf:type, Student)
is Σ-equivalent to Select?s((?s, rdf:type, Student) And (?s, matric, ?m)). We there-
fore conclude that JQfKD = ∅holds on every document D |= Σ.
2
140

5.4. Semantic Query Optimization for SPARQL
Having discussed the idea behind the rules, we next prove them formally.
Proof of Lemma 5.4
Rule (FSI): ⇒: Assume that Q2 ≡Σ Q2 Filter (?x =?y) and consider a mapping
µ ∈JSelectS(Q2)KD. Then µ is obtained from some µl ∈JQ2KD by projecting S.
By precondition, µl is also contained in JQ2 Filter (?x =?y)KD, so we know that
?x, ?y ∈dom(µl) and µl(?x) = µl(?y). It is easily veriﬁed that in this case there is a
mapping µr ∈Q2
?x
?y that agrees with µl on all variables dom(µl)\?y and is unbound
for ?y (cf. the proof of rule (FElimI) from Lemma 4.3). Given that ?y ̸∈S and
the observation that µ is obtained from µl by projecting S, we conclude that µ
is also obtained from µr when projecting S. Consequently, µ is generated by the
right side expression SelectS(Q2
?x
?y). ⇐: Assume that Q2 ≡Σ Q2 Filter (?x =?y)
and consider a mapping µ ∈JSelectS(Q2
?x
?y)KD. Then µ is obtained from some
µr ∈JQ2
?x
?yKD by projecting the variables S. It can be shown that then the mapping
µl := µr ∪{?y 7→µr(?x)} is contained in JQ2KD (cf. the proof of rule (FElimI) from
Lemma 4.3). Given that ?y ̸∈S and the observation that µ is obtained from µr
by projecting S, we conclude that µ is also obtained from µl when projecting S.
Consequently, the left side expression JSelectS(Q2)KD generates mapping µ.
Rule (FSII): Follows directly by assumption, i.e. each µ ∈JQ2KD is also generated
by JQ2 Filter (?x =?y)KD and therefore cannot satisfy the ﬁlter condition ¬?x =?y.
Rule (FSIII): First observe that precondition ?x ∈pVars(JQ2KD) and Q2 ∈A
implies that ?x ∈cVars(JQ2KD). From Proposition 4.2 we obtain that ?x ∈dom(µ2)
foreach µ2 ∈JQ2KD and it easily follows from Deﬁnition 4.2 that ?x ∈dom(µ)
foreach mapping µ ∈JQ1 And Q2KD. Now consider the expression
JQ1 Opt Q2KD = (JQ1KD 1 JQ2KD) ∪(JQ1KD \ JQ2KD)
and put Ω1 := JQ1KD 1 JQ2KD = JQ1 And Q2KD, Ω\ := JQ1KD \ JQ2KD. From the
above considerations we know that ?x ∈dom(µ1) foreach µ1 ∈Ω1. We now argue
that Ω\ = ∅, which implies that the equivalence holds, because then ?x ∈dom(µ)
for every µ ∈JQ1 Opt Q2KD and therefore no mapping satisﬁes the ﬁlter condi-
tion ¬bnd(?x). To show that Ω\ = ∅let us for the sake of contradiction assume there
is µ\ ∈Ω\. This implies that µ\ ∈JQ1KD and there is no compatible mapping µ2 ∼µ\
in JQ2KD. Now observe that by assumption µ\ ∈JSelectpVars(JQ1KD)(Q1 And Q2)KD.
Hence, there must be µ1 ∈JQ1KD, µ2 ∈JQ2KD such that µ1 ∼µ2 and µ1 ∪µ2 ⊇µ\.
Consequently, it trivially holds that µ2 ∼µ\, which contradicts the initial assump-
tion that there is no compatible mapping µ2 ∼µ\ in JQ2KD.2
Having presented semantic rewriting rules for the Filter operator, we ﬁnally turn
towards operator Opt, the most complex construct in the SPARQL query language
(as discussed in Chapter 3). The following lemma lists two useful semantic rewriting
rules for the Opt operator, similar in style to the previous ﬁlter rewriting rules.
141

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
Lemma 5.5 Let Q1, Q2, Q3 ∈A and S ⊂V .
(OSI)
If Q1 ≡Σ SelectpVars(JQ1KD)(Q1 And Q2), then
Q1 Opt Q2 ≡Σ Q1 And Q2.
(OSII)
If Q1 ≡Σ Q1 And Q2, then
Q1 Opt (Q2 And Q3) ≡Σ Q1 Opt Q3.
2
Analogical to Lemma 5.4, we can check the preconditions using the chase algo-
rithm. Rule (OSI) shows that in some cases Opt can be replaced by And; informally
speaking, this rule applies when the expression in the Opt clause is implied by the
constraints set. We argue that it is particularly useful when the author of the query
is not fully aware of all integrity constraints that hold on the input database: in
such cases, she may specify parts of the query that are implicit by the constraints
as optional, just to be sure not to miss relevant answers. Rule (OSII) is useful to
eliminate redundant And-only subexpressions in Opt clauses. Before proving the
rules, we demonstrate (OSI) by means of a small, easy-to-verify example.
Example 5.18 We sketch a possible optimization for query
Qo := Select?p,?gn,?sn(t1 Opt (t2 And t3)), with
t1 := (?p, rdf:type, Person), t2 := (?p, givenname, ?gn), t3 := (?p, surname, ?sn)
under constraint set Σ := Total•(Person, givenname) ∪Total•(Person, surname).
Note that, when instantiating the constraint templates according to their deﬁni-
tion in Figure 5.3, we obtain a set of TGDs and EGDs and it is straightforward to
show that this constraint set is weakly acyclic, so the chase will always terminate.
Next, it is easily veriﬁed that t1 is Σ-equivalent to Select?p(t1 And (t2 And t3)),
so optimization rule (OSI) from Lemma 5.5 is applicable. We therefore conclude that
Q′
o := Select?p,?gn,?sn(t1 And (t2 And t3))
is Σ-equivalent to Qo. This transformation also clears the way for further opti-
mizations. For instance, using the algebraic equivalences we now could reorder the
triples t1, t2, t3 in Q′
o according to rules (JIdem) and (JAss) from Figure 4.1.
2
We conclude our discussion of SQO for SPARQL with the proof of Lemma 5.5:
Proof of Lemma 5.5
Rule (OSI): We transform Q := JQ1 Opt Q2KD systematically. Let D be an RDF
database such that D satisﬁes all constraints in Σ. Then
JQKD = JQ1 Opt Q2KD
= (JQ1KD 1 JQ2KD) ∪(JQ1KD \ JQ2KD)
= JQ1 And Q2KD ∪(πpVars(JQ1KD)(JQ1 And Q2KD) \ JQ2KD)
142

5.5. Related Work
It is easy to verify that each mapping in JQ1 And Q2KD is compatible with at least
one mapping in Q2, and the same holds for πpVars(JQ1KD)(JQ1 And Q2KD). Hence, the
right side union subexpression can be dropped and we obtain Q ≡Σ Q1 And Q2.
Rule (OSII): Let D be an RDF database such that D |= Σ. We transform the
expression Q := JQ1 Opt (Q2 And Q3)KD schematically:
JQKD = J(Q1 Opt (Q2 And Q3))KD
= JQ1 And Q2 And Q3KD ∪(JQ1KD \ JQ2 And Q3KD)
= JQ1 And Q3KD ∪(JQ1 And Q2KD \ JQ2 And Q3KD)
(∗)
= JQ1 And Q3KD ∪(JQ1 And Q2KD \ JQ3KD)
= (JQ1KD 1 JQ3KD) ∪(JQ1KD \ JQ3KD)
= JQ1 Opt Q3KD,
where step (*) follows from the observation that the equation
JQ1 And Q2KD \ JQ2 And Q3KD ≡JQ1 And Q2KD \ JQ3KD
holds; the formal proof of this equation is similar in style to the rewriting presented
in Example 4.13 (at algebraic level). We omit the details.2
5.5. Related Work
Data Dependencies. Short time after the invention of the relational model [Cod69;
Cod70], Codd proposed functional dependencies for relational databases and used
them to develop the notions of 2NF and 3NF [Cod71; Cod72]. Since then, consid-
erable eﬀort has been spent in the investigation of integrity constraints for the rela-
tional model. New constraint classes beyond functional dependencies have been pro-
posed and systematically explored, such as multi-valued dependencies [Fag77], join
and inclusion dependencies [Dat81], or embedded multivalued dependencies [Fag77]
(see [AHV] for an overview of these classes and a summary of important results).
Closely related to the study of constraints is the theory of data integrity and
normal forms for the relational model, which are important to avoid update, insert,
and deletion anomalies in database instances. Beyond the above-mentioned 2NF and
3NF, more advanced normal forms have been proposed over the years, such as the
Boyce-Codd normal form [Cod74] or the fourth normal form [Fag77].
Another central problem in the context of data dependencies is the constraint im-
plication problem for diﬀerent classes of constraints [BB79; MMS79; BV84; CV85],
i.e. the question whether, given a constraint set Σ and a single constraint ϕ, Σ im-
plies ϕ. Whether this problem is decidable or not depends on the classes of the input
constraints. To give an example, the implication problem for the class of functional
dependencies is decidable, but it becomes undecidable when adding inclusion de-
pendencies [CV85]. The interested reader will ﬁnd initial results on the decidability
143

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
of the implication problem for RDF constraints in [LMS08], where we identify sub-
classes of RDF constraints for which the implication is decidable and undecidable.
Related to the formalization of constraints in ﬁrst-order logic, in [Nic78] it was
shown that constraint classes like functional, join, and inclusion dependencies (orig-
inally deﬁned using ad-hoc syntax) have natural representations in ﬁrst-order logic.
A recent summary of ﬁrst-order logic representations of integrity constraints was
given in [Deu08]. The formalization in this chapter followed these two approaches.
Semantic Query Optimization and Chase. Beyond their investigation in the
context of data integrity, data dependencies have been extensively studied in the
area of constraint-based query optimization (see e.g. [ASU79; Kin81; JK82; SO87;
CGM90; BFW99; DT01; DT05; CGK08]). Most of these semantic optimization ap-
proaches build upon the chase algorithm [ASU79; MMS79; JK82; BV84], which
has originally been proposed to tackle the implication problem for data dependen-
cies [MMS79; BV84] and to optimize conjunctive queries under data dependen-
cies [ASU79; JK82]. Since then, the chase has been successfully applied in many
other application areas, such as data exchange [FKMP05], data integration [Len02],
query answering using views [Hal01], and probabilistic databases [OHK09].
Given the central role of the chase and the observation that the algorithm does
not necessarily terminate (cf. the discussion in Section 5.1.4), much research ef-
fort has been spent in ﬁnding suﬃcient conditions that guarantee its termina-
tion [FKMP05; DNR08; Mar09; SML08; MSL09d; MSL09a]. The common idea of
all these termination conditions is to statically assert that there are no positions
in the database schema where fresh labeled null values might be cyclically created
in. We exemplarily presented weak acyclicity as a suﬃcient termination conditions
in Section 5.1.4. In [DNR08], weak acyclicity was generalized to a condition called
stratiﬁcation. In [MSL09d; MSL09a] we introduced termination conditions that fur-
ther generalize stratiﬁcation, namely safe restriction, inductive restriction, and the
so-called T-hierarchy of constraint classes.4 Like weak acyclicity and stratiﬁcation,
these suﬃcient conditions guarantee chase termination on every database instance.
Further, in these publications we studied the problem of data-dependent chase ter-
mination and, as a key result, developed speciﬁc termination conditions for the chase
algorithm w.r.t. ﬁxed database instances. We will not go into more detail here and
refer the interested reader to the original publications for details.
SQO for SPARQL. By the best of our knowledge, constraint-based optimiza-
tion in the context of RDF has ﬁrst been proposed in [SKCT05]; we refer the reader
back to Section 5.4 for a short discussion of that work. In addition, we discussed con-
straints for RDF and SQO for SPARQL in [LMS08; SML08]. The results presented
in this chapter extend the ideas of the two previous publications.
4There is a minor bug in the stratiﬁcation condition from [DNR08], which also carries over to some
of the conditions presented in [MSL09d; MSL09a]. We propose a ﬁx for this bug in [MSL09b].
144

5.6. Conclusion
5.6. Conclusion
In this chapter we discussed diﬀerent aspects of constraints in the context of RDF
and RDFS data. In particular, we showed that such constraints typically fall into
well-known constraints classes that have been extensively studied in the context of
other data formats and that SPARQL can easily be extended to check and specify
constraints, similar in style to the SQL query language, which oﬀers mechanisms
to test and deﬁne constraints for the relational model. Furthermore, we presented
a semantic query optimization approach for SPARQL. Falling back on established
algorithms like the chase and the C&B algorithm for conjunctive query optimiza-
tion, our scheme gives us guarantees to ﬁnd minimal conjunctive queries if the chase
terminates. Tackling the issue of more complex SPARQL queries, we presented se-
mantic optimization rules for queries involving operators Filter and Opt.
We conclude with some ﬁnal remarks on our SQO scheme for SPARQL. First, we
want to note that semantic optimization strategies are basically orthogonal to alge-
braic optimization schemes. Hence, the semantic optimization approach presented
in this chapter can be coupled with the algebraic rewriting rules from Chapter 4. To
give an example, we may get better optimization results when combining the rules
for ﬁlter decomposition, elimination, and pushing from Figure 4.4 with the semantic
rewriting rules for ﬁlter expressions stated in Lemma 5.4. As another example, in
Example 5.15 we demonstrated that the projection pushing rules from Figure 4.3
can be used to increase the beneﬁt of our semantic optimization scheme.
A second important property of our SQO approach is that, as proposed in [DPT06],
the C&B algorithm can be enhanced by a cost function, which makes it easy to factor
in cost-based query optimization approaches for SPARQL such as [SSB+08]. This
ﬂexibility strengthens the prospectives and practicability of our semantic optimiza-
tion scheme. The study of rewriting heuristics and the integration of a cost function,
though, is beyond the scope of this thesis and is left as future work.
145

Chapter 5. Constraints for RDF and Semantic SPARQL Query Optimization
146

Chapter 6.
SP2Bench: A SPARQL Performance
Benchmark
Roy:
“We’re ﬁnally done with optimization. Now others can use our
algebraic and semantic optimization schemes to develop new
SPARQL optimization approaches and build eﬃcient engines!”
Jen:
“But how do they prove they’re better than others?”
Moss: “And how do they show they’re eﬃcient at all?”
Roy:
“Right, we need a comprehensive benchmark for SPARQL!”
As discussed in Section 4.5, over the last years a variety of proposals for the eﬃ-
cient SPARQL evaluation have been made [ACKP01; BKvH02; HG03; Cyg05; HD05;
SKCT05; TCK05; CLJF06; PAG06a; AMMH07; GGL07; HH07; Pol07; AG08a;
FB08; LMS08; NW08; SSB+08; WKB08; NW09]. As a proof of concept, most of
these approaches have been evaluated experimentally either in user-deﬁned scenar-
ios, on top of the LUBM benchmark [GPH05], or on top of the Barton Library
benchmark [AMMH]. We argue that none of these scenarios is adequate for testing
SPARQL implementations in a general and comprehensive way.
1. User-deﬁned scenarios are typically designed to demonstrate very speciﬁc prop-
erties of engines and, for this reason, often lack generality. To assess the per-
formance of engines in an independent way, though, it is desirable to have a
comprehensive benchmark platform that covers a broad range of challenges
that may arise in the context of SPARQL query evaluation.
2. The LUBM benchmark was primarily designed to test the reasoning and infer-
ence mechanisms of Knowledge Base Systems. In this line, the main challenge
of the LUBM queries lies in the eﬃciency of the inferencing process, which is
not even part of the SPARQL evaluation process, where it is assumed that
inferencing is realized by a separate layer (see the discussion in Section 5.4.1).
Coming along with this design decision, in the LUBM benchmark queries cen-
tral SPARQL operators like Union and Opt are missing. We argue that, to
test the performance of SPARQL evaluation approaches, one should rather
consider SPARQL evaluation on top of a ﬁxed data set, i.e. without inference
or where the inferred triples have already been materialized.
147

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
3. The Barton Library benchmark is strongly application-oriented. Its queries,
which have been derived from a typical browsing session through the Barton
Library online catalog, are speciﬁed in SQL and it is assumed that RDF is
stored in a relational database. One problem is that not all of these queries
can be expressed in SPARQL, due to missing language support for aggre-
gation. Beyond that, in the corresponding translations of those queries that
can be expressed, central SPARQL operators like Opt are missing. Further,
a closer investigation of the Barton queries reveals that the join patterns in
these queries are rather uniform (cf. the discussion in [SHK+08]). A compre-
hensive benchmark should cover all SPARQL operators, important operator
constellations, as well as a variety of data access and join patterns.
Based on the observations above, there is an urgent need for a benchmark frame-
work that allows to assess the performance of SPARQL engines and, more gener-
ally, RDF storage schemes in an extensive and uniform way. In response, we have
developed a benchmark suite for the SPARQL query language that fulﬁlls these
requirements, called SPARQL Performance Benchmark (SP2Bench).1
The focus and design goals of SP2Bench vary from the benchmark projects dis-
cussed before. In particular, SP2Bench diﬀers in that it is neither application-
oriented nor use-case driven, but falls into the class of language-speciﬁc benchmarks.
This means that, compared to the other benchmarks, the document and query de-
sign in SP2Bench is not driven by a speciﬁc use-case, but instead is speciﬁcally laid
out to test common SPARQL constructs, operator constellations, and a variety of
RDF data access patterns. In this line, SP2Bench covers a broad range of challenges
that SPARQL engines may face in diﬀerent contexts and constitutes a framework
that allows for comprehensive performance evaluation, rather than performance as-
sessment in a speciﬁc, application-driven scenario. The SP2Bench queries are not
intended to be evaluated in a work load setting, but rather on a one by one basis,
where each query poses diﬀerent challenges to the tested SPARQL engine and may
help to identify deﬁciencies in the underlying evaluation strategy. With these design
decisions, SP2Bench allows to assess the generality of optimization approaches and
to compare evaluation strategies in a universal, application-independent setting. We
want to emphasize that such language-speciﬁc benchmarks (such as, for instance, the
XQuery benchmark XMark [SWK+02]) have found broad acceptance in the past.
The ﬁrst component of SP2Bench is a data generator, which supports the cre-
ation of DBLP-style models in RDF format. DBLP [Ley] is a well-known biblio-
graphic library that contains publications made in the area of databases and, more
generally, computer science. The generated DBLP-like documents mirror vital key
characteristics and distributions found in the original DBLP database, making it
possible to create arbitrarily large documents with realistic data that exhibits many
1The SP2Bench data generator and benchmark queries can be downloaded in a ready-to-use
format at http://dbis.informatik.uni-freiburg.de/index.php?project=SP2B.
148

real-world characteristics. The data mimics natural correlations between entities,
such as power law distributions and limited growth curves. Complementarily to the
generator, SP2Bench comprises 17 meaningful queries speciﬁed over the generated
documents. These queries cover important SPARQL constructs, operator constel-
lations, and vary in their characteristics, such as complexity and result size. The
detailed knowledge of data characteristics, which is obtained by an elaborate anal-
ysis of the original DBLP database, makes it possible to predict the challenges that
the queries impose on SPARQL engines. This contributes to the understanding of
the SP2Bench queries and facilitates the interpretation of benchmark results.
The key contributions of this chapter are the following.
• We present SP2Bench, a comprehensive benchmark for SPARQL. Our frame-
work comprises a data generator and a collection of 17 benchmark queries.
• The benchmark data generator supports the creation of arbitrarily large doc-
uments in RDF format, reﬂecting key characteristics and social-world distri-
butions found in the DBLP database, a well-known bibliographic library. In
addition, the generated documents cover a variety of RDF constructs, such
as blank nodes and RDF containers. While, in the context of SP2Bench, this
data forms the basis for the design of challenging and predictable benchmark
queries, the data generator may also be useful in other Semantic Web projects,
i.e. whenever large amounts of test data with natural distributions are needed.
• The benchmark queries have been carefully designed to test common opera-
tor constellations, data access patterns, and SPARQL optimization strategies.
They comprise both SPARQL Select and Ask queries. In the exhaustive
discussion of these queries we also highlight the speciﬁc challenges that these
queries impose on SPARQL engines and discuss possible evaluation strategies.
• We ﬁnally propose a set of performance metrics that capture diﬀerent aspects
of the evaluation process, such as runtime and memory consumption. These
metrics allow for a schematic analysis and interpretation of SP2Bench bench-
mark results, as well as a comparison of results from diﬀerent engines.
Structure. We start with a discussion of general and SPARQL-speciﬁc desider-
ata for benchmarks in Section 6.1, including design decisions made in the SP2Bench
framework. Subsequently, in Sections 6.2 and 6.3 we turn towards a study of the
SP2Bench data generator. This discussion includes an analysis of key characteristics
of the DBLP data set, which forms the basis for the implementation of the data gen-
erator. The profound knowledge of DBLP characteristics helps to understand the key
challenges of the SP2Bench queries, which are presented in Section 6.4. The chapter
ends with a discussion of performance metrics for the SP2Bench suite (Section 6.5),
a summary of related work (Section 6.6) and a short conclusion (Section 6.7).
149

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
6.1. Benchmark Design Decisions
A central aspect in the design of a benchmark is the choice of an appropriate domain.
Clearly, the domain of a language-speciﬁc benchmark should not only constitute a
representative scenario that captures the philosophy behind the data format, but
also leave room for challenging benchmark queries. With the choice of the DBLP
library [Ley], a bibliographic database that contains a large collection of publications
made in the area of computer science, SP2Bench satisﬁes both desiderata. First, the
RDF data format has been particularly designed to encode meta data (cf. [rdfb]),
which makes DBLP an excellent candidate for an RDF scenario. Further, as shown
in [EL05], DBLP reﬂects interesting social-world distributions. One may expect that
such distributions are frequently found in the Semantic Web, which integrates a great
many of individual databases into one global database and therefore can be seen as
a large social network. As an example, it has been shown in [TTKC08] that power-
law distributions are naturally contained in large RDF Schema speciﬁcations. These
observations justify the choice of DBLP as the underlying scenario and facilitate the
design of interesting queries on top of these natural social-world distributions.
In the context of semi-structured data, one often distinguishes between data-
and document-centric scenarios. Document-centric design typically involves large
amounts of free-form text, while data-centric documents are more structured and
usually processed by machines rather than humans. As discussed in Section 2.2, one
major design goal of RDF was to provide a mechanism to encode information in
a machine-readable way, so the RDF data format basically follows the data-centric
approach. We point out that also in this respect, the DBLP domain – which provides
structured data and only little free text – constitutes an adequate RDF scenario.
Rather than using an RDF version of the existing DBLP data set (such as [BC07]),
SP2Bench comes with a generator that supports the creation of arbitrarily large
DBLP-like documents in RDF format, hence overcoming an upper limit on the
size of benchmark documents.2 The generator itself relies on an in-depth study
of characteristics and relationships between entities found in the original DBLP
database, comprising the analysis of data entities (such as articles and authors),
their properties, frequency, and also their interaction. Consequently, the generated
documents mimic a broad range of natural, social-world distributions such as power
laws (found in the citation system or the distribution of publications among authors)
and limited growth curves (e.g., the increasing number of publications over time).
Complementarily to the data generator, we have designed 17 meaningful bench-
mark queries that operate on top of the generated documents. As we will show later,
these queries cover not only the most important SPARQL constructs and operator
constellations, but also vary in other characteristics, such as complexity, result size,
2In September 2009, the XML-to-RDF translation of the original DBLP database in [BC07]
comprised only about 15 million RDF triples, which is far away from a large-scale RDF scenario.
150

6.2. The DBLP Data Set
and join patterns. We want to stress that the detailed knowledge of data charac-
teristics is crucial to design meaningful, challenging queries and makes it possible
to predict the challenges that the benchmark queries impose on SPARQL engines.
This, in turn, facilitates the interpretation of the benchmark results.
Benchmark Design Principles. The Benchmark Handbook [Gra93] postulates
four key requirements for domain speciﬁc benchmarks. First, such a benchmark
should be (1) relevant, thus testing representative and typical operations within
the speciﬁc domain. Second, benchmarks should be (2) portable, i.e. should be exe-
cutable on diﬀerent platforms. Third, each benchmark should be (3) scalable, which
in particular means that it should be possible to run the benchmark on both small
and very large data sets. Last but not least, every benchmark must be (4) under-
standable, since otherwise it will with high probability not be adopted in practice.
For a language-speciﬁc benchmark, the relevance requirement (1) suggests that
queries implement realistic requests on top of the data. Moreover, the benchmark
queries should not focus on verifying the correctness of the tested engine, but rather
on common operator constellations that impose representative, particular challenges.
To give an example for the manifestation of these ideas in SP2Bench, two of our
queries test (closed-world) negation, which – as discussed previously in Section 4.2.6
– can be expressed in SPARQL through a characteristic combination of operators
Opt, Filter, and ﬁlter predicate bnd (see queries Q6 and Q7 in Section 6.4).
Requirements (2) portability and (3) scalability bring along technical challenges
concerning the implementation of the data generator. Addressing those, our data
generator is deterministic, platform independent, and accurate w.r.t. the desired size
of generated documents. It is eﬃcient and gets by with a constant amount of main
memory, and hence supports the generation of arbitrarily large RDF documents.
From the viewpoint of an engine developer, a benchmark should give hints on
deﬁciencies in the design and implementation of the respective engine. This is where
requirement (4) understandability comes into play, i.e. it is important to keep queries
simple and understandable. At the same time, the queries should leave room for
diverse optimizations. In this regard, we took special care to design them in such a
way that they are amenable to a wide range of optimization strategies.
6.2. The DBLP Data Set
We start our presentation of SP2Bench with a study of the DBLP data set, which
lays the foundations for the design and implementation of the generator. To this end,
we analyze properties, relations, and distributions of bibliographic entities and per-
sons found in the original DBLP data set. Such analysis of frequency distributions
in scientiﬁc production is not new, but has ﬁrst been discussed in [Lot26]. Similar in
spirit, a study of characteristics of the DBLP database has been presented in [EL05].
The latter work considers a subset of DBLP, called DBLP-DB, restricting the dis-
151

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
cussion to publications in database venues. As a key contribution, it is shown that
DBLP-DB, reﬂects vital relations that are typically encountered in social networks,
thus forming a “small world” on its own. We relax the restriction from [EL05] and
consider the whole DBLP database (instead of DBLP-DB). Our analysis shows that
similar social-world relations can also be found in the complete database. We note
that, although the analysis in [EL05] forms valuable groundwork for the study pre-
sented in this section, our approach diﬀers in that it is of more pragmatic nature:
the ultimate goal of our investigation is to approximate real-world distributions by
concrete functions that can be used for the implementation of a data generator.
In order to approximate distributions, we use function families that naturally re-
ﬂect the scenarios, such as logistics curves for modeling limited growth or power
equations for power law distributions. All approximations presented in this sec-
tion have been computed with the ZunZun [zun] data modeling tool and the gnu-
plot [gnu] curve ﬁtting module. Our starting point for the study of DBLP was the
February 25, 2008 XML version of the DBLP data. Data extraction from the DBLP
XML database (such as, for instance, the extraction of authors or diﬀerent types of
publications) was realized using the MonetDB/XQuery [CWI] processor.
Apart from the pure extraction and approximation of distributions found in the
DBLP data set, an important objective of this section is to provide insights into key
characteristics of DBLP data. Although it is impossible to mirror all relations found
in the original data, we work out a variety of interesting relationships, considering
diﬀerent types of publications, the role of persons as authors and editors, as well as
the citation system. In this line, we mainly study the development of such entities
over time, on a year-by-year basis. The insights that we gain establish a deep under-
standing of the benchmark queries and their speciﬁc challenges. As an example, the
benchmark queries Q3a, Q3b, and Q3c (discussed in Section 6.4) look quite similar,
but pose diﬀerent challenges based on the probability distribution of article proper-
ties (i.e. swrc:pages, swrc:month, swrc:isbn), which will be discussed within this
section. As another example, Q7 (also given in Section 6.4) heavily depends on the
DBLP citation system, which will be another central topic in this section.
In the end, our analysis allows us to implement a data generator that creates data
that is very similar to the original DBLP data, at least for years up to present. Of
course we cannot guarantee that the generated data goes hand in hand with the
original DBLP data when generating data for future years. However, and this is
even more important, by implementing reasonable social-world distributions, also
data for the future reﬂects real-world relationships between data entities, which is
clearly preferable to purely synthetic data. We ﬁnally stress that the benchmark
queries are designed to operate on top of exactly those relations and distributions
that are mirrored by our data generator, which makes them realistic, predictable
and understandable. For instance, some queries cover the citation system, which is
mirrored by our generator. As a counterexample, the generator abstracts from a
realistic distribution of article release months, so no query relies on this property.
152

6.2. The DBLP Data Set
<!ELEMENT dblp
(article|inproceedings|proceedings|book|
incollection|phdthesis|mastersthesis|www)*>
<!ENTITY % field
"author|editor|title|booktitle|pages|year|address|
journal|volume|number|month|url|ee|cdrom|cite|
publisher|note|crossref|isbn|series|school|chapter">
<!ELEMENT article (%field;)*>
<!ELEMENT inproceedings (%field;)*>
...
<!ELEMENT www (%field;)*>
Figure 6.1.: Extract of the DBLP DTD.
Article
Inproc.
Proc.
Book
WWW
PhDTh.
MastTh.
Incoll
author
0.9895
0.9970
0.0001
0.8937
0.9973
1.0000
1.0000
0.8459
cite
0.0048
0.0104
0.0001
0.0079
0.0000
0.0000
0.0000
0.0047
editor
0.0000
0.0000
0.7992
0.1040
0.0004
0.0000
0.0000
0.0000
isbn
0.0000
0.0000
0.8592
0.9294
0.0000
0.0222
0.0000
0.0073
journal
0.9994
0.0000
0.0004
0.0000
0.0000
0.0000
0.0000
0.0000
month
0.0065
0.0000
0.0001
0.0008
0.0000
0.0333
0.0000
0.0000
pages
0.9261
0.9489
0.0000
0.0000
0.0000
0.0000
0.0000
0.6849
title
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
Table 6.1.: Probability distribution for selected attributes.
6.2.1. Structure of Document Classes
Our starting point for the discussion of the DBLP data set is the DBLP DTD, which
comes with the public release of the DBLP XML data set.3 An extract of the DTD
is provided in Figure 6.1. The DTD entry point dblp deﬁnes eight child entities,
namely Article, Inproceedings, Proceedings, . . . , and WWW resources. In
the following, we will call these entities document classes, and instances thereof
documents. Furthermore, we distinguish between Proceedings documents, called
conferences, and instances of the remaining classes, called publications.
The DTD deﬁnes 22 possible child tags for each document class, such as author,
editor, and title. These tags describe documents and we call them attributes
in the following. As can be seen, according to the DTD each document might be
described by an arbitrary combination of attributes and even repeated occurrences
of the same attribute are allowed. For instance, a document may have one title
attribute specifying its title and, in addition, several author attributes. As one may
expect, in practice only a subset of all document class/attribute combinations occur.
3See http://www.informatik.uni-trier.de/∼ley/db/.
153

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
 0.04
 0.03
 0.02
 0.01
 60
 50
 40
 30
 20
 10
 1
probability for x citations
number of citations = x
probability for number of citations
approx. probability for number of citations
Figure 6.2.: Distribution of citations for documents having at least one citation.
For instance, it turns out that attribute pages is never associated with WWW
documents, but instead is typically used in combination with Article documents.
We summarize these connections between document classes and attributes in Ta-
ble 6.1. It shows, for selected document class/attribute pairs, the probability that
the attribute describes a document of this class.4 To give an example, about 92.61%
of Article documents are described by the attribute pages, but none of them has
an editor associated. In contrast, about 79.92% of all Proceedings documents are
described by one or more editor attributes. It is important to note that this anal-
ysis does not account for multiple occurrences of an attribute in a single document.
We will discuss such repeated occurrences of attributes separately in Section 6.2.2.
The probability distribution in Table 6.1 forms the basis for generating document
class instances of any type. Note that we simplify and assume that the presence
of an attribute does not depend on the presence of other attributes, i.e. we ignore
conditional probabilities. We will further elaborate on this decision in Section 6.7.
6.2.2. Repeated Attributes
We now turn towards an investigation of attributes that occur repeatedly within a
single document, called repeated attributes. A closer investigation of DBLP reveals
that in practice only few attributes occur repeatedly and, for the majority of them,
the number of repeated occurrences is very small. We decided to restrict ourselves to
an analysis of the most frequent repeated attributes, i.e. cite, editor, and author.
Figure 6.2 exempliﬁes our analysis for attribute cite. It shows, for each docu-
ment with at least one cite occurrence, the probability (y-axis) that the document
has exactly n cite attributes (x-axis). Recall that, according to Table 6.1, only a
small fraction of documents are described by cite (e.g., only 0.48% of all Article
documents). Arguably, this value should be close to 100% in a complete scenario,
4The full correlation matrix can be found in the technical report [SHLP08].
154

6.2. The DBLP Data Set
which shows that DBLP contains only a fraction of all existing citations. This is also
why, in our analysis (and hence in Figure 6.2), we consider only those documents
with at least one outgoing citation, since otherwise over 99% would get assigned zero
citations. Still, when assigning the number of citations to documents of a speciﬁc
class later on, we ﬁrst use the probability distribution of attributes in Table 6.1 to
estimate the number of documents with at least one outgoing citation and after-
wards apply the citation distribution in Figure 6.2 to only those documents that
– according to the previous step – get assigned citations. We emphasize that this
strategy exactly mirrors the distribution found in the original DBLP data.
Based on experiments with diﬀerent function families, we decided to use bell-
shaped Gaussian curves for approximating the distribution of citations from Fig-
ure 6.2.5 It is well-known that Gaussian curves are typically used to model normal
distributions. Although, strictly speaking, our data is not normally distributed (due
to the left limit at position x = 1), these curves nicely ﬁt the data for x ≥1 (see
Figure 6.2). Formally, Gaussian curves are described by functions
p(µ,σ)
gauss(x) =
1
σ
√
2πe−0.5( x−µ
σ )2,
(6.1)
where the expected value µ ∈R ﬁxes the x-position of the peak and σ ∈R>0
speciﬁes the statistical spread. For instance, the approximation function for the
cite distribution in Figure 6.2 is deﬁned by instantiating Equation (6.1):
dcite(x) := p(16.82,10.07)
gauss
(x).
(6.2)
The analysis and the resulting distribution of repeated editor attributes is struc-
turally similar to that of the citation attribute, so we omit the details; the concrete
approximation function for editor attributes is the Gaussian curve
deditor(x) := p(2.15,1.18)
gauss
(x).
(6.3)
The approximation function for repeated author attributes is based on a Gaussian
curve, too. However, we observed that the average number of authors per publication
has increased over the years. The same observation was made in [EL05] and explained
by an increasing pressure to publish and the proliferation of new communication
platforms, like the Internet. Due to the prominent role of authors, we decided to
mimic this property. As a consequence, parameters µ and σ are not ﬁxed for the
author attribute (as it was the case for the distributions dcite(x) and deditor(x)), but
modeled as functions over time. We make the assumption that the average number of
authors per publication will eventually stabilize and model both µ and σ as limited
growth functions (also called logistic curves). We consider logistic curves of the form
5We experimented with other distributions, such as Gamma distributions, log normal distribu-
tions, and Chi distributions (cf. [McL99]), but obtained the best ﬁtting for Gaussian curves.
155

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
 1
 1.5
 2
 2.5
 3
 3.5
 1936
 1950
 1975
 2000
 2025
 2050
expected value in year x
year = x
expected value
 0.5
 1
 1.5
 2
 1936
 1950
 1975
 2000
 2025
 2050
statistical spread in year x
year = x
statistical spread
Figure 6.3.: Development of author (a) expected value (µauth(yr)) and (b) statistical
spread (σauth(yr)) for publications having at least one author.
flogistic(x) =
a
1 + be−cx,
(6.4)
where a, b, c ∈R>0. For this parameter setting, a constitutes the upper asymptote
and the x-axis forms the lower asymptote. The curve is “caught” in-between its
asymptotes and increases continuously, i.e. it is S-shaped (two examples for logistic
curves are plotted in Figure 6.3; we will discuss them in the following).
The resulting distribution dauth(x, yr) for the author attribute, which gives the
probability for a publication in year yr having exactly x authors given that the
publication has at least one author, is then described by the Gaussian function
dauth(x, yr) := p(µauth(yr),σauth(yr))
gauss
(x), where
µauth(yr) :=
2.05
1 + 17.59e−0.11(yr−1975) + 1.05, and
σauth(yr) :=
1.00
1 + 6.46e−0.10(yr−1975) + 0.50.
(6.5)
Figures 6.3 (a) and (b) show the logistic curves for functions µauth(yr) and σauth(yr).
We can observe that both µauth(yr) and σauth(yr) increase steadily over time, but
stabilize in future years. The concrete limited growth approximation was obtained
by ﬁtting original DBLP data up to year 2005. The distribution of the author at-
tribute (i.e., function dauth(x, yr)) builds on µauth(yr) and σauth(yr). We plot this
distribution for selected years (1980, 2000, and the expected distribution for years
2020, 2040) in Figure 6.4(a). According to the growth of the expected value and
statistical spread, we can observe that the average number of authors increases over
time. Due to the fact that the growth is limited, this increase is signiﬁcant for early
years (cf. year 1980 vs. 2000), but diminishes in future years (cf. year 2020 vs. 2040).
156

6.2. The DBLP Data Set
 0.2
 0.4
 0.6
 1
 2
 3
 4
 5
probability of x authors per publication in given year
number of authors per publication = x
year 1980
year 2000
year 2020
year 2040
100k
10k
1k
 100
 10
 2005
 2000
 1990
 1980
 1970
 1960
number of documents in year x
year = x
proceedings
journals
inproceedings
articles
approx. proceedings
approx. journals
approx. inproceedings
approx. articles
Figure 6.4.: (a) Expected number of authors for publications having at least one
author (dauth(x, yr)); (b) Development of documents over time.
6.2.3. Development of Document Classes over Time
We next investigate the quantity of document class instances over time. We noticed
that DBLP contains only little and incomplete information in its early years, and
also found anomalies in the ﬁnal years, mostly in form of lowered growth rates. We
suspect that, in the coming years, some more conferences for the last years may
be added belatedly (i.e. data may not yet be complete) and therefore we restrict
ourselves to the years in-between 1960 and 2005 in the subsequent discussion
Figure 6.4(b) plots the number of Proceedings, Journal, Inproceedings,
and Article documents as a function of time. The y-axis is in log scale. Note that
Journal is not an explicit document class according to the DBLP DTD in Fig-
ure 6.1, but we assume that this class is implicitly deﬁned by the journal attribute
of Article documents, which speciﬁes the journal an article appeared in. As can
be seen in Figure 6.4(b), the inproceedings and articles are closely coupled to the
proceedings and journals, respectively. For instance, there are always about 50-60
times more inproceedings than proceedings. This indicates that the average number
of inproceedings per proceeding remains stable over time. Similar observations hold
with respect to the article documents and the journals they appeared in.
As a further observation, Figure 6.4(b) shows exponential growth for all docu-
ment classes under consideration, where the growth rate of Journal and Article
documents decreases in the ﬁnal years. This observation strongly suggests a limited
growth scenario, to be modeled by the logistic curve formula introduced in Equa-
tion (6.4). For instance, we approximate the number of Journal documents over
time, which is also visualized in Figure 6.4(b), by the logistic curve
fjournal(yr) :=
740.43
1 + 426.28e−0.12(yr−1950).
(6.6)
Approximation functions for Article, Proceedings, Inproceedings, Book,
157

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
farticle(yr) :=
58519.12
1+876.80e−0.12(yr−1950)
fbook(yr)
:=
52.97
40739.38e−0.32(yr−1950)
fproc(yr)
:=
5502.31
1+1250.26e−0.14(yr−1965)
fphd(yr)
:= random[0..20]
finproc(yr) :=
337132.34
1+1901.05e−0.15(yr−1965)
fmasters(yr) := random[0..10]
fincoll(yr) :=
3577.31
196.49e−0.09(yr−1980)
fwww(yr)
:= random[0..10]
Figure 6.5.: Approximation functions for document class counts.
and Incollection documents are also modeled as logistic curves and diﬀer only
in the parameters. Concerning the remaining document classes PhDThesis, Mas-
tersThesis, and WWW we found that documents thereof were distributed un-
steadily, so we modeled them by random functions random[x..y] (which generate
random numbers in the interval [x..y]) that reﬂect their distribution in the origi-
nal DBLP data set. The concrete approximation functions for all these document
classes are summarized in Figure 6.5, where we name each formula according to the
document class, e.g. farticle(yr) denotes the function that belongs to document class
Article. It is worth mentioning that the number of articles and inproceedings per
year clearly dominates the number of instances of the remaining classes.
Based on the previous analysis, we can easily estimate the total number of docu-
ments fdocs(yr) in year yr by summing up the individual counts:
fdocs(yr) := fjournal(yr) + farticle(yr) + fproc(yr) + finproc(yr)+
fincoll + fbook(yr) + fphd(yr) + fmasters(yr) + fwww(yr).
(6.7)
6.2.4. Authors and Editors
Authors. An estimation for the total number of authors in a given year, which we
deﬁne as the number of author attributes associated to the year, is obtained as
follows. First, we estimate the number of documents described by attribute author
for each document class individually (using the distribution in Table 6.1). All these
counts are summed up, which gives us an estimation for the total number of doc-
uments with one or more author attributes. Finally, this value is multiplied with
the expected average number of authors per paper in the respective year (which is
implicitly given by the distribution dauth(x, yr) discussed Section 6.2.2).
To be close to reality, we also consider the number of distinct persons that appear
as authors (relative to the year), called distinct authors, as well as the number of
new authors in a given year, i.e. those persons that publish for the ﬁrst time. We
found that the number of distinct authors fdauth(yr) per year can be approximated
in dependence of fauth(yr) according to the formula
fdauth(yr) := (
−0.67
1 + 169.41e−0.07(yr−1936) + 0.84) ∗fauth(yr).
(6.8)
158

6.2. The DBLP Data Set
 100000
 10000
 1000
 100
 10
 80
 50
 10
 5
 1
number of authors with publication count x
publication count = x
in 1975
in 1985
in 1995
in 2005
approx. for 1975
approx. for 1985
approx. for 1995
approx. for 2005
 1000
 5000
 1000
 500
 100
 50
 10
 1
 500
 100
 50
 10
 5
 1
publications being cited x times
number of citations =x
up to 1975
up to 1985
up to 1995
up tu 2005
approx. up to 1975
approx. up to 1985
approx. up to 1995
approx. up to 2005
Figure 6.6.: Powerlaw distributions in DBLP: (a) Number of publications per author
over time; (b) Distribution of citations among publications.
The distribution indicates that the number of distinct authors relative to the total
authors decreases steadily, from 0.84% to 0.17% (= 0.84% −0.67%). This reﬂects
the increasing productivity of authors over time discussed in Section 6.2.2.
The approximation formula for the number fnew(yr) of new authors builds on the
previous one and is also based on the logistic curve function in Equation (6.4):
fnew(yr) := (
−0.29
1749.00e−0.14(yr−1937) + 0.628) ∗fdauth(yr).
(6.9)
Publications. In Figure 6.6(a) we plot, for selected year and publication count x,
the number of authors with exactly x publications in this year. The graph is in log-
log scale. Here, we observe a typical power law distribution, which is characterized by
the fact that there are only a couple of authors having a large number of publications,
while a majority of the authors has only few publications.
Power law distributions are modeled by functions of the form
fpowerlaw(x) = axk + b,
(6.10)
with constants a ∈R>0, b ∈R, and exponent k ∈R<0. In such functions, param-
eter a aﬀects the x-axis intercept, exponent k deﬁnes the gradient, and parameter b
constitutes a shift in y-direction. For the given parameter restriction, the powerlaw
functions decrease steadily for increasing x ≥0. Examples for powerlaw functions
are the approximation functions shown in Figure 6.6(a) and 6.6(b).
Another interesting trend that can be observed in Figure 6.6(a) is that the curves
move upwards throughout the years. Informally speaking, this means that the pub-
lication count of the leading author(s) has steadily increased over the last 30 years
and it also reﬂects an increasing number of authors over time. We estimate the
number of authors with x publications in year yr as
159

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
fawp(x, yr) := 1.50fpubl(yr)x−f′
awp(yr) −5, where
f ′
awp(yr) :=
−0.60
1 + 216223e−0.20(yr−1936) + 3.08.
(6.11)
In the formula above, fpubl(yr) returns the total number of publications in yr.6
Note that the logistic curve fawp′(yr) reﬂects the increasing number of publications
of the leading author and is used in fawp(yr). Parameters k := f ′
awp(yr) and b in
function fawp(x, yr) were ﬁtted experimentally, using the original DBLP data.
Coauthors. With respect to coauthor characteristics, we also investigated rela-
tions between the publication count of authors and its numbers of total and distinct
coauthors in DBLP. In particular, given an author with a total of x publications,
we approximate the average number of total coauthors by
µcoauth(x) := 2.12 ∗x
(6.12)
and the number of its distinct coauthors by
µdcoauth(x) := x0.81.
(6.13)
These functions are used later when distributing publications among authors.
Editors. The analysis of authors is complemented by a study of their relations to
editors. As one may expect, editors tend to be persons that have published before,
i.e. persons that are well-known in the community. Following the distributions in
the original DBLP database, we thus assign editor activity primarily to persons
that have published in (earlier) venues. We omit the concrete analysis and formula,
which is rather technical and does not bring further insights into this relationship.
6.2.5. Citations
In Section 6.2.1 we studied repeated occurrences of attribute cite, namely outgoing
citations. This analysis is complemented by a study of incoming references, i.e. the
distribution of incoming citations among publications. Figure 6.6(b) plots the ci-
tation count distribution in DBLP, up to selected years. For instance, up to 1995
there were about 500 papers that have been cited 5 times. Similar to the distribu-
tion of publications, we observe a typical powerlaw distribution: most of the papers
have only a small fraction of citations, while only few papers have lots of citations.
As for the distribution of publications among authors, we use powerlaw functions
(cf. Equation (6.10)) to model this distribution. We omit the concrete formula.
As another observation, we found that in DBLP the number of incoming citations
is smaller than the number of outgoing citations. This may be surprising at ﬁrst
6It is obtained by subtracting the venue-type documents from fdocs(yr) in Equation (6.7), i.e. it
is deﬁned as fpubl(yr) := fdocs(yr) −(fjournal(yr) + fproc(yr)).
160

6.3. The SP2Bench Data Generator
glance, but is simply explained by the fact that DBLP contains many untargeted ci-
tations (in form of empty cite tags). Recalling that only a fraction of all papers have
outgoing citations (cf. Section 6.2.1), we conclude that the DBLP citation system is
very incomplete, although in some sense natural in that it follows natural distribu-
tions such as power law distributions (w.r.t. incoming citations, see Figure 6.6(b))
and the Gaussian distribution (w.r.t. outgoing references, see Figure 6.2).
6.3. The SP2Bench Data Generator
Having studied the key characteristics of the DBLP database, we now give some
background on the data generator implementation, including a discussion of the
RDF scheme for DBLP, design decisions, and implementation details.
6.3.1. An RDF Scheme for DBLP
The ﬁrst task in data generator implementation is to ﬁnd an appropriate RDF
scheme for the DBLP domain. We decided to follow the approach taken in [BC07],
an existing XML-to-RDF mapping of the original DBLP database. However, with
the goal to generate arbitrarily-sized documents, the SP2Bench data generator uses
lists of ﬁrst and last names, publishers, and random words, rather than real au-
thor, publication, and conference names. The generated SP2Bench conference and
journal names are always strings of the form “Conference $i ($year)” and “Journal
$i ($year)”, where $i is a unique conference (respectively journal) number in year
$year. Further, lists of random words are used to generate string content, e.g. for
titles. Analogously, fresh person names are built using the list of ﬁrst and last names
mentioned above.7 Finally, domain-speciﬁc string-content, such as the pages spec-
iﬁcation or the ISBN number of documents, are ﬁlled with strings of a reasonable
domain (for instance, we generate ISBN-like random strings for ISBN numbers).
Similar to [BC07], we use existing RDF vocabularies to describe resources in a uni-
form way. We borrow vocabulary from FOAF [foa] for describing persons, and from
SWRC [swr] (namespace swrc) and Dublin Core [dub] (namespace dc) for describ-
ing scientiﬁc resources. Additionally, we introduce a fresh namespace bench, which
deﬁnes DBLP-speciﬁc document classes, such as bench:Book and bench:Article.
In the sense of RDF, each XML attribute is mapped to an RDF property. For
instance, the attribute author is mapped to the RDF property dc:creator, a prede-
ﬁned URI from the dc namespace for specifying the creator of bibliographic entities.
Consequently, an author attribute in the XML database will be translated into a
single triple with predicate dc:creator, which provides the author for the respective
7The name lists that we provide to the data generator are large enough to generate distinct
authors for DBLP data containing billions of RDF triples. However, in case these lists are not
large enough, the data generator will notify the user and request larger name lists as input.
161

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
Attribute
Mapped to property
Type
address
swrc:address
xsd:string
author
dc:creator
foaf:Person
booktitle
bench:booktitle
xsd:string
cdrom
bench:cdrom
xsd:string
chapter
swrc:chapter
xsd:integer
cite
dcterms:references
foaf:Document
crossref
dcterms:partOf
foaf:Document
editor
swrc:editor
foaf:Person
ee
rdfs:seeAlso
xsd:string
isbn
swrc:isbn
xsd:string
journal
swrc:journal
bench:Journal
month
swrc:month
xsd:integer
note
bench:note
xsd:string
number
swrc:number
xsd:integer
page
swrc:pages
xsd:string
publisher
dc:publisher
xsd:string
school
dc:publisher
xsd:string
series
swrc:series
xsd:integer
title
dc:title
xsd:string
url
foaf:homepage
xsd:string
volume
swrc:volume
xsd:integer
year
dcterms:issued
xsd:integer
Figure 6.7.: Translation of DBLP attributes into RDF properties.
document. Figure 6.7 surveys the translation of XML attributes to RDF properties.
For each attribute, we also list its range restriction, i.e. the type of elements it refers
to. For instance, property dc:creator always refers to a URI of type foaf:Person.
Unfortunately, the XML-to-RDF translation of DBLP from [BC07] neither con-
tains blank nodes nor RDF containers, such as lists or bags. With the goal to build a
comprehensive benchmark, though, we want to design queries on top of such RDF-
speciﬁc constructs. For this reason, we decided to model persons in the data set as
(unique) blank nodes :ﬁrstname lastname, instead of URIs (using the lists of ﬁrst-
and lastnames mentioned above). To have RDF containers present in the generated
data, we model outgoing citations of documents using standard rdf:Bag containers.
In addition to this modiﬁcation, we enrich a small fraction of Article and Inpro-
ceedings documents with the new property bench:abstract (only about 1%, to
keep the modiﬁcation low), which constitutes comparably large strings.8
8We use a Gaussian distribution with µ = 150 expected words and a statistical spread of σ = 30.
162

6.3. The SP2Bench Data Generator
Figure 6.8.: Sample RDF database.
Figure 6.8 shows a sample DBLP instance in RDF format, as it may be generated
by the SP2Bench data generator. Dashed edges are typing edges (i.e., rdf:type)
and sc is an abbreviation for rdfs:subClassOf. On the logical level we distinguish
between the schema layer (gray) and the instance layer (white). As discussed before,
reference lists are modeled as blank nodes of type rdf:Bag (see e.g. :references1),
while both authors and editors are modeled as blank nodes of type foaf:Person.
On the schema level, class foaf:Document splits up into the individual DBLP doc-
ument classes bench:Journal, bench:Article, . . . . In summary, the sample graph
deﬁnes three persons, one proceeding, two inproceedings, one journal, and one arti-
cle. For readability reasons, we plot only selected properties of these entities. As also
illustrated, property dcterms:partOf links inproceedings and proceedings together,
while swrc:journal connects articles to the journals they appeared in.
In order to provide an entry point for queries that access authors and to provide
a person with ﬁxed characteristics, we created a special author, named after the
famous mathematician Paul Erd¨os (this person is not shown in Figure 6.8). Per year,
we assign 10 publications and 2 editor activities to this prominent person, starting
from year 1940 and up to year 1996. For the ease of access, Paul Erd¨os is modeled
as a ﬁxed URI.9 As an example query consider Q8 in Section 6.4, which extracts
9Recall that blank nodes cannot be accessed directly through SPARQL queries.
163

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
foreach year:
calculate counts for documents and generate document class instances;
calculate number of total, new, distinct, and retiring authors;
choose publishing authors;
assign number of new publications, number of coauthors, and
number of distinct coauthors to publishing authors;
// such that constraints for number of publications per author hold
assign from publishing authors to papers;
// satisfying authors per paper and coauthor constraints
choose editors and assign editors to papers;
// such that constraints for number of publications and editors hold
generate outgoing citations;
assign expected incoming and outgoing citations to papers;
write output until done or until output limit reached;
// permanently keeping output consistent
Figure 6.9.: Data generation algorithm.
all persons with Erd¨os number one or two (the Erd¨os number of a scientist is its
distance to Paul Erd¨os in the coauthor graph: persons that have published directly
with Paul Erd¨os have Erd¨os number one, those that have published with a coauthor
of Paul Erd¨os have Erd¨os number two etc., see http://www.oakland.edu/enp/).
6.3.2. Data Generator Implementation
We implemented the scheme described in the previous section in a data generator,
written in C++. It takes into account all relationships and characteristics that have
been studied in Section 6.2. Figure 6.9 shows the key steps in data generation. The
data generator simulates data year by year and takes into account all the structural
constraints implied by the distributions studied within Section 6.2, in a carefully
selected order. The generation process is simulation-based, which, amongst others,
means that we assign life times to authors and individually estimate their future
behavior, thereby taking into account global publications and coauthor constraints,
as well as the fraction of distinct and new authors (cf. Section 6.2.4).
The generator writes RDF output in N-Triples format [ntr] (cf. Section 2.2.3). It
oﬀers two command line parameters, to ﬁx either a triple count limit or the year up to
which data will be generated. When the triple count limit is set, the implementation
asserts that the generated data is in a “consistent” state, e.g. whenever proceedings
164

6.3. The SP2Bench Data Generator
#Triples
103
104
105
106
107
108
109
Elapsed time [s]
0.08
0.13
0.60
5.76
70
1011
13306
Table 6.2.: Performance of document generation.
have been written to the output, the corresponding conference will also included.
All random functions (which, for example, are used to assign the attributes ac-
cording to Table 6.1, or to sample data according to the approximation functions
in Figure 6.2) are based on a ﬁxed seed. This makes data generation determinis-
tic, i.e. the parameter setting uniquely identiﬁes the outcome. In addition, the data
generator is implemented in plain ANSI C++, which asserts that it also is platform-
independent. The ﬁxed random seed and the platform-independent implementation
ensure that documents generated on diﬀerent platforms are identical, so experi-
mental results from diﬀerent platforms remain comparable (modulo hardware and
system settings). Further, this asserts that data generation is incremental, which
means that small documents are always contained in larger documents.
6.3.3. Data Generator Evaluation
In order to prove the practicability of our data generator implementation, we per-
formed experiments and measured data generation times for documents of diﬀerent
size. The experiments were conducted under Linux ubuntu v7.10 gutsy, on top of an
Intel Core2 Duo E6400 2.13GHz CPU and 3GB DDR2 667 MHz nonECC physical
memory, using a 250GB Hitachi P7K500 SATA-II hard drive with 8MB Cache.
Table 6.2 summarizes the performance results for documents containing up to
one billion RDF triples. We observe that the generator scales almost linearly with
document size and creates even large documents very fast. For instance, the 109
triples document, which has a physical size of about 103GB, was generated in less
than four hours. An important prerequisite for this eﬃciency and scaling is that the
generator runs with constant main memory consumption (it gets by with at most
1.2GB RAM, independent from the size of the generated documents).
In addition to the performance assessment, we veriﬁed the implementation of
all characteristics from Section 6.2. Table 6.3 shows selected data generator and
output document characteristics for documents up to 25M RDF triples. We list
the size of the output ﬁle, the year up to which data was simulated, the number
of total authors and distinct authors contained in the data set (cf. Section 6.2.4),
and the counts of the document class instances (cf. Section 6.2.3). The summary
shows superlinear growth for the number of authors relative to the number of triples
in the data set, which is primarily caused by the increasing average number of
authors per publication, as discussed in Section 6.2.1. The growth of proceedings and
inproceedings is also superlinear, while the number of journals and articles increases
165

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
#Triples
10k
50k
250k
1M
5M
25M
File size [MB]
1.0
5.1
26
106
533
2694
Data up to year
1955
1967
1979
1989
2001
2015
#Total authors
1.5k
6.8k
34.5k
151.0k
898.0k
5.4M
#Distinct authors
0.9k
4.1k
20.0k
82.1k
429.6k
2.1M
#Journals
25
104
439
1.4k
4.6k
11.7k
#Articles
916
4.0k
17.1k
56.9k
207.8k
642.8k
#Proceedings
6
37
213
903
4.7k
24.4k
#Inproceedings
169
1.4k
9.2k
43.5k
255.2k
1.5M
#Incollections
18
56
173
442
1.4k
4.5k
#Books
0
0
39
356
973
1.7k
#PhD theses
0
0
0
101
237
365
#Masters theses
0
0
0
50
95
169
#WWWs
0
0
0
35
92
168
Table 6.3.: Characteristics of generated documents.
sublinear. These observations reﬂect the development of proceedings, inproceedings,
journals, and articles sketched in Figure 6.4(b). According to the approximation
functions in Figure 6.5, we also can observe that the number of inproceedings and
articles clearly dominates the remaining document classes. Finally, we remark that –
like in the original DBLP database – in the early years instances of selected document
classes are missing, e.g. there are no Book documents for these years.
6.4. The SP2Bench Queries
In addition to the data generator, the SP2Bench framework provides a set of bench-
mark queries. These queries have been designed to test common SPARQL operator
constellations, RDF data access patterns, and also particular SPARQL optimization
strategies. In the following, we discuss RDF and SPARQL characteristics that were
considered in query design (in Section 6.4.1 and Section 6.4.2, respectively), before
presenting and discussing the benchmark queries one by one in Section 6.4.3.
6.4.1. RDF Characteristics of Interest
Like in the context of relational data, decisions on how RDF data is stored and
accessed by engines may heavily inﬂuence system performance (see for instance the
discussions in [AMMH07; WKB08; SHK+08]). Consequently, a SPARQL benchmark
should consider the speciﬁcs of the underlying RDF data representation language.
The ﬁrst aspect that is interesting with respect to the RDF data format is that
RDF constitutes elements from three diﬀerent sets, namely URIs, blank nodes, and
166

6.4. The SP2Bench Queries
Query
1
2
3abc
4
5ab
6
1
Data access: Blank nodes, Literals, URIs,
L,U
L,U,La
L,U
B,L,U
B,L,U
B,L,U
Large Literals, Containers
2
Access pattern: Subj., Pred., Obj., None.
P,PO
P,PO
N,PO
P,PO
P,PO
P,PO
3
Operators: And, Filter, Union, Opt
A
A,O
A,F
A,F
A,F
A,F,O
4
Modiﬁers: Distinct, Limit, Offset, Order by
-
Ob
-
D
D
5
Filter pushing possible?
-
-
✓
-
✓/-
✓
6
Reusing of graph patterns possible?
-
-
-
✓
-
✓
Query
7
8
9
10
11
12c
1
Data access: Blank nodes, Literals, URIs,
L,U,C
B,L,U
B,L,U
U
L,U
U
Large Literals, Containers
2
Access pattern: Subj., Pred., Obj., None.
P,PO
P,PO
N,PO
O
P
SPO
3
Operators: And, Filter, Union, Opt
A,F,O
A,F,U
A,U
-
-
-
4
Modiﬁers: Distinct, Limit, Offset, Order by
D
D
D
-
L,Ob,Of
-
5
Filter pushing possible?
✓
✓
-
-
-
-
6
Reusing of graph patterns possible?
✓
✓
✓
-
-
-
Table 6.4.: Selected properties of the SP2Bench benchmark queries.
literals. SPARQL engines might represent elements from these domains diﬀerently,
e.g. it could make sense to have a special index for literals to accelerate text search.
The SP2Bench queries therefore access all three entities and diﬀerent combinations
thereof; line 1 in Table 6.4 surveys this characteristic for the SP2Bench queries, where
abbreviations are indicated by bold font (e.g. we use B as a shortcut for Blank
Nodes).10 Line 1 also indicates queries that access comparably large literals (namely
the abstracts of documents) and RDF containers (i.e. outgoing references, which are
of type rdf:Bag). Containers are of particular interest due to their special semantics
and the fact that they induce a (possibly large) set of membership properties rdf:_1,
rdf:_2, rdf:_3, . . . . As argued in [SHK+08], these membership properties may
induce problems for RDF storage schemes like Vertical Partitioning [AMMH07].
A second important requirement imposed by the RDF data format is to test
diﬀerent data access patterns at triple pattern level. Triple patterns in SPARQL
queries may contain variables in any position and the eﬃcient evaluation of single
patterns forms the basis for the fast evaluation of more complex queries. On the
one hand, it seems reasonable to assume that in most triple patterns the predi-
cate is ﬁxed, such as is the case in patterns like (?book,dc:creator,?name) or
(Book1,dc:creator,?name), asking for authors of all or a ﬁxed book, respectively.
Such patterns can be seen as the natural counterpart of SQL queries, which select a
ﬁxed set of properties from some entities, by accessing exactly those table attributes
that contain the desired properties. On the other hand, as also argued in [WKB08],
10As we shall see in Section 6.4.3, Q12a and Q12b are Ask-counterparts of the Select queries
Q5a and Q8, respectively. The Ask versions are not explicitly listed in the table.
167

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
one strength of RDF querying is the ﬂexibility of having variables in predicate posi-
tion. This makes it possible to write patterns like (Book1,?prop,?val), to obtain a
compact description of a certain resource, or (?subj,?prop,Person1), to extract all
entities that stand in some relation to Person1. Suchlike data access patterns may
be of particular importance if the underlying domain contains rather unstructured
data and the properties of entities are not or only partially known.
We survey the data access patterns that are used in the SP2Bench queries in
Table 6.4, Line 2. To give an example, the shortcut PO denotes that the respective
query contains at least one triple pattern with ﬁxed predicate and object position and
a variable in subject position. Although in most triple patterns the predicate position
is ﬁxed, we took special care to design queries with other access patterns, such as
triple patterns containing only variables (cf. Q3a, Q3b, Q3c, and Q9), patterns where
only the object is bound (cf. Q10), or variable-free triple patterns (cf. Q12c). We
will resume this discussion when presenting the queries individually in Section 6.4.3.
As a ﬁnal RDF-related aspect, it is important that RDF provides graph-structured
data. Hence, SPARQL engines should perform well on diﬀerent kinds of graph pat-
terns. Unfortunately, up to the present there exist only few real-world SPARQL
scenarios. It would be necessary to analyze a large set of such scenarios, to extract
graph patterns that frequently occur in practice. In the absence of this possibility,
we roughly distinguish between long path chains, i.e. nodes linked to each other via
a long path, and bushy patterns, i.e. single nodes that are linked to a multitude of
other nodes, in the style of star-joins. It is impossible to give a precise characteriza-
tion of “long” and “bushy”, so we designed meaningful queries containing comparably
long chains (i.e. Q4, Q6) and bushy patterns (i.e. Q2) w.r.t. our scenario.
6.4.2. SPARQL Characteristics of Interest
Next, we turn towards a discussion of SPARQL characteristics that were of par-
ticular interest in query design. Rows 3 and 4 in Table 6.4 survey the operators
and solution modiﬁers used in the Select benchmark queries. It can be observed
that the queries cover various operator constellations, combined with selected solu-
tion modiﬁer combinations. We want to stress that we took special care at operator
Opt, which has been shown to be the most complex operator in the SPARQL query
language in Section 3.2.3 and can be used to encode closed-world negation (cf. the
discussion in Section 4.2.6). Many interesting queries involve negation and therefore
we explicitly test it in two benchmark queries, namely Q6 and Q7 (see Section 6.4.3).
Query Optimization. Another important objective of SP2Bench was to design
queries that are amenable to a wide range of SPARQL optimization approaches. One
promising approach to SPARQL optimization is the reordering of triple patterns
based on selectivity estimations [NW08; SSB+08], similar in idea to join reordering
in relational algebra optimization. A beneﬁcial ordering of triple patterns depends on
both the selectivity of triple patterns and data access paths provided by the engine.
168

6.4. The SP2Bench Queries
Query
Q1
Q2
Q3a
Q3b
Q3c
Q4
Q5a
Q5b
10k
1
147
846
9
0
23226
155
155
50k
1
965
3647
25
0
104746
1085
1085
250k
1
6197
15853
127
0
542801
6904
6904
1M
1
32770
52676
379
0
2586733
35241
35241
5M
1
248738
192373
1317
0
18362955
210662
210662
25M
1
1876999
594890
4075
0
n/a
696681
696681
Query
Q6
Q7
Q8
Q9
Q10
Q11
Q12a
Q12b
Q12c
10k
229
0
184
4
166
10
yes
yes
no
50k
1769
2
264
4
307
10
yes
yes
no
250k
12093
62
332
4
452
10
yes
yes
no
1M
62795
292
400
4
572
10
yes
yes
no
5M
417625
1200
493
4
656
10
yes
yes
no
25M
1945167
5099
493
4
656
10
yes
yes
no
Table 6.5.: Number of query results (Q1–Q11) and results of ASK queries (Q12abc)
on documents containing up to 25 million RDF triples.
Actually, most SP2Bench queries may beneﬁt from such an optimization, because
most of them contain large And-connected blocks and require series of joins.
Closely related to triple reordering is ﬁlter pushing, which aims at an early eval-
uation of ﬁlter conditions (cf. Section 4.2.5). Like triple pattern reordering, ﬁlter
pushing may speed up evaluation by decreasing the intermediate result size. Row 5
in Table 6.4 identiﬁes SP2Bench queries that are amenable to such techniques.
Another reasonable idea is to reuse evaluation results of graph pattern evalua-
tion (or even of whole subqueries). This strategy is applicable whenever the same
pattern or subquery is used multiple times in the same query. As a simple example
consider Q4 in Section 6.4.3. In that query, ?article1 and ?article2 in the ﬁrst
and second triple pattern will be bound to exactly the same nodes of the input RDF
graph, so it suﬃces to evaluate this pattern only once and use this result for both
subqueries. We investigate the applicability of this technique in Table 6.4, row 6.
We will sketch more optimization approaches, such as semantic query optimization
(cf. Chapter 5), when discussing the individual queries in the following subsection.
6.4.3. Discussion of Benchmark Queries
Before starting our discussion, we survey the result sizes of the individual queries on
RDF documents of diﬀerent size in Table 6.5. The overview shows that the queries
vary in their result size (e.g., SP2Bench contains queries with increasing, constant,
and empty result size). This survey forms the basis for the subsequent discussion.
In the following we discuss the SPARQL versions of the SP2Bench queries, which
169

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
can be processed directly by SPARQL engines (complementarily, there exist SQL-
translations of these queries for relational storage schemes like a triple table approach
or Vertical Partitioning [AMMH07] at the SP2Bench project page, see [SHK+08] for
a discussion of these translations). In particular, we study the challenges the queries
impose to SPARQL engines. Thereby, we distinguish between in-memory engines,
which load the document from ﬁle and process queries in main memory (e.g., the
Jena ARQ engine [arq]), and native engines, which rely on a physical database
system (e.g., the Virtuoso engine [Bla07]). When discussing the challenges for native
engines, we always assume that the document has been loaded into the database prior
to query processing. We refer the reader to the survey of namespaces in Appendix A
for a complete listing of all namespaces that are used in the benchmark queries.
Benchmark Query Q1: Return the year of publication of “Journal 1 (1940)”.
SELECT ? yr
WHERE {
? j o u r n a l
r d f : type
bench : Journal .
? j o u r n a l
dc : t i t l e
” Journal 1 (1940)”ˆˆ xsd : s t r i n g .
? j o u r n a l
dcterms : i s s u e d
? yr }
Benchmark query Q1 returns exactly one result (for suﬃciently large documents).
Native engines may use index lookups to answer this query in (almost) constant time,
i.e. execution time should be independent from document size. In-memory engines
must scan the whole document and should scale linearly with document size.
Benchmark Query Q2: Extract all inproceedings with properties dc:creator,
bench:booktitle, dcterms:issued, dcterms:partOf, rdfs:seeAlso, dc:title, swrc:pages, foaf:homepage,
and optionally bench:abstract, including the respective values.
SELECT ? inproc
? author ? b o o k t i t l e
? t i t l e
? proc ? ee ? page ? u r l
? yr ? a b s t r a c t
WHERE {
? inproc
r d f : type
bench : I n p r o c e e d i n g s .
? inproc
dc : c r e a t o r
? author .
? inproc
bench : b o o k t i t l e
? b o o k t i t l e .
? inproc
dc : t i t l e
? t i t l e .
? inproc
dcterms : partOf ? proc .
? inproc
r d f s : seeAlso
? ee .
? inproc
swrc : pages ? page .
? inproc
f o a f : homepage ? u r l .
? inproc
dcterms : i s s u e d
? yr
OPTIONAL { ? inproc
bench : a b s t r a c t
? a b s t r a c t
}
} ORDER BY ? yr
This query implements a large star-join pattern, where diﬀerent properties of
inproceedings (variable ?inproc) are requested. It contains a simple Opt clause,
170

6.4. The SP2Bench Queries
and accesses large strings (i.e. the abstracts). Result size grows with database size
and a ﬁnal result ordering is necessary due to operator Order By. Both native and
in-memory engines may reach evaluation times almost linear to document size.
Benchmark Queries Q3abc: Select all articles with property (a) swrc:pages,
(b) swrc:month, or (c) swrc:isbn.
( a ) SELECT ? a r t i c l e
WHERE { ? a r t i c l e
r d f : type
bench : A r t i c l e .
? a r t i c l e
? property
? value
FILTER (? property=swrc : pages ) }
(b) Q3a ,
but ” swrc : month”
i n s t e a d
of ” swrc : pages ”
( c ) Q3a ,
but ” swrc : isbn ”
i n s t e a d
of ” swrc : pages ”
These three queries test Filter expressions with varying selectivity. According
to Table 6.1, the Filter expression in Q3a is not very selective (i.e. retains about
92.61% of all articles). Data access through a secondary index for Q3a is probably
not eﬃcient, but may work well for Q3b, which selects only 0.65% of all articles.
The ﬁlter in Q3c is never satisﬁed, because articles never have predicate swrc:isbn.
Native engines may use statistics to answer Q3c in constant time. As an alternative
strategy, ﬁlter elimination techniques in the style of Lemma 4.3 may be applied.
Benchmark Query Q4: Select all distinct pairs of article author names for
authors that have published in the same journal.
SELECT DISTINCT ?name1 ?name2
WHERE { ? a r t i c l e 1
r d f : type
bench : A r t i c l e .
? a r t i c l e 2
r d f : type
bench : A r t i c l e .
? a r t i c l e 1
dc : c r e a t o r
? author1 .
? author1
f o a f : name ?name1 .
? a r t i c l e 2
dc : c r e a t o r
? author2 .
? author2
f o a f : name ?name2 .
? a r t i c l e 1
swrc : j o u r n a l
? j o u r n a l .
? a r t i c l e 2
swrc : j o u r n a l
? j o u r n a l
FILTER (? name1<?name2) }
Q4 contains a rather long graph chain, i.e. variables ?name1, and ?name2 are linked
through the articles that diﬀerent authors have published in the same journal. As
one may expect, the result is very large (cf. Table 6.5). Instead of evaluating the
outer pattern block and applying the Filter afterwards, engines may embed the
Filter expression in the computation of the inner block, e.g. by exploiting indices
on author names. The Distinct modiﬁer further complicates the query. We shall
expect superlinear behavior for this query, even for native engines.
171

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
Benchmark Queries Q5ab: Return the names of all persons that occur as
author of at least one inproceeding and at least one article.
( a ) SELECT DISTINCT ? person ?name
WHERE { ? a r t i c l e
r d f : type
bench : A r t i c l e .
? a r t i c l e
dc : c r e a t o r
? person .
? inproc
r d f : type
bench : I n p r o c e e d i n g s .
? inproc
dc : c r e a t o r
? person2 .
? person
f o a f : name ?name .
? person2
f o a f : name ?name2
FILTER(?name=?name2) }
(b) SELECT DISTINCT ? person ?name
WHERE { ? a r t i c l e
r d f : type
bench : A r t i c l e .
? a r t i c l e
dc : c r e a t o r
? person .
? inproc
r d f : type
bench : I n p r o c e e d i n g s .
? inproc
dc : c r e a t o r
? person .
? person
f o a f : name ?name }
Q5a and Q5b test diﬀerent join variants: Q5a implements an implicit join on
author names (encoded in the Filter condition), while Q5b explicitly joins the au-
thors on variable ?name. Although the queries are not equivalent in the general case,
the one-to-one mapping between authors and their names in the SP2Bench scenario
implies equivalence (i.e., the attribute foaf:name is a primary key for objects of
type foaf:Person). In Section 5.4, semantic query optimization using such keys for
RDF has been proposed. When coupled with ﬁlter manipulation rules (e.g. in the
style of Lemma 4.3), such approaches may detect the equivalence of Q5a and Q5b
in this scenario and choose the more eﬃcient version among Q5a and Q5b.
Benchmark Query Q6: Return, for each year, the set of all publications
authored by persons that have not published in years before.
SELECT ? yr ?name ?doc
WHERE {
? c l a s s
r d f s : subClassOf
f o a f : Document .
?doc
r d f : type ? c l a s s .
?doc dcterms : i s s u e d
? yr .
?doc dc : c r e a t o r
? author .
? author
f o a f : name ?name
OPTIONAL {
? c l a s s 2
r d f s : subClassOf
f o a f : Document .
? doc2
r d f : type ? c l a s s 2 .
? doc2
dcterms : i s s u e d
? yr2 .
? doc2 dc : c r e a t o r
? author2
FILTER (? author=?author2 && ? yr2 <?yr ) }
FILTER ( ! bound (? author2 )) }
172

6.4. The SP2Bench Queries
This query implements closed-world negation, expressed through the combination
of the operators Opt, Filter, and ﬁlter predicate bnd (denoted as bound in the
oﬃcial syntax) sketched in Section 4.2.6. To shortly discuss the query, the idea of the
construction is that the block outside the Opt expression computes all publications,
while the inner one constitutes earlier publications from authors that appear outside.
The outer Filter expression then retains publications for which variable ?author2
is unbound, i.e. exactly those publications from authors that have not published
in earlier years. For this query, SPARQL-speciﬁc optimization like the rewriting of
closed-world negation discussed in Section 4.2.6 may be beneﬁcial.
Benchmark Query Q7: Return the titles of all publications that have been cited
at least once, but not by any paper that has not been cited itself.
SELECT DISTINCT ? t i t l e
WHERE {
? c l a s s
r d f s : subClassOf
f o a f : Document .
?doc
r d f : type ? c l a s s .
?doc dc : t i t l e
? t i t l e .
?bag2 ?member2 ?doc .
? doc2
dcterms : r e f e r e n c e s
?bag2
OPTIONAL {
? c l a s s 3
r d f s : subClassOf
f o a f : Document .
? doc3
r d f : type ? c l a s s 3 .
? doc3
dcterms : r e f e r e n c e s
?bag3 .
?bag3 ?member3 ?doc
OPTIONAL {
? c l a s s 4
r d f s : subClassOf
f o a f : Document .
? doc4
r d f : type ? c l a s s 4 .
? doc4
dcterms : r e f e r e n c e s
?bag4 .
?bag4 ?member4 ? doc3
} FILTER ( ! bound (? doc4 ))
} FILTER ( ! bound (? doc3 ))
}
Benchmark query Q7 tests double negation, which requires the encoding of nested
closed-world negation through a nesting of the Opt + Filter + bnd construction
used in the previous query. Given that the citation system of DBLP contains only
few incoming and outgoing citations (cf. Section 6.2.5), the query returns only few
results, even on very large documents (see Table 6.5). However, we expect the query
to be challenging, due to the double negation. As for Q7, engines may use the rewrit-
ing scheme for closed-world negation presented in Section 4.2.6. Another possible
optimization strategy is to reuse graph pattern evaluation results. For instance, the
And-connected block $1 rdfs:subClassOf foaf:Document. $2 rdf:type $1 oc-
curs three times in the query, for (i) $1:=?class, $2:=?doc, for (ii) $1:=?class3,
$2:=?doc3, and also for (iii) $1:=?class4, $2:=?doc4.
173

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
Benchmark Query Q8: Compute authors with Erd¨os number one or two.
SELECT DISTINCT ?name
WHERE {
? erdoes
r d f : type
f o a f : Person .
? erdoes
f o a f : name ” Paul
Erdoes ”ˆˆ xsd : s t r i n g .
{ ?doc dc : c r e a t o r
? erdoes .
?doc dc : c r e a t o r
? author .
? doc2 dc : c r e a t o r
? author .
? doc2 dc : c r e a t o r
? author2 .
? author2
f o a f : name ?name
FILTER (? author !=? erdoes &&
? doc2!=?doc &&
? author2 !=? erdoes &&
? author2 !=? author )
} UNION {
?doc dc : c r e a t o r
? erdoes .
?doc dc : c r e a t o r
? author .
? author
f o a f : name ?name
FILTER (? author !=? erdoes ) }
}
Here, the evaluation of the second Union part is basically “contained” in the
evaluation of the ﬁrst part. Hence, techniques like graph pattern (or subexpression)
reusing are applicable. Another possible strategy is to decompose the ﬁrst ﬁlter
expression and push its components down in the operator tree (cf. Section 4.2.5), to
apply atomic ﬁlter conditions early and decrease the size of intermediate results.
Benchmark Query Q9: Return incoming and outgoing properties of persons.
SELECT DISTINCT ? p r e d i c a t e
WHERE {
{ ? person
r d f : type
f o a f : Person .
? s u b j e c t
? p r e d i c a t e
? person } UNION
{ ? person
r d f : type
f o a f : Person .
? person ? p r e d i c a t e
? o b j e c t
} }
Q9 has been primarily designed to test non-standard data access patterns. Naive
implementations would compute the triple patterns of the Union subexpressions
separately, thus evaluating patterns where no component is bound. Then, pattern
?subject ?predicate ?person selects all graph triples, which is rather ineﬃcient.
Another approach is to evaluate the ﬁrst triple in each Union subexpression, af-
terwards using the bindings for variable ?person to evaluate the second pattern
eﬃciently. Note that the result size is exactly 4 for suﬃciently large documents
(see Table 6.5). RDF-speciﬁc statistics about incoming and outgoing properties of
foaf:Person-typed objects (in native engines) may help to answer this query in
174

6.4. The SP2Bench Queries
constant time, even without data access. In-memory engines, however, must always
load the whole document and therefore should scale linearly with document size.
Benchmark Query Q10: Return all subjects that stand in some relation to
person “Paul Erd¨os”, including the type of their relation.
SELECT ? subj
? pred
WHERE { ? subj
? pred
person : Paul Erdoes }
In the SP2Bench scenario, this query can be reformulated as: Return publications
and venues in which “Paul Erd¨os” is involved as author or as editor. It implements
an object bound-only access pattern. In contrast to Q9, statistics are not imme-
diately useful, because the result includes the subjects (i.e., the subjects must be
extracted from the data set). Recalling that Paul Erd¨os is active only between 1940
and 1996 (cf. Section 6.3.1), the result size stabilizes for large documents. Native
engines that exploit indices may reach (almost) constant execution time.
Benchmark Query Q11: Return (up to) 10 electronic edition URLs starting
from the 51th publication, in lexicographical order.
SELECT ? ee
WHERE { ? p u b l i c a t i o n
r d f s : seeAlso
? ee }
ORDER BY ? ee LIMIT 10 OFFSET 50
The focus of this query lies on the combination of solution modiﬁers Order By,
Limit, and Offset. In-memory engines have to read, process, and sort electronic
editions prior to application of the Limit and Offset modiﬁers. In contrast, native
engines may exploit indices to access only a fraction of all electronic editions and,
as the result size is limited to 10 due to the Limit modiﬁer, would optimally reach
constant runtime, independently from the size of the input document.
Benchmark Query Q12: (a) Return yes if a person occurs as author of at least
one inproceeding and at least one article, no otherwise; (b) Return yes if there is
an author with Erd¨os number one or two in the database, and no otherwise; (c)
Return yes if the person “John Q Public” is contained in the database.
(Q12a) Q5a as ASK query
(Q12b) Q8 as ASK query
(Q12c) ASK { person : John Q Public
r d f : type
f o a f : Person }
All three queries are boolean queries, designed to test the eﬃcient implementation
of the SPARQL Ask query form. Q12a and Q12b share the properties of their Se-
lect counterparts Q5a and Q8, respectively. They always return yes for suﬃciently
175

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
large documents. When evaluating Ask queries, engines should stop the evaluation
process as soon as a solution has been found. A reasonable optimization approach
would be to adapt the query execution plan, trying to eﬃciently locate a witness
(e.g. using the rewriting rules for Ask queries in Lemma 4.14, Section 4.4.1). For
instance, based on execution time estimations it may be favorable to evaluate the
(simpler) second part of the Union in Q12b ﬁrst. Q12c asks for a single triple that
is not present in the database. With indices, native engines may answer Q12c in
constant time. Again, in-memory engines must read and scan the whole document.
6.5. Benchmark Metrics
In this section we propose several benchmark metrics that cover diﬀerent aspects
of the evaluation process. These metrics reﬂect the scope and design decisions of
SP2Bench and can be used to systematically evaluate benchmark results.
We propose to perform three runs over documents comprising 10k, 50k, 250k, 1M,
5M, and 25M RDF triples, using a ﬁxed timeout of 30min per query and document.
The reported time should include the average value over all three runs and, if sig-
niﬁcant, the errors within these runs. This setting was tested in [SHK+08; SHLP09]
and can be evaluated in reasonable time for state-of-the-art engines (typically within
few days). If the tested engine is fast enough, nothing prevents the user from adding
larger documents. We recommend the following ﬁve benchmark metrics.
1. Success Rate: As a ﬁrst indicator we propose to survey the success rates
for the engine on top of all document sizes, distinguishing between Success,
Timeout (e.g. an execution time > 30min), Memory Exhaustion (if an additional
memory limit was set), and general Errors. This metric gives a survey over
scaling properties and ﬁrst insights into the engine’s overall behavior.
2. Loading Time: The loading time for documents of diﬀerent sizes is interesting
to get insights into the eﬃciency and, particularly, to see how document loading
scales with document size. This metric primarily applies to engines with a
physical database backend and may be irrelevant for in-memory engines, where
document loading is usually part of the query evaluation process.
3. Memory Consumption: In particular for engines with a physical backend,
the main memory consumption for the individual queries and also the average
memory consumption over all queries may be of interest. Optimally, physi-
cal backend database engines should get by with a constant main memory
consumption, independently from the size of the input document.
4. Per-Query Performance: Individual performance results for all queries
over all document sizes give more detailed insights into the behavior than the
Success Rate metric discussed before. The Per-Query Performance
metric forms the basis for a deep study of the results and allows to investigate
176

6.6. Related Work
strengths, weaknesses, and scaling of the tested implementation based on a one
by one discussion of the engine results for the individual benchmark queries.
5. Global Performance: This metric integrates the individual per-query re-
sults into a global performance measure. It contains, for each tested document
size, the arithmetic and the geometric mean11 of the engine’s average execu-
tion time over all queries. We propose to penalize timeouts and other errors
with 3600s (i.e., twice the evaluation time limit). Global Performance
is well-suited to investigate the scaling properties of engines and to compare
the performance of diﬀerent evaluation approaches. Note that the arithmetic
mean imposes a stronger penalty on outlier queries than the geometric mean.
Generally speaking, the geometric mean better reﬂects the average behavior
of engines, while the arithmetic mean captures the worst-case behavior.
6.6. Related Work
The Benchmark Handbook [Gra93] provides a summary of important database
benchmarks. Probably the most “complete” benchmark suite for relational systems
is TPC [tpc], which deﬁnes performance and correctness benchmarks for a large va-
riety of scenarios. Going beyond relational data, a variety of benchmarks have been
developed for other data models, such as the OO7 benchmark [CDN93] for object-
oriented databases and the XMark benchmark [SWK+02] for XML data processing.
Coming along with the proliferation of the Semantic Web, benchmarking has be-
come an increasingly important topic in the context of data formats like RDF(S) and
OWL. In response, also in this context several benchmark platforms have been devel-
oped. These platforms address both structural aspects of the data (e.g., [MACP02])
as well as eﬃcient data processing (e.g., [GPH05; AMMH; BS09]).
The LUBM and the Barton Library benchmark have been discussed in the be-
ginning of this chapter and we will not further elaborate on them here. Another
notable benchmark project in the context of SPARQL is the application-oriented
Berlin SPARQL Benchmark (BSBM) [BS; BS08; BS09]. BSBM has been developed
independently, in parallel to the SP2Bench framework. It tests the performance of
SPARQL engines in a prototypical e-commerce scenario, so it is use-case driven.
Compared to the SP2Bench queries, the BSBM queries are typically simpler and
laid out to be evaluated in a work load setting. This makes the BSBM benchmark
particularly interesting to compare the performance of engines that expose SPARQL
endpoints via the SPARQL protocol and gives the benchmark a more industrial ﬂa-
vor. Hence, with its focus BSBM is supplementary to the SP2Bench suite.
The RDF(S) data model benchmark in [MACP02] focuses on structural properties
of RDF Schemas. In [TTKC08], graph features of RDF Schemas are studied. The
results constitute a valuable basis for synthetic schema generation. With their focus
11The geometric mean is deﬁned as the nth root of the product over n numbers.
177

Chapter 6. SP2Bench: A SPARQL Performance Benchmark
on schemas, however, both [MACP02] and [TTKC08] are complementary to our
work. As an interesting observation, the investigations in [TTKC08] reveal that
power law distributions are naturally encountered in large RDF Schema descriptions,
which justiﬁes the use of such distributions in the SP2Bench data.
The topic of data generation is not new either. In the context of the Semantic
Web, a synthetic data generation approach for OWL based on test data is described
in [WGQH05]. There, the focus is on rapidly generating large data sets from rep-
resentative sample data of a ﬁxed domain. Our data generation approach is more
ﬁne-grained, as we analyze the development of entities (e.g. articles) over time and
the generated documents reﬂect many characteristics found in social communities.
6.7. Conclusion
The SP2Bench performance benchmark for SPARQL constitutes the ﬁrst methodical
approach for testing the performance of engines w.r.t. diﬀerent operator constella-
tions, RDF access paths, typical RDF constructs, and a variety of possible opti-
mization approaches. In this line, our framework allows to assess the performance of
SPARQL in a general, comprehensive way and is a useful tool for quality assessment
for both research prototypes and industrial-strength SPARQL implementations.
The SP2Bench data generator relies on an in-depth study of DBLP. Although it
is impossible to mirror all correlations found in the original DBLP data (e.g., we
simpliﬁed when assuming independence between attributes in Section 6.2.1), many
aspects are modeled in faithful detail. Hence, the data generator forms a contribution
on its own and may be useful in other Semantic Web projects where large amounts
of test data with natural distributions are needed. In SP2Bench, this data forms
the foundation for the design of challenging benchmark queries, which – due to the
real-world distributions – are realistic, understandable, and predictable.
We conclude with the remark that, with the choice of the well-known DBLP sce-
nario, we also clear the way for coming extensions of the SPARQL query language.
For instance, SPARQL update and aggregation support are currently planned as
extensions [spaa]. Updates, for example, require only minor modiﬁcations to the
SP2Bench data generator. Concerning aggregations, we argue that the detailed
knowledge of the document class counts and distributions (cf. Section 6.2) facili-
tates the design of challenging aggregate queries with predictable characteristics.
178

Chapter 7.
Conclusion and Outlook
Although over the last years a variety of research has been done in the context of
RDF data and SPARQL query processing, both technologies are still in their infancy.
As witnessed by our experimental results presented in [SHK+08; SHLP09], current
SPARQL implementations like ARQ [arq], Sesame [ses], or Virtuoso [Bla07] still
suﬀer from severe performance bottlenecks when dealing with medium- and large-
scale RDF databases, even for presumably simple queries that can be processed
eﬃciently in a comparable relational setting. Tackling the deﬁciencies of these im-
plementations, a variety of promising optimization schemes and research prototypes
have been developed (e.g. [AMMH07; NW08; WKB08; NW09]). However, as dis-
cussed earlier in the introduction of Chapter 4, these approaches typically focus on
the optimization of And-only queries. Experiments that conﬁrm their eﬃciency for
larger SPARQL fragments (and hence, more complex queries) are still outstanding.
In addition, other important issues that may arise in the context of real-world
data management systems for RDF, such as concurrency control, transaction man-
agement, or recovery, have not or only marginally been addressed to date. On the
one hand, it seems reasonable to assume that – like for query optimization – the
Semantic Web community can beneﬁt from established approaches that have been
proposed for relational database systems. On the other hand, one may expect that
also in these areas new challenges arise, due to the speciﬁcs of the RDF data format
and the SPARQL query language. To give an example, the discussion in [Muy08] in-
dicates deﬁciencies of traditional serialization strategies for concurrent transactions
in the context of RDF data (such as e.g. two-phase locking [EGLT76]) and proposes
a concurrency control protocol speciﬁcally designed for RDF databases.
Beyond those mentioned above, other new challenges in SPARQL query process-
ing and RDF data management will arise in response to future extensions of the
SPARQL query language. In its current Working Draft [spaa], the W3C SPARQL
Working Group tackles a variety of new features for SPARQL, such as aggregate
functions, nested subqueries, negation, property paths, and updates, to increase the
practicability of the SPARQL query language. While some of them are only syn-
tactic sugar (e.g., as shown in [AG08a], negation can be simulated using operators
Opt, Filter, and ﬁlter predicate bnd), others like aggregate functions increase the
179

Chapter 7. Conclusion and Outlook
expressive power of SPARQL and therefore complicate query processing. Again, the
community may fall back on experience gathered in the relational context, as a basis
for the development of eﬃcient solutions for the SPARQL query language.
Based on all these observations, we expect that there is still a long way to go
to industrial-strength SPARQL implementations that cope eﬃciently and reliably
with challenging SPARQL queries in the context of large-scale RDF repositories.
We hope that our investigations of SPARQL complexity, optimization, and bench-
marking presented in this thesis are one more step towards the understanding of
the theoretical foundations of SPARQL, towards the understanding of its relation-
ship to established data models, towards the design, implementation, and testing of
SPARQL engines, and lastly towards the realization of the Semantic Web vision.
180

Bibliography
[AB07] Sanjeev Arora and Boaz Barak. Computational Complexity: A Modern Ap-
proach. Cambridge University Press, 2007.
[ABC+76] Morton M. Astrahan, Mike W. Blasgen, Donald D. Chamberlin, Kapali P.
Eswaran, Jim Gray, Patricia P. Griﬃths, William F. King III, Raymond A.
Lorie, Paul R. McJones, James W. Mehl, Gianfranco R. Putzolu, Irving L.
Traiger, Bradford W. Wade, and Vera Watson. System R: Relational Ap-
proach to Database Management. ACM Trans. Database Syst., 1(2):97–137,
1976.
[ABE09] Faisal Alkhateeb, Jean-Fran¸cois Baget, and J´erˆome Euzenate.
Extending
SPARQL with regular expression patterns (for querying RDF). Web Seman-
tics, 7(2):57–73, 2009.
[ACKP01] Soﬁa Alexaki, Vassilis Christophides, Gregory Karvounarakis, and Dimitris
Plexousakis. On Storing Voluminous RDF Descriptions: The case of Web
Portal Catalogs. In WebDB, pages 43–48, 2001.
[AG08a] Renzo Angles and Claudio Gutierrez. The Expressive Power of SPARQL. In
ISWC, pages 114–129, 2008.
[AG08b] Renzo Angles and Claudio Gutierrez. The Expressive Power of SPARQL.
Technical report, Universidad de Chile, TR/DCC-2008-5, 2008.
[AHV] Serge Abiteboul, Richard Hull, and Victor Vianu. Foundations of Databases.
Addison-Wesley 1995.
[AMMH] Daniel J. Abadi, Adam Marcus, Samuel Madden, and Katherine J. Hollen-
bach. Using the Barton libraries dataset as an RDF benchmark. TR MIT-
CSAIL-TR-2007-036, MIT.
[AMMH07] Daniel J. Abadi, Adam Marcus, Samuel Madden, and Katherine J. Hollen-
bach. Scalable Semantic Web Data Management Using Vertical Partitioning.
In VLDB, pages 411–422, 2007.
[arq] ARQ SPARQL Processor for Jena. http://jena.sourceforge.net/ARQ/.
[ASU79] Alfred V. Aho, Yehoshua Sagiv, and Jeﬀrey D. Ullman. Eﬃcient Optimization
of a Class of Relational Expressions. ACM Trans. Database Syst., 4(4):435–
454, 1979.
181

Bibliography
[AU79] Alfred V. Aho and Jeﬀrey D. Ullman. The Universality of Data Retrieval
Languages. In POPL, pages 110–120, 1979.
[BB79] Catriel Beeri and Philip A. Bernstein. Computational Problems Related to
the Design of Normal Form Relational Schemas. ACM Trans. Database Syst.,
4(1):30–59, 1979.
[BC07] Christian Bizer and Richard Cyganiak. D2R Server publishing the DBLP
Bibliography Database, 2007. http://www4.wiwiss.fu-berlin.de/dblp/.
[BCM+03] Franz Baader, Diego Calvanese, Deborah McGuinness, Daniele Nardi, and
Peter Patel-Schneider. The Description Logic Handbook: Theory, Implemen-
tation and Applications. Cambridge University Press, 2003.
[BEE+07] Fran¸cois Bry, Norbert Eisinger, Thomas Eiter, Tim Furche, Georg Gottlob,
Clemens Ley, Benedikt Linse, Reinhard Pichler, and Fang Wei. Foundations
of Rule-based Query Answering. In Reasoning Web, pages 1–153, 2007.
[BFL+08] Fran¸cois Bry, Tim Furche, Clemens Ley, Benedikt Linse, and Bruno Marnette.
RDFLog: It’s Like Datalog for RDF. In WLP, 2008.
[BFW99] Peter Buneman, Wenfei Fan, and Scott Weinstein. Query Optimization for
Semistructured Data Using Path Constraints in a Deterministic Data Model.
In DBLP, pages 208–223, 1999.
[BKS07] Abraham Bernstein, Christoph Kiefer, and Markus Stocker.
OptARQ: A
SPARQL Optimization Approach based on Triple Pattern Selectivity Esti-
mation. Technical report, University of Zurich, 2007.
[BKvH02] Jeen Broekstra, Arjohn Kampman, and Frank van Harmelen.
Sesame: A
Generic Architecture for Storing and Querying RDF and RDF Schema. In
ISWC, pages 54–68, 2002.
[Bla07] Carl Blakeley. Mapping Relational Data to RDF with Virtuoso’s RDF Views,
2007. OpenLink Software.
[BLHL01] Tim Berners-Lee, James Hendler, and Ora Lassila. The Semantic Web. Sci-
entiﬁc American, 2001.
[BM72] Rudolf Bayer and Edward M. McCreight. Organization and Maintenance of
Large Ordered Indices. Acta Inf., 1:173–189, 1972.
[BS] Christian Bizer and Andreas Schultz. The Berlin SPARQL Benchmark. http:
//www4.wiwiss.fu-berlin.de/bizer/BerlinSPARQLBenchmark/.
[BS08] Christian Bizer and Andreas Schultz. Benchmarking the Performance of Stor-
age Systems that expose SPARQL Endpoints. In SSWS, 2008.
182

Bibliography
[BS09] Christian Bizer and Andreas Schultz. The Berlin SPARQL Benchmark. In
International Journal On Semantic Web and Information Systems - Special
Issue on Scalability and Performance of Semantic Web Systems, 2009.
[BV84] Catriel Beeri and Moshe Y. Vardi. A Proof Procedure for Data Dependencies.
J. ACM, 31(4):718–741, 1984.
[CB74] Donald D. Chamberlin and Raymond F. Boyce.
SEQUEL: A Structured
English Query Language.
In SIGMOD Workshop, Vol. 1, pages 249–264,
1974.
[CDES05] Eugene I. Chong, Souripriya Das, George Eadon, and Jagannathan Srini-
vasan.
An Eﬃcient SQL-based RDF Querying Scheme.
In VLDB, pages
1216–1227, 2005.
[CDN93] Michael J. Carey, David J. DeWitt, and Jeﬀrey F. Naughton.
The OO7
Benchmark. In SIGMOD, pages 12–21, 1993.
[CGK08] Andrea Cali, Georg Gottlob, and Michael Kifer. Taming the Inﬁnite Chase:
Query Answering under Expressive Relational Constraints. In Descr. Logics,
2008.
[CGL09] Andrea Cali, Georg Gottlob, and Thomas Lukasiewicz. Datalog±: a Uniﬁed
Approach to Ontologies and Integrity Constraints. In ICDE, pages 14–30,
2009.
[CGM90] Upen S. Chakravarthy, John Grant, and Jack Minker. Logic-based Approach
to Semantic Query Optimization. TODS, 15(2):162–207, 1990.
[CH80] Ashok K. Chandra and David Harel. Structure and Complexity of Relational
Queries. In FOCS, pages 333–347, 1980.
[Cha98] Surajit Chaudhuri. An Overview of Query Optimization in Relational Sys-
tems. In PODS, pages 34–43, 1998.
[Che76] Peter P. Chen. The Entity-Relationship Model – Toward a Uniﬁed View of
Data. ACM Trans. Database Syst., 1(1):9–36, 1976.
[CLJF06] Artem Chebotko, Shiyong Lu, Hasan M. Jamil, and Farshad Fotouhi. Se-
mantics Preserving SPARQL-to-SQL Query Translation for Optional Graph
Patterns. Technical report, Wayne State University, TR-DB-052006-CLJF,
2006.
[CM77] Ashok K. Chandra and Philip M. Merlin. Optimal Implementation of Con-
junctive Queries in Relational Data Bases. In STOC, pages 77–90, 1977.
[Cod69] Edgar F. Codd.
Derivability, Redundancy and Consistency of Relations
Stored in Large Data Banks. IBM Research Report, San Jose, California,
RJ599, 1969.
183

Bibliography
[Cod70] Edgar F. Codd. A Relational Model of Data for Large Shared Data Banks.
Commun. ACM, 13(6):377–387, 1970.
[Cod71] Edgar F. Codd. Further Normalization of the Data Base Model. IBM Re-
search Report, San Jose, California, RJ909, 1971.
[Cod72] Edgar F. Codd.
Relational Completeness of Data Base Sublanguages.
Database Systems, pages 65–98, 1972.
[Cod74] Edgar F. Codd. Recent Investigations into Relational Data Base Systems.
IBM Research Report, San Jose, California, RJ1385, 1974.
[Cod79] Edgar F. Codd. Extending the Data Base Model to Capture More Meaning.
ACM Trans. Database Syst., 4(4):397–434, 1979.
[CPST03] Vassilis Christophides, Dimitris Plexousakis, Michel Scholl, and Sotirios Tour-
tounis. On Labeling Schemes for the Semantic Web. In WWW, pages 544–
555, 2003.
[CV85] Ashok K. Chandra and Moshe Y. Vardi.
The Implication Problem for
Functional and Inclusion Dependencies is Undecidable. SIAM J. Comput.,
14(3):671–677, 1985.
[CWI] CWI Amsterdam. MonetDB. http://monetdb.cwi.nl/.
[Cyg05] Richard Cyganiac. A relational algebra for SPARQL. Technical report, HP
Laboratories Bristol, 2005.
[Dat81] Christopher J. Date. Referential Integrity. In VLDB, pages 2–12, 1981.
[Deu08] Alin Deutsch. FOL Modeling of Integrity Constraints (Dependencies). DB
Encyclopedia, 2008.
[DNR08] Alin Deutsch, Alan Nash, and JeﬀRemmel. The Chase Revisited. In PODS,
pages 149–158, 2008.
[DPT06] Alin Deutsch, Lucian Popa, and Val Tannen.
Query Reformulation with
Constraints. SIGMOD Record, 35(1):65–73, 2006.
[DT01] Alin Deutsch and Val Tannen. Containment for Classes of XPath Expressions
Under Integrity Constraints. In Knowledge Representation Meets Databases,
2001.
[DT05] Alin Deutsch and Val Tannen. XML Queries and Constraints, Containment
and Reformulation. Theor. Comput. Sci., 336(1):57–87, 2005.
[dub] Dublin Core Metadata Initiative. http://dbulincore.org.
184

Bibliography
[EFT94] Heinz-Dieter Ebbinghaus, J¨org Flum, and Wolfgang Thomas. Mathematical
Logic. Springer, 1994.
[EGLT76] Kapali P. Eswaran, Jim Gray, Raymond A. Lorie, and Irving L. Traiger. The
Notions of Consistency and Predicate Locks in a Database System. Commun.
ACM, 19(11):624–633, 1976.
[EGM97] Thomas Eiter, Georg Gottlob, and Heikki Mannila.
Disjunctive Datalog.
ACM Trans. Database Syst., 22(3):364–418, 1997.
[EL05] Ergin Elmacioglu and Dongwon Lee. On Six Degrees of Separation in DBLP-
DB and More. SIGMOD, 34(2), 2005.
[Fag77] Ronald Fagin. Multivalued Dependencies and a New Normal Form for Rela-
tional Databases. ACM Trans. Database Syst., 2(3):262–278, 1977.
[Fag82] Ronald Fagin. Horn Clauses and Database Dependencies. J. ACM, 29(4):952–
985, 1982.
[FB08] George H. L. Fletcher and Peter W. Beck. A Role-free Approach to Indexing
Large RDF Data Sets in Secondary Memory for Eﬃcient SPARQL Evalu-
ation. Technical report, arXiv:0811.1083 cs.DB, 2008. http://arxiv.org/
abs/0811.1083.
[FKMP05] Ronald Fagin, Phokion G. Kolaitis, Ren´ee J. Miller, and Lucian Popa. Data
Exchange: Semantics and Query Answering. Theor. Comput. Sci., 336(1):89–
124, 2005.
[FL01] W. Fan and L. Libkin. On XML Integrity Constraints in the Presence of
DTDs. In J. ACM, pages 114–125, 2001.
[foa] The Friend of a Friend (FOAF) Project. http://www.foaf-project.org/.
[FS03] Wenfei Fan and J´erˆome Sim´eon. Integrity Constraints for XML. J. Comput.
Syst. Sci., 66(1):254–291, 2003.
[FT05] Enrico Franconi and Sergio Tessaris. The Semantics of SPARQL, 2005. http:
//www.inf.unibz.it/krdb/w3c/sparql/. Editors working draft.
[GdMB08] Aurona Gerber, Alta Van der Merwe, and Andries Barnard. A Functional
Semantic Web Architecture. In ESWC, pages 273–287, 2008.
[GGK09] Jinghua Groppe, Sven Groppe, and Jan Kolbaum. Optimization of SPARQL
by using coreSPARQL. In ICEIS, pages 107–112, 2009.
[GGL07] Sven Groppe, Jinghua Groppe, and Volker Linnemann. Using an Index of
Precomputed Joins in order to speed up SPARQL Processing.
In ICEIS,
pages 13–20, 2007.
185

Bibliography
[GHM04] Claudio Gutierrez, Carlos A. Hurtado, and Alberto O. Mendelzon. Founda-
tions of Semantic Web Databases. In PODS, pages 95–106, 2004.
[GKP03] Georg Gottlob, Christoph Koch, and Reinhard Pichler. The Complexity of
XPath Query Evaluation. In PODS, pages 179–190, 2003.
[GKPS05] Georg Gottlob, Christoph Koch, Reinhard Pichler, and Luc Segouﬁn. The
Complexity of XPath Query Evaluation and XML Typing. J. ACM, 52(2),
2005.
[GLR97] C´esar A. Galindo-Legaria and Arnon Rosenthal. Outerjoin Simpliﬁcation and
Reordering for Query Optimization. ACM Trans. Database Syst., 22(1):43–
73, 1997.
[GMUW00] Hector Garcia-Molina, Jeﬀrey D. Ullman, and Jennifer Widom.
Database
System Implementation. Prentice Hall, 2000.
[gnu] Gnuplot. http://www.gnuplot.info/.
[GPH05] Yuanbo Guo, Zhengxiang Pan, and JeﬀHeﬂin. LUBM: A Benchmark for
OWL Knowledge Base Systems.
Web Semantics: Science, Services and
Agents on the WWW, 3(2-3):158–182, 2005.
[Gra93] Jim Gray. The Benchmark Handbook for Database and Transaction Systems.
Morgan Kaufmann, 1993.
[Gru02] Thorsten Grust. Accelerating XPath Location Steps. In SIGMOD, pages
109–120, 2002.
[Hal75] Patrick A. V. Hall. Optimization of a Single Expression in a Relational Data
Base System. Technical report, IBM UKSC Report 76, 1975.
[Hal01] Alon Y. Halevy. Answering Queries Using Views: A Survey. VLDB Journal,
pages 270–294, 2001.
[Har09] Olaf Hartig. Trustworthiness of Data on the Web. In STI Berlin & CSW
PhD Workshop, 2008/09.
[HBEV04] Peter Haase, Jeen Broekstra, Andreas Eberhart, and Raphael Volz. A Com-
parison of RDF Query Languages. In ISWC, pages 502–517, 2004.
[HD05] Andreas Harth and Stefan Decker. Optimized Index Structures for Querying
RDF from the Web. In LA-WEB, pages 71–80, 2005.
[HG03] Stephen Harris and Nicholas Gibbins. 3store: Eﬃcient Bulk RDF Storage. In
PSSS, 2003.
[HH07] Olaf Hartig and Ralf Heese. The SPARQL Query Graph Model for Query
Optimization. In ESWC, pages 564–578, 2007.
186

Bibliography
[HM75] Michael M. Hammer and Dennis J. McLeod. Semantic Integrity in a Rela-
tional Data Base System. In VLDB, pages 25–47, 1975.
[HS08] Huahai He and Ambuj K. Singh.
Graphs-at-a-time: Query Language and
Access Methods for Graph Databases. In SIGMOD, pages 405–418, 2008.
[JK82] David S. Johnson and Anthony C. Klug. Testing Containment of Conjunctive
Queries under Functional and Inclusion Dependencies. In PODS, pages 164–
169, 1982.
[JK84] Matthias Jarke and J¨urgen Koch. Query Optimization in Database Systems.
ACM Comput. Surv., 16(2):111–152, 1984.
[KACP02] Gregory Karvounarakis, Soﬁa Alexaki, Vassilis Christophides, and Dimitris
Plexousakis. RQL: a Declarative Query Language for RDF. In WWW, pages
592–603, 2002.
[Kin81] Jonathan J. King.
QUIST: a system for semantic query optimization in
relational databases. In VLDB, pages 510–517, 1981.
[KJ07] Krys Kochut and Maciej Janik. SPARQLeR: Extended SPARQL for Seman-
tic Association Discovery. In ESWC, pages 145–159, 2007.
[KLW95] Michael Kifer, Georg Lausen, and James Wu. Logical Foundations of Object-
Oriented and Frame-Based Languages. J. ACM, 42(4):741–843, 1995.
[KM01] Marja R. Koivunen and Eric Miller. W3C Semantic Web Activity. In Se-
mantic Web Kick-Oﬀin Finland – Vision, Technologies, Research, and Ap-
plications, pages 27–43, 2001.
[Len02] Maurizio Lenzerini. Data Integration: A Theoretical Perspective. In PODS,
pages 233–246, 2002.
[Ley] Michael Ley.
DBLP Database.
http://www.informatik.uni-trier.de/
∼ley/db/.
[lin] Linked Data. http://linkeddata.org/.
[LM01] Quanzhong Li and Bongki Moon.
Indexing and Querying XML Data for
Regular Path Expressions. In VLDB, pages 361–370, 2001.
[LMS08] Georg Lausen, Michael Meier, and Michael Schmidt. SPARQLing Constraints
for RDF. In EDBT, pages 499–509, 2008.
[Lot26] Alfred J. Lotka. The Frequency Distribution of Scientiﬁc Production. Acad.
Sci., 16:317–323, 1926.
187

Bibliography
[MACP02] Aimilia Magkanaraki, Soﬁa Alexaki, Vassilis Christophides, and Dimitris
Plexousakis. Benchmarking RDF Schemas for the Semantic Web. In ISWC,
pages 132–146, 2002.
[Mar04] Draltan Marin.
RDF Formalization.
Technical report, Universidad de
Chile, TR/DCC-2006-8, 2004.
http://www.dcc.uchile.cl/∼cgutierr/
ftp/draltan.pdf.
[Mar09] Bruno Marnette.
Generalized Schema-Mappings: From Termination To
Tractability. In PODS, pages 13–22, 2009.
[McL99] Michael P. McLaughlin. A Compendium of Common Probability Distribu-
tions, 1999.
[MD88] M. Muralikrishna and David J. DeWitt. Equi-Depth Histograms for Estimat-
ing Selectivity Factors For Multi-Dimensional Queries. In SIGMOD, pages
28–36, 1988.
[MHS07] Boris Motive, Ian Horrocks, and Ulrike Sattler. Adding Integrity Constraints
to OWL. In OWLED-07, 2007.
[MMS79] David Maier, Alberto Mendelzon, and Yehoshua Sagiv. Testing Implications
of Data Dependencies. TODS, pages 455–469, 1979.
[MPG07] Sergio Mu˜noz, Jorge P´erez, and Claudio Gutierrez. Minimal Deductive Sys-
tems for RDF. In ESWC, pages 53–67, 2007.
[MSL09a] Michael Meier, Michael Schmidt, and Georg Lausen. On Chase Termination
Beyond Stratiﬁcation. In VLDB, 2009.
[MSL09b] Michael Meier, Michael Schmidt, and Georg Lausen.
On Chase Termina-
tion Beyond Stratiﬁcation. Technical report, arXiv:0906.4228v2 cs.DB, 2009.
http://arxiv.org/abs/0906.4228.
[MSL09c] Michael Meier, Michael Schmidt, and Georg Lausen. Stop the Chase. Tech-
nical report, arXiv:0901.3984 cs.DB, 2009. http://arxiv.org/abs/0901.
3984.
[MSL09d] Michael Meier, Michael Schmidt, and Georg Lausen. Stop the Chase: Short
Contribution. In Alberto Mendelzon Workshop on Foundations of Data Man-
agement, 2009.
[Muy08] Andrae Muys.
A Concurrency Protocol for Multiversion RDF Datastores
(Discussion Paper). In Mulgara Workshop, San Francisco, 2008. http://
www.netymon.com/papers/XA2-Discussion-Papers/XA2-CC-Design.pdf.
[MW88] David Maier and David S. Warren. Computing with logic: logic programming
with Prolog. Benjamin-Cummings Publishing Co., 1988.
188

Bibliography
[Nic78] Jean-Marie Nicloas. First Order Logic Formalization for Functional, Multi-
valued, and Mutual Dependencies. In SIGMOD, pages 40–46, 1978.
[not] Notation 3 (N3). http://www.w3.org/DesignIssues/Notation3.
[ntr] N-Triples. http://www.w3.org/TR/rdf-testcases/#ntriples.
[NW08] Thomas Neumann and Gerhard Weikum. RDF-3X: a RISC-style engine for
RDF. PVLDB, 1(1):647–659, 2008.
[NW09] Thomas Neumann and Gerhard Weikum. Scalable Join Processing on Very
Large RDF Graphs. In SIGMOD, pages 627–640, 2009.
[OH08] Kieron O’Hara and Wendy Hall.
Trust on the Web: Some Web Science
Research Challenges. e-Journal on the Knowledge Society, 2008.
[OHK09] Dan Olteanu, Jiewen Huang, and Christoph Koch. SPROUT: Lazy vs. Eager
Query Plans for Tuple-Independent Probabilistic Databases. In ICDE, 2009.
[ope] openRDF.org.
The SeRQL query language (revision 1.2).
http://www.
openrdf.org/doc/sesame/users/ch06.html.
[owl] Web Ontology Language OWL. http://www.w3.org/2004/OWL/.
[PAG06a] Jorge P´erez, Marcelo Arenas, and Claudio Gutierrez. Semantics and Com-
plexity of SPARQL. Technical report, arXiv:0605124 cs.DB, 2006. http:
//arxiv.org/abs/0605124.
[PAG06b] Jorge P´erez, Marcelo Arenas, and Claudio Gutierrez. Semantics of SPARQL.
Technical report, Universidad de Chile, TR/DCC-2006-16, 2006.
[PAG08] Jorge P´erez, Marcelo Arenas, and Claudio Gutierrez. nSPARQL: A Naviga-
tional Language for RDF. In ISWC, pages 66–81, 2008.
[PAG09] Jorge P´erez, Marcelo Arenas, and Claudio Gutierrez. Semantics and Com-
plexity of SPARQL. ACM Trans. Database Syst., 2009.
[Pap94] Christos H. Papadimitriou.
Computational Complexity.
Addison-Wesley,
1994.
[Pol07] Axel Polleres. From SPARQL to Rules (and back). In WWW, pages 787–796,
2007.
[PSS07] Axel Polleres, Francois Scharﬀe, and Roman Schindlauer. SPARQL++ for
Mapping Between RDF Vocabularies. On the Move to Meaningful Internet
Systems 2007: CoopIS, DOA, ODBASE, GADA, and IS, pages 878–896, 2007.
[rdfa] Resource Description Framework (RDF): Concepts and Abstract Syn-
tax.
W3C Recommendation, 10 February 2004. http://www.w3.org/TR/
rdf-concepts/.
189

Bibliography
[rdfb] RDF Primer. W3C Recommendation, 10 February 2004. http://www.w3.
org/TR/REC-rdf-syntax/.
[rdfc] RDF Vocabulary Description Language 1.0: RDF Schema. W3C Recommen-
dation, 10 February 2004. http://www.w3.org/TR/rdf-schema/.
[rdfd] RDF Semantics. W3C Recommendation, 10 February 2004. http://www.w3.
org/TR/rdf-mt/.
[rdfe] RDF/XML Syntax Speciﬁcation (Revised).
W3C Recommendation, 10
February 2004. http://www.w3.org/TR/rdf-syntax-grammar/.
[rifa] RIF Working Group.
W3C Initiative. http://www.w3.org/2005/rules/
wiki/RIF Working Group.
[rifb] Rule Interchange Format Working Group Charter.
http://www.w3.org/
2005/rules/wg/charter.html.
[SC75] John M. Smith and Philip Y. Chang. Optimizing the Performance of a Rela-
tional Algebra Database Interface. Commun. ACM, 18(10):568–579, 1975.
[SD01] Michael Sintek and Stefan Decker. TRIPLE – An RDF Query, Inference, and
Transformation Language. In INAP, pages 47–56, 2001.
[ses] openRDF.org
–
home
of
Sesame.
http://www.openrdf.org/
documentation.jsp.
[SGK+08] Lefteris Sidirourgos, Romulo Goncalves, Martin L. Kersten, Niels Nes, and
Stefan Manegold. Column-store Support for RDF Data Management: not all
swans are white. In VLDB, pages 1553–1563, 2008.
[SHK+08] Michael Schmidt, Thomas Hornung, Norbert K¨uchlin, Georg Lausen, and
Christoph Pinkel. An Experimental Comparison of RDF Data Management
Approaches in a SPARQL Benchmark Scenario. In ISWC, pages 82–97, 2008.
[SHLP08] Michael Schmidt, Thomas Hornung, Georg Lausen, and Christoph Pinkel.
SP2Bench:
A
SPARQL
Performance
Benchmark.
Technical
report,
arXiv:0806.4627 cs.DB, 2008. http://arxiv.org/abs/0806.4627.
[SHLP09] Michael Schmidt, Thomas Hornung, Georg Lausen, and Christoph Pinkel.
SP2Bench: A SPARQL Performance Benchmark. In ISWC, 2009.
[SHM+09] Michael Schmidt, Thomas Hornung, Michael Meier, Christoph Pinkel, and
Georg Lausen. SP2Bench: A SPARQL Performance Benchmark. In Semantic
Web Information Management: a model based perspective. Springer, 2009.
[Siz07] Sergej Sizov.
What Makes You Think That? The Semantic Web’s Proof
Layer. IEEE Intelligent Systems, 22(6):94–99, 2007.
190

Bibliography
[SKCT05] Giorgos Serﬁotis, Ioanna Koﬃna, Vassilis Christophides, and Val Tannen.
Containment and Minimization of RDF/S Query Patterns. In ISWC, pages
607–623, 2005.
[SML08] Michael Schmidt, Michael Meier, and Georg Lausen. Foundations of SPARQL
Query Optimization. Technical report, arXiv:0812.3788 cs.DB, 2008. http:
//arxiv.org/abs/0812.3788.
[SO87] Sreekumar T. Shenoy and Zehra M. Ozsoyoglu. A System for Semantic Query
Optimization. In SIGMOD, pages 181–195, 1987.
[SOO99] Lei Sheng, Zehra M. Ozsoyoglu, and Gultekin Ozsoyoglu. A Graph Query
Language and Its Query Processing. In ICDE, pages 572–581, 1999.
[spaa] SPARQL New Features and Rationale. W3C Working Draft, 2 July 2009.
http://www.w3.org/TR/2009/WD-sparql-features-20090702/.
[spab] SPARQL Protocol for RDF. W3C Recommendation, 15 January 2008. http:
//www.w3.org/TR/rdf-sparql-protocol/.
[spac] SPARQL Query Language for RDF.
W3C Recommendation, 15 January
2008. http://www.w3.org/TR/rdf-sparql-query/.
[SS86] Leon Sterling and Ehud Y. Shapiro. The Art of Prolog - Advanced Program-
ming Techniques. MIT Press, 1986.
[SSB+08] Markus Stocker, Andy Seaborne, Abraham Bernstein, Christoph Kiefer, and
Dave Reynolds. SPARQL Basic Graph Pattern Optimization Using Selectiv-
ity Estimation. In WWW, pages 595–604, 2008.
[Sto76] Larry J. Stockmeyer. The polynomial-time hierarchy. Theor. Comput. Sci.,
3:1–22, 1976.
[SWK+02] Albrecht Schmidt, Florian Waas, Martin L. Kersten, Michael J. Carey, Ioana
Manolescu, and Ralph Busse. XMark: A Benchmark for XML Data Manage-
ment. In VLDB, pages 974–985, 2002.
[swr] SWRC – Semantic Web Research Community Ontology. http://ontoware.
org/projects/swrc/.
[sww] W3C Semantic Web Activity. http://www.w3org/2001/sw/.
[Tau] Joshua Tauberer. U.S. Census RDF Data. http://www.rdfabout.com/demo/
census/.
[TCK05] Yannis Theoharis, Vassilis Christophides, and Grigoris Karvounarakis.
Benchmarking Database Representations of RDF/S Stores. In ISWC, pages
685–701, 2005.
191

Bibliography
[Tod75] Stephen Todd.
PRTV: An Eﬃcient Implementation for Large Relational
Data Bases. In VLDB, pages 554–556, 1975.
[tpc] TPC. http://www.tpc.org.
[TTKC08] Yannis
Theoharis,
Yannis
Tzitzikas,
Dimitris
Kotzinos,
and
Vassilis
Christophides. On Graph Features of Semantic Web Schemas. IEEE Trans.
Knowl. Data Eng., 20(5):692–702, 2008.
[tur] Turtle – Terse RDF Triple Language. W3C Team Submission, 14 January
2008. http://www.w3.org/TeamSubmission/turtle/.
[Var82] Moshe Y. Vardi. The Complexity of Relational Query Languages (Extended
Abstract). In STOC, pages 137–146, 1982.
[W3Ca] W3C.
The Semantic Web layer cake.
http://www.w3.org/2007/03/
layerCake.png.
[w3cb] Internationalized Resource Identiﬁers (IRIs). RFC 3987, January 2005. http:
//tools.ietf.org/html/rfc3987.
[w3cc] Uniform Resource Identiﬁer (URI): Generic Syntax. RFC 3986, January 2005.
http://tools.ietf.org/html/rfc3986.
[w3cd] Universal Resource Identiﬁers in WWW.
RFC 1630, June 1994. http://
tools.ietf.org/html/rfc1630.
[w3ce] Extensible Markup Language (XML). http://www.w3.org/XML/.
[WGQH05] Sui-Yu Wang, Yuanbo Guo, Abir Qasem, and JeﬀHeﬂin. Rapid Benchmark-
ing for Semantic Web Knowledge Base Systems. In ISWC, pages 758–772,
2005.
[WKB08] Cathrin Weiss, Panagiotis Karras, and Abraham Bernstein. Hexastore: Sex-
tuple Indexing for Semantic Web Data Management. In VLDB, pages 1008–
1019, 2008.
[xml] XML Schema Part 0: Primer Second Edition. W3C Recommendation, 28
October 2004. http://www.w3.org/TR/xmlschema-0.
[xpa] XML Path Language (XPath) 2.0. W3C Recommendation, 23 January 2007.
http://www.w3.org/TR/xpath20/.
[xqu] XQuery 1.0: An XML Query Language. W3C Recommendation, 23 January
2007. http://www.w3.org/TR/xquery/.
[zun] ZunZun Curve Fitting and Surface Fitting. http://zunzun.com/.
192

Appendix A.
Survey of Namespaces
Preﬁx
Description
bench
Shortcut for http://localhost/vocabulary/bench/
User-deﬁned namespace used in SP2Bench (cf. Chapter 6).
dc
Shortcut for http://purl.org/dc/elements/1.1/
Namespace of the Dublin Core Metadata project [dub], used for
describing bibliographic entities.
dcterms
Shortcut for http://purl.org/dc/terms/
Namespace of the Dublin Core Metadata project [dub], used for
describing bibliographic entities.
foaf
Shortcut for http://xmlns.com/foaf/0.1/
Namespace of the Friend-of-a-Friend project [foa], which provides
vocabulary for describing persons.
myns
Shortcut for http://my.namespace.com#
User-deﬁned namespace used in various examples throughout the thesis.
person
Shortcut for http://localhost/persons/
User-deﬁned namespace used in SP2Bench (cf. Chapter 6).
rdf
Shortcut for http://www.w3.org/1999/02/22-rdf-syntax-ns#
Standard RDF [rdfa] namespace.
rdfs
Shortcut for http://www.w3.org/2000/01/rdf-schema#
Standard RDFS [rdfc] namespace.
swrc
Shortcut for http://swrc.ontoware.org/ontology#
Namespace of the SWRC [swr] (Semantic Web for Research
Communities) project for modeling entities of research communities.
xsd
Shortcut for http://www.w3.org/2001/XMLSchema#
Contains common RDF datatype deﬁnitions, such as xsd:integer.
Figure A.1.: Survey of namespaces used in the thesis.
193

Appendix A. Survey of Namespaces
194

Appendix B.
Proofs of Complexity Results
B.1. Proof of Theorem 3.5
We start with a more general form of the QBF problem, which will be required
later in the main proof of Theorem 3.5. The new version of QBF diﬀers from the
QBF versions used in the proofs of Theorems 3.3 and 3.4 (cf. Section 3.2.3) in that
we relax the condition that the inner, quantiﬁer-free part of the formula is in CNF.
We call this generalized version QBF* and deﬁne it as follows.
QBF*: given a quantiﬁed boolean formula ϕ := ∀x1∃y1∀x2∃y2 . . . ∀xm∃ymψ,
as input, where ψ is a quantiﬁer-free formula: is ϕ valid?
Lemma B.1 There is a polynomial-time reduction from QBF* to the SPARQL
Evaluation problem for class AFO.1
2
Proof of Lemma B.1
The correctness of this lemma follows from the observations that (i) QBF* is
known to be PSpace-complete (like QBF), (ii) the subfragment AO ⊂AFO is
PSpace-hard and (iii) the superfragment E ⊃AFO is contained in PSpace. Thus,
fragment AFO also is PSpace-complete, which implies the existence of a reduction.
We are, however, interested in some speciﬁc properties of the reduction, so we
will shortly sketch the construction. We restrict ourselves on showing how to encode
the inner, quantiﬁer-free boolean formula ϕ (which is not necessarily in CNF) using
operators And and Filter. The second part of the reduction, namely the encoding
of the quantiﬁer sequence, is the same as in the proof of Theorem 3.3.
Let us start with a quantiﬁed boolean formula of the form
ϕ := ∀x1∃y1∀x2∃y2 . . . ∀xm∃ymψ,
1The same result was proven in [PAG09]. This lemma, however, was developed independently
from [PAG09]. We informally published it already several months earlier in [SML08].
195

Appendix B. Proofs of Complexity Results
where ψ is a quantiﬁer-free boolean formula. We assume w.l.o.g. that ψ is con-
structed using the boolean connectives ∧, ∨and ¬. By Vψ := {v1, . . . , vn} we denote
the set of boolean variables in formula ψ. We ﬁx the database
D := {(a, false, 0), (a, true, 1), (a, tv, 0), (a, tv, 1)}
and encode the formula ψ as
Pψ := ((a, tv, ?V1) And . . . And (a, tv, ?Vn)) Filter f(ψ),
where ?V1, . . . , ?Vn represent the boolean variables v1, . . . , vn and function f(ψ)
generates a SPARQL ﬁlter condition that precisely mirrors the boolean formula ψ.
Formally, function f(ψ) is deﬁned inductively on the structure of ψ as follows.
f (vi)
:= ?Vi = 1
f (ψ1 ∧ψ2):= f (ψ1) ∧f (ψ2)
f (ψ1 ∨ψ2):= f (ψ1) ∨f (ψ2)
f (¬ψ1)
:= ¬ f (ψ1)
In the expression Pψ, the And-block generates all possible valuations for the
variables in ψ, while the Filter-expression retains exactly those valuations that
satisfy formula ψ. It is straightforward to verify that ψ is satisﬁable iﬀthere is
a mapping µ ∈JPψKD. Even more, for each µ ∈JPψKD the truth assignment ρµ
deﬁned as ρµ(v) := µ(?V ) for all variables v ∈Vψ satisﬁes the formula ψ and, vice
versa, for each truth assignment ρ that satisﬁes ψ there is a mapping µ ∈JPψKD
that deﬁnes ρ. The rest of the proof (i.e., the encoding of the surrounding quantiﬁer
sequence) is the same as in the proof of Theorem 3.3. Ultimately, this gives us a
SPARQL expression Pϕ (which contains Pψ above as a subexpression) such that the
formula ϕ is valid if and only the mapping µ := {B0 7→1} is contained in JPϕKD.2
The next lemma follows from the construction in the previous lemma:
Lemma B.2 Let
D := {(a, false, 0), (a, true, 1), (a, tv, 0), (a, tv, 1)}
be an RDF database and ϕ := ∀x1∃y1 . . . ∀xm∃ymψ (m ≥1) be a quantiﬁed
boolean formula, where ψ is quantiﬁer-free. There is an encoding enc(ϕ) such that
1. enc(ϕ) ∈E≤2m,
2. ϕ is valid iﬀ{?B0 7→1} ∈Jenc(ϕ)KD, and
3. ϕ is invalid iﬀfor each µ ∈Jenc(ϕ)KD it holds that µ ⊇{?B0 7→1, ?A1 7→1}.2
196

B.1. Proof of Theorem 3.5
Proof of Lemma B.2
We argue that expression Pϕ from the proof of Lemma B.1 is an encoding that
satisﬁes all three conditions (observe that the database D deﬁned in Lemma B.2 cor-
responds to the database D used in the proof of Lemma B.1). To prove Lemma B.2,
we thus set enc(ϕ) := Pϕ and verify the claims one by one.
Lemma B.2(1): It is easy to see that the Opt-rank of Pϕ is exactly 2m: the subex-
pression Pψ presented in the proof of Lemma B.1 is Opt-free and the surrounding
construction that simulates the quantiﬁer alternation requires a nesting of 2m Opt
expressions (see the proof of Theorem 3.3). Hence, Pϕ =: enc(ϕ) ∈E≤2m.
Lemma B.2(2): Follows directly from the proof of Lemma B.1.
Lemma B.2(3): We consider the expression Pϕ deﬁned in Theorem 3.3:
Pϕ := (a, true, ?B0) Opt (P1 Opt (Q1
Opt (P2 Opt (Q2
. . .
Opt (Pm Opt (Qm And Pψ)) . . . )))),
where the subexpression Pψ denotes the AF encoding for ψ from the proof of
Lemma B.1. First consider subexpression P1, which is deﬁned as
P1
:= ((a, tv, ?X1) And (a, false, ?A0) And (a, true, ?A1)), so we have
JP1KD = {{?X1 7→0, ?A0 7→0, ?A1 7→1}, {?X1 7→1, ?A0 7→0, ?A1 7→1}}.
Further, J(a, true, ?B0)KD = {{?B0 7→1}}. We now study the subexpression
P ′ := P1 Opt (Q1
Opt (P2 Opt (Q2
. . .
Opt (Pm Opt (Qm And Pψ)) . . . ))),
of Pϕ. Recall that m ≥1 by assumption. It is easy to see that, by semantics of
Opt and the previous evaluation result for JP1KD, in each mapping µ′ ∈JP ′KD,
variable ?A1 is bound to 1. Furthermore, JP ′KD contains at least two mappings,
because the Opt operator either joins the two mappings from JP1KD with others
or retains them without joining. Finally, consider the evaluation of expression Pϕ.
We can assume that there is at least one mapping µ′ ∈JP ′KD s.t. µ′ ∼{?B0 7→1}
(otherwise {?B0 7→1} ∈JPϕKD, which implies that ϕ is valid and the third case of
Lemma B.1 would not apply). Consequently, each mapping µ ∈JPϕKD is of the form
µ := {?B0 7→1} ∪µ′, where µ′ ∈JP ′KD. From the previous argumentation it follows
that µ maps both variable ?B0 and ?A1 to 1. This completes the proof.2
Lemma B.3 Let P1 and P2 be SPARQL expressions for which the evaluation prob-
lem is in ΣP
i , i ≥1, and let R be a ﬁlter condition. The following claims hold.
197

Appendix B. Proofs of Complexity Results
1. The Evaluation problem for the expression P1 Union P2 is in ΣP
i .
2. The Evaluation problem for the expression P1 And P2 is in ΣP
i .
3. The Evaluation problem for the expression P1 Filter R is in ΣP
i .
2
Proof of Lemma B.3
Lemma B.3(1): According to the semantics we have that µ ∈JP1 Union P2KD if
and only if µ ∈JP1KD or µ ∈JP2KD. By assumption, both conditions can be checked
individually by a ΣP
i -algorithm, and so both can be checked in sequence in ΣP
i .
Lemma B.3(2): It is easy to see that µ ∈JP1 And P2KD iﬀµ can be decomposed
into two mappings µ1 ∼µ2 such that µ = µ1 ∪µ2 and µ1 ∈JP1KD and µ2 ∈JP2KD.
By assumption, both testing µ1 ∈JP1KD and µ2 ∈JP2KD is in ΣP
i . Since i ≥1, we
have that ΣP
i ⊇ΣP
1 = NP. Hence, we can guess a decomposition µ = µ1 ∪µ2 and
check the two conditions one after the other. The whole procedure is in ΣP
i .
Lemma B.3(3): The condition µ ∈JP1 Filter RKD holds iﬀµ ∈JP1KD (which can
be tested in ΣP
i by assumption) and R satisﬁes µ (which can be tested in polynomial
time). We have that ΣP
i ⊇NP ⊇PTime for i ≥1, so the algorithm is in ΣP
i .2
Proof of Theorem 3.5
We are now ready to tackle Theorem 3.5. The completeness proof divides into two
parts, namely hardness and membership. We start with the hardness part, which
is a reduction from QBFn, a variant of the QBF problem used in previous proofs
where the number n of quantiﬁer alternations is ﬁxed. We formally deﬁne QBFn:
QBFn: given a quantiﬁed boolean formula ϕ := ∃x1∀x2∃x3 . . . Qxnψ as input,
where ψ is a quantiﬁer-free formula, Q := ∃if n is odd, and Q := ∀
if n is even: is the formula ϕ valid?
It is known that QBFn is ΣP
n -complete for n ≥1 (see e.g. [Pap94]).
(Hardness) Recall that our goal is to show that fragment E≤n is ΣP
≤n+1-hard. To
prove this claim, we present a reduction from QBFn+1 to the Evaluation problem
for class E≤n, i.e. we encode a quantiﬁed boolean formula with n + 1 quantiﬁer
alternations by an E expression with Opt-rank ≤n. We distinguish two cases.
(1) Let Q := ∃, so the quantiﬁed boolean formula is of the form
ϕ := ∃y0∀x1∃y1 . . . ∀xm∃ymψ.
Formula ϕ has 2m+1 quantiﬁer alternations, so we need to ﬁnd an E≤2m encoding
for this expressions. We rewrite ϕ into an equivalent formula ϕ := ϕ1 ∨ϕ2, where
ϕ1 := ∀x1∃y1 . . . ∀xm∃ym(ψ ∧y0),
ϕ2 := ∀x1∃y1 . . . ∀xm∃ym(ψ ∧¬y0).
198

B.1. Proof of Theorem 3.5
According to Lemma B.2 there is a ﬁxed document D and E≤2m encodings enc(ϕ1)
and enc(ϕ2) (for ϕ1 and ϕ2, respectively) s.t. Jenc(ϕ1)KD (resp. Jenc(ϕ2)KD) contains
the mapping µ := {?B0 7→1} iﬀϕ1 (resp. ϕ2) is valid. It is easy to see that the
expression enc(ϕ) := enc(ϕ1) Union enc(ϕ2) contains µ iﬀϕ1 or ϕ2 is valid, i.e. iﬀ
ϕ := ϕ1∨ϕ2 is valid. Given that enc(ϕ1) and enc(ϕ2) are E≤2m expressions, it follows
that enc(ϕ) := enc(ϕ1) Union enc(ϕ2) is in E≤2m, which completes part (1).
(2) Let Q := ∀, so the quantiﬁed boolean formula is of the form
ϕ := ∃x0∀y0∃x1∀x1 . . . ∃xm∀ymψ.
ϕ has 2m+2 quantiﬁer alternations, so we need to ﬁnd a reduction to the E≤2m+1
fragment. We eliminate the outer ∃-quantiﬁer by rewriting ϕ as ϕ := ϕ1 ∨ϕ2, where
ϕ1 := ∀y0∃x1∀y1 . . . ∃xm∀ym(ψ ∧y0),
ϕ2 := ∀y0∃x1∀y1 . . . ∃xm∀ym(ψ ∧¬y0).
Abstracting from the details of the inner formula, both ϕ1 and ϕ2 are of the form
ϕ′ := ∀y0∃x1∀y1 . . . ∃xm∀ymψ′,
where ψ′ is a quantiﬁer-free boolean formula. We now proceed as follows: we
show (*) how to encode ϕ′ by an E≤2m+1 expression enc(ϕ′) that, when evaluated
on a ﬁxed document D, yields a ﬁxed mapping µ exactly if ϕ′ is valid. This is
suﬃcient, because then expression enc(ϕ1) Union enc(ϕ2) is an E≤2m+1 encoding
that contains µ exactly if the original formula ϕ := ϕ1 ∨ϕ2 is valid (analogously to
the argumentation in case (1) of the proof). We ﬁrst rewrite ϕ′:
ϕ′ := ∀y0∃x1∀y1 . . . ∃xm∀ymψ′
= ¬∃y0∀x1∃y1 . . . ∀xm∃ym¬ψ′
= ¬(ϕ′
1 ∨ϕ′
2), where
ϕ′
1 := ∀x1∃y1 . . . ∀xm∃ym(¬ψ′ ∧y0),
ϕ′
2 := ∀x1∃y1 . . . ∀xm∃ym(¬ψ′ ∧¬y0).
According to Lemma B.2, each ϕ′
i can be encoded by an E≤2m expressions enc(ϕ′
i)
such that, on the ﬁxed database D given there, (1) µ := {?B0 7→1} ∈Jϕ′
iKD iﬀϕ′
i is
valid and (2) if ϕ′
i is not valid, then all mappings Jenc(ϕ′
i)KD bind both variable ?A1
and ?B0 to 1. It follows that (1’) µ ∈enc(ϕ′
1) Union enc(ϕ′
2) iﬀϕ′
1 ∨ϕ′
2 and (2’) all
mappings µ ∈enc(ϕ′
1) Union enc(ϕ′
2) bind both ?A1 and ?B0 to 1 iﬀ¬(ϕ′
1 ∨ϕ′
2).
Now consider the expression Q := (a, false, ?A1) Opt (enc(ϕ′
1) Union enc(ϕ′
2)).
From claims (1’) and (2’) it follows that µ′ := {?A1 7→0} ∈JQKD iﬀ¬(ϕ′
1 ∨ϕ′
2).
Now recall that ϕ′ = ¬(ϕ′
1 ∨ϕ′
2), hence µ′ ∈JQKD iﬀϕ′ is valid. We know that
both enc(ϕ′
1) and enc(ϕ′
2) are E≤2m expressions, so Q ∈E≤2m+1. This implies that
claim (*) holds and completes the hardness part of the proof.
199

Appendix B. Proofs of Complexity Results
And
Opt
. . .
. . .
•
•
Opt
. . .
. . .
t
•
Opt
. . .
. . .
And
Evaluation in ΣP
n+2
•
•
Evaluation in ΣP
n+2
Evaluation in PTime
•
Evaluation in ΣP
n+2
Figure B.1.: (a) AND-expression with increased OPT-rank; (b) Associated complex-
ity classes for Opt-subexpressions and triple patterns.
(Membership) We next prove membership of E≤n expressions in ΣP
n+1 by induction
on the Opt-rank. Let us assume that for each E≤n expression (n ∈N0) Evaluation
is in ΣP
n+1. As stated in Theorem 3.1(2), Evaluation is ΣP
1 = NP-complete for
Opt-free expressions (i.e., E≤0), so the hypothesis holds for the basic case. In the
induction step we increase the Opt-rank from n to n + 1 and show that, for the
resulting E≤n+1 expression, the Evaluation problem can be solved in ΣP
n+2. We
consider an expression Q with rank(Q) := n + 1 and distinguish four cases.
(1) Assume that Q := P1 Opt P2. By assumption, Q ∈E≤n+1 and from the
deﬁnition of the Opt-rank (cf. Deﬁnition 3.2) it follows that both P1 and P2 are
in E≤n. Hence, by induction hypothesis, both P1 and P2 can be evaluated in ΣP
n+1.
By semantics, we have that JP1 Opt P2KD = JP1 And P2KD ∪(JP1KD \ JP2KD), so it
holds that µ ∈JP1 Opt P2KD iﬀit is generated by (i) JP1 And P2KD or generated
by (ii) JP1KD \ JP2KD. According to Lemma B.3(2), condition (i) can be checked
in ΣP
n+1. The more interesting part is to check if (ii) holds. Applying the semantics
of operator \, this check can be formulated as C := C1 ∧C2, where C1 := µ ∈JP1KD
and C2 := ¬∃µ′ ∈JP2KD : µ ∼µ′. By induction hypothesis, C1 can be checked
in ΣP
n+1. We now argue that ¬C2 = ∃µ′ ∈JP2KD : µ ∼µ′ can be checked in ΣP
n+1: we
can guess a mapping µ′ (because ΣP
n+1 ⊇NP) and then check if µ ∈JP2KD (which,
by application of the induction hypothesis, can be done by a ΣP
n+1-algorithm), and
test if µ and µ′ are compatible (in polynomial time). Checking the inverse problem,
i.e. if C2 holds, is then possible in coΣP
n+1 = ΠP
n+1. Summarizing cases (i) and (ii) we
observe that (i) ΣP
n+1 and (ii) ΠP
n+1 are both contained in ΣP
n+2, so the two checks
in sequence can be performed in ΣP
n+2. This completes case (1).
(2) Assume that Q := P1 And P2. Figure B.1(a) shows the structure of a sam-
ple And expression, where the • symbols represent non-Opt operators (i.e. And,
Union, or Filter), and t stands for triple patterns. Expression Q has an arbitrary
number of Opt subexpressions (which might, of course, contain Opt subexpres-
sions themselves). Each of these subexpressions has Opt-rank ≤n + 1. Using the
same argumentation as in case (1), the evaluation problem for all of them is in ΣP
n+2.
Further, each leaf node of the tree carries a triple pattern, which can be evaluated in
200

B.1. Proof of Theorem 3.5
PTime ⊆ΣP
n+2. Figure B.1(b) illustrates the tree that is obtained when replacing all
Opt-expressions and triple patterns by the complexity of their Evaluation prob-
lem. This simpliﬁed tree is now Opt-free, i.e. carries only operators And, Union,
and Filter. We then proceed as follows. We apply Lemma B.3(1)-(3) repeatedly,
folding the remaining And, Union, and Filter subexpressions bottom up. The
lemma guarantees that these folding operations do not increase the complexity class,
so it follows that the Evaluation problem falls in ΣP
n+2 for the whole expression.
Cases (3) Q := P1 Union P2 and (4) Q := P1 Filter R are analogical to (2).2
201

Appendix B. Proofs of Complexity Results
202

Appendix C.
Proofs of Algebraic Optimization
Results
C.1. Proof of Lemma 4.2
Proof of Lemma 4.2(1): Trivial (by counterexample).
Proof of Lemma 4.2(2): We provide counterexamples that rule out distributivity
of operators
1 and \ over ∪, designed for the ﬁxed database D := {(0, c, 1)}:
• The equivalence A1 \ (A2 ∪A3) ≡(A1 \ A2) ∪(A1 \ A3) does not hold, as
witnessed by A1 := J(0, c, ?x)KD, A2 := J(?x, c, 1)KD, and A3 := J(0, c, ?y)KD.
• The equivalence A1
1 (A2 ∪A3) ≡(A1
1 A2)∪(A1
1 A3) does not hold, as
witnessed by A1 := J(0, c, ?x)KD, A2 := J(?x, c, 1)KD, and A3 := J(0, c, ?y)KD.
Proof of Lemma 4.2(3): We provide counterexamples for all operator constella-
tions that are listed in the lemma. Again, the counterexamples are designed for the
database D := {(0, c, 1)}. We start with invalid distributivity rules over operator 1:
• The equivalence A1 ∪(A2 1 A3) ≡(A1 ∪A2) 1 (A1 ∪A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(0, c, ?y)KD.
• The equivalence (A1 1 A2) ∪A3 ≡(A1 ∪A3) 1 (A2 ∪A3) does not hold (the
counterexample is symmetrical to the previous one).
• The equivalence A1 \ (A2 1 A3) ≡(A1 \ A2) 1 (A1 \ A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(0, c, ?y)KD.
• The equivalence (A1 1 A2) \ A3 ≡(A1 \ A3) 1 (A2 \ A3) does not hold, as
witnessed by A1 := J(0, c, ?x)KD, A2 := J(0, c, ?y)KD, and A3 := J(?x, c, 1)KD.
• The equivalence A1
1 (A2 1 A3) ≡(A1
1 A2) 1 (A1
1 A3) does not hold,
as witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(0, c, ?x)KD.
• The equivalence (A1 1 A2)
1 A3 ≡(A1
1 A3) 1 (A2
1 A3) does not hold,
as witnessed by A1 := J(0, c, ?x)KD, A2 := J(0, c, ?y)KD, and A3 := J(?x, c, 1)KD.
Next, we provide counterexamples for distributivity rules over \:
203

Appendix C. Proofs of Algebraic Optimization Results
• The equivalence A1 ∪(A2 \ A3) ≡(A1 ∪A2) \ (A1 ∪A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(0, c, ?x)KD, and A3 := J(?x, c, 1)KD.
• The equivalence (A1 \ A2) ∪A3 ≡(A1 ∪A3) \ (A2 ∪A3) does not hold (the
counterexample is symmetrical to the previous one).
• The equivalence A1 1 (A2 \ A3) ≡(A1 1 A2) \ (A1 1 A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(0, c, ?x)KD.
• The equivalence (A1 \ A2) 1 A3 ≡(A1 1 A3) \ (A2 1 A3) does not hold (the
counterexample is symmetrical to the previous one).
• The equivalence A1
1 (A2 \ A3) ≡(A1
1 A2) \ (A1
1 A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(?y, c, 1)KD.
• The equivalence (A1 \ A2)
1 A3 ≡(A1
1 A3) \ (A2
1 A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(0, c, ?y)KD.
Finally, we provide counterexamples for invalid distributivity rules over
1:
• The equivalence A1 ∪(A2
1 A3) ≡(A1 ∪A2)
1 (A1 ∪A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(c, c, c)KD, and A3 := J(?y, c, 1)KD.
• The equivalence (A1
1 A2) ∪A3 ≡(A1 ∪A3)
1 (A2 ∪A3) does not hold (the
counterexample is symmetrical to the previous one).
• The equivalence A1 1 (A2
1 A3) ≡(A1 1 A2)
1 (A1 1 A3) does not hold,
as witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(0, c, ?x)KD.
• The equivalence (A1
1 A2) 1 A3 ≡(A1 1 A3)
1 (A2 1 A3) does not hold
(the counterexample is symmetrical to the previous one).
• The equivalence A1 \ (A2
1 A3) ≡(A1 \ A2)
1 (A1 \ A3) does not hold, as
witnessed by A1 = J(?x, c, 1)KD, A2 = J(?y, c, 1)KD, and A3J(0, c, ?x)KD.
• The equivalence (A1
1 A2) \ A3 ≡(A1 \ A3)
1 (A2 \ A3) does not hold, as
witnessed by A1 := J(?x, c, 1)KD, A2 := J(?y, c, 1)KD, and A3 := J(0, c, ?y)KD.
The list of counterexamples is exhaustive.2
C.2. Proof of the Equivalences in Figure 4.3
We introduce some notation. Given a mapping µ and variable set S ⊆V , we deﬁne
the mapping µ|S as the mapping obtained when projecting S in µ. To give an
example, {?x 7→1, ?y 7→2}|{?x} = {?x 7→1}. Further, given two mappings µ1,
µ2 and a variable ?x we say that µ1 and µ2 agree on ?x iﬀeither it holds that
?x ∈dom(µ1) ∩dom(µ2) ∧µ1(?x) = µ2(?x) or ?x ̸∈dom(µ1) ∪dom(µ2).
(PBaseI). Follows from the deﬁnition of the projection operator (see Deﬁni-
tion 2.10) and the observation that pVars(A) extracts all variables that are po-
tentially bound in any result mapping, as stated in Proposition 4.1.
(PBaseII). For each set of variables S∗it holds that S = (S ∩S∗) ∪(S \ S∗),
so we can rewrite the left side of the equation as π(S∩pVars(A))∪(S\pVars(A))(A). This
204

C.2. Proof of the Equivalences in Figure 4.3
shows that, compared to the right side expression of the equation, the left side
projection diﬀers in that it additionally considers variables in S\pVars(A). However,
as stated in Proposition 4.1, for each mapping µ that is generated by A we have
that dom(µ) ⊆pVars(A), so S \ pVars(A) contains only variables that are unbound
in each result mapping and thus can be dropped without changing the semantics.
(PUPush). The equivalence follows easily from the deﬁnition of the projection and
the union operator (cf. Deﬁnition 2.10). We omit the details.
(PJPush). See Section 4.2.4.
(PMPush). ⇒: Let µ ∈πS(A1 \ A2). By semantics, µ is obtained from some
mapping µ1 ∈A1 that is incompatible with each mapping in A2, by project-
ing S, i.e. µ = µ1|S. We show that µ is also generated by the right side expression
πS(πS′(A1) \ πS′′(A2)). First observe that πS′(A1) generates a mapping µ′
1 ⊆µ1 that
agrees with µ1 on all variables in S and also on all variables in pVars(A1)∩pVars(A2),
because A1 generates µ1 and S′ := S ∪(pVars(A1)∩pVars(A2)). We distinguish two
cases. (a) Assume that µ′
1 is incompatible with each mapping generated by πS′′(A2).
Then πS′(A1)\πS′′(A2) generates µ′
1 and, going one step further, the whole expression
at the right side (i.e., including the outermost projection for S) generates µ′
1|S. We
know that µ′
1 agrees with µ1 on all variables in S, so µ′
1|S = µ1|S = µ and the right
side generates µ. (b) Assume there is a mapping µ′
2 ∈πS′′(A2) that is compatible
with µ′
1, i.e. for all ?x ∈dom(µ′
1)∩dom(µ′
2) : µ′
1(?x) = µ′
2(?x). From before we know
that µ1 ⊇µ′
1 and that µ1 agrees with µ′
1 on all variables in pVars(A1) ∩pVars(A2).
From µ′
2 ∈πS′′(A2) it follows that there is a mapping µ2 ∈A2 such that µ2 ⊇µ′
2
and µ2 agrees with µ′
2 on all variables in S′′ := pVars(A1) ∩pVars(A2). Taking
both observations together, we conclude that µ1 ∼µ2, because all shared variables
in-between µ1 and µ2 are contained in pVars(A1) ∩pVars(A2) and each of these
variables either maps to the same value in µ1 (µ2) and µ′
1 (µ′
2) or, alternatively, is
unbound in both. This is a contradiction to the initial claim that µ1 is incompatible
with each mapping in A2 and we conclude that assumption (b) was invalid.
⇐: Assume that µ′ ∈πS(πS′(A1) \ πS′′(A2)). We show that µ′ is also generated
by the left side of the equivalence. By semantics, µ′ is obtained from a mapping
µ′
1 ∈πS′(A1) by projecting S, i.e. µ′ = µ′
1|S. where µ′
1 is incompatible with each
mapping in πS′′(A2). First observe that the left side subexpression A1 generates a
mapping µ1 ⊇µ′
1 that agrees with µ′
1 on all variables in S′. From the observation
that µ′
1 is incompatible with each mapping in πS′′(A2) we conclude that also µ1 ⊇µ′
1
is incompatible with each mapping in A2 (which contains only mappings of the
form µ2 ⊇µ′
2 for some µ′
2 ∈πS′′(A2)). Hence, also the left side expression A1 \ A2
generates µ1. From µ1 ⊇µ′
1 and the observation that µ1 and µ′
1 agree on all variables
in S′ we conclude that µ1 and µ′
1 also agree on the variables in S ⊆S′. Consequently,
µ1|S = µ′
1|S = µ′ and it follows that the left side expression generates mapping µ′.
(PLPush). The following rewriting proves the claim, where we use the shortcuts
205

Appendix C. Proofs of Algebraic Optimization Results
S′ := S ∪(pVars(A1) ∩pVars(A2)) and S′′ := pVars(A1) ∩pVars(A2).
πS(A1
1 A2)
= πS((A1 1 A2) ∪(A1 \ A2))
[semantics]
= πS(A1 1 A2) ∪πS(A1 \ A2)
[(PUPush)]
= πS(πS′(A1) 1 πS′(A2)) ∪πS(πS′(A1) \ πS′′(A2))
[(PJPush),(PMPush)]
= πS(πS′(A1) 1 πS′(A2)) ∪πS(πS′(A1) \ πS′(A2))
[(∗)]
= πS((πS′(A1) 1 πS′(A2)) ∪(πS′(A1) \ πS′(A2)))
[(PUPush)]
= πS(πS′(A1)
1 πS′(A2))
[semantics]
Most interesting is step (∗), where we replace S′′ by S′ (all other steps are straight-
forward). This rewriting step is justiﬁed by the equivalence
πS(πS′(A1) \ πS′′(A2)) ≡πS(πS′(A1) \ πS′(A2)).
The idea behind this equivalence is the following. First note that S′ can be written
as S′ = S′′ ∪(S \ (pVars(A1) ∪pVars(A2)), which shows that S′ and S′′ diﬀer only
by variables that are contained in S but not in pVars(A1) ∩pVars(A2). Intuitively,
these variables are harmless because they cannot induce incompatibility between
the A1 and the A2 part on either side of the equivalence, since they occur at most
in one of both mapping sets.
(PFPush). Follows from the semantics of operator π and operator σ in Deﬁni-
tion 2.11. The crucial observation is that ﬁltering leaves mappings unchanged, and
if we do not project away variables that are required to evaluate the ﬁlter (which is
implicit by the equation), then the preprojection does not change the semantics.
(PMerge). The rule follows trivially from the deﬁnition of operator π.2
C.3. Proof of the Equivalences in Figure 4.4
(FDecompI). Follows from Lemma 1(1) in [PAG06a].
(FDecompII). Follows from Lemma 1(2) in [PAG06a].
(FReord). Follows from (FDecompI) and the commutativity of operator ∧.
(FBndI). Follows from Proposition 4.2.
(FBndII). Follows from Proposition 4.1.
(FBndIII). Follows from Proposition 4.2.
(FBndIV). Follows from Proposition 4.1.
(FUPush). Follows from Proposition 1(5) in [PAG06a].
206

C.4. Proof of Lemma 4.3
(FMPush). ⇒: Let µ ∈σR(A1 \ A2). By semantics, µ ∈A1, there is no µ2 ∈A2
compatible with µ1, and µ |= R. From these preconditions it follows immediately
that µ ∈σR(A1) \ A2. ⇐: Let µ ∈σR(A1) \ A2. Then µ ∈A1, µ |= R, and there is
no compatible mapping in A2. Clearly, then also µ ∈A1 \ A2 and µ ∈σR(A1 \ A2).
(FJPush). See Section 4.2.5.
(FLPush). We rewrite the expression schematically:
σR(A1
1 A2)
= σR((A1 1 A2) ∪(A1 \ A2))
[semantics]
= σR(A1 1 A2) ∪σR(A1 \ A2)
[(FUPush)]
= (σR(A1) 1 A2) ∪(σR(A1) \ A2)
[(FJPush),(FMPush)]
= σR(A1)
1 A2
[semantics]2
C.4. Proof of Lemma 4.3
(FElimI). We ﬁrst introduce three functions rem?x : M 7→M, add?x7→c : M 7→M,
and subst ?y
?x : M 7→M, which manipulate mappings as follows.
• rem?x(µ) removes ?x from µ (if it is bound), i.e. outputs mapping µ′ such that
dom(µ′) := dom(µ) \ {?x} and µ′(?y) := µ(?y) for all ?y ∈dom(µ′).
• add?x7→c(µ) binds variable ?x to c in µ, i.e. outputs mapping µ′ := µ∪{?x 7→c}
(we will apply this function only if ?x ̸∈dom(µ), so µ′ is deﬁned).
• subst ?y
?x(µ) := rem?x(add?y7→µ(?x)(µ)) replaces variable ?x by ?y in µ (we will
apply this function only if ?x ∈dom(µ) and ?y ̸∈dom(µ)).
We ﬁx document D. To prove that (FElimI) holds, we show that, for every ex-
pression A built using operators 1, ∪, and triple patterns JtKD (i.e., expressions as
deﬁned in rule (FElimI)) the following ﬁve claims hold (abusing notation, we write
µ ∈A if µ is contained in the result of evaluating expression A on document D).
(C1) If µ ∈A, dom(µ) ⊇{?x, ?y}, and µ(?x) = µ(?y) then rem?x(µ) ∈A ?y
?x.
(C2) If µ ∈A and ?x ̸∈dom(µ) then µ ∈A ?y
?x.
(C3) If µ ∈A and ?x ∈dom(µ), and ?y ̸∈dom(µ) then subst ?y
?x(µ) ∈A ?y
?x.
(C4) If µ ∈A ?y
?x and ?y ̸∈dom(µ) then µ ∈A.
(C5) If µ ∈A ?y
?x and ?y ∈dom(µ) then µ ∈A or add?x7→µ(?y)(µ) ∈A or subst ?x
?y (µ) ∈A.
Before proving that these conditions hold for every expression A build using only
operators 1, ∪, and triple patterns JtKD, we argue that the above ﬁve claims in com-
bination with the precondition ?x, ?y ∈cVars(A) stated in Lemma 4.3 imply equiv-
alence (FElimI). ⇒: Let µ ∈πS\{?x}(σ?x=?y(A)). From the semantics of operators π
207

Appendix C. Proofs of Algebraic Optimization Results
and σ it follows that µ is obtained from some µ′ ⊇µ s.t. µ′ ∈A, ?x, ?y ∈dom(µ′),
µ′(?x) = µ′(?y), and πS\{?x}({µ′}) = {µ}. Given all these prerequisites, condi-
tion (C1) implies that µ′′ := rem?x(µ′) is generated by A ?y
?x. Observe that mapping µ′′
agrees with µ′ on all variables but ?x. Hence, πS\{?x}({µ′′}) = πS\{?x}({µ′}) = {µ},
which shows that µ is generated by the right side expression πS\{?x}(A ?y
?x). ⇐: Con-
sider a mapping µ ∈πS\{?x}(A ?y
?x). Then there is some mapping µ′ ∈A ?y
?x such that
µ′ ⊇µ and πS\{?x}({µ′}) = {µ}. By assumption we have that ?x ∈cVars(A) and
it is easily veriﬁed that this implies ?y ∈cVars(A ?y
?x). Hence, variable ?y is bound
in µ′ (according to Proposition 4.2). Condition (C5) now implies that (i) µ′ ∈A,
or (ii) add?x7→µ′(?y)(µ′) ∈A, or (iii) subst ?x
?y (µ′) ∈A holds. Concerning case (i) ﬁrst
observe that ?x ̸∈dom(µ′), since all occurrences of ?x have been replaced by ?y
in A ?y
?x. On the other hand, we know that ?x ∈cVars(A) →?x ∈dom(µ′), so we
have a contradiction (i.e., assumption (i) was invalid). With similar argumentation,
we obtain a contradiction for case (iii), because ?y ∈cVars(A) →?y ∈dom(µ′′) for
all µ′′ ∈A, but obviously ?y ̸∈dom(subst ?x
?y (µ′)). Therefore, given that condition
(C5) is valid by assumption, we conclude that case (ii) µ′′ := add?x7→µ′(?y)(µ′) ∈A
must hold. Observe that µ′′(?x) = µ′′(?y) by construction and that µ′′ diﬀers from µ′
only by an additional binding for variable ?x. Hence, µ′′ passes the ﬁlter σ?x=?y in
the left side expression and from πS\{?x}({µ′′}) = πS\{?x}({µ′}) = {µ} we deduce
that the left side expression πS\{?x}(σ?x=?y(A)) generates µ.
Having shown that the ﬁve claims imply the equivalence, we now prove the claims
by structural induction (over expressions built using operators 1, ∪and triple pat-
terns of the form JtKD). We leave the basic case A := JtKD as an exercise to the
reader and assume that the induction hypothesis holds. In the induction step, we
distinguish two cases. (1) Let A := A1 1 A2. Consider a mapping µ ∈A. Then µ
is of the form µ = µ1 ∪µ2 where µ1 ∈A1 and µ2 ∈A2 are compatible mappings.
Observe that A ?y
?x = A1
?y
?x 1 A2
?y
?x. (1.1) To see why condition (C1) holds ﬁrst note
that by induction hypothesis conditions (C1)-(C3) hold for A1, A2. Further assume
that dom(µ) ⊇{?x, ?y}, and µ(?x) = µ(?y) (otherwise we are done). It is straight-
forward to verify that conditions (C1), (C2), and (C3) imply that A1
?y
?x 1 A2
?x
?y
generates rem?x(µ): the claim follows when distinguishing several cases, covering
the possible domains of µ1 and µ2, and applying the induction hypothesis; we omit
the details. (1.2) To prove condition (C2) let us assume that ?x ̸∈dom(µ). This
implies that ?x ̸∈dom(µ1) and ?x ̸∈dom(µ2), so µ1 and µ2 are also generated by
A1
?y
?x and A2
?y
?x (by induction hypothesis and claim (C2)). Hence, µ is generated by
A ?y
?x = A1
?y
?x 1 A2
?y
?x. (1.3) The proof that condition (C3) holds follows by applica-
tion of the induction hypothesis and conditions (C2), (C3). (1.4) Claim (C4) can
be shown by application of the induction hypothesis in combination with condi-
tion (C4). (1.5) Claim (C5) can be shown by application of the induction hypothesis
and conditions (C4), (C5). (2) Let A := A1∪A2 and consequently A ?y
?x = A1
?y
?x∪A2
?y
?x.
(2.1) Assume that µ ∈A, dom(µ) ⊇{?x, ?y}, and µ(?x) = µ(?y). Then µ is gener-
208

C.5. Proof of Proposition 4.4
ated by A1 or by A2. Let us w.l.o.g. assume that µ is generated by A1. By induction
hypothesis, rem?x(µ) is generated by A1
?y
?x, and consequently also by A ?y
?x. The proofs
for the remaining conditions (C2)-(C5) proceed analogously.
(FElimII). Similar in idea to (FElimI).2
C.5. Proof of Proposition 4.4
(MReord). We ﬁx a mapping µ and show that it is contained in the left side
expression if and only if it is contained in the right side expression. First observe
that if µ is not contained in A1, then it is neither contained in the right side nor
in the left side of the expressions (both are subsets of A1). So let us assume that
µ ∈A1. We distinguish three cases. Case (1): consider a mapping µ ∈A1 and assume
there is a compatible mapping in A2. Then µ is not contained in A1 \ A2, and also
not in (A1 \ A2) \ A3, which by deﬁnition is a subset of the former. Now consider
the right-hand side of the equation and let us assume that µ ∈A1 \ A3 (otherwise
we are done). Then, given that there is a mapping in A2 that is compatible to µ,
the expression (A1 \ A3) \ A2 will not contain µ. Case (2): The case of µ ∈A1 being
compatible with any mapping from A3 is symmetrical to (2). Case (3): Let µ ∈A1
be a mapping that is not compatible with any mapping in A2 and A3. Then both
(A1 \ A2) \ A3 on the left side and (A1 \ A3) \ A2 on the right side contain µ. In all
cases, µ is contained in the right side iﬀit is contained in the left side.
(MMUCorr). We show both directions of the equivalence. ⇒: Let µ ∈(A1\A2)\A3.
Then µ ∈A1 and there is neither a compatible mapping µ2 ∈A2 nor a compatible
mapping µ3 ∈A3. Then both A2 and A3 contain only incompatible mappings,
and clearly A2 ∪A3 contains only incompatible mappings. Hence, the right side
A1 \ (A2 ∪A3) produces µ. ⇐: Let µ ∈A1 \ (A2 ∪A3). Then µ ∈A1 and there is
no compatible mapping in A2 ∪A2, which means that there is neither a compatible
mapping in A2 nor in A3. It follows that A1\A2 generates µ (as there is no compatible
mapping in A2 and µ ∈A1) and, going one step further, from the fact that there is
no compatible mapping in A3 we deduce that µ ∈(A1 \ A2) \ A3.
(MJ). See Lemma 3(2) in [PAG06a].
(f
LJ). Let f
A1, f
A2 be eA-expressions. The following sequence of rewriting steps
proves the equivalence.
f
A1
1 f
A2
= (f
A1 1 f
A2) ∪(f
A1 \ f
A2)
[by semantics]
= (f
A1 1 (f
A1 1 f
A2)) ∪(f
A1 \ (f
A1 1 f
A2))
[( ^
JIdem),(JAss),(MJ)]
= (f
A1
1 (f
A1 1 f
A2))
[by semantics] 2
209

Appendix C. Proofs of Algebraic Optimization Results
C.6. Proof of Lemma 4.4
(FLBndI). See Section 4.2.6.
(FLBndII). First recall that by assumption ?x ∈cVars(A2) \ pVars(A1), which
implies that ?x ̸∈pVars(A1 \ A2) and ?x ∈cVars(A1 1 A2). The following step-by-
step rewriting proves the equivalence.
σbnd(?x)(A1
1 A2)
= σbnd(?x)((A1 1 A2) ∪(A1 \ A2))
[semantics]
= σbnd(?x)(A1 1 A2) ∪σbnd(?x)(A1 \ A2)
[(FUPush)]
= σbnd(?x)(A1 1 A2) ∪∅
[(FBndII)]
= σbnd(?x)(A1 1 A2)
[semantics]
= A1 1 A2
[(FBndI)]2
C.7. Proof of Lemma 4.9
Recall that by Lemma 4.5 the result of evaluating set and bag algebra expres-
sions diﬀers at most in the associated cardinality, so (given that the rules hold for
SPARQL set algebra) in all cases it suﬃces to show that, for a ﬁxed mapping µ that
is contained in (by assumption both) the left and right side of the equivalence, the
associated left and right side cardinalities for the mapping coincide. We ﬁx docu-
ment D. Further, given a SPARQL bag algebra expression A+
i with some index i,
we denote by (Ωi, mi) the mapping multi-set obtained when evaluating A+
i on D.
In subsequent proofs we exploit the following well-known rewriting rules for sums.
Proposition C.1 (Sum Rewriting Rules, Folklore) Let ax, bx, denote expres-
sions that depend on some x, λ be an expression that does not depend on x, and
let Cx be a condition that depends on x. The following rewritings are valid.
(S1) P
x∈X λ ∗ax = λ ∗P
x∈X ax,
(S2) P
x∈{x∗∈X|Cx∗}
P
y∈{y∗∈Y |Cy∗} ax ∗by = P
(x,y)∈{(x∗,y∗)∈(X,Y )|Cx∗∧Cy∗} ax ∗by,
(S3) P
x∈X ax + bx = P
x∈X ax + P
x∈X bx.
2
In the following, we shall refer to these equivalences as (S1), (S2), and (S3).
(UAss+). Let A+
1 , A+
2 , A+
3 ∈A+. We deﬁne expressions A+
l := (A+
1 ∪A+
2 ) ∪A+
3
and A+
r := A+
1 ∪(A+
2 ∪A+
3 ). Consider a mapping µ that is contained both in the
result of evaluating A+
l and A+
r on D. We apply the semantics of operator ∪for
multi-set expressions and rewrite the multiplicity that is associated with µ for A+
l
step-by-step: ml(µ) = (m1(µ)+m2(µ))+m3(µ) = m1(µ)+(m2(µ)+m3(µ)) = mr(µ).
210

C.7. Proof of Lemma 4.9
(JAss+). Let A+
1 , A+
2 , A+
3 ∈A+. We deﬁne the shortcuts A+
l := (A+
1 1 A+
2 ) 1 A+
3 ,
A+
r := A+
1 1 (A+
2 1 A+
3 ), A+
112 := A+
1 1 A+
2 , and A+
213 := A+
2 1 A+
3 . Consider a
mapping µ that is contained in both the result of evaluating A+
l and A+
r . We apply
the semantics from Deﬁnition 2.14 and rewrite the left side multiplicity ml(µ):
ml(µ) = P
(µ112,µ3)∈{(µ∗
112,µ∗
3)∈Ω112×Ω3|µ∗
112∪µ∗
3=µ}(m112(µ112) ∗m3(µ3))
= P
(µ112,µ3)∈{(µ∗
112,µ∗
3)∈Ω112×Ω3|µ∗
112∪µ∗
3=µ}(
(P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|µ∗
1∪µ∗
2=µ112}(m1(µ1) ∗m2(µ2))) ∗m3(µ3))
(S1)
= P
(µ112,µ3)∈{(µ∗
112,µ∗
3)∈Ω112×Ω3|µ∗
112∪µ∗
3=µ}(
P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|µ∗
1∪µ∗
2=µ112}(m1(µ1) ∗m2(µ2) ∗m3(µ3)))
(S2)
= P
((µ112,µ3),(µ1,µ2))∈{((µ∗
112,µ∗
3),(µ∗
1,µ∗
2))∈(Ω112×Ω3)×(Ω1×Ω2)|µ∗
112∪µ∗
3=µ∧µ∗
1∪µ∗
2=µ∗
112}
(m1(µ1) ∗m2(µ2) ∗m3(µ3))
= P
(µ1,µ2,µ3)∈{(µ∗
1,µ∗
2,µ∗
3)∈Ω1×Ω2×Ω3|µ∗
1∪µ∗
2∪µ∗
3=µ}(m1(µ1) ∗m2(µ2) ∗m3(µ3))
= P
((µ1,µ213),(µ2,µ3))∈{((µ∗
1,µ∗
213),(µ∗
2,µ∗
3))∈(Ω1×Ω213)×(Ω2×Ω3)|µ∗
1∪µ∗
213=µ∧µ∗
2∪µ∗
3=µ∗
213}
(m1(µ1) ∗m2(µ2) ∗m3(µ3))
(S2)
= P
(µ1,µ213)∈{(µ∗
1,µ∗
213)∈Ω1×Ω213|µ∗
1∪µ∗
213=µ}(
P
(µ2,µ3)∈{(µ∗
2,µ∗
3)∈Ω2×Ω3|µ∗
2∪µ∗
3=µ213}(m1(µ1) ∗m2(µ2) ∗m3(µ3)))
(S1)
= P
(µ1,µ213)∈{(µ∗
1,µ∗
213)∈Ω1×Ω213|µ∗
1∪µ∗
213=µ}(
m1(µ1) ∗P
(µ2,µ3)∈{(µ∗
2,µ∗
3)∈Ω2×Ω3|µ∗
2∪µ∗
3=µ213}(m2(µ2) ∗m3(µ3)))
= P
(µ1,µ213)∈{(µ∗
1,µ∗
213)∈Ω1×Ω213|µ∗
1∪µ∗
213=µ}(m1(µ1) ∗m213(µ213))
= µr(µ)
(UComm+). Let A+
1 , A+
2 ∈A+. Put A+
l := A+
1 ∪A+
2 and A+
r := A+
2 ∪A+
1 . Consider
a mapping µ that is contained in both the result of evaluating A+
l and A+
r . We apply
the semantics of operator ∪(cf. Deﬁnition 2.14) and rewrite the left side multiplicity
step-by-step: ml(µ) = m1(µ) + m2(µ) = m2(µ) + m1(µ) = mr(µ).
(JComm+). Let A+
1 , A+
2 ∈A+. Put A+
l := A+
1 1 A+
2 , A+
r := A+
2 1 A+
1 . Consider a
mapping µ that is contained in both the result of evaluating A+
l and A+
r . Applying
the semantics of operator 1 (cf. Deﬁnition 2.14) we rewrite the left side multiplicity:
ml(µ) = P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|µ∗
1∪µ∗
2=µ}(m1(µ1) ∗m2(µ2))
= P
(µ2,µ1)∈{(µ∗
2,µ∗
1)∈Ω2×Ω1|µ∗
2∪µ∗
1=µ}(m2(µ2) ∗m1(µ1))
= mr(µ)
(JUDistR+). See Section 4.3.2.
(JUDistL+). Symmetrical to the proof of (JUDistR+).
(MUDistR+). Let A+
1 , A+
2 , A+
3 ∈A+. We deﬁne expressions A+
l := (A+
1 ∪A+
2 )\A+
3 ,
A+
r := (A+
1 \A+
3 )∪(A+
2 \A+
3 ), A+
1∪2 := A+
1 ∪A+
2 , A+
1\3 := A+
1 \A+
3 , and A+
2\3 := A+
2 \A+
3 .
211

Appendix C. Proofs of Algebraic Optimization Results
Consider a mapping µ that is contained in the result of evaluating A+
l and A+
r . It is
easily veriﬁed that ml(µ) = m1∪2(µ) = m1(µ)+m2(µ) = m1\3(µ)+m2\3(µ) = mr(µ).
(LUDistR+). Let A+
1 , A+
2 , A+
3 ∈A+. The following rewriting proves the claim.
(A+
1 ∪A+
2 )
1 A+
3
= ((A+
1 ∪A+
2 ) 1 A+
3 ) ∪((A+
1 ∪A+
2 ) \ A+
3 )
[semantics]
= ((A+
1 1 A+
3 ) ∪(A+
2 1 A+
3 )) ∪((A+
1 ∪A+
2 ) \ A+
3 )
[(JUDistR+)]
= ((A+
1 1 A+
3 ) ∪(A+
2 1 A+
3 )) ∪((A+
1 \ A+
3 ) ∪(A+
2 \ A+
3 )) [(MUDistR+)]
= ((A+
1 1 A+
3 ) ∪(A+
1 \ A+
3 )) ∪((A+
2 1 A+
3 ) ∪(A+
2 \ A+
3 )) [(UAss+), (UComm+)]
= (A+
1
1 A+
3 ) ∪(A+
2
1 A+
3 )
[semantics]2
C.8. Proof of Lemma 4.10
As before in the proof of Lemma C.7 we exploit the assumption that by Lemma 4.5
the results of evaluating set and bag algebra expressions diﬀers at most in the as-
sociated cardinality, so (given that the rules hold for SPARQL set algebra) in all
cases it suﬃces to show that, for a ﬁxed mapping µ that is contained in the left
and right side of the equivalence, the associated left and right side cardinalities for
the mapping coincide. We ﬁx a document D. Further, given a SPARQL bag algebra
expression A+, we denote by (Ω, m) the mapping multi-set obtained when evaluat-
ing A+. Similarly, for a bag algebra expression A+
i with some index i, we denote by
(Ωi, mi) the mapping multi-set obtained when evaluating A+
i . We shall again use
the rewritings for sum expressions from Proposition C.1 (see Appendix C.7), which
we denote by (S1), (S2), and (S3), respectively.
(PBaseI+). Let A+ ∈A+ and S ⊂V . Consider a mapping µ that is contained
in the result of evaluating A+
l
:= πpVars(A+)∪S(A+) and A+
r := A+. We apply the
semantics from Deﬁnition 2.14 and rewrite the multiplicity ml(µ) step-by-step:
ml(µ) = P
µ+∈{µ∗
+∈Ω|πpVars(A+)∪S({µ∗
+})={µ}} m(µ+)
(∗)
= P
µ+∈{µ} m(µ+)
= m(µ)
= mr(µ),
where step (∗) follows from the observation that πpVars(A+)∪S({µ∗
+}) = {µ} holds
if and only if µ∗
+ = µ (this claim follows easily from Proposition 4.1, the deﬁnition
of operator π, and the fact that µ ∈Ωr = Ωby assumption).
(PBaseII+). Let A+ ∈A+ and S ⊂V . Consider a mapping µ that is contained in
the result of evaluating A+
l := πS(A+) and A+
r := πS∩pVars(A+)(A+). We apply the
semantics from Deﬁnition 2.14 and rewrite the (right side) multiplicity mr(µ):
212

C.8. Proof of Lemma 4.10
mr(µ)= P
µ+∈{µ∗
+∈Ω|πS∩pVars(A+)({µ∗
+})={µ}} m(µ+)
(∗)
= P
µ+∈{µ∗
+∈Ω|πS({µ∗
+})={µ}} m(µ+)
= ml(µ),
where step (∗) follows by semantics and Proposition 4.1.
(PUPush+). Let A+
1 , A+
2 ∈A+ and S ⊂V . Consider a mapping µ that is contained
in the result of evaluating A+
l := πS(A+
1 ∪A+
2 ) and A+
r := πS(A+
1 ) ∪πS(A+
2 ). Put
A+
1∪2 := A+
1 ∪A+
2 , A+
π1 := πS(A+
1 ), and A+
π2 := πS(A+
2 ). We apply the semantics from
Deﬁnition 2.14 and rewrite the multiplicity ml(µ) step-by-step:
ml(µ) = P
µ+∈{µ∗
+∈Ω1∪2|πS({µ∗
+})={µ}} m1∪2(µ+)
= P
µ+∈{µ∗
+∈Ω1∪2|πS({µ∗
+})={µ}}(m1(µ+) + m2(µ+))
(S3)
= (P
µ+∈{µ∗
+∈Ω1∪2|πS({µ∗
+})={µ}} m1(µ+))+
(P
µ+∈{µ∗
+∈Ω1∪2|πS({µ∗
+})={µ}} m2(µ+))
(∗)
= (P
µ+∈{µ∗
+∈Ω1|πS({µ∗
+})={µ}} m1(µ+)) + (P
µ+∈{µ∗
+∈Ω2|πS({µ∗
+})={µ}} m2(µ+))
= mπ1(µ) + mπ2(µ)
= mr(µ),
where step (∗) follows by semantics of operator ∪.
(PJPush+). To facilitate the proof, we establish the following proposition:
Proposition C.2 Let A+
1 , A+
2 ∈A+ and S′ ⊂V with S′ ⊇pVars(A+
1 )∩pVars(A+
2 ).
Then the following equivalence holds.
πS′(A+
1 1 A+
2 ) ≡πS′(A+
1 ) 1 πS′(A+
2 )
(FJPush2 +)
2
To see why Proposition C.2 implies (FJPush), consider the latter equivalence,
recall that S′ := S ∪(pVars(A+
1 )∩pVars(A+
2 )), and observe that by construction we
have S′ ⊇pVars(A+
1 ) ∩pVars(A+
2 ). We rewrite the left side of (FJPush):
πS(A+
1 1 A+
2 ) = πS(πS′(A+
1 1 A+
2 ))
[(PMerge+)]
= πS(πS′(A+
1 ) 1 πS′(A+
2 ))
[(FJPush2+)]
Given this rewriting, it remains to show that (FJPush2+) is valid. We split this
proof into two parts. First, we show that the mapping sets coincide. To this end, we
show that (FJPush2+) holds for SPARQL set algebra (the result carries over to bag
algebra by Lemma 4.5). Let A1, A2 ∈A and S′ ⊇pVars(A1) ∩pVars(A2).
⇒: Consider a mapping µ generated by the left side expression πS′(A1 1 A2).
Then µ is obtained from some mapping µ′ ⊇µ s.t. πS′({µ′}) = {µ}. Further, µ′ is
of the form µ′
1 ∪µ′
2 where µ′
1 and µ′
2 are compatible mappings that are generated by
213

Appendix C. Proofs of Algebraic Optimization Results
A1 and A2, respectively. We observe that the right side subexpressions πS′(A1) and
πS′(A2) then generate mappings µ′′
1 ⊆µ′
1 and µ′′
2 ⊆µ′
2 that agree with µ′
1 and µ′
2
on all variables in S′, respectively (where “agree” means that each such variable is
either bound to the same value in the two mappings or unbound in both mappings).
Clearly, µ′′
1 ⊆µ′
1∧µ′′
2 ⊆µ′
2∧µ′
1 ∼µ′
2 →µ′′
1 ∼µ′′
2, so the right side expression generates
the mapping µ′′ := µ′′
1 ∪µ′′
2. It is easily veriﬁed that (i) dom(µ′′) ⊆S′ and that (ii) µ′′
agrees with µ′ on all variables in S′. This implies that µ′′ = µ and we conclude that µ
is generated by the right side expression. ⇐: Consider a mapping µ′ that is generated
by the right side expression πS′(A1) 1 πS′(A2). Then µ′ is of the form µ′ = µ′
1 ∪µ′
2,
where µ′
1 ∼µ′
2 are generated by the subexpressions πS′(A1) and πS′(A2), respectively.
Consequently, A1 and A2 generate mappings µ1 ⊇µ′
1 and µ2 ⊇µ′
2 such that µ1 and
µ2 agree with µ′
1 and µ′
2 on all variables in S′, respectively. We distinguish two cases.
First, (i) if µ1 and µ2 are compatible then µ := µ1∪µ2 agrees with µ′ on all variables
in S′, and therefore πS′({µ}) = µ′, so the left side expression generates µ′. Second,
(ii) if µ1 and µ2 are not compatible then there is ?x ∈dom(µ1) ∩dom(µ2) such that
µ1(?x) ̸= µ2(?x). From precondition S′ ⊇pVars(A1)∩pVars(A2) and Proposition 4.1
it follows that ?x ∈S′. We know that µ′
1 and µ′
2 agree with µ1 and µ2 on all variables
in S′. It follows that µ′
1(?x) ̸= µ′
2(?x), which contradicts the assumption µ′
1 ∼µ′
2.
Having shown that the mapping sets coincide, it remains to show that the left
and right side multiplicities under bag semantics agree for each result mapping. We
therefore switch to SPARQL bag algebra again. Let A+
1 , A+
2 ∈A+ and S′ ⊂V
such that S′ ⊇pVars(A1) ∩pVars(A2) holds. We deﬁne A+
l
:= πS′(A+
1 1 A+
2 ),
A+
r := πS′(A+
1 ) 1 πS′(A+
2 ), A+
112 := A+
1 1 A+
2 , A+
π1 := πS′(A+
1 ), and A+
π2 := πS′(A+
2 ).
Using the notation introduced in the beginning of Appendix C.2, we write µ|S for the
mapping obtained when projecting S in µ (e.g. {?x 7→1, ?y 7→2}|{?x} = {?x 7→1}).
Applying the semantics from Deﬁnition 2.14 we rewrite ml(µ) schematically:
ml(µ) = P
µ+∈{µ∗
+∈Ω112|πS′({µ∗
+})={µ}} m112(µ+)
= P
µ+∈{µ∗
+∈Ω112|πS′({µ∗
+})={µ}}
P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|µ∗
1∪µ∗
2=µ+}(m1(µ1) ∗m2(µ2))
(S2)
= P
(µ+,(µ1,µ2))∈{(µ∗
+,(µ∗
1,µ∗
2))∈Ω112×(Ω1×Ω2)|πS′({µ∗
+})={µ}∧µ∗
1∪µ∗
2=µ∗
+}
(m1(µ1) ∗m2(µ2))
= P
(µ+,(µ1,µ2))∈{(µ∗
+,(µ∗
1,µ∗
2))∈Ω112×(Ω1×Ω2)|πS′({µ∗
1∪µ∗
2})={µ}∧µ∗
1∪µ∗
2=µ∗
+}
(m1(µ1) ∗m2(µ2))
= P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|πS′({µ∗
1∪µ∗
2})={µ}}(m1(µ1) ∗m2(µ2))
= P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|(µ∗
1∪µ∗
2)|S′=µ}(m1(µ1) ∗m2(µ2))
(∗)
= P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ω1×Ω2|µ∗
1|S′∪µ∗
2|S′=µ}(m1(µ1) ∗m2(µ2))
= P
(µ1,µ2)∈{(µ∗
1,µ∗
2)∈Ωπ1×Ωπ2|µ∗
1∪µ∗
2=µ}(mπ1(µ1) ∗mπ2(µ2))
= mr(µ),
where step (∗) follows from the precondition S′ ⊇pVars(A1) ∩pVars(A2).
214

C.8. Proof of Lemma 4.10
(PMPush+). Let A+
1 , A+
2 ∈A+ and S ⊂V be a set of variables. Further recall that
by deﬁnition S′ := S ∪(pVars(A1) ∩pVars(A2)) and S′′ := pVars(A1) ∩pVars(A2).
We deﬁne A+
l
:= πS(A+
1 \ A+
2 ), A+
r := πS(πS′(A+
1 ) \ πS′′(A+
2 )), A+
1\2 := A+
1 \ A+
2 ,
A+
π1 := πS′(A+
1 ), A+
π2 := πS′′(A+
2 ), A+
π1\π2 := A+
π1 \ A+
π2, and ﬁx document D and
a mapping µ that is contained both in Ωl and Ωr. Applying the semantics from
Deﬁnition 2.14, we rewrite the (right side) multiplicity mr(µ) schematically:
mr(µ)= P
µ+∈{µ∗
+∈Ωπ1\π2|πS({µ∗
+})={µ}} mπ1\π2(µ+)
(∗1)
= P
µ+∈{µ∗
+∈Ωπ1\π2|πS({µ∗
+})={µ}} mπ1(µ+)
= P
µ+∈{µ∗
+∈Ωπ1\π2|πS({µ∗
+})={µ}}
P
µ′
+∈{µ•
+∈Ω1|πS′({µ•
+})={µ+}} m1(µ′
+)
(S2)
= P
(µ+,µ′
+)∈{(µ∗
+,µ•
+)∈Ωπ1\π2×Ω1|πS({µ∗
+})={µ}∧πS′({µ•
+})={µ∗
+}} m1(µ′
+)
= P
(µ+,µ′
+)∈{(µ∗
+,µ•
+)∈Ωπ1\π2×Ω1|πS(πS′({µ•
+}))={µ}∧πS′({µ•
+})={µ∗
+}} m1(µ′
+)
(∗2)
= P
(µ+,µ′
+)∈{(µ∗
+,µ•
+)∈Ωπ1\π2×Ω1|πS({µ•
+})={µ}∧πS′({µ•
+})={µ∗
+}} m1(µ′
+)
(∗3)
= P
(µ+,µ′
+)∈{(µ∗
+,µ•
+)∈Ωπ1\π2×Ω1\2|πS({µ•
+})={µ}∧πS′({µ•
+})={µ∗
+}} m1(µ′
+)
(∗4)
= P
µ′
+∈{µ•
+∈Ω1\2|πS({µ•
+})={µ}} m1(µ′
+)
(∗5)
= P
µ′
+∈{µ•
+∈Ω1\2|πS({µ•
+})={µ}} m1\2(µ′
+)
= ml(µ),
where step (∗1) follows from the observation that mπ1\π2(µ∗
+) = mπ1(µ∗
+) for all
µ∗
+ ∈Ωπ1\π2, rewriting step (∗2) holds because S ⊆S′, step (∗3) follows from the
observation that only those mappings from Ω1 contribute to the result that are
also contained in Ω1\2, step (∗4) holds because every mapping µ•
+ ∈Ω1\2 uniquely
determines a mapping µ∗
+ ∈Ωπ1\π2 through condition πS′({µ•
+}) = µ∗
+, and step
(∗5) follows from the observation that m1(µ•
+) = m1\2(µ•
+) for all µ•
+ ∈Ω1\2.
(PLPush+). Analogical to the proof of (PLPush) for SPARQL set algebra from
Appendix C.2 (observe that all rules that are used in the latter proof are also valid
in the context of SPARQL bag algebra).
(PFPush+). Similar in idea to (PMPush+).
(PMerge+). Let A+ ∈A+ and S1, S2 ⊂V . Deﬁne expressions A+
l := πS1(πS2(A+)),
A+
r := πS1∩S2(A+), and A+
π2 := πS2(A+). According to Lemma 4.5, it suﬃces to show
that for each mapping µ that is contained in Ωl and Ωr it holds that ml(µ) = mr(µ).
We rewrite the left side multiplicity ml(µ) schematically:
ml(µ) = P
µ+∈{µ∗
+∈Ωπ2|πS1({µ∗
+})={µ}} mπ2(µ+)
= P
µ+∈{µ∗
+∈Ωπ2|πS1({µ∗
+})={µ}}
P
µ′
+∈{µ•
+∈Ω|πS2({µ•
+})={µ+}} m(µ′
+)
(S2)
= P
(µ+,µ′
+)∈{(µ∗
+,µ•
+)∈Ωπ2×Ω|πS1({µ∗
+})={µ}∧πS2({µ•
+})={µ∗
+}} m(µ′
+)
= P
(µ+,µ′
+)∈{(µ∗
+,µ•
+)∈Ωπ2×Ω|πS1(πS2({µ•
+}))={µ}∧πS2({µ•
+})={µ∗
+}} m(µ′
+)
215

Appendix C. Proofs of Algebraic Optimization Results
(∗1)
= P
µ′
+∈{µ•
+∈Ω|πS1(πS2({µ•
+}))={µ}} m(µ′
+)
(∗2)
= P
µ′
+∈{µ•
+∈Ω|πS1∩S2({µ•
+})={µ}} m(µ′
+)
= mr(µ),
where step (∗1) follows from the observation that mapping µ∗
+ is uniquely deter-
mined by µ•
+ and (∗2) follows directly from the semantics of operator π.2
C.9. Proof of Lemma 4.11
As before in the proof of Lemma 4.10 in Appendix C.8 we exploit the assumption
that by Lemma 4.5 the results of evaluating set and bag algebra expressions diﬀer
at most in the associated cardinality, so (given that the rules hold for SPARQL
set algebra) in all cases it suﬃces to show that, for a ﬁxed mapping µ that is
contained in the left and right side of the equivalence, the associated left and right
side cardinalities for the mapping coincide. We ﬁx a document D. Further, given a
SPARQL bag algebra expression A+, we denote by (Ω, m) the mapping multi-set
obtained when evaluating A+. Similarly, for a bag algebra expression A+
i with some
index i, we denote by (Ωi, mi) the mapping multi-set obtained when evaluating A+
i .
(FDecompI+). Let A+ ∈A+ and R be a ﬁlter condition. Put A+
l := σR1∧R2(A+),
A+
r := σR1(σR2(A+)), and A+
σ2 := σR2(A+). Consider a mapping µ that is contained
in the result of evaluating A+
l
and A+
r , which implies that µ ∈Ωand µ |= R1,
µ |= R2, µ |= R1 ∧R2. Applying the semantics from Deﬁnition 2.14 we can easily
derive that ml(µ) = m(µ) = mσ2(µ) = mr(µ).
(FReord+). Follows from equivalence (FDecompI+) for SPARQL bag algebra and
the commutativity of the boolean operator ∧.
(FBndI+) - (FBndIV+). Follow from the semantics of σ in Deﬁnition 2.10.
(FUPush+). Follows from the semantics of σ and ∪in Deﬁnition 2.10.
(FMPush+). Let A+
1 , A+
2 ∈A+ and R be a ﬁlter condition. Put A+
l := σR(A+
1 \A+
2 ),
A+
r := σR(A+
1 ) \ A+
2 , A+
1\2 := A+
1 \ A+
2 , and A+
σ1 := σR(A+
1 ). Consider a mapping µ
that is contained in the result of evaluating A+
l and A+
r . This implies that µ |= R,
µ ∈Ω1\2, µ ∈Ω1, and µ ∈Ωσ1. Combining the semantics from Deﬁnition 2.14 with
the above observations we obtain ml(µ) = m1\2(µ) = m1(µ) = mσ1(µ) = mr(µ).
(FJPush+). See Section 4.3.2.
(FLPush+). Analogical to the proof of (FLPush) for SPARQL set algebra from
Appendix C.3 (observe that all rules that are used in the latter proof are also valid
in the context of SPARQL bag algebra).2
216

C.10. Proof of Lemma 4.13
C.10. Proof of Lemma 4.13
(MReord+), (MMUCorr+), (MJ+). The three equivalences follow easily from the
semantics of operator \ from Deﬁnition 2.14.
(f
LJ
+). Analogical to the proof of (f
LJ) for SPARQL set algebra from Appendix C.5
(observe that all rules that are used in the latter proof are also valid in the context
of SPARQL bag algebra).
(FLBndI+), (FLBndII+). Analogical to the proof of (FLBndI) and (FLBndII) for
SPARQL set algebra from Lemma 4.4 (observe that all rules that are used in the
proof of the latter lemma are also valid in the context of SPARQL bag algebra).2
217

