Ac
hiev
emen
ts
and
Prosp
ects
of
Learning
W
ord
Morphology
with
Inductiv
e
Logic
Programming
Dimitar
Kazak
o
v
Univ
ersit
y
of
Y
ork,
Heslington,
Y
ork
YO0
DD,
UK,
kazakov@cs.york.ac.uk,
WWW
home
page:
http://www.cs.york.ac.uk/kazako
v/
Abstract.
This
article
presen
ts
an
o
v
erview
of
existing
ILP
and
non-
ILP
approac
hes
to
w
ord
morphology
learning,
and
sets
targets
for
future
researc
h.
The
article
claims
that
new
c
hallenges
to
the
ILP
comm
unit
y
with
more
app
eal
to
computational
linguists
should
b
e
sough
t
in
a
whole
new
range
of
unexplored
learning
tasks
in
whic
h
ILP
w
ould
ha
v
e
to
mak
e
a
more
extensiv
e
use
of
relev
an
t
linguistic
kno
wledge,
and
b
e
more
closely
in
tegrated
with
other
learning
tec
hniques
for
data
prepro
cessing.

In
tro
duction
Matthews
(	)
denes
morphology
as
\that
branc
h
of
linguistics
whic
h
is
con-
cerned
with
the
`forms
of
w
ords'
in
dieren
t
uses
and
constructions".
According
to
another
denition
b
y
the
same
author,
morphology
is
\the
study
of
the
gram-
matical
structure
of
w
ords
and
the
categories
realized
b
y
them"
(Matthews,
		).
In
order
to
compare
w
ords
and
study
their
dierences,
w
ords
are
often
se
gmente
d
in
to
a
n
um
b
er
of
constituen
ts.
Wor
d
se
gmentation
|
here
the
expres-
sion
is
used
as
related
to
w
ord
morphology
,
and
not
to
tokenisation,
where
lexical
constituen
ts
are
inden
tied
in
the
text
|
is
an
imp
ortan
t
subtask
of
Natural
Language
Pro
cessing
(NLP)
with
a
range
of
applications
from
h
yphenation
to
more
detailed
morphological
analysis
and
text-to-sp
eec
h
con
v
ersion.
Sev
eral
approac
hes
aiming
to
learn
w
ord
morphology
ha
v
e
b
een
published
recen
tly
.
In
man
y
of
them,
annotated
data
is
required.
F
or
instance,
Brill
(		)
uses
a
text
corpus
tagged
with
the
corresp
onding
part
of
sp
eec
h
(P
oS)
to
learn
morphological
rules
for
the
prediction
of
the
P
oS
of
unkno
wn
w
ords;
v
an
den
Bosc
h,
Daelemans,
and
W
eijters
(		)
mak
e
use
of
morphologically
analysed
w
ords
to
deriv
e
a
text-to-sp
eec
h
con
v
ersion
theory
in
an
inductiv
e
fashion.
There
are
also
algorithms
for
unsup
ervise
d
learning
of
w
ord
morphology
that
use
plain
text
(Deligne,
		)
or
lists
of
w
ords
(Harris,
	),
(Pirelli,
		),
(Yv
on,
		b)
as
training
data.
Inductiv
e
Logic
Programming
(ILP)
aims
at
the
inductiv
e
learning
of
con-
cepts
from
examples
and
bac
kground
kno
wledge
in
a
rst-order
logic
framew
ork
(La
vra

c
&
D

zeroski,
		).
ILP
has
pro
v
ed
to
b
e
a
feasible
w
a
y
to
learn
linguis-
tic
kno
wledge
in
dieren
t
domains,
suc
h
as
morphological
analysis
(Blo
c
k
eel,
		),
(Mo
oney
&
Cali,
		),
part-of-sp
eec
h
taging
(Cussens,
		)
and
pars-
ing
(Zelle
&
Mo
oney
,
		),
(Kazak
o
v,
			).
Unlik
e
statistical
or
connectionist

approac
hes,
these
results
can
b
e
easily
understo
o
d
and
mo
died
b
y
a
h
uman
exp
ert.
This
article
presen
ts
an
o
v
erview
of
existing
applications
of
ILP
to
w
ord
morphology
learning,
and
sets
targets
for
future
researc
h.
The
article
claims
that
new
c
hallenges
for
the
ILP
comm
unit
y
,
and
a
higher
in
terest
from
computational
linguists
should
b
e
sough
t
in
a
whole
new
range
of
unexplored
learning
tasks
in
whic
h
ILP
will
b
e
more
closely
in
tegrated
with
other
learning
tec
hniques
and
relev
an
t
linguistic
kno
wledge.
The
exp
osition
starts
with
an
o
v
erview
of
basic
concepts
and
approac
hes
in
w
ord
morphology
and
ILP
.
It
then
pro
ceeds
to
describ
e
t
w
o
of
the
relev
an
t
asp
ects
in
mac
hine
learning
of
language:
{
training
data
annotation
and
the
p
ossiblilities
of
its
augmen
tation
through
learning
{
bac
kground
concepts
used
b
y
the
learner.
The
ab
o
v
e
can
b
e
seen
as
t
w
o
orthogonal
dimensions
dening
a
space
of
p
ossible
learning
settings
according
to
the
obje
ct
language
in
whic
h
the
training
data
is
describ
ed,
and
the
c
onc
ept
language
used
to
express
the
theories
learned.
Eac
h
learning
task
can
then
b
e
represen
ted
as
a
mo
v
e
along
either
or
b
oth
axes,
for
instance,
adding
w
ord
segmen
tation
to
the
existing
annotation
of
training
data
or
learning
morphology
rule
templates
with
the
help
of
some
basic
linguistic
concepts,
suc
h
as
v
o
w
els
and
consonan
ts.
This
article
describ
es
a
n
um
b
er
of
biases
for
pre-pro
cessing
of
lexical
data,
and
suggests
w
a
ys
of
com
bining
the
so
obtained
additional
information
with
linguistic
concepts
of
v
arious
complexit
y
for
the
ILP
learning
of
w
ord
morphology
to
ols
and
theories.

W
ord
Morphology
It
is
usually
assumed
that
a
w
ord
consists
of
a
n
um
b
er
of
constituen
ts
whic
h
co
v
er
the
en
tire
w
ord,
do
not
o
v
erlap
and
follo
w
eac
h
other.
The
last
condition
is
generally
v
alid
for
the
Indo-Europ
ean
languages.
In
Arabic,
ho
w
ev
er,
a
more
complex
op
erator
than
concatenation
is
used
to
com
bine
morphemes
(Matthews,
	,
p.
).
.
Morphemes
The
r^
ole
of
the
w
ord
constituen
ts
has
v
aried
b
et
w
een
the
dieren
t
linguistic
sc
ho
ols
and
with
the
dev
elopmen
t
of
linguistics.
The
w
ord
constituen
ts
can
b
e
p
erceiv
ed
merely
as
dierences
allo
wing
us
to
distinguish
b
et
w
een
t
w
o
w
ords
with
no
particular
meaning
assigned
to
them
(de
Saussure,
	).
Ho
w
ev
er,
in
what
is
no
w
considered
to
b
e
the
classical
approac
h
in
morphology
,
w
ord
constituen
ts
are
paid
somewhat
more
atten
tion.
Three
basic
assumptions
are
often
made,
whic
h
concern
w
ord
constituen
ts,
the
w
ords
and
their
phonological/orthographic
represen
tation

(F
radin,
		):

The
w
a
y
in
whic
h
the
w
ords
are
actually
pronounced
or
written.

.
There
are
minimal
w
ord
constituen
ts,
morphemes,
with
whic
h
morphology
op
erates.
.
The
w
ord
is
made
up
of
morphemes
whic
h
follo
w
eac
h
other.
.
There
is
a
set
of
rules
that
pro
duce
the
actual
pron
unciation/sp
elling
of
eac
h
morpheme
according
to
its
con
text.
F
or
instance,
it
can
b
e
said
that
the
w
ord
truthful
is
made
of
t
w
o
morphemes,
truth
and
ful.
Also,
one
migh
t
w
an
t
to
sa
y
that
the
w
ord
studie
d
consists
of
the
morphemes
study
and
e
d
where
a
sp
ecial
rule
handles
the
c
hange
of
the
nal
`y'
in
study
in
to
`i'
in
the
con
text
of
-e
d.
So,
there
are
t
w
o
dieren
t
meanings
of
`morpheme':
.
An
y
sequence
of
c
haracters
that
has
either
grammatical
or
lexical
meaning
.
An
in
v
arian
t
lexical
or
grammatical
unit
realised
b
y
one
or
more
sequences
of
c
haracters.
In
the
ab
o
v
e
denitions,
based
on
Matthews
(		),
`sequence
of
c
haracters'
w
as
substituted
for
`conguration
of
phonological
units',
since
a
lot
of
NLP
researc
h
aims
at
segmen
tation
and
morphological
analysis
of
text,
and
in
man
y
languages
the
pron
unciation
is
not
directly
reected
in
the
sp
elling.
That
c
hange
in
terminology
ma
y
raise
the
ob
jection
that
it
w
ould
mak
e
p
ossible
to
split
a
single
phonological
unit
represen
ted
b
y
t
w
o
or
more
c
haracters
in
the
text.
Ho
w
ev
er,
the
requiremen
t
that
the
sequence
of
c
haracters
ha
v
e
grammatical
or
lexical
meaning
eliminates
those
cases.
If
`morpheme'
is
used
in
its
second
sense,
it
represen
ts
a
certain
kind
of
ab-
straction
o
v
er
a
n
um
b
er
of
v
arian
ts,
called
morphs.
One
can
imagine
a
w
ord
represen
ted
at
t
w
o
lev
els.
If
the
lexic
al
level
con
tains
the
morphemes
included
in
the
w
ord,
the
surfac
e
level
sho
ws
the
actual
w
ord
as
formed
b
y
the
concate-
nation
of
the
corresp
onding
morphs.
F
or
instance,
one
can
sp
eak
of
the
pairs
of
morphemes
dog
+
`plural
morpheme',
boss
+
`plural
morpheme',
leaf
+
`plural
morpheme',
`nega
tive
morpheme'
+
do
whic
h
at
the
surface
lev
el
pro
duce
do
gs,
b
osses,
le
aves,
undo,
resp
ectiv
ely
.
A
distinction
is
usually
made
b
et
w
een
derivational
and
ine
ctional
morph-
ology
.
The
latter
studies
ho
w
all
the
forms
of
eac
h
w
ord
in
the
lexicon
are
pro-
duced,
e.g.
walk,
walks,
walke
d
giv
en
the
lexical
en
try
to
walk,
Verb.
All
these
w
ord-forms
share
the
same
part
of
sp
eec
h
and
meaning,
and
it
is
only
the
mor-
phosyn
tactic
features,
suc
h
as
T
ense
or
P
erson,
that
c
hange.
On
the
other
hand,
deriv
ational
morphology
describ
es
transformations
c
hanging
the
w
ord
meaning.
In
this
case,
the
P
oS
of
the
initial
and
resulting
w
ords
ma
y
dier.
F
or
instance,
adding
the
sux
-able
to
a
v
erb
can
pro
duce
an
adjectiv
e
conrming
the
appli-
cabilit
y
of
the
action
denoted
b
y
the
v
erb.
.
Morphology
Represen
tation
F
rom
a
practical
p
oin
t
of
view,
it
is
imp
ortan
t
to
represen
t
a
large
n
um
b
er
of
w
ord-forms
and
the
corresp
onding
set
of
categories
in
a
more
concise
w
a
y
than
b
y
exhaustiv
e
en
umeration.
Suc
h
a
compact
theory
of
w
ord-formation
has

also
a
generativ
e
p
o
w
er,
enabling
prediction
of
new
w
ord-forms
from
the
kno
wn
ones
and
analysis
of
unseen
w
ords.
A
certain
lev
el
of
complexit
y
is
required
if
the
theory
is
to
reect
the
subtleties
of
the
describ
ed
language;
ho
w
ev
er,
a
detailed
description
of
language
morphology
can
b
e
costly
and,
in
practice,
the
complexit
y
of
the
mo
del
used
is
often
c
hosen
to
eect
a
trade-o
b
et
w
een
its
cost
and
p
erformance.
Since
concatenation
is
the
basic
op
erator
to
arrange
morphemes
in
to
w
ords
in
the
ma
jorit
y
of
Europ
ean
languages,
simple
and
ecien
t
represen
tations
of
w
ord
paradigms
can
b
e
obtained
as
a
com
bination
of
an
inectionally
in
v
ariable
stem
and
a
set
of
endings.
The
use
of
sev
eral
stems
corresp
onding
to
dieren
t
parts
of
the
paradigm
can
simplify
the
represen
tation,
decreasing
the
n
um
b
er
of
endings
used.
The
paradigms
of
sev
eral
w
ords
(lexemes)
can
b
e
further
merged
in
to
larger
clusters
called
ine
ctional
classes.
The
c
hoice
of
represen
tation
also
v
aries
according
to
the
purp
ose.
F
or
in-
stance,
one
ma
y
b
e
in
terested
in
mo
delling
the
idealised
nativ
e
sp
eak
er-hearer.
Also,
one
ma
y
wish
\to
assist
the
se
c
ond
language
le
arner
by
c
onstructing
an
optimal
p
e
dago
gic
al
gr
ammar,
to
plumb
the
depths
of
what
ther
e
is
to
b
e
le
arne
d
ab
out
the
history
of
the
language
thr
ough
the
metho
d
of
internal
r
e
c
onstruction,
or
to
pr
e
dict
futur
e
dir
e
ctions
apt
to
o
c
cur
in
a
language"
(Bender,
		).
F
or
instance,
when
a
second
language
is
studied,
practical
reasons
are
put
forw
ard,
or,
in
Matthews's
w
ords
(Matthews,
	):
An
y
language
is
learned
b
y
a
mixture
of
rote-learning,
rules
and
practice
and
it
is
the
job
of
the
language
teac
her
to
w
ork
out
what
com
bination
is
the
most
ecien
t.
Indeed,
it
is
often
simpler
for
the
h
uman
learner
and
the
NLP
dev
elop
er
alik
e
to
co
v
er
some
of
the
cases
as
exceptions
b
y
simply
memorising
them,
rather
than
to
use
a
complex
set
of
rules
needed
otherwise.
.
Generativ
e
Phonology
and
Tw
o-Lev
el
Morphology
Sev
eral
approac
hes
to
morphology
use
the
notion
of
abstract,
lexical-lev
el,
mor-
phemes.
In
these
formalisms,
w
ords
are
formed
b
y
rstly
com
bining
morphemes
at
lexical
lev
el,
and
then
applying
rules
to
deriv
e
the
w
ord
surface-lev
el
represen-
tation.
In
the
framew
ork
of
generativ
e
phonology
(Chomsky
&
Halle,
	),
the
initial,
lexical-lev
el
sequence
of
morphemes
is
mo
died
b
y
sequen
tial
application
of
rewriting
[generativ
e]
rules,
and
a
w
ord
can
go
through
sev
eral
in
termediate
stages
b
efore
it
tak
es
its
nal,
surface-lev
el
form.
Some
of
the
criticism
of
this
approac
h
is
related
to
the
pro
cedural
c
haracter
of
the
rules,
as
they
ha
v
e
to
b
e
applied
in
a
certain
order,
and
op
erate
only
in
an
underlying-to-surface
direction.
The
follo
wing
example,
b
orro
w
ed
from
An
t
w
orth
(		),
illustrates
the
prin-
ciples
of
generativ
e
phonology
.
Consider
the
rules:
Rule

(V
o
w
el
Raising)
e
b
ecomes
(is
rewritten
as)
i
if
it
is
follo
w
ed
b
y
a
zero
or
more
consonan
ts,
follo
w
ed
b
y
i.
Rule

(P
alatalisation)
t
b
ecomes
c
preceding
i.

d
v
E
k
+
a
d
v
k
+
a
e
d
v
E
k
+
d
v
k
+
Fig.
.
Lexical
vs
surface
lev
el
in
t
w
o-lev
el
morphology
A
sample
deriv
ation
of
forms
to
whic
h
these
rules
apply
follo
ws.
Lexical
level:
temi
Rule
:
timi
Rule
:
cimi
Surface
level:
cimi
Kosk
enniemi
(	)
in
tro
duces
the
Tw
o-Lev
el
Morphology
approac
h.
The
approac
h
is
based
on
t
w
o
lexicons
con
taining
morphemes
(ro
ots
and
axes),
and
a
set
of
morphological
rules.
The
rules
establish
whether
a
giv
en
sequence
of
c
haracters
at
the
surface
lev
el
(as
it
app
ears
in
the
text)
can
corresp
ond
to
another
giv
en
sequence
of
sym
b
ols
used
to
represen
t
the
morphemes
in
the
lexicon.
The
morphological
rules
are
implemen
ted
as
nite
state
transducers
(FST).
FST
are
automata
with
t
w
o
bands,
i.e.
they
are
based
on
alphab
ets
comp
osed
b
y
pairs
of
sym
b
ols.
F
or
instance,
the
Nominativ
e
Singular
of
the
Czec
h
w
ord
d

vka
(girl)
can
b
e
represen
ted
at
lexical
lev
el
as
a
com
bination
of
the
morphemes
d

vEk+a.
The
corresp
onding
string
at
surface
lev
el
is
d

v;k+a
(Figure
).
P
airs
of
corresp
ond-
ing
sym
b
ols
at
the
t
w
o
lev
els
ha
v
e
to
b
e
aligned,
p
ossibly
making
use
of
nil
(`;')
sym
b
ols.
One
p
ossible
rule
ma
y
p
ostulate
that
the
sym
b
ol
E
corresp
onds
to
the
nil
sym
b
ol
;
if
the
righ
t-hand
side
con
text
of
E
is
ka.
There
could
b
e
another
rule
matc
hing
E
to
e
if
the
righ
t-hand
side
con
text
of
E
is
k;.
F
rom
a
theoretical
p
oin
t
of
view,
there
are
sev
eral
asp
ects
whic
h
mak
e
the
t
w
o-lev
el
approac
h
in
teresting.
It
mak
es
use
of
morphemes
close
to
the
surface
lev
el,
and
it
do
es
not
need
in
termediary
descriptions
whic
h
cannot
b
e
directly
related
to
empirical
observ
ations.
F
urthermore,
this
approac
h
is
defended
b
y
its
supp
orters
as
declarativ
e,
allo
wing
to
separate
the
conditions
go
v
erning
a
linguis-
tic
phenomenon
from
their
application
(F
radin,
		).
Also,
t
w
o-lev
el
rules
are
bi-directional,
i.e.
they
can
op
erate
in
an
underlying
to
surface
direction
(genera-
tion
mo
de)
or
in
a
surface
to
underlying
direction
(recognition
mo
de)
(An
t
w
orth,
		).
The
declarativ
e
and
bi-directional
c
haracter
of
t
w
o-lev
el
approac
h
mak
es
its
logic
programming
implemen
tation
suitable
and
easy
.
Another
related
ap-
proac
h
w
orth
men
tioning
is
Kaplan
and
Ka
y's
m
ultilev
el
morphology
(Kaplan
&
Ka
y
,
		).


ILP
T
o
ols
This
section
describ
es
the
ILP
to
ols
whic
h
ha
v
e
b
een
applied
to
morphology
learning
so
far.
.
Decision
List
Learners
A
rst-or
der
de
cision
list
is
a
logic
program
in
whic
h
the
last
literal
in
all
clauses
but
the
last
one
is
a
cut
(!).
F
or
example,
in
Figure
,
the
predicate
number/
predicts
the
morpho-syn
tactic
category
Number
of
English
nouns
according
to
their
ending.
The
order
of
clauses
do
es
matter,
and
when
the
predicate
is
called,
only
the
rst
of
all
applicable
clauses
is
red.
The
gure
sho
ws
a
decision
list
    append(_,[a,t,a],Word), !.
number(Word,plural):-
number(Word,singular):-
    append(_,[s,s],Word), !.
    append(_,[s],Word), !.
number(Word,plural):-
Legend
:  category assigned
singular
song :  input word
:  clause used
:  clause scope
mass
plural
singular
singular
plural
song
birds
lemmata
number(Word,singular).
Order of testing - only the first applicable clause is used.
Fig.
.
Scop
e
and
priorit
y
of
decision
list
clauses
of
four
clauses
and
ho
w
it
is
applied
on
four
English
nouns.
The
noun
lemmata
res
the
rst
clause
in
the
list,
and
is
classied
as
b
eing
a
plural
w
ord-form.
The
noun
mass
is
co
v
ered
b
y
the
second,
third,
and
fourth
clauses;
ho
w
ev
er,
the
clause
order
guaran
tees
that
only
the
rst
of
the
three
is
used.
The
scop
e
of
eac
h
clause
is
sc
hematically
represen
ted
with
an
ellipse;
these
are
pro
jected
on
a
plane
to
allo
w
for
their
comparison.
The
last
clause
represen
ts
the
default
case,
i.e.
co
v
ers
all
examples
whic
h
are
not
subsumed
b
y
an
y
of
the
preceding
clauses.

F
oidl
F
oidl
(Mo
oney
&
Cali,
		)
is
a
greedy
top-do
wn
ILP
system
for
learning
rst-order
decision
lists
from
examples
and
is
closely
related
to
F
OIL
(Quinlan,
		0).
One
of
the
main
reasons
that
a
system
suc
h
as
F
oidl
is
a
go
o
d
candidate
for
NLP
applications
is
that
it
can
b
e
used
to
learn
from
p
ositiv
e
examples
only
.
F
oidl
uses
an
assumption
kno
wn
as
output
c
ompleteness
to
generate
implicit
negativ
e
examples
from
p
ositiv
e
data.
The
notion
of
output
completeness
can
b
e
b
est
explained
b
y
an
example.
Let
the
p
ositiv
e
examples
b
e:
past(sleep,slept
),
past(like,liked)
,
past(walk,walked)
.
Let
the
target
predicate
past/
b
e
queried
using
the
mo
de
past(+,-).
This
mo
de
describ
es
that
the
rst
argumen
t
is
an
input
v
ariable,
i.e.
it
m
ust
b
e
instan-
tiated
in
an
y
query
,
and
the
second
one
is
an
output
v
ariable,
i.e.
one
that
is
to
b
e
instan
tiated
b
y
the
clause
b
eing
learned.
The
only
query
ab
out
that
clause
that
is
allo
w
ed
b
y
the
mo
de
declaration
is
of
the
t
yp
e
past(sleep,X).
An
y
instan
ti-
ation
of
the
output
v
ariables
that
do
es
not
exactly
matc
h
a
p
ositiv
e
example
is
p
erceiv
ed
as
a
negativ
e
one.
Suc
h
implicit
negativ
e
examples
for
the
examples
sho
wn
are
past(sleep,X),
past(sleep,liked)
,
past(sleep,walked
).
Clog
Clog
(Manandhar,
D

zeroski,
&
Erja
v
ec,
		)
is
another
system
for
learning
rst-order
decision
lists.
It
shares
a
fair
amoun
t
of
similarit
y
with
F
oidl
(Cali
&
Mo
oney
,
		).
In
b
oth
F
oidl
and
Clog,
the
decision
list
is
learned
from
the
b
ottom
up
w
ards,
i.e.
the
most
general
clauses
are
learned
rst.
Lik
e
F
oidl
,
Clog
can
learn
rst-order
decision
lists
from
p
ositiv
e
examples
only
using
the
output
completeness
assumption.
In
the
curren
t
implemen
tation,
the
generali-
sations
relev
an
t
to
an
example
are
supplied
b
y
a
user-dened
predicate
whic
h
tak
es
as
input
an
example
and
generates
a
list
of
all
generalisations
that
co
v
er
that
example.
Clog
treats
the
set
of
generalisations
of
an
example
as
a
gener-
alisation
set.
It
then
cycles
ev
ery
input
example
through
the
generalisation
set
in
a
single
iteration
c
hec
king
whether
a
candidate
generalisation
co
v
ers
the
ex-
ample
p
ositiv
ely
or
negativ
ely
.
Once
this
pro
cess
is
complete
the
b
est
candidate
generalisation
w.r.t.
a
user-dened
gain
function
is
c
hosen.
A
comparison
b
et
w
een
F
oidl
and
Clog
is
clearly
in
fa
v
our
of
the
latter.
Unlik
e
F
oidl
,
Clog
do
es
not
require
theory
constan
ts
to
b
e
sp
ecied
prior
to
learning.
Clog
is
m
uc
h
faster
than
F
oidl
(Manandhar
et
al.,
		),
(Kazak
o
v
&
Manandhar,
		),
and
it
can
pro
cess
m
uc
h
larger
data
sets
as
it
do
es
not
require
all
data
to
b
e
loaded
in
the
memory
.
A
relativ
e
shortcoming
of
Clog
in
comparison
with
F
oidl
is
that
the
space
of
target
concepts
is
limited
to
a
n
um
b
er
of
hard-co
ded
clauses.
A
p
ossible
w
a
y
around
is
to
apply
F
oidl
to
a
feasible
subset
of
the
data,
and
then
pass
the
t
yp
e
of
clauses
learned
to
Clog
and
pro
cess
the
whole
data
set
with
it.

.
Analogical
Prediction
Analogical
Prediction
(AP)
is
a
learning
tec
hnique
that
com
bines
some
of
the
main
adv
an
tages
of
Instance-Based
Learning
(IBL)
and
standard
ILP
.
It
tak
es
bac
kground
kno
wledge
and
training
examples,
then
for
eac
h
test
example
e
it
searc
hes
for
the
most
compressiv
e
single-clause
theory
that
co
v
ers
e
and
classi-
es
it
similarly
to
the
largest
class
of
training
examples
co
v
ered.
AP
and
IBL
share
the
robustness
to
the
c
hanges
in
the
training
data
t
ypical
for
lazy
learn-
ing
approac
hes.
In
terms
of
preference
bias,
AP
replaces
the
similarit
y
metric
needed
in
IBL
with
a
compression-based
metric.
Also,
AP
inherits
from
ILP
its
greatest
adv
an
tage
o
v
er
IBL,
namely
the
explicit
h
yp
othesis
serving
as
an
explanation
of
the
decisions
made.
AP
has
b
een
sho
wn
to
b
e
particularly
w
ell
suited
for
domains
in
whic
h
a
large
prop
ortion
of
examples
m
ust
b
e
treated
as
exceptions
(Muggleton
&
Bain,
			).
AP
has
b
een
implemen
ted
in
Progol.
b
y
Muggleton.

Asp
ects
of
Mac
hine
Learning
of
Natural
Language
.
Learning
from
Annotated
Data
A
lo
ok
at
existing
w
ork
sho
ws
t
w
o
principal
settings
in
whic
h
a
computational
linguist
w
ould
learn
w
ord
morphology
.
The
data
in
the
rst
case
is
a
corpus
(list,
lexicon)
of
annotated
w
ords,
where
eac
h
one
is
tagged
with
some
relev
an
t
information.
Whether
it
is
a
part
of
sp
eec
h
(Brill,
		),
(Mikheev,
		),
a
more
complex
list
of
morphosyn
tactic
categories
(Kazak
o
v,
Manandhar,
&
Erja
v
ec,
			)
or
a
string
of
phonemes
describing
the
w
ord
pron
unciation
(Yv
on,
		b),
(v
an
den
Bosc
h,
		),
the
information
con
tained
in
the
tag
is
related
to
the
w
ord
as
a
whole.
F
or
instance,
if
the
corpus
pairs
w
ords
with
their
pron
unciation,
the
t
w
o
strings
of
letters
and
phonemes
ma
y
b
e
aligned,
so
that
eac
h
phoneme
corresp
onds
to
one
or
more
letters.
Ho
w
ev
er,
it
is
the
con
text
of
the
whole
w
ord
that
denes
the
phonetic
v
alue
of
eac
h
letter.
Giv
en
a
tagged
list
of
w
ords,
one
can
try
to
iden
tify
constituen
ts
within
the
w
ord
and
nd
a
mapping
b
et
w
een
these
and
the
information
in
the
tag.
One
can
so
attempt
to
disco
v
er
whether
a
certain
w
ord
ending
can
b
e
asso
ciated
with
a
sp
ecic
part
of
sp
eec
h,
or
morphological
category
,
suc
h
as
Num
b
er,
Gender
or
T
ense.
Also,
kno
wing
the
segmen
ts
that
preserv
e
their
pron
unciation
throughout
the
corpus
can
b
e
v
ery
v
aluable,
as
it
can
help
predict
the
pron
unciation
of
unkno
wn
w
ords
comp
osed
out
of
suc
h
segmen
ts.
One
can
also
imagine
the
case
in
whic
h
the
data
set
con
tains
w
ords
with
mark
ed
morpheme
b
oundaries.
The
aim
of
learning
in
this
case
dep
ends
on
the
additional
annotation
pro
vided.
Segmen
ted
data
can
supply
the
learning
algo-
rithm
with
morphemes
or
concatenations
of
morphemes
that
can
b
e
used
as
theory
constan
ts
in
the
rules
mapping
w
ords
to
categories.
Learning
segmen
ta-
tion
rules
can
pro
vide
the
computational
linguist
with
a
to
ol
that
can
b
e
used
as
a
pre-pro
cessor
of
the
morphological
analyser
or
alone.
Segmen
tation
pro
ducing

the
w
ord
stem
can
b
e
seen
as
assigning
a
w
ord
its
meaning
as
most
general-
purp
ose
systems
for
information
retriev
al
assume
that
the
w
ord
meaning
is
fully
sp
ecied
b
y
its
stem,
ignoring
asp
ects
of
lexical
am
biguit
y
and
con
textual
in
ter-
pretation.
In
the
setting
describ
ed
so
far,
w
ord
morphology
aims
to
iden
tify
w
ord
con-
stituen
ts
(morphemes)
and
assign
them
certain
prop
erties,
suc
h
as
pron
uncia-
tion,
p
ossible
parts
of
sp
eec
h,
etc.
The
matc
hing
b
et
w
een
morphemes
and
w
ord
prop
erties
can
b
e
used
b
oth
for
w
ord
generation
(Mo
oney
&
Cali,
		),
(Mug-
gleton
&
Bain,
			)
and
analysis
(Brill,
		),
(Mikheev,
		).
.
Unsup
ervised
Learning
of
Language
In
the
second
setting
in
whic
h
mac
hine
learning
is
applied
to
w
ord
morphology
,
the
corpus
con
tains
no
annotation.
Here
the
in
terest
is
fo
cussed
on
the
searc
h
for
the
b
est
mo
del
of
the
p
oten
tially
innite
language
or
of
its
nite
sample
a
v
ailable.
The
qualit
y
of
the
mo
del
can
b
e
ev
aluated
b
y
its
generativ
e
p
o
w
er,
i.e.
the
abilit
y
to
generate
all
w
ords
of
the
language,
b
y
the
lik
eliho
o
d
with
whic
h
the
corpus
is
pro
duced
b
y
the
mo
del
(Deligne,
		)
or
b
y
a
com
bination
of
these
t
w
o
criteria,
whic
h
are
clearly
con
tradictory
.
The
t
w
o
extremes
of
the
scale
could
b
e
mark
ed
b
y
the
most
general
mo
del,
the
one
that
describ
es
w
ords
as
strings
of
arbitrary
length
and
comp
osition,
and
the
mo
del
represen
ted
b
y
the
training
data
itself.
Unsup
ervised
learning
com
bines
a
new
data
represen
tation
framew
ork
(e.g.
replace
w
ords
with
concatenations
of
a
prex
and
sux)
with
a
preference
bias
in
tro
ducing
an
order
among
all
p
ossible
represen
tations
(use
the
shortest
p
os-
sible
lexicons
of
prexes
and
suxes),
and
a
searc
h
tec
hnique
(e.g.
a
genetic
algorithm)
to
nd
the
b
est
represen
tation
of
data
w.r.t.
the
bias.
In
this
con-
text,
t
w
o
principles
are
often
used.
Occam's
razor
recommends
the
use
of
the
simplest
(shortest)
mo
del
(theory
,
h
yp
othesis)
that
can
explain
ho
w
the
data
w
as
pro
duced.
The
Minimal
Description
Length
(MDL)
principle
com
bines
the
mo
del
length
with
the
length
of
the
data
description
based
on
the
mo
del;
the
mo
del
whic
h
minimises
the
n
um
b
er
of
bits
enco
ding
b
oth
mo
del
and
data,
is
giv
en
preference
(Mitc
hell,
		).
Unsup
ervised
learning
has
b
een
used
to
annotate
texts
or
lists
of
w
ords
with
their
segmen
tations
(Deligne,
		),
(Harris,
	),
(Kazak
o
v,
		),
(Bren
t,
Lundb
erg,
&
Murth
y
,
		),
and
to
assign
the
most
probable
pron
unciation
to
eac
h
segmen
t
(Yv
on,
		b).
The
biases
used
in
the
cited
w
ork,
and
the
w
a
y
they
con
tribute
to
the
data
annotation,
are
sho
wn
in
the
upp
er
part
of
T
able

on
page
.
F
rom
a
practical
p
oin
t
of
view,
unsup
ervised
learning
is
attractiv
e
b
ecause
of
its
lo
w
cost,
as
it
do
es
not
require
the
presence
of
a
linguist.
Ho
w
ev
er,
there
is
also
a
more
fundamen
tal
reason
to
study
unsup
ervised
learning.
In
an
area
as
abundan
t
in
mo
dels
and
theories
as
linguistics
is,
supp
ort
from
information
theory
p
ersp
ectiv
e
can
help
consolidate
the
opinions.

.
ILP
and
Bac
kground
Kno
wledge
for
Language
Learning
According
to
Shieb
er
(	),
there
are
t
w
o
classes
of
linguistic
formalisms:
lin-
guistic
to
ols
and
linguistic
theories.
The
former
are
used
to
describ
e
natural
languages,
the
latter
serv
e
to
dene
the
class
of
p
ossible
natur
al
languages.
One
can
extend
this
taxonom
y
b
y
taking
in
to
accoun
t
the
fact
that
natural
languages
are
y
et
another
class
of
comm
unication
co
des,
and
as
suc
h
should
follo
w
the
prin-
ciples
of
Information
Theory
(Shannon
&
W
ea
v
er,
	),
and
could
b
e
studied
and
mo
delled
from
the
its
general
p
ersp
ectiv
e.
Indeed,
a
considerable
part
of
this
article
is
dedicated
to
the
description
of
existing
researc
h
applying
general
Information
Theory
principles
to
natural
language.
ILP
theory
claims
that
learning
is
more
accurate
when
the
concept
language
is
extende
d
to
include
appropriate
bac
kground
concepts.
Limiting
the
concept
language
to
some
of
these
concepts
can
impro
v
e
learning
eciency
.
Ev
en
at
presen
t,
with
all
ongoing
w
ork
on
predicate
in
v
en
tion
(Khan,
Muggleton,
&
P
arson,
		)
and
m
ulti-predicate
learning
(Muggleton,
		),
the
use
of
w
ell-
c
hosen
domain
kno
wledge
is
crucial.
Ha
ving
existed
in
its
mo
dern
form
for
more
than
00
y
ears
(Lancelot
&
Arnauld,
0),
linguistics
seems
lik
ely
to
pro
vide
suc
h
relev
an
t
concepts.
These
could
b
e
used
in
three
w
a
ys
in
conjunction
with
ILP
learning.
.
Learning
could
help
sp
ecialise
an
existing
theory
,
e.g.
pro
ducing
rules
from
templates
to
obtain
a
theory
or
to
ol
for
a
particular
language
in
a
w
a
y
reminding
of
Explanation-Based
Learning
(Mitc
hell,
		).
.
On
the
other
hand,
it
migh
t
b
e
p
ossible
to
use
learning
for
the
generalisations
of
existing
theories
or
to
ols.
F
or
instance,
existing
morpho-lexical
analysers
for
a
n
um
b
er
of
languages
L
i
could
b
e
compiled
in
to
a
single
to
ol
for
use
in
a
m
ultilingual
con
text.
In
this
case,
learning
could
also
result
in
a
metatheory
whic
h
w
ould
subsume
eac
h
of
the
languages
L
i
,
and
y
et
b
e
more
sp
ecic
than
the
general
framew
ork
initially
used.
.
Finally
,
an
existing
theory
(to
ol)
whic
h
is
incomplete
or
inconsisten
t
w.r.t.
the
corpus
could
b
e
extended
or
ne-tuned
b
y
learning.
W
e
w
an
t
to
suggest
a
classication
of
all
p
ossible
applications
of
ILP
learning
to
w
ord
morphology
based
on
t
w
o
factors,
the
exten
t
to
whic
h
linguistic
concepts
are
used
as
bac
kground
concepts,
and
the
amoun
t
of
annotation
in
data.
The
lo
w
er
part
of
T
able

(see
page
)
giv
es
a
graphical
represen
tation
of
that
clas-
sication.
The
horizon
tal
axis
represen
ts
a
partial
ordering
b
et
w
een
data
sets|if
t
w
o
data
sets
con
taining
the
same
w
ords
are
displa
y
ed
in
dieren
t
columns,
the
one
on
the
left-hand
side
con
tains
al
l
annotation
from
the
rst
data
set
plus
some
additional
one.
The
ro
ws
in
the
table
sho
w
whether
the
bac
kground
con-
cepts
used
for
morphology
learning
consist
only
of
string
relations
or
mak
e
use
of
increasingly
complex
linguistic
concepts
and
theories.
No
w
a
learning
task
can
b
e
describ
ed
as
(O
in
;
C
in
)
!
(O
out
;
C
out
),
i.e.
as
a
v
ector,
the
initial
co-ordinates
of
whic
h
sp
ecify
the
t
yp
e
of
data
O
in
and
bac
kground
kno
wledge
C
in
a
v
ailable.
The
v
ector
end
p
oin
t
is
giv
en
b
y
the
t
yp
e

of
target
theory
learned
C
out
,
and
also
reects
c
hanges
in
the
t
yp
e
of
annotation
a
v
ailable
in
the
data.
The
application
of
an
y
preference
bias
for
unsup
ervised
learning
from
the
upp
er
part
of
T
able

w
ould
result
in
a
righ
t-to-left
mo
v
e
in
parallel
to
the
horizon
tal
axis.
Indeed,
eac
h
of
these
biases
w
ould
add
extra
annotation
to
the
data
y
et
their
con
tribution
to
the
concept
language
w
ould
usually
b
e
limited
to
the
creation
of
new
theory
constan
ts,
t
ypically
segmen
ts
of
w
ords.
The
v
ector
describing
unsup
ervised
learning
w
ould
usually
not
ha
v
e
an
y
v
ertical
comp
onen
t,
as
the
(v
ery
simple)
relations
it
in
tro
duces,
suc
h
as
prex
and
sux
(Kazak
o
v,
		),
w
ould
already
b
e
presen
t
in
the
bac
kgound
kno
wledge
for
an
y
learning
setting.
One
could
only
imagine
the
h
yp
othetical
case
in
whic
h
the
use
of
a
preference
bias
based
on
a
general
principle
of
Information
Theory
w
ould
actually
result
in
a
less
trivial
concept,
e.g.
lexical-lev
el
morpheme
(see
the
last
ro
w
of
the
PREFERENCE
BIAS
part
of
the
table).
Unlik
e
unsup
ervised
learning,
one
w
ould
exp
ect
ILP
learning
to
c
hange
the
conceptual
lev
el
of
represen
tation
b
y
in
tro
ducing
new,
more
complex
concepts,
and
their
gradual
sp
ecialisation,
or
b
y
generalising
existing
case-sp
ecic
theories.
In
either
case,
the
learning
task
v
ector
will
b
e
v
ertical.
Can
ILP
b
e
applied
to
the
task
usually
assigned
to
unsup
ervised
learners?
Strictly
sp
eaking,
the
answ
er
is
y
es.
Ho
w
ev
er,
to
do
so,
the
ILP
learner
should
incorp
orate
the
three
already
men
tioned
comp
onen
ts
of
unsup
ervised
learners.
As
it
uses
rst-order
logic
as
a
represen
tation
formalism,
ILP
w
ould
hardly
ha
v
e
an
y
diculties
incorp
orating
the
represen
tation
formalism
and
the
preference
bias
of
almost
an
y
unsup
ervised
learner.
It
is
the
searc
h
tec
hnique
that
w
ould
mak
e
ILP
a
p
o
or
substitute
for
a
giv
en
unsup
ervised
learner.
T
o
mak
e
the
searc
h
maximally
ecien
t,
eac
h
of
these
programs
uses
a
sp
ecialised
algorithm
whic
h
w
ould
b
e
dicult
to
replicate
in
the
con
text
of
declarativ
e
programming
or
for
whic
h
a
n
um
b
er
of
standard
libraries
exist
in
other,
imp
erativ
e
languages.
The
list
of
suc
h
algorithms
includes
searc
h
for
a
minimal-cost
path
in
a
graph
(Deligne,
		),
and
genetic
algorithms
(Kazak
o
v,
		).
The
implication
is
that
non-ILP
algorithms
for
unsup
ervised
learning
and
ILP
are
not
m
utually
exclusiv
e.
Quite
the
opp
osite,
they
could,
and
should,
b
e
com
bined
in
fruitful
marriages,
in
whic
h
the
unsup
ervised
learner
is
used
as
a
pre-pro
cessor
for
ILP
.
An
example
of
suc
h
h
ybrid
approac
h
using
the
Na

v
e
Theory
of
Morphology
bias
already
exists
(Kazak
o
v
&
Manandhar,
		).
This,
and
sev
eral
other
relev
an
t
biases
are
describ
ed
in
the
next
section.

Preference
Biases
for
W
ord
Segmen
tation
.
Deligne's
T
ext
Segmen
tation
Deligne
(		)
describ
es
a
tec
hnique
for
the
segmen
tation
of
a
long
single
string,
p
ossibly
con
taining
sev
eral
sen
tences
with
no
delimiters
b
et
w
een
w
ords.
The
metho
d
is
lo
oking
for
a
mo
del
from
whic
h
the
corpus
could
b
e
generated
with
a
maximal
lik
eliho
o
d.
All
p
ossible
segmen
tations
are
represen
ted
as
a
directed
graph
with
one
start
and
one
end
no
de.
Eac
h
edge
corresp
onds
to
a
sequence
of
letters
forming
a
segmen
t.
Eac
h
path
from
the
start
to
the
end
no
de
represen
ts

a
particular
segmen
tation
of
the
whole
text.
Eac
h
edge
of
the
graph
is
assigned
w
eigh
t
represen
ting
the
probabilit
y
of
this
segmen
t.
This
probabilit
y
is
estimated
simply
as
the
relativ
e
frequency
of
the
string
in
the
text,
and
the
searc
h
for
the
b
est
segmen
tation|the
one
whic
h
maximises
the
pro
duct
of
all
segmen
ts'
probabilities|is
reform
ulated
as
a
searc
h
for
a
minimal-cost
path
in
the
graph.
Once
an
initial
segmen
tation
is
obtained,
it
is
used
to
re-estimate
the
probabilit
y
of
eac
h
segmen
t,
and
to
segmen
t
the
text
again.
The
pro
cess
stops
when
the
segmen
tation
b
ecomes
stable.
This
metho
d
can
b
e
adapted
to
the
task
of
w
ord
segmen
tation
b
y
in
tro
ducing
rm
b
oundaries
in
the
text
corresp
onding
to
the
w
ord
b
orders.
The
algorithmic
complexit
y
of
the
metho
d
requires
that
the
maximal
length
of
segmen
ts
should
b
e
limited
(Deligne
(		)
sets
it
to

c
haracters).
Ev
en
in
the
con
text
of
segmen
ting
lists
of
w
ords,
rather
than
con
tin
uous
text,
remo
ving
this
restriction
ma
y
cause
problems,
as
the
n
um
b
er
of
p
ossible
constituen
ts
b
ecomes
exp
onen
tial
w.r.t.
the
w
ord
length.
It
follo
ws
directly
from
the
fact
that
a
segmen
tation
of
a
string
in
to
substrings
can
b
e
enco
ded
as
a
string
of
s
and
0s
where
a

marks
a
split
p
oin
t.
Then
for
a
string
of
length
n,
there
are

n 
p
ossible
segmen
tations.
This,
additionally
m
ultiplied
b
y
the
complexit
y
O
(n

)
of
the
searc
h
for
a
minimal-
cost
path
(n
is
the
n
um
b
er
of
no
des
in
the
graph)
can
easily
b
ecome
an
issue,
esp
ecially
for
aglutinativ
e
languages
where
w
ords
can
b
e
of
considerable
length.
F
or
instance,
a
fourt
y-one-letter
w
ord,
whic
h
is
not
unheard
of
in
T
urkish,
has

0

0

p
ossible
segmen
tations.
.
Bren
t
et
al.
Another
w
ord
segmen
tation
metho
d
based
on
information
theory
is
emplo
y
ed
b
y
Bren
t
et
al.
(		).
The
article
describ
es
a
binary
enco
ding
of
a
list
of
w
ords
based
on
lexicons
of
w
ord
constituen
ts,
and
a
table
describing
ho
w
these
con-
stituen
ts
are
com
bined
to
form
w
ords.
Then
the
Minimal
Description
Length
(MDL)
principle
is
applied,
i.e.
the
table
corresp
onding
to
the
enco
ding
requir-
ing
the
minimal
n
um
b
er
of
bits
is
assumed
to
describ
e
the
optimal
segmen
tation
of
the
list
of
w
ords.
As
the
searc
h
space
for
p
ossible
enco
dings
is
v
ery
large,
the
approac
h
limits
the
n
um
b
er
of
constituen
ts
to
t
w
o
p
er
w
ord.
Also,
suxes
(righ
t-hand-side
constituen
ts)
whic
h
share
a
sux
b
et
w
een
themselv
es
are
not
allo
w
ed.
The
suggested
searc
h
tec
hnique
is
iterativ
e,
trying
to
reduce
the
binary
en-
co
ding
size
b
y
alternately
adding
new
suxes
to
their
lexicon,
or
b
y
remo
ving
suxes
from
that
lexicon
if
the
addition
of
new
suxes
do
es
not
result
in
shorter
represen
tations.
The
eciency
of
this
searc
h
tec
hnique
is
unsatisfactory
|
\the
b
ad
news
is
se
ar
ch"
(Bren
t
et
al.,
		),
whic
h
lea
v
es
the
bias
with
no
practical
use
so
far.
.
Na

v
e
Theory
of
Morphology
Kazak
o
v
(		)
in
tro
duces
a
bias
for
the
segmen
tation
of
all
w
ords
in
a
list.
It
is
assumed
that
eac
h
w
ord
can
b
e
represen
ted
as
a
concatenation
of
t
w
o
(p
ossibly

empt
y)
strings
Pr
ef
+
Suf
.
F
or
eac
h
t
w
o
segmen
tations
of
all
w
ords
in
the
list,
the
bias
giv
es
a
preference
to
the
one
for
whic
h
the
t
w
o
lexicons
of
prexes
and
suxes
con
tain
less
c
haracters
in
total.
The
segmen
tation
of
eac
h
w
ord
can
b
e
represen
ted
with
an
in
teger
denoting
the
prex
length,
and
the
segmen
tation
of
a
list
of
w
ords
can
b
e
enco
ded
as
a
v
ector
of
in
tegers
called
a
Na

v
e
Theory
of
Morphology
(NTM).
Then
a
genetic
algorithm
can
b
e
easily
applied
to
the
searc
h
for
a
suitable
segmen
tation
of
the
input
where
eac
h
c
hromosome
is
an
NTM,
and
its
tness
function
is
minimising
the
n
um
b
er
of
c
haracters
in
the
corresp
onding
prex
and
sux
lexicons.
Initially
,
the
algorithm
generates
a
n
um
b
er
of
na

v
e
theories
of
morphology
at
random,
and
then
rep
eatedly
applies
the
genetic
op
erators
cr
ossover,
mutation
and
sele
ction
.
.
Harris's
Segmen
tation
Harris
(	)
describ
es
an
unsup
ervised
approac
h
to
the
segmen
tation
of
utter-
ances
sp
elt
phonetically
.
The
approac
h
coun
ts
the
n
um
b
er
of
dieren
t
phonemes
succ(Pr
ex
)
whic
h
can
app
ear
in
an
utterance
of
the
language
after
a
giv
en
ini-
tial
sequence
Pr
ex
of
phonemes
(the
notation
succ(n),
where
n
is
the
prex
length,
will
also
b
e
used).
F
or
eac
h
utterance
a
v
ailable,
its
left
substrings
Pr
ex
i
are
pro
duced
and
the
coun
ts
succ(Pr
ex
i
)
computed.
Then
the
utterance
is
seg-
men
ted
where
the
function
succ(Pr
ex
)
reac
hes
its
lo
cal
maxima.
It
is
p
ossible
to
adapt
the
metho
d
for
the
segmen
tation
of
w
ords
instead
of
whole
utterances,
also
replacing
phonemes
with
letters.
Figure

giv
es
the
v
alue
of
succ(Pr
ex
)
for
eac
h
w
ord
in
the
data
set
but,
cut,
cuts,
br
e
ad,
sp
ot,
sp
ots,
sp
otte
d
|
note
the
use
of
^
and
@
as
unique
sym
b
ols
for
a
start
and
end
of
w
ord.
As
already
but
:
^-,
b-,
u-,
t-,
@-0
cut
:
^-,
c-,
u-,
t-,
@-0
cut-s
:
^-,
c-,
u-,
t-,
s-,
@-0
bread
:
^-,
b-,
r-,
e-,
a-,
d-,
@-0
spot
:
^-,
s-,
p-,
o-,
t-,
@-0
spot-s
:
^-,
s-,
p-,
o-,
t-,
s-,
@-0
spot-ted
:
^-,
s-,
p-,
o-,
t-,
t-,
e-,
d-,
@-0
Fig.
.
Segmen
tation
p
oin
ts
and
succ(Pr
ex
)
(sho
wn
after
the
last
letter
of
eac
h
prex)
stated,
a
w
ord
w
is
segmen
ted
after
the
initial
substring
of
length
n,
if
for
n
succ(n)
reac
hes
a
lo
cal
maxim
um,
whic
h
is
greater
than
b
oth
succ(n
 )
and
succ(n
+
).
In
case
a
plateau
is
found,
all
no
des
on
the
plateau
are
considered
as
segmen
tation
p
oin
ts
pro
vided
a
do
wnhill
slop
e
follo
ws.
Using
this
criterion
one
obtains
cut-,
cut-s,
spot-,
spot-s,
spot-ted
while
but
and
bread
are
not
segmen
ted.
Although
not
men
tioned
in
the
original
pap
er
(Harris,
	),
Harris's
metho
d
can
b
e
based
on
the
represen
tation
of
all
utterances
(or
w
ords)
as
a
prex,
resp.
sux
tree.

.
de
Saussure's
Analogy
In
his
Course
in
Gener
al
Linguistics,
de
Saussure
(	)
describ
es
a
principle
of
analogy
according
to
whic
h,
in
the
long
term,
the
w
ord-forms
in
a
giv
en
language
c
hange
and
tend
to
form
the
follo
wing
patterns:
Pr
ef

+
Suf

:
Pr
ef

+
Suf

=
()
Pr
ef

+
Suf

:
Pr
ef

+
Suf

The
principle
is
often
used
for
w
ord
segmen
tation
(Pirelli,
		),
(Yv
on,
		b).
A
w
ord
is
split
in
to
a
prex
and
sux
only
if
one
can
nd
in
the
corpus
another
three
w
ords
forming
the
prop
ortion
in
Equation
,
for
example,
sle
ep-s,
sle
ep-ing,
r
e
ad-s,
r
e
ad-ing.
A
simpler
approac
h
using
just
one
ro
w
or
column
of
this
pattern
can
b
e
also
adopted
(Brill,
		),
(Mikheev,
		).
Ho
w
ev
er,
using
the
complete
analogy
principle
helps
lter
out
the
n
umerous
spurious
segmen
tations
generated
when
prexes
or
suxes
are
factored
out
of
pairs
of
w
ords.
F
or
instance,
although
the
prex
on-
app
ears
in
the
w
ords
onyx
and
ontolo
gy,
one
w
ould
nev
er
segmen
t
on-yx
if
the
corpus
do
es
not
con
tain
the
w
ords
Pr
ef+`yx',
Pr
ef+`tolo
gy'
for
an
y
v
alue
of
Pr
ef.
There
are
situations
when
the
principle
of
analogy
will
fail
ev
en
if
there
is
a
v
ery
ob
vious
segmen
tation
at
hand.
A
lo
ok
at
the
follo
wing
example

sho
ws
that,
for
instance,
W

cannot
b
e
segmen
ted,
as
the
w
ord
Pr
ef

+
Suf

is
missing
in
the
data.
W

=
Pr
ef

+
Suf

W

=
Pr
ef

+
Suf

()
W

=
Pr
ef

+
Suf

W

=
Pr
ef

+
Suf

W

=
Pr
ef

+
Suf

W

=
Pr
ef

+
Suf


Existing
Applications
of
ILP
to
Morphology
Learning
ILP
has
b
een
applied
to
t
w
o
t
yp
es
of
morphology-related
learning
tasks
so
far,
learning
of
inectional
morphology
and
w
ord
segmen
tation.
In
our
new
taxon-
om
y
,
these
tasks
are
enco
ded
as
(O

;
C

)
!
(O

;
C

)
and
(O

;
C

)
!
(O

;
C

)
resp
ectiv
ely
.
.
Learning
of
Inectional
Morphology
Most
ILP
applications
to
w
ord
morphology
ha
v
e
aimed
to
learn
a
theory
matc
h-
ing
pairs
of
w
ord-forms
of
the
same
lexical
en
try
(lemma),
eac
h
of
whic
h
cor-
resp
onds
to
a
certain
set
of
morphosyn
tactic
categories.
The
setting
can
b
e
sc
hematically
represen
ted
as
W

/Cat

:
W

/Cat

,
for
instance
sleep/V
erbPresen
t:
slept/V
erbP
ast.
T
ypically
,
one
of
the
forms
pro
vided
(e.g.
W

)
is
the
standard
lexical
en
try
,
whic
h
has
for
eac
h
language
and
part
of
sp
eec
h
a
xed
set
of
morphosyn
tactic
categories,
e.g.
for
the
Czec
h
Noun
it
is
Case=Nominativ
e,
Gender=Masculine,

Num
b
er=Singular.
That
allo
ws
for
the
data
to
b
e
represen
ted
as
a
set
of
facts
of
a
binary
predicate
whic
h
has
the
form
cat(LexEntry,W
)
.
There
are
t
w
o
v
ariations
of
the
task,
()
giv
en
the
lexical
en
try
nd
the
inected
form,
or,
()
giv
en
the
inected
form
nd
the
lexical
en
try
.
In
ILP
terms,
it
corresp
onds
to
learning
the
target
predicate
with
mo
des
cat(+,-)
or
cat(-,+),
resp
ectiv
ely
.
Existing
w
ork
includes
learning
of
Dutc
h
dimin
utiv
es
(Blo
c
k
eel,
		),
En-
glish
P
ast
T
ense
(Mo
oney
&
Cali,
		),
(Muggleton
&
Bain,
			),
Slo
v
ene
nominal
paradigms
(D

zeroski
&
Erja
v
ec,
		),
and
nominal
and
adjectiv
al
paradigms
in
English,
Romanian,
Czec
h,
Slo
v
ene
and
Estonian
(Manandhar
et
al.,
		).
In
all
but
one
case,
a
rst-order
decision
list
learner
has
b
een
used,
the
exception
b
eing
the
application
of
Progol
and
Analogical
Prediction
to
English
P
ast
T
ense
data
b
y
Muggleton
and
Bain
(			).
The
rules
learned
are
based
on
the
presence
of
one
or
t
w
o
axes
in
the
w
ord.
F
or
instance,
one
could
ha
v
e
rules
of
the
follo
wing
template:
(Stem
+
Suf

)=Cat

:
(Stem
+
Suf

)=Cat

()
If
the
concatenation
of
a
giv
en
stem
and
sux
Suf

is
a
w
ord-form
corre-
sp
onding
to
category
Cat

,
then
b
y
replacing
Suf

with
Suf

one
can
ob-
tain
the
w
ord-form
corresp
onding
to
category
Cat

,
e.g.
(appl+y)/Pr
esent:
(appl+ie
d)/Past.
The
English
P
ast
T
ense
data
set
has
b
ecome
a
standard
test
b
ed
for
b
oth
ILP
(Mo
oney
&
Cali,
		),
(Muggleton
&
Bain,
			)
and
non-ILP
learn-
ing
approac
hes
related
to
morphology
(Rumelhart
&
McClelland,
	),
(Ling
&
Marino
v,
		).
The
w
ork
of
Rumelhart
and
McClelland
(	)
is
the
rst
in
a
series
of
articles
to
apply
learning
to
the
task
of
pro
ducing
the
past
tense
of
English
v
erbs
from
their
presen
t
form.
The
simple
p
erceptron-based
learner
used
b
y
them
w
as
follo
w
ed
b
y
a
more
sophisticated
use
of
Articial
Neural
Net-
w
orks
(ANN)
b
y
MacWhinney
and
Lein
bac
h
(		).
Ling
and
Marino
v
(		)
ha
v
e
subsequen
tly
applied
the
Sym
b
olic
P
attern
Asso
ciator
(SP
A),
a
C.-based
learner,
to
the
same
task.
The
principle
of
analogy
has
also
b
een
used
b
y
Yv
on
(		a).
The
data
consists
of
pairs
of
presen
t
and
past
tense
forms
of
the
same
v
erb,
for
instance,
(sle
ep,
slept)
.
It
comes
in
t
w
o
a
v
ours,
alphab
etic,
using
standard
sp
elling,
and
phonetic,
reecting
the
pron
unciation.
Direct
comparison
of
learn-
ing
metho
ds
is
made
dicult
b
y
the
fact
that
dieren
t
approac
hes
do
not
use
training
and
test
data
of
the
same
size
and
distribution
of
regular
and
irregular
v
erb
forms.
F
or
instance,
for
a
sample
of
00
training
and
0
test
v
erb
forms
sp
elt
phonetically
,
SP
A
sho
ws
	%
predictiv
e
accuracy
as
compared
to
the
0%
of
MacWhinney's
ANN
(Ling,
		).
The
ANN
here
is
an
impro
v
ed
v
ersion
of
the
one
previously
published
b
y
MacWhinney
and
Lein
bac
h
(		).
The
original
ANN
ac
hiev
es
.%
accuracy
on
a
sample
of
00
training
and
00
test
en
tries
in
phonetic
represen
tation
as
compared
to
Ling's
.%.
The
ILP
learner
F
OIL
outp
erforms
SP
A
on
the
same
data
with
.%
(Quinlan,
		).
The
alphab
etical
data
set
with
the
same
00/00
training-to-test
sample
ratio
allo
ws
for
the
b
est
comparison
of
ILP
metho
ds
applied
to
the
task.
T
able

sho
ws

the
results
for
F
oidl
,
IF
OIL
and
F
OIL
rep
orted
b
y
Mo
oney
and
Cali
(		)
(Quinlan's
result
with
F
OIL
for
the
same
data
is
.%).
Results
with
standard
Progol
learning
pure
logic
programs
and
Progol
using
Analogical
Prediction
ha
v
e
b
een
rep
orted
b
y
Muggleton
and
Bain
(			).
All
results
should
b
e
compared
to
the
ma
jorit
y
class
baseline
of
	.%
(P
ast
=
Present+`ed').
Comparison
sho
ws
that
ILP
o
v
erp
erforms
the
other
approac
hes.
Within
ILP
,
Analogical
Prediction
ac
hiev
es
the
b
est
results
(Muggleton
&
Bain,
			).
T
able
.
Learning
of
English
past
tense
non-ILP
learning
ILP
learning
Data
set
Neural
SP
A
F
OIDL
IF
OIL
F
OIL
AP
PR
OGOL
net
w
ork
Phonetic
.
.
N/A
N/A
.
N/A
N/A
Alphab
etic
N/A
N/A

0

	.	
.0
.
Hybrid
Learning
of
W
ord
Segmen
tation
Rules
Kazak
o
v
and
Manandhar
(		)
ha
v
e
prop
osed
a
h
ybrid
approac
h
to
the
segmen-
tation
of
an
unannotated
list
of
w
ords
in
whic
h
the
w
ords
are
initially
segmen
ted
b
y
applying
the
Na

v
e
Theory
of
Morphology
bias,
and
then
a
rst-order
deci-
sion
list
of
segmen
tation
rules
is
learned
with
the
help
of
either
F
oidl
or
Clog.
These
rules
sho
w
a
go
o
d
degree
of
generalisation,
and
can
b
e
used
for
the
seg-
men
tation
of
unkno
wn
w
ords.
A
c
haracteristic
feature
of
the
metho
d
is
that
the
searc
h
tec
hnique
(GA)
used
in
the
unsup
ervised
learner
ma
y
not
b
e
able
to
nd
the
b
est
segmen
tations
w.r.t.
the
bias.
The
w
ords
that
are
not
segmen
ted
in
an
optimal
w
a
y
are
often
co
v
ered
b
y
an
exception
in
the
decision
list,
that
is,
a
rule
applicable
to
just
one
w
ord
with
no
p
oten
tial
for
generalisation.
Remo
ving
these
exceptions
from
the
decision
list
do
es
not
inuence
its
p
erformance
on
other,
unseen,
w
ords,
and
the
application
of
the
remaining
rules
in
the
decision
list
to
the
training
list
of
w
ords
usually
results
in
segmen
tations
that
are
b
etter
b
oth
w.r.t.
the
bias,
and
from
a
linguistic
p
oin
t
of
view.
Exp
erimen
ts
with
the
learning
of
segmen
tation
rules
for
Slo
v
ene
ha
v
e
sho
wn
that
the
segmen
ts
pro
duced
for
eac
h
w
ord
are
strongly
correlated
with
its
morpho-
syn
tactic
categories,
th
us
supp
orting
the
h
yp
othesis
that
these
segmen
ts
can
b
e
in
terpreted
as
morphemes
(Kazak
o
v
et
al.,
			).

F
uture
W
ork
The
taxonom
y
in
tro
duced
in
this
article
denes
automatically
a
whole
space
of
learning
tasks
most
of
whic
h
remain
unexplored.
The
follo
wing
list
con
tains
the
dening
v
ectors
of
some
of
them
and
their
p
ossible
in
terpretation.

(O

;
C

)
!
(O

;
C

)
Replace
NTM
with
other
biases,
suc
h
as
Deligne's,
Harris's
or
analogy
,
and
learn
w
ord
segmen
tation
rules.
(O

;
C

)
!
(O

;
C

)
In
v
en
t
new
predicates
to
b
e
used
as
rule
templates
in
task
(O

;
C

)
!
(O

;
C

).
The
template
in
Equation

(page
)
giv
es
a
go
o
d
idea
of
the
t
yp
e
of
templates
one
could
exp
ect
to
learn.
(O

;
C

)
!
(O

;
C

)
Use
some
basic
linguistic
concepts,
suc
h
as
consonan
ts,
v
o
w
els,
semiv
o
w
els,
diph
thongs
(unary
predicates)
or
the
p
ossibilit
y
of
map-
ping
sev
eral
morphs
on
to
a
single
lexical-lev
el
morpheme
(a
binary
predicate
with
app
end/
literals
in
the
b
o
dy)
to
learn
a
mo
del
of
a
giv
en
language
that
is
a
more-than-one-lev
el
morphology
.
(O

;
C

)
!
(O

;
C

)
Same
as
ab
o
v
e,
but
use
the
t
w
o-lev
el
morphology
and,
p
os-
sibly
,
relev
an
t
rule
templates
as
bac
kground
kno
wledge.
(O

;
C

)
!
(O

;
C

)
Learn
the
t
w
o-lev
el
morphology
mo
del
of
language
L
from
w
ords
tagged
with
their
morphosyn
tactic
categories.
Use
a
com
bination
of
unsup
ervised
learning
to
segmen
t
the
w
ords
(Harris,
Na

v
e
Theory
of
Mor-
phology)
and
ILP
.
(O

;
C

)
!
(O

;
C

)
Learn
the
t
w
o-lev
el
morphology
mo
del
of
language
L
from
w
ords
tagged
with
their
lexical-lev
el
morphemes
and
morphosyn
tactic
cate-
gories.
This
kind
of
annotation
is
the
t
ypical
output
of
a
t
w
o-lev
el
morpho-
lexical
analyser.
Here
is
a
transliterated
example
of
so
analysed
T
urkish
w
ord:
['CIkarIlmakla',[[cat:noun,s
tem:
[cat
:verb
,roo
t:'CI
K',v
oice
:caus
,
voice:pass,sense:pos],suffix
:mak
,typ
e:inf
init
ive,a
gr:'
SG'
,poss
:
'NONE',case:ins]]].
(O

;
C

)
!
(O

;
C
+
)
Start
with
an
incomplete
and/or
inaccurate
t
w
o-lev
el
de-
scription
of
morphology
and
learn
the
complete
and
correct
set
of
rules.
This
is
a
particularly
in
teresting
application,
as
it
can
help
to
adapt
an
existing
t
w
o-lev
el
mo
del
of
the
language
norm
to
accoun
t
for
the
p
eculiarities
of
a
v
ernacular.
(O

;
C

)
!
(O

;
C

)
Learn
metarules
(rule
templates)
from
inection
or
segmen-
tation
rules.
The
latter
can
b
e
pro
vided
b
y
linguist,
or
obtained
in
previous
learning
steps.
(O

;
C

)
!
(O

;
C

)
Use
the
existing
t
w
o-lev
el
mo
dels
of
the
morphology
of
sev-
eral
languages,
e.g.
F
renc
h,
Spanish
and
Italian,
to
learn
a
common
set
of
rules
and/or
templates
p
oten
tially
relev
an
t
to
a
family
of
languages
(e.g.
Romance
languages).

Conclusions
This
pap
er
describ
es
some
existing
w
ork
in
ILP
learning
of
w
ord
morphology
,
and
iden
ties
the
issues
addressed
so
far.
In
the
con
text
of
the
curren
t
ac
hiev
e-
men
ts
in
computational
morphology
,
it
seems
that
com
bining
ILP
with
existing
linguistic
theories,
rather
than
substituting
it
for
them,
w
ould
lead
to
appli-
cations
whic
h
w
ould
c
hallenge
the
ILP
comm
unit
y
and
attract
more
atten
tion
from
computational
linguists.
Researc
h
in
this
direction
could
b
e
help
ed
b
y
the

T
able
.
Belo
w:
space
of
ML
applications
to
w
ord
morphology
.
Ab
o
v
e:
preference
biases
used
to
augmen
t
training
data
annotation.
- consonants, vowels, 
semivowels, diphthongs;
- morphemes: lexical vs
surface level
No annotation
Segmentation
No annotation
Segmentation
No annotation
Segmentation
No annotation
Segmentation
Words annotated with
lex. level morphemes
and categories
2L framework for 
Romance languages
Language L
(Word,Cat) in
- 2L morphology rules
of language L
CONCEPT
LANGUAGE
palatalisation, nasal
- specific rules: vowel raising,
and rules
tool T for language L
Conjugation / 
declension rules
No annotation
Segmentation
Segmentation
PREFERENCE BIAS
Segmentation and
lexical level morphemes
No annotation
Morphosyntactic
categories
Two-level morphology
Two-level morphology
More annotation
No annotation
?
segmentation
Deligne’s text
Harris
NTM
Brent et al.
strings
Some annotation
- segmentation or
- morphosyntactic
    categories, etc.
DATA LANGUAGE
TRAINING
Words
Words
Slovene nominal
English Past Tense,
paradigms
Implementation of
Abstract machines
Basic linguistic concepts
Relations between
- 2L morphology: finite state
transducers, rule templates
Rule templates
Segmentation (Harris)
morphosynt. cat.
Pronunciation or
- append, prefix, suffix
analogy
de Saussure’s
assimilation
Words
Two-level morphology
of French, Spanish,
Italian...
Segmentation rules
Segmentation (NTM)
* incomplete/inconsistent
* complete/accurate
C1
C2
C3
C4
O2
additional use of
- segmentation or
- morphosyntactic
    categories or
- lex. level morphemes
O3
O1

dev
elopmen
t
of
standard
ILP
libraries
of
bac
kground
predicates
represen
ting
linguistic
concepts
or
theories.
Also,
the
in
tegration
of
ILP
with
metho
ds
for
unsup
ervised
learning
in
a
h
ybrid
framew
ork
w
ould
increase
the
abilit
y
of
ILP
to
learn
from
imp
o
v
erished
data
con
taining
little
or
no
annotation.
The
newly
in
tro
duced
taxonom
y
of
learning
approac
hes
to
morphology
describ
es
a
v
ariet
y
of
tasks
that
ha
v
e
not
b
een
addressed
y
et,
and
could
serv
e
as
an
inspiration
for
future
researc
h
in
the
eld.
Bibliograph
y
An
t
w
orth,
E.
(		).
In
tro
duction
to
t
w
o-lev
el
phonology
.
Notes
on
Linguistics,
,
pp.
{.
Bender,
B.
W.
(		).
L
atin
V
erb
Ine
ction.
Univ
ersit
y
of
Ha
w
ai`i,
h
ttp://www.ha
w
aii.edu/b
ender/Latin
V
erb.h
tml.
Blo
c
k
eel,
H.
(		).
Application
of
inductiv
e
logic
programming
to
natural
lan-
guage
pro
cessing.
Master's
thesis,
Katholiek
e
Univ
ersiteit
Leuv
en,
Bel-
gium.
Bren
t,
M.,
Lundb
erg,
A.,
&
Murth
y
,
S.
(		).
Disco
v
ering
morphemic
suxes:
A
case
study
in
minim
um
description
length
induction.
In
Pr
o
c.
of
the
Fifth
International
Workshop
on
A
rticial
Intel
ligenc
e
and
Statistics.
Brill,
E.
(		).
Some
adv
ances
in
transformation-based
part
of
sp
eec
h
tagging.
In
Pr
o
c.
of
the
Twelfth
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
{.
AAI
Press/MIT
Press.
Cali,
M.
E.,
&
Mo
oney
,
R.
J.
(		).
Adv
an
tages
of
decision
lists
and
implicit
negativ
es
in
inductiv
e
logic
programming.
T
ec
h.
rep.,
Univ
ersit
y
of
T
exas
at
Austin.
Chomsky
,
N.,
&
Halle,
M.
(	).
The
Sound
Pattern
of
English.
Harp
er
and
Ro
w,
New
Y
ork.
Cussens,
J.
(		).
P
art-of-sp
eec
h
tagging
using
Progol.
In
Pr
o
c.
of
the
Seventh
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
pp.
	{0.
de
Saussure,
F.
(	).
Course
in
Gener
al
Linguistics
(		
edition).
Philo-
sophical
Library
,
New
Y
ork.
Deligne,
S.
(		).
Mo
d

eles
de
s

equenc
es
de
longueurs
variables:
Applic
ation
au
tr
aitement
du
langage

ecrit
et
de
la
p
ar
ole.
Ph.D.
thesis,
ENST
P
aris,
F
rance.
D

zeroski,
S.,
&
Erja
v
ec,
T.
(		).
Induction
of
Slo
v
ene
nominal
paradigms.
In
Pr
o
c.
of
the
Seventh
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
am-
ming,
pp.
{
Prague,
Czec
h
Republic.
Springer.
F
radin,
B.
(		).
L'appro
c
he

a
deux
niv
eaux
en
morphologie
computationnelle
et
les
d

ev
elopmen
ts
r

ecen
ts
de
la
morphologie.
T
r
aitement
automatique
des
langues,

(),
	{.
Harris,
Z.
S.
(	).
F
rom
phoneme
to
morpheme.
L
anguage,

().
Kaplan,
R.
M.,
&
Ka
y
,
M.
(		).
Regular
mo
dels
of
phonological
rule
systems.
Computational
Linguistics,
0
(),
{	.

Kazak
o
v,
D.
(		).
Unsup
ervised
learning
of
na

v
e
morphology
with
genetic
algorithms.
In
Daelemans,
W.,
Bosc
h,
A.,
&
W
eijters,
A.
(Eds.),
Workshop
Notes
of
the
ECML/MLnet
Workshop
on
Empiric
al
L
e
arning
of
Natur
al
L
anguage
Pr
o
c
essing
T
asks,
pp.
0{
Prague,
Czec
h
Republic.
Kazak
o
v,
D.
(			).
Com
bining
lapis
and
W
ordNet
for
the
learning
of
LR
parsers
with
optimal
seman
tic
constrain
ts.
In
D

zeroski,
S.,
&
Flac
h,
P
.
(Eds.),
Pr
o
c.
of
The
Ninth
International
Workshop
on
Inductive
L
o
gic
Pr
o-
gr
amming,
pp.
0{
Bled,
Slo
v
enia.
Springer-V
erlag.
Kazak
o
v,
D.,
&
Manandhar,
S.
(		).
A
h
ybrid
approac
h
to
w
ord
segmen
ta-
tion.
In
P
age,
D.
(Ed.),
Pr
o
c.
of
the
Eighth
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
pp.
{
Madison,
Wisconsin.
Springer-
V
erlag.
Kazak
o
v,
D.,
Manandhar,
S.,
&
Erja
v
ec,
T.
(			).
Learning
w
ord
segmen
tation
rules
for
tag
prediction.
In
D

zeroski,
S.,
&
Flac
h,
P
.
(Eds.),
Pr
o
c.
of
The
Ninth
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
pp.
{

Bled,
Slo
v
enia.
Springer-V
erlag.
Khan,
K.,
Muggleton,
S.,
&
P
arson,
R.
(		).
Rep
eat
learning
using
predicate
in
v
en
tion.
In
P
age,
D.
(Ed.),
Pr
o
c.
of
the
Eighth
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
pp.
{
Madison,
Wisconsin.
Springer-
V
erlag.
Kosk
enniemi,
K.
(	).
Two-level
morpholo
gy:
A
Gener
al
Computational
Mo
del
for
Wor
d-F
orm
R
e
c
o
gnition
and
Pr
o
duction.
Univ
ersit
y
of
Helsinki,
Dept.
of
General
Linguistics,
Finland.
Lancelot,
C.,
&
Arnauld,
A.
(0).
Gr
ammair
e
g

en

er
ale
et
r
aisonn

ee
(de
Port-
R
oyal)
(	
facsimile
edition).
The
Scolar
Press,
Menston,
England.
La
vra

c,
N.,
&
D

zeroski,
S.
(		).
Inductive
L
o
gic
Pr
o
gr
amming
T
e
chniques
and
Applic
ations.
Ellis
Horw
o
o
d,
Chic
hester.
Ling,
C.
X.
(		).
Learning
the
past
tense
of
English
v
erbs:
The
sym
b
olic
pat-
tern
asso
ciatior
vs.
connectionist
mo
dels.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
,
0	{	.
Ling,
C.
X.,
&
Marino
v,
M.
(		).
Answ
ering
the
connectionist
c
hallenge:
A
sym
b
olic
mo
del
of
learning
the
past
tense
of
English
v
erbs.
Co
gnition,
	
(),
{	0.
MacWhinney
,
B.,
&
Lein
bac
h,
J.
(		).
Implemen
tations
are
not
conceptual-
izations:
Revising
the
v
erb
mo
del.
Co
gnition,
0,
	{	.
Manandhar,
S.,
D

zeroski,
S.,
&
Erja
v
ec,
T.
(		).
Learning
Multilingual
Mor-
phology
with
CLOG.
In
P
age,
D.
(Ed.),
Pr
o
c.
of
The
Eighth
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
pp.
{
Madison,
Wis-
consin.
Matthews,
P
.
H.
(	).
Morpholo
gy:
an
Intr
o
duction
to
the
The
ory
of
Wor
d-
Structur
e
(First
edition).
Cam
bridge
Univ
ersit
y
Press.
Matthews,
P
.
H.
(		).
The
Concise
Oxfor
d
Dictionary
of
Linguistics.
Oxford
Univ
ersit
y
Press.
Mikheev,
A.
(		).
Automatic
rule
induction
for
unkno
wn
w
ord
guessing.
Com-
putational
Linguistics,

(),
0{.
Mitc
hell,
T.
M.
(		).
Machine
L
e
arning.
McGra
w-Hill,
New
Y
ork.

Mo
oney
,
R.
J.,
&
Cali,
M.
E.
(		).
Induction
of
rst{order
decision
lists:
Results
on
learning
the
past
tense
of
English
v
erbs.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
,
{.
Muggleton,
S.
(		).
Adv
ances
in
ILP
theory
and
implemen
tations.
In
P
age,
C.
(Ed.),
Pr
o
c.
of
The
Eighth
International
Workshop
on
Inductive
L
o
gic
Pr
o-
gr
amming,
p.
	
Madison,
Wisconsin.
Springer-V
erlag.
Abstract
of
k
eynote
presen
tation.
Muggleton,
S.,
&
Bain,
M.
(			).
Analogical
prediction.
In
Pr
o
c.
of
the
Ninth
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming
Bled,
Slo
v
enia.
Springer-V
erlag.
Pirelli,
V.
(		).
Morpholo
gy,
A
nalo
gy
and
Machine
T
r
anslation.
Ph.D.
thesis,
Salford
Univ
ersit
y
,
UK.
Quinlan,
J.
R.
(		).
P
ast
tenses
of
v
erbs
and
rst-order
learning.
In
Deb
en-
ham,
J.,
&
Luk
ose,
D.
(Eds.),
Pr
o
c
e
e
dings
of
the
Seventh
A
ustr
alian
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
{0
Singap
ore.
W
orld
Scien-
tic.
Quinlan,
J.
(		0).
Learning
logical
denitions
from
relations.
Machine
L
e
arning,
,
	{.
Rumelhart,
D.
E.,
&
McClelland,
J.
(	).
Par
al
lel
Distribute
d
Pr
o
c
essing,
V
ol.
I
I,
c
hap.
On
Learning
the
P
ast
T
ense
of
English
V
erbs,
pp.
{.
MIT
Press,
Cam
bridge,
MA.
Shannon,
C.
E.,
&
W
ea
v
er,
W.
(	).
The
Mathematic
al
The
ory
of
Communi-
c
ation.
Univ
ersit
y
of
Illinois
Press,
Urbana.
Shieb
er,
S.
(	).
A
n
intr
o
duction
to
unic
ation-b
ase
d
appr
o
aches
to
gr
am-
mar.
No.

in
CSLI
Lecture
Notes.
Cen
ter
for
the
Study
of
Language
and
Information,
Stanford,
CA.
v
an
den
Bosc
h,
A.
(		).
L
e
arning
to
Pr
onounc
e
Written
Wor
ds:
A
Study
in
Inductiv
e
Language
Learning.
Ph.D.
thesis,
Univ
ersit
y
of
Maastric
h
t,
The
Netherlands.
v
an
den
Bosc
h,
A.,
Daelemans,
W.,
&
W
eijters,
T.
(		).
Morphological
analysis
as
classication:
an
inductiv
e
learning
approac
h.
In
Oazer,
K.,
&
Somers,
H.
(Eds.),
Pr
o
c.
of
NeMLaP-,
pp.
	{	
Ank
ara,
T
urk
ey
.
Yv
on,
F.
(		a).
P
ersonal
comm
unication.
Yv
on,
F.
(		b).
Pr
ononc
er
p
ar
analo
gie:
motivations,
formalisations
et

evaluations.
Ph.D.
thesis,
ENST
P
aris,
F
rance.
Zelle,
J.
M.,
&
Mo
oney
,
R.
J.
(		).
Learning
seman
tic
grammars
with
con-
structiv
e
inductiv
e
logic
programming.
In
Pr
o
c
e
e
dings
of
AAAI-	,
pp.
{.
AAI
Press/MIT
Press.

