POSE AND MOTION ESTIMATION FROM VISION
USING DUAL QUATERNION-BASED EXTENDED
KALMAN FILTERING
A Dissertation
Presented for the
Doctor of Philosophy
Degree
The University of Tennessee, Knoxville
James Samuel Goddard, Jr.
December 1997

Copyright c⃝1997 by James Samuel Goddard, Jr.
All rights reserved
ii

ACKNOWLEDGEMENTS
I would like to thank my adviser, Dr. Mongi Abidi, whose continued encouragement
and guidance have enabled me to bring this extensive eﬀort to a successful conclusion. In
addition, I would like to express gratitude to my committee members, Drs. Rajiv Dubey,
Walter Green, Michael Roberts, and Ross Whitaker for their extremely helpful assistance
and comments while completing my dissertation. Lastly, I would like to express special
appreciation to my wife and children for their support and patience during this long
process.
iii

ABSTRACT
Determination of relative three-dimensional (3–D) position, orientation, and rela-
tive motion between two reference frames is an important problem in robotic guidance,
manipulation, and assembly as well as in other ﬁelds such as photogrammetry. A solution
to this problem that uses two-dimensional (2–D), intensity images from a single camera
is desirable for real-time applications. Where the object geometry is unknown, the esti-
mation of structure is also required. A single camera is advantageous because a standard
video camera is low in cost, setup and calibration are simple, physical space requirements
are small, reliability is high, and low-cost hardware is available for digitizing and processing
the images. A diﬃculty in performing this measurement is the process of projecting 3–D
object features to 2–D images, a nonlinear transformation. Noise is present in the form of
perturbations to the assumed process dynamics, imperfections in system modeling, and
errors in the feature locations extracted from the 2–D images. This dissertation presents
solutions to the remote measurement problem for a dynamic system given a sequence of
2–D intensity images of an object where feature positions of the object are known relative
to a base reference frame and where the feature positions are unknown relative to a base
reference frame.
The 3–D transformation is modeled as a nonlinear stochastic system
with the state estimate providing six degree-of-freedom motion and position values. The
stochastic model uses the iterated extended Kalman ﬁlter as an estimator and as a screw
representation of the 3–D transformation based on dual quaternions. Dual quaternions
provide a means to represent both rotation and translation in a uniﬁed notation. The
method has been implemented and tested with both simulated and actual experimental
data. Simulation results are provided along with comparisons to a point-based method
using rotation and translation to show the relative advantages of the proposed method.
Experimental test results using a camera mounted on the end eﬀector of a robot arm to
demonstrate relative motion and position control are also presented.
iv

TABLE OF CONTENTS
Chapter 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Related Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.1
Pose Using Point-Based Methods . . . . . . . . . . . . . . . . .
8
1.3.2
Methods for Real-Time Pose Determination . . . . . . . . . . .
10
1.3.3
Pose Using Model-Based and Higher-Level Primitives Methods
17
1.3.4
Methods Incorporating Noise Estimation and Uncertainty . . .
26
1.3.5
Methods Using Kalman Filtering for Direct Pose and Motion
Estimation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.4
Synopsis of Dissertation . . . . . . . . . . . . . . . . . . . . . . . . . .
33
Chapter 2. Theoretical Foundation . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.1
Coordinate Frame Transformations . . . . . . . . . . . . . . . . . . . .
35
2.2
Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.3
Dual Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.4
Dual Number Quaternions . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.5
3-D to 2-D Perspective Projection
. . . . . . . . . . . . . . . . . . . .
41
2.6
Representation of 3-D Rotation by Quaternions . . . . . . . . . . . . .
43
2.7
Representation of 3–D Rotation and Translation by Dual Number
Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.8
Iterated Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . .
47
v

Chapter 3. Pose and Motion Estimation Method . . . . . . . . . . . . . . . . . . .
54
3.1
Pose and Motion Estimation Using IEKF and Dual Number Quaternions
55
3.2
State Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.3
System Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.4
Measurement Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.5
Representation of Lines in a Plane
. . . . . . . . . . . . . . . . . . . .
62
3.6
Linearization of the State Transition Function . . . . . . . . . . . . . .
65
3.7
Linearization of the Measurement Function
. . . . . . . . . . . . . . .
67
3.8
Iterated Extended Kalman Filter Representation
. . . . . . . . . . . .
72
3.9
Extension of Iterated Extended Kalman Filter to Estimate Structure .
73
3.9.1
Structure Representation
. . . . . . . . . . . . . . . . . . . . .
73
3.9.2
Extension to State Transition . . . . . . . . . . . . . . . . . . .
74
3.9.3
Extension to Measurement Update . . . . . . . . . . . . . . . .
74
Chapter 4. Noise Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.1
Analysis of Line Noise . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.1.1
Probability Density Function of Line-Point Parameters . . . . .
77
4.1.2
Covariance of xlp and ylp
. . . . . . . . . . . . . . . . . . . . .
87
4.2
Real-Time Implementation
. . . . . . . . . . . . . . . . . . . . . . . .
93
Chapter 5. Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5.1
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5.1.1
Dynamic Model . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.1.2
Model Trajectory . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.1.3
Camera Model
. . . . . . . . . . . . . . . . . . . . . . . . . . .
100
5.1.4
Object Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
5.1.5
Estimation Model
. . . . . . . . . . . . . . . . . . . . . . . . .
101
vi

5.1.6
Comparison Model . . . . . . . . . . . . . . . . . . . . . . . . .
102
5.1.7
Initial Conditions . . . . . . . . . . . . . . . . . . . . . . . . . .
102
5.1.8
Measurement Noise Model . . . . . . . . . . . . . . . . . . . . .
102
5.1.9
Test Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
5.1.10 Simulation Using Adaptive Noise Estimation
. . . . . . . . . .
130
5.2
Robot Arm Tests
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.2.1
Experimental Setup
. . . . . . . . . . . . . . . . . . . . . . . .
136
5.2.2
Dynamic Model . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
5.2.3
Measurement Model . . . . . . . . . . . . . . . . . . . . . . . .
136
5.2.4
Initial Conditions . . . . . . . . . . . . . . . . . . . . . . . . . .
139
5.2.5
Measurement Noise Results . . . . . . . . . . . . . . . . . . . .
139
5.2.6
Target Motion Results . . . . . . . . . . . . . . . . . . . . . . .
139
5.3
Test Results From Unknown Object Geometry
. . . . . . . . . . . . .
148
5.3.1
Structure Estimation . . . . . . . . . . . . . . . . . . . . . . . .
148
5.3.1.1
Calculation of Actual Structure for Reference
. . . .
150
5.3.1.2
Initial State Estimate . . . . . . . . . . . . . . . . . .
151
5.3.1.3
Transformation of Structure Estimate Frame to Ref-
erence Frame
. . . . . . . . . . . . . . . . . . . . . .
153
5.3.2
Response to Step Change in Motion
. . . . . . . . . . . . . . .
154
Chapter 6. Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . .
159
6.1
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
6.2
Conclusions and Contributions
. . . . . . . . . . . . . . . . . . . . . .
160
6.3
Future Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
Vita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
vii

LIST OF TABLES
5.1
Actual and assumed initial states of the extended Kalman ﬁlter for the
four-point simulation tests. The initial states are the same for both the
dual quaternion method and the point method. . . . . . . . . . . . . . . .
104
5.2
Dual quaternion method and point method initial error covariance matrix
and process noise matrix diagonal terms of the extended Kalman ﬁlter for
the four-point simulation tests. . . . . . . . . . . . . . . . . . . . . . . . .
104
5.3
Actual and assumed initial states of the dual quaternion and point-based
extended Kalman ﬁlter for simulation comparison with the adaptive mea-
surement noise line method, the nonadaptive line method, and the com-
parison point method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
5.4
Dual quaternion method and point method initial error covariance matrix
and process noise matrix diagonal terms of the extended Kalman ﬁlter for
simulation comparison with the adaptive measurement noise line method,
the nonadaptive line method, and the comparison point method. . . . . .
131
5.5
Reference pipe line data as calculated by two image views separated by an
x translation of 50 mm. The data show the ⃗l and ⃗m components of the
3–D lines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
viii

LIST OF FIGURES
1.1
Transformation between two reference frames illustrating the fundamental
problem to be solved.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Block diagram of a robot arm feedback control loop showing an application
for pose and motion estimation. A camera mounted on the end eﬀector of
the robot arm is used as the measurement sensor.
. . . . . . . . . . . . .
6
2.1
3–D to 2–D perspective projection model assumed in pose estimation prob-
lem. The center of projection is the camera reference origin. . . . . . . . .
42
2.2
Diagram showing screw form of 3–D transformation between two reference
frames. Eight parameters are needed to specify the complete transforma-
tion.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.3
Extended Kalman ﬁlter block diagram showing functionality along with
the inputs and outputs. The ﬁlter is seen to have a closed-loop feedback
control where the measurement error is minimized. . . . . . . . . . . . . .
52
3.1
Functional block diagram of estimation problem showing operations in-
volved in pose estimation method from optical imaging to state estimate
produced by the iterated extended Kalman ﬁlter. . . . . . . . . . . . . . .
58
3.2
Line point for a line in 2-D image plane is deﬁned as the intersection point
of the line and a perpendicular line passing through the origin. . . . . . .
64
4.1
Diagram showing a point pair with a noise radius of one standard deviation
and the corresponding range of variation of the line connecting the points.
77
ix

4.2
Probability density function of xlp for 1.25 mm. length horizontal line
centered about the y axis, ys= 3.125 mm., point noise 0.02 mm. standard
deviation. A best-ﬁt Gaussian pdf is also plotted, but the diﬀerences are
seen to be small.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
4.3
Probability density function of ylp for 1.25 mm. length horizontal line
centered about the y axis, ys= 3.125 mm., point noise 0.02 mm. standard
deviation. A best-ﬁt Gaussian pdf is also plotted, but the diﬀerences are
seen to be small.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.4
Probability density function of xlp for 1.25 mm. length downward diago-
nal line at xs=12.5 mm., ys= 12.5 mm., point noise 0.02 mm. standard
deviation. A best-ﬁt Gaussian pdf is also plotted, but the diﬀerences are
seen to be small.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
4.5
Probability density function of xlp for 1.25 mm. length upward diagonal
line at xs=12.5 mm., ys= 12.5 mm., point noise 0.02 mm. standard devi-
ation. A best-ﬁt Gaussian pdf is also plotted, but the diﬀerences are seen
to be small. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.6
Variance of line-point parameters for 1.25 mm. horizontal line centered on
the y axis as ys varies, point noise 0.02 mm. standard deviation, compared
with point noise variance. ylp variance is always less than the point vari-
ance while the xlp variance becomes greater after only a small displacement.
91
4.7
Variance of line-point parameters for 1.25 mm. diagonal line (45 degree
angle) centered on the y axis as ys varies, point noise 0.02 mm. standard
deviation, compared with point noise variance. xlp variance is always less
than the point variance while the ylp variance becomes greater after only
a small displacement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
x

5.1
Functional block diagram of the estimation problem showing the opera-
tions involved in the pose estimation method from optical imaging to state
estimate produced by the iterated extended Kalman ﬁlter. . . . . . . . . .
98
5.2
Target shapes and dimensions used in the simulation experiments: (a)
four-point coplanar target, and (b) six-point polyhedral 3-D target. . . . .
101
5.3
tx translation pose estimate simulation results with a four-point target
showing linear translation over time for the line method, the comparison
point method, and the true x translation value from a sample run. Both
methods are seen to give good estimation results for this axis.
. . . . . .
106
5.4
ty translation pose estimate simulation results with a four-point target
showing linear translation over time for the line method, the comparison
point method, and the true y translation value from a sample run. Both
methods are seen to give good estimation results for this axis.
. . . . . .
106
5.5
tz translation pose estimate simulation results with a four-point target
showing linear translation over time for the line method, the comparison
point method, and the true z translation value from a sample run. More
deviation from the true value is seen for this axis than in x and y although
the errors are not large.
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
5.6
q0 quaternion pose estimate simulation results with a four-point target
showing constant rotation over time for the line method, the comparison
point method, and the true q0 quaternion value from a sample run. Both
methods are seen to give good estimation results for this state variable.
.
107
xi

5.7
q1 quaternion pose estimate simulation results with a four-point target
showing constant rotation over time for the line method, the comparison
point method, and the true q1 quaternion value from a sample run. More
deviation from the true value is seen for this axis than in the q0 estimate
with signiﬁcant errors occurring at the beginning and at the end. . . . . .
108
5.8
q2 quaternion pose estimate simulation results with a four-point target
showing constant rotation over time for the line method, the comparison
point method, and the true q2 quaternion value from a sample run. More
deviation from the true value is seen for this axis than in the q0 estimate
with signiﬁcant errors occurring at the beginning and at the end. . . . . .
108
5.9
q3 quaternion pose estimate simulation results with a four-point target
showing constant rotation over time for the line method, the comparison
point method, and the true q3 quaternion value from a sample run. Both
methods are seen to give good estimation results for this state variable.
.
109
5.10 vx translation velocity pose estimate simulation results with a four-point
target showing constant linear velocity over time for the line method, the
comparison point method, and the true vx velocity value of -5 mm./sec.
from a sample run. Both methods are seen to give good estimation results
for this axis after the initial settling time. . . . . . . . . . . . . . . . . . .
109
5.11 vy translation velocity pose estimate simulation results with a four-point
target showing constant linear velocity over time for the line method, the
comparison point method, and the true vy velocity value of 2 mm./sec.
from a sample run. Both methods are seen to give good estimation results
for this axis after the initial settling time. . . . . . . . . . . . . . . . . . .
110
xii

5.12 vz translation velocity pose estimate simulation results with a four-point
target showing constant linear velocity over time for the line method, the
comparison point method, and the true vz velocity value of -5 mm./sec.
from a sample run. The line method estimates are seen to settle faster to
the true value and with less error than the point method.
. . . . . . . . .
110
5.13 ωx angular velocity pose estimate simulation results with a four-point tar-
get showing constant angular velocity over time for the line method, the
comparison point method, and the true ωx angular velocity value of -0.03
rad./sec. from a sample run. Both methods are seen to give good estima-
tion results for this rotation angle after the initial settling time, although,
higher divergence is present for the point method near the end of the
simulation.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
5.14 ωy angular velocity pose estimate simulation results with a four-point tar-
get showing constant angular velocity over time for the line method, the
comparison point method, and the true ωy angular velocity value of 0.05
rad./sec. from a sample run. Both methods are seen to give good estima-
tion results for this rotation angle after the initial settling time, although,
higher divergence is present for the point method near the end of the
simulation.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
5.15 ωz angular velocity pose estimate simulation results with a four-point tar-
get showing constant angular velocity over time for the line method, the
comparison point method, and the true ωz angular velocity value of -0.2
rad./sec. from a sample run. Both methods are seen to give good estima-
tion results for this rotation angle after the initial settling time.
. . . . .
112
xiii

5.16 tx translation pose estimate simulation results for a four-point target show-
ing linear translation estimation error over time for the line method and
the comparison point method from a sample run. The deviation for both
methods is seen to be small with similar errors over the simulation time. .
112
5.17 ty translation pose estimate simulation results for a four-point target show-
ing linear translation estimation error over time for the line method and
the comparison point method from a sample run. The deviation for both
methods is seen to be small with similar errors over the simulation time. .
113
5.18 tz translation pose estimate simulation results for a four-point target show-
ing linear translation estimation error over time for the line method and
the comparison point method from a sample run. The deviation for both
methods is larger than for the x and y translations. Response time is faster
for the line method with somewhat smaller errors over the simulation time. 113
5.19 q0 quaternion pose estimate simulation results for a four-point target show-
ing rotation estimation error over time for the line method and the com-
parison point method from a sample run. The deviation for both methods
is seen to be small with similar errors over the simulation time. . . . . . .
114
5.20 q1 quaternion pose estimate simulation results for a four-point target show-
ing rotation estimation error over time for the line method and the com-
parison point method from a sample run. The deviation for both methods
is seen to be small with similar errors over the simulation time except at
the end where the point method has somewhat larger error. . . . . . . . .
114
xiv

5.21 q2 quaternion pose estimate simulation results for a four-point target show-
ing rotation estimation error over time for the line method and the com-
parison point method from a sample run. The deviation for both methods
is seen to be small with similar errors over the simulation time except at
the end where the point method has somewhat larger error. . . . . . . . .
115
5.22 q3 quaternion pose estimate simulation results for a four-point target show-
ing rotation estimation error over time for the line method and the com-
parison point method from a sample run. The deviation for both methods
is seen to be small with similar errors over the simulation time. . . . . . .
115
5.23 vx translation velocity pose estimate simulation results for a four-point
target showing linear translation velocity estimation error over time for
the line method and the comparison point method from a sample run.
The deviation for both methods is seen to be small; the line method errors
are somewhat less over the simulation time. . . . . . . . . . . . . . . . . .
116
5.24 vy translation velocity pose estimate simulation results for a four-point
target showing linear translation velocity estimation error over time for
the line method and the comparison point method from a sample run.
The deviation for both methods is seen to be small; the line method errors
are somewhat less over the simulation time. . . . . . . . . . . . . . . . . .
116
5.25 vz translation velocity pose estimate simulation results for a four-point
target showing linear translation velocity estimation error over time for
the line method and the comparison point method from a sample run.
The deviation for both methods is seen to be larger than for the x and y
velocity estimates. The line method has faster settling time with somewhat
lower errors than the point method over the simulation time.
. . . . . . .
117
xv

5.26 ωx angular velocity pose estimate simulation results for a four-point target
showing constant angular velocity error over time for the line method and
the comparison point method from a sample run. The deviation for both
methods is relatively large compared with the true velocity, although, the
mean error is near zero.
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.27 ωy angular velocity pose estimate simulation results for a four-point target
showing constant angular velocity error over time for the line method and
the comparison point method from a sample run. The deviation for both
methods is relatively large compared with the true velocity, although, the
mean error is near zero.
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
5.28 ωz angular velocity pose estimate simulation results for a four-point target
showing constant angular velocity error over time for the line method and
the comparison point method from a sample run. The deviation for both
methods is small compared with the true velocity over the simulation time. 118
5.29 tx translation pose estimate simulation results for a four-point target show-
ing linear translation root mean square estimation error over time for the
line method and the comparison point method calculated from 100 sample
runs. The square root of the predicted error covariance from the Kalman
ﬁlter for both methods is also shown. The root mean square error is seen
to be signiﬁcantly greater for the point method than for the line method
over most of the time interval. The Kalman ﬁlter predicted error is close
to the actual error for the line method. . . . . . . . . . . . . . . . . . . . .
120
xvi

5.30 ty translation pose estimate simulation results for a four-point target show-
ing linear translation root mean square estimation error over time for the
line method and the comparison point method calculated from 100 sample
runs. The square root of the predicted error covariance from the Kalman
ﬁlter for both methods is also shown. The root mean square error is seen
to be signiﬁcantly greater for the point method than for the line method
over most of the time period. The Kalman ﬁlter predicted error is close to
the actual error for the line method. . . . . . . . . . . . . . . . . . . . . .
120
5.31 tz translation pose estimate simulation results for a four-point target show-
ing linear translation root mean square estimation error over time for the
line method and the comparison point method calculated from 100 sample
runs. The square root of the predicted error covariance from the Kalman
ﬁlter for both methods is also shown. The root mean square error is seen to
be signiﬁcantly greater for the point method than for the line method over
the last half of the simulated time period. The Kalman ﬁlter predicted
error is close to the actual error for the line method. . . . . . . . . . . . .
121
5.32 q0 quaternion pose estimate simulation results for a four-point target show-
ing rotation root mean square estimation error over time for the line
method and the comparison point method calculated from 100 sample
runs. The square root of the predicted error covariance from the Kalman
ﬁlter for both methods is also shown. The root mean square error is seen
to be small for both methods over the simulated time period. The Kalman
ﬁlter predicted error for the line method greatly overestimates the actual
error.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
xvii

5.33 q1 quaternion pose estimate simulation results for a four-point target show-
ing rotation root mean square estimation error over time for the line
method and the comparison point method calculated from 100 sample
runs. The square root of the predicted error covariance from the Kalman
ﬁlter for both methods is also shown. The root mean square error is seen
to be considerably larger for the point method than the line method over
the simulated time period. The Kalman ﬁlter predicted error for the point
method greatly underestimates the actual error while the predicted error
for the line method is close to the actual error except near the end of the
time period.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
5.34 q2 quaternion pose estimate simulation results for a four-point target show-
ing rotation root mean square estimation error over time for the line
method and the comparison point method calculated from 100 sample
runs. The square root of the predicted error covariance from the Kalman
ﬁlter for both methods is also shown. The root mean square error is seen
to be considerably larger for the point method than the line method over
the simulated time period. The Kalman ﬁlter predicted error for the point
method greatly underestimates the actual error while the predicted error
for the line method is close to the actual error except near the end of the
time period.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
xviii

5.35 q3 quaternion pose estimate simulation results for a four-point target show-
ing rotation root mean square estimation error over time for the line
method and the comparison point method calculated from 100 sample
runs. The square root of the predicted error covariance from the Kalman
ﬁlter for both methods is also shown. The root mean square error is seen
to be small for both methods over the simulated time period. The Kalman
ﬁlter predicted error for the line method greatly overestimates the actual
error.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
5.36 vx translation velocity pose estimate simulation results for a four-point
target showing linear translation root mean square estimation velocity
error over time for the line method and the comparison point method
calculated from 100 sample runs. The square root of the predicted error
covariance from the Kalman ﬁlter for both methods is also shown. The root
mean square error is seen to be signiﬁcantly greater for the point method
than for the line method over most of the time period. The Kalman ﬁlter
predicted error is close to the actual error for the line method.
. . . . . .
124
5.37 vy translation velocity pose estimate simulation results for a four-point
target showing linear translation root mean square estimation velocity
error over time for the line method and the comparison point method
calculated from 100 sample runs. The square root of the predicted error
covariance from the Kalman ﬁlter for both methods is also shown. The root
mean square error is seen to be signiﬁcantly greater for the point method
than for the line method over most of the time period. The Kalman ﬁlter
predicted error is close to the actual error for the line method.
. . . . . .
125
xix

5.38 vz translation velocity pose estimate simulation results for a four-point
target showing linear translation root mean square estimation velocity
error over time for the line method and the comparison point method
calculated from 100 sample runs. The square root of the predicted error
covariance from the Kalman ﬁlter for both methods is also shown. The
root mean square error is seen to be signiﬁcantly greater for the point
method than for the line method, particularly during the initial settling
time. The Kalman ﬁlter predicted error is close to the actual error for the
line method but considerably underestimates the actual error for the point
method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
5.39 ωx angular velocity pose estimate simulation results for a four-point target
showing angular root mean square estimation velocity error over time for
the line method and the comparison point method calculated from 100
sample runs. The square root of the predicted error covariance from the
Kalman ﬁlter for both methods is also shown.
The root mean square
error is seen to be signiﬁcantly greater for the point method than for the
line method over the latter portion of the simulation time. The Kalman
ﬁlter predicted error is close to the actual error for the line method but
considerably underestimates the actual error for the point method. . . . .
127
xx

5.40 ωy angular velocity pose estimate simulation results for a four-point target
showing angular root mean square estimation velocity error over time for
the line method and the comparison point method calculated from 100
sample runs. The square root of the predicted error covariance from the
Kalman ﬁlter for both methods is also shown.
The root mean square
error is seen to be signiﬁcantly greater for the point method than for the
line method over the latter portion of the simulation time. The Kalman
ﬁlter predicted error is close to the actual error for the line method but
considerably underestimates the actual error for the point method. . . . .
128
5.41 ωz angular velocity pose estimate simulation results for a four-point target
showing angular root mean square estimation velocity error over time for
the line method and the comparison point method calculated from 100
sample runs. The square root of the predicted error covariance from the
Kalman ﬁlter for both methods is also shown. The root mean square error
is seen to be signiﬁcantly greater for the point method than for the line
method over the latter portion of the simulation time. The Kalman ﬁlter
predicted error is close to the actual error for the line method and for the
point method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
xxi

5.42 tx translation pose estimate simulation results for a four-point target oﬀ-
set from the image center. The ﬁgure shows linear translation root mean
square estimation error over time for the adaptive line method, the non-
adaptive line method and the comparison point method calculated from
100 sample runs. x and y translations were both initially 400 mm. The
RMS error is seen to be signiﬁcantly less for the point method than for
the adaptive and nonadaptive line methods over most of the time period.
Initial error overshoot is also much less with the point method. The adap-
tive line method shows less error than the nonadaptive method over most
of the simulation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
5.43 ty translation pose estimate simulation results for a four-point target oﬀ-
set from the image center. The ﬁgure shows linear translation root mean
square estimation error over time for the adaptive line method, the non-
adaptive line method and the comparison point method calculated from
100 sample runs. x and y translations were both initially 400 mm. The
root mean square error is seen to be signiﬁcantly less for the point method
than for the adaptive and nonadaptive line methods over most of the time
period. Initial error overshoot is also much less with the point method.
The adaptive line method shows less error than the nonadaptive method
over most of the simulation. . . . . . . . . . . . . . . . . . . . . . . . . . .
134
xxii

5.44 tz translation pose estimate simulation results for a four-point target oﬀ-
set from the image center. The ﬁgure shows linear translation root mean
square estimation error over time for the adaptive line method, the non-
adaptive line method and the comparison point method calculated from
100 sample runs. x and y translations were both initially 400 mm. The
root mean square error is seen to be signiﬁcantly less for the point method
than for the adaptive and nonadaptive line methods over most of the time
period. Initial error overshoot is also much less with the point method.
The adaptive line method shows less error than the nonadaptive method
over most of the simulation. . . . . . . . . . . . . . . . . . . . . . . . . . .
135
5.45 Mitsubishi RV-E2 six-axis robot arm and the Cidtec camera used in the
experimental tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
5.46 Block diagram showing the system hardware used in the experimental
setup. The hardware includes a Cidtec camera, frame grabber, PC com-
patible computer, and Mitsubishi robot arm and controller. . . . . . . . .
138
5.47 Measurement functions applied to obtain line point measurements from
the camera image in robot arm experiments. . . . . . . . . . . . . . . . . .
138
5.48 Histogram of the distribution of x line point measurement values from 500
camera images for a stationary target. . . . . . . . . . . . . . . . . . . . .
140
5.49 Histogram of the distribution of y line point measurement values from 500
camera images for a stationary target. . . . . . . . . . . . . . . . . . . . .
140
5.50 tx translation pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a sinusoidal variation corre-
sponding to the true x variation of the target. Note the step response at
the beginning of the target motion. . . . . . . . . . . . . . . . . . . . . . .
141
xxiii

5.51 ty translation pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a sinusoidal variation corre-
sponding to the true y variation of the target. Note the step response at
the beginning of the target motion. . . . . . . . . . . . . . . . . . . . . . .
142
5.52 tz translation pose estimate from experimental testing for constant target
motion about z axis. The estimate shows a noisy variation about a mean
z value with a small sinusoidal component. The true target z value is
nominally constant. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
5.53 q0 quaternion pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a full 360 degree sinusoidal
variation corresponding to the true q0 variation of the target as it moves
through two complete revolutions.
. . . . . . . . . . . . . . . . . . . . . .
143
5.54 q1 quaternion pose estimate from experimental testing for constant target
motion about the z axis.
The estimate shows a relatively small noisy
variation about zero in comparison to a true q1 constant value of zero. . .
143
5.55 q2 quaternion pose estimate from experimental testing for constant target
motion about the z axis.
The estimate shows a relatively small noisy
variation about zero in comparison to a true q2 constant value of zero. . .
144
5.56 q3 quaternion pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a full 360 degree sinusoidal
variation corresponding to the true q3 variation of the target as it moves
through two complete revolutions.
. . . . . . . . . . . . . . . . . . . . . .
144
5.57 vx translation velocity pose estimate from experimental testing for con-
stant target motion about the z axis. The estimate shows a noisy sinusoidal
variation compared to the true x sinusoidal velocity variation of the target.
Note the step response at the beginning of the target motion. . . . . . . .
145
xxiv

5.58 vy translation velocity pose estimate from experimental testing for con-
stant target motion about the z axis. The estimate shows a noisy sinu-
soidal variation compared to the true y sinusoidal velocity variation of the
target. Note the step response at the beginning of the target motion. . . .
145
5.59 vz translation velocity pose estimate from experimental testing for con-
stant target motion about z axis. The estimate shows a noisy variation
about a mean z velocity value of zero. The true target vz velocity value is
nominally zero. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.60 ωx angular velocity pose estimate from experimental testing for constant
target motion about the z axis.
The estimate shows a noisy variation
about a mean ωx velocity value of zero. The true target ωx velocity value
is nominally zero. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.61 ωy angular velocity pose estimate from experimental testing for constant
target motion about the z axis.
The estimate shows a noisy variation
about a mean ωy velocity value of zero. The true target ωy velocity value
is nominally zero. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.62 ωz angular velocity pose estimate from experimental testing for constant
target motion about the z axis. The estimate shows a noisy variation about
a mean ωz velocity value of -0.06 rad./sec. The true target ωz velocity
value is nominally constant at -0.06 rad./sec.
. . . . . . . . . . . . . . . .
147
5.63 Pipe assembly used in the structure estimation robot arm experiments.
The picture shows the four segments of white PVC pipe joined together. .
149
5.64 Structure estimate of the three ⃗l line vector components for line zero with
motion along the z axis.
The true structure values are also shown for
comparison. The x and y components are close to the true values while
the z component has a constant oﬀset error.
. . . . . . . . . . . . . . . .
155
xxv

5.65 Structure estimate of the three ⃗m line vector components for line zero
with motion along the z axis. The true structure values are also shown for
comparison. The y and z components are accurately estimated while the
x component has signiﬁcant error.
. . . . . . . . . . . . . . . . . . . . . .
155
5.66 Structure estimate of the three ⃗l line vector components for line one with
motion along the z axis.
The true structure values are also shown for
comparison. The x and y components have small errors in the estimate
while the z component is seen to converge slowly to the true value. . . . .
156
5.67 Structure estimate of the three ⃗m line vector components for line one
with motion along the z axis. The true structure values are also shown
for comparison.
The z component is estimated accurately while the x
component is seen to converge to the correct value. Error in y, though
initially small, increases with time. . . . . . . . . . . . . . . . . . . . . . .
156
5.68 Structure estimate of the three ⃗l line vector components for line two with
motion along the z axis.
The true structure values are also shown for
comparison. Accurate estimates are obtained for all three components. . .
157
5.69 Structure estimate of the three ⃗m line vector components for line two with
motion along the z axis.
The true structure values are also shown for
comparison. Accurate estimates are obtained for all three components. . .
157
5.70 Structure estimate of the three ⃗l line vector components for line four with
motion along the z axis.
The true structure values are also shown for
comparison. The x and y components are close to the true values while
the z component has a constant oﬀset error.
. . . . . . . . . . . . . . . .
158
xxvi

5.71 Structure estimate of the three ⃗m line vector components for line four
with motion along the z axis. The true structure values are also shown for
comparison. The x and z components have relatively small errors while a
constant oﬀset error is present for the y component.
. . . . . . . . . . . .
158
xxvii

Chapter 1
Introduction
1.1
Problem Statement
Estimation of relative three-dimensional (3–D) position and orientation (pose) and
structure as well as relative motion between two reference frames is an important problem
in robotic guidance, manipulation, and assembly as well as in other areas such as pho-
togrammetry, tracking, object recognition, and camera calibration. Pose is deﬁned for an
object in 3–D cartesian space consisting of an object reference frame and a base reference
frame. The pose with respect to the base frame comprises the three position coordinates
of the object reference frame origin and the three orientation angles of the object frame.
Remote estimation of the relative pose and motion of a 3–D object without physically
touching the object or without human intervention is the fundamental problem examined
here. For example, an autonomous robot that moves along a path may need to determine
its position relative to an obstacle. Similarly, a mechanical hand grasping a moving object
requires that the gripper motion be matched to the object and then placed at the correct
position and orientation. Automated spacecraft docking requires the measurement of pose
relative to the docking port. The pose can be expressed relative to the sensing element
frame or with respect to the base frame.
This pose estimation or determination problem has been a subject of considerable
research for many years in computer vision, photogrammetry, and robotics with many
solutions having been proposed. The methods available are generally deterministic and
use single-vision cameras, stereo-vision cameras, or more direct 3–D measuring techniques
1

such as range images from laser, ultrasonic, and structured lighting devices. Two general
classiﬁcations of sensors can be made: active and passive.
Active devices (e.g., laser
range ﬁnders) are those which emit energy and sense its reﬂection to determine the sensor
response, while passive devices such as video cameras sense the natural or background
energy (e.g., visible light or infrared) reﬂected oﬀan object. This distinction is important
in an application such as a battleﬁeld environment, where a sensor that emits energy
could be detected by enemy surveillance. However, relying on background energy alone
may be insuﬃcient in providing a good signal-to-noise ratio for feature detection and
object recognition. Poor visibility caused by fog, smoke, or rain can produce images that
are highly variable in quality with diﬃcult to recognize features. Noise considerations,
therefore, must be taken into account to a much greater extent than would be the case
in a laboratory environment where illumination can be easily controlled. Even in a well-
controlled setting, sensor noise must be considered since intrinsic sources can introduce a
substantial amount of measurement noise. Depending on the accuracy required for pose
measurement, this noise can either be ignored or the eﬀects can be minimized. Generally,
the highest accuracy is needed for camera calibration while model matching or grasping
may permit larger errors in the estimate. Accuracy requirements vary for docking and
tracking applications.
The general problem developed here is to locate an object and measure its motion
in three dimensions based on three position coordinate parameters and three rotation
coordinate parameters either relative to an observer or with respect to a ﬁxed-reference
frame. Figure 1.1 shows the basic problem with each reference frame and transformation
shown.
Known as the exterior orientation problem in photogrammetry [1], this question has
been addressed for photographs using a number of manual methods dating back to 1879
[2]. More recently, beginning in the 1960’s, methods using computer vision techniques
2

ZP
XP
YP
Camera Reference Frame
XS
ZS
YS
TSP
Object Reference Frame
TPS
ωz
ωx
ωy
ωx
ωz
ωy
,vx
,vy
,vz
,vx
,vz
,vy
Fig. 1.1.
Transformation between two reference frames illustrating the fundamental
problem to be solved.
3

have been developed. Most measurement techniques for pose determination or estimation
are image-based and can be classiﬁed into two major categories.
These categories are
point-based methods and model-based methods using higher-order geometric primitives.
Each type involves acquiring an image, either two-dimensional (2–D) or 3–D, and then
processing that image to arrive at a value for the pose. Noise eﬀects may also be analyzed
for either type. Speciﬁc techniques and applications that have been proposed are described
in Section 1.3.
The particular sensor used can determine the ultimate accuracy achieved.
The
precise layout of the elements in the sensing charge coupled device (CCD) array along
with the use of low distortion lenses gives low geometric distortion for CCD imaging
cameras. Calibration for lens distortion can reduce the error still further. Feature detection
and extraction software can then locate an object’s feature in 2–D spatial coordinates
to subpixel accuracy provided the signal-to-noise ratio is high.
Noise and uncertainty
are present due to the response nonuniformity of the individual CCD pixels along with
photon and thermal noise. Laser range ﬁnders likewise have uncertainty in both range as
well as geometric location correspondence. The imaged scene also inﬂuences the accuracy
achieved.
Extracting features from an object of low contrast with its surroundings is
not as straightforward nor as accurate as one with high contrast. To overcome some of
the limitations of natural objects, artiﬁcial markings may be placed on the object whose
position relative to the object is known. These markings can consist of a high-contrast
pattern that maximizes signal-to-noise ratio and optimizes a particular pose determination
algorithm.
Other factors to be considered for pose and motion determination are the compu-
tational complexity and robustness of a method. The algorithm chosen may require a
large computational overhead due to an optimization solution technique requiring many
iterations. The choice of method usually involves a trade-oﬀbetween accuracy and noise
4

rejection versus computation time. Robustness in this context is deﬁned as the ability
of an algorithm to provide a result within a known error bound or to indicate that the
error condition cannot be met. Measurement data outside a known error bound relative
to a working model may be ignored or given low weight. Obtaining accurate results for
autonomous pose determination in everyday environments in which people operate with
ease is a diﬃcult problem and one that has not yet been fully solved. Variable conditions
such as lighting and visibility, along with the presence of noise from various sources, are
principle diﬃculties that need to be examined. This work is an attempt to model these
uncertainties and imprecisions in such environments as applied to pose and motion esti-
mation. The term estimation is used as opposed to determination since the modeling is
done in a statistical sense where estimated results are computed. Other methods may
determine the pose by directly calculating a set of equations or by calculating a least
squared error solution that does not model the noise or process stochastically.
1.2
Motivation
A solution to this problem that uses two 2–D intensity images from a single camera
is desirable in many instances where a priori geometric knowledge about an object in the
unknown reference frame is available. One direct application for this problem is in robotic
assembly and navigation. Figure 1.2 shows in block diagram form how pose and motion
estimation could be applied in a robot arm feedback control loop. The diagram illustrates
the measurement of the pose and motion of a camera mounted on the end of a robot arm
with respect to an object of interest. This measurement is used as feedback for controlling
the position of the arm.
Use of a single camera is advantageous for the following reasons: a CCD video
camera is low in cost; setup and calibration is simple; physical space requirements are
small; reliability is high; and standard low-cost hardware is available for digitizing and
5

Relative Pose /
Pose Velocity 
Estimate
Rules
Speed
Axis
Commands
2-D Feature Locations
for Gaze Control
Robot Arm
Dynamics
Eye to Hand
Transformation
Pose and
Motion
Estimator
(EKF)
Feature
Extraction/
Correspondence
Arm
Controller
Path
Calculation
Camera
Fig. 1.2.
Block diagram of a robot arm feedback control loop showing an application for
pose and motion estimation. A camera mounted on the end eﬀector of the robot arm is
used as the measurement sensor.
processing the images.
The primary diﬃculty in performing this measurement is that
depth information is lost in the process of projecting 3–D object features to 2–D images,
a nonlinear transformation.
Noise is also present in the form of perturbations to the
assumed process dynamics, imperfect system modeling, and feature locations extracted
from the 2–D images.
This dissertation presents a solution to the remote measurement problem for a
dynamic system given a sequence of 2–D intensity images of an object whose position
and orientation are known relative to a base reference frame. The 3–D transformation is
modeled as a nonlinear stochastic system with the state estimate providing the six degree-
of-freedom motion and position values. The stochastic model uses the iterated extended
Kalman ﬁlter (IEKF) as an estimator and a screw representation of the 3–D transfor-
mation based on dual quaternions. Dual quaternions, whose elements are dual numbers,
provide a means to represent both rotation and translation in a uniﬁed notation. In the
following, prior related work is reviewed pertaining to general methods of autonomous
6

pose estimation and determination of 3–D objects and to methods that model noise as
part of the solution.
Previous solutions have used point-based image features in estimating the structure,
pose, and motion. This work, instead, uses image line features as measurement inputs for
the estimation. Line features are present in many scenes and objects to a greater extent
than point features. They may be more visible, as well, under a wider range of lighting and
environmental conditions than points. Also, straightforward techniques such as the Hough
transform and line ﬁtting to edges are available to extract the lines from the images. For
these reasons, an approach was taken to develop the nonlinear estimation problem strictly
in terms of 3–D lines used to describe an object or scene and 2–D image lines as measured
quantities.
1.3
Related Work
This section provides a summary of previous work relating to noncontact position
and orientation measurement methods as described in the published literature. Existing
research shows a large number of position determination methods applicable to a variety of
applications. While not as numerous, pose estimation methods have also been described,
and these are summarized as well. Variations in the methods include type and location
of the sensor, illumination requirements, the object or scene feature on which the pose
is calculated, relative motion of the robot or object, iterative versus direct solution, and
modeling of uncertainty or noise in an attempt to improve the robustness and accuracy of
the results. Applications proposed are for research programs or for commercial prototype
development in the general areas of robot location, manufacturing, camera calibration,
and tracking of a moving object.
7

1.3.1
Pose Using Point-Based Methods
Point-based methods rely on the identiﬁcation and location of feature points on
a target object from a 2–D image of the scene. A rigid body is generally assumed but
no explicit geometric model is given. Information concerning the geometric shape other
than size is not used in calculating the pose. Coordinates of the points in a local or world
reference frame may or may not be known.
Methods of this class, referred to as N-point perspective, were the ﬁrst to be studied
and, as a result, have been more extensively developed than model-based methods [3]. A
perspective model that assumes the projection of a 3–D object onto a 2–D image plane
through a pinhole camera model is generally used
[1].
Both single-image and stereo
methods have been reported; however, single-vision techniques have, by far, the greatest
number of solutions. One reason for this is that point correspondence with an object from
a single image is easier to determine than correspondences between two images and the
object as required in stereo. The general framework is, given N corresponding points in
the object and in the image, to solve for the relative pose between the camera and the
object. The minimum N that produces a ﬁnite number of solutions is three, although, up
to four solutions are possible. Four coplanar, noncollinear points give a unique solution.
Four or ﬁve noncoplanar, noncollinear points may result in two solutions. For N greater
than ﬁve noncollinear points, the result is unique and consists of an overdetermined set
that can be solved using least squared error methods [4]. In general, as N increases, the
accuracy of the results increases [5]. These overdetermined solutions are used for camera
calibration in which a large number of points are needed through minimization of an error
criteria to achieve the desired accuracy and to calculate both the external and internal
camera parameters [5]. Three- and four-point coplanar targets have been directly used
for pose determination. With stereo cameras, three corresponding points on an object
are suﬃcient to uniquely identify the relative pose of the object although uncertainty
8

may be reduced through a larger number of points
[6]. Range images, similarly, can
determine pose with a minimum of three points. An advantage of 3–D range images over
stereo is that the correspondence problem is not present since the three coordinates of
an object point are determined directly. Algorithms for these techniques are primarily
iterative. For the three- and four-point special cases, however, closed-form solutions have
been demonstrated [7, 4, 8, 9].
Trabasso and Zielinski [10] describe an approximate calibration method for calcu-
lating exterior orientation parameters and the x and y scaling factors. A calibration block
consisting of four coplanar points is assumed to be perpendicular to the optical axis of
the camera. This method has been tested in an experimental work cell for robot grasping
and placement of automobile bodyshells whose plane is approximately perpendicular to
the camera axis. Placement accuracies of one millimeter were achieved.
Tsai has described a technique for high-accuracy, 3–D camera calibration using
standard television cameras and lenses [5]. This paper gives a good survey of the cali-
bration and pose determination techniques at the time of its publication. It also proposes
a new two-stage method based on the radial alignment constraint that determines the
six extrinsic and intrinsic parameters including focal length, two radial lens distortion
coeﬃcients, and the x coordinate scale factor. The ﬁrst stage is a direct linear solution
neglecting lens distortion. This result is used as the initial estimate in the nonlinear second
stage that takes into account the lens distortion. An overdetermined set of points is used
to achieve high accuracies using least squared error ﬁtting. Using the radial alignment
constraint, solutions are given for both coplanar calibration points and noncoplanar points.
Test results are given where the number of calibration points used is 60. Total accuracy in
3–D is about one part in 2000. Fewer points gave higher errors while more points did not
give a signiﬁcant improvement in accuracy. The error obtained is approximately one-half
the total theoretical predicted error.
9

A 3–D location method based on dual number quaternions is given by Walker et al.
[11]. The method is formulated as a single-cost function to be minimized where the real
part and dual part of the dual quaternion are used to represent rotation and translation.
Conversions between the conventional homogeneous transformation matrix and the dual
quaternion parts are given. An algorithm for the localization is detailed. Inputs are in
terms of 3–D measured points and unit vectors corresponding to surface normals or to
edge directions. Phong et al.
[12] also describe a pose estimation method based on dual
number quaternions. Their method is a 2–D to 3–D pose estimation rather than the 3–D
to 3–D as given by Walker. Line features, or lines deﬁned by pairs of corresponding points,
are used as correspondences between the image and the 3–D object. The problem is set
up as a sum of quadratic constraints. A trust region optimization algorithm is used to
solve for the real part and dual part of the transformation. Experimental results are given
for various numbers of line correspondences and varying amounts of added noise. Two
sets of results are given. In the ﬁrst set, the real quaternion is solved ﬁrst and then used
to solve for the dual part. The second set of results is from the simultaneous solution of
both parts of the dual quaternion. In general, the simultaneous solution had less error but
required more computation. This method is extended in [13] to point features based on
the collinearity between the object point, the center of projection, and the image point.
1.3.2
Methods for Real-Time Pose Determination
Real-time methods are intended for those applications requiring fast response for
control or needing low computational overhead for operation with low-cost hardware.
These methods generally use a relatively small number of feature points, with four copla-
nar points being most widely used. Abidi and Chandra [8] propose a new closed-form
algorithm for relative pose determination based on the volume measurement of tetrahe-
dra. Using triples of the corner points of a quadrangular target, the volumes of the solids
10

deﬁned by these points and the lens center are calculated. Object pose is recovered using
these volumes without prior knowledge of the lens focal length as long as the coplanar tar-
get and the optical axis are not perpendicular. A solution is also given that uses the focal
length as an input to remove this restriction. Lens distortion correction is applied to the
image feature points before using the algorithm. To achieve higher accuracies, a nonlinear
iterative optimization stage is given that is applied to the results of the direct algorithm.
Experimental results and simulations show that average errors of 0.84% in distance and
0.66% for angles were obtained using the direct method alone, assuming pixel standard
deviation noise of 0.5 pixels. Applying the optimization step gave errors of 0.19% and
0.05% respectively. Results are also given showing sensitivities of the pose to parameter
changes and to the shape and relative size of the target. Target shape was not found to
have a signiﬁcant eﬀect on accuracy. Abidi and Gonzalez [14] give an alternate method
to compute the relative pose based on an algebraic solution. Three points are shown to
be required to give a ﬁnite number of solutions. A fourth coplanar point is then shown to
give a unique solution as long as no subset of three points are collinear.
An early paper on pose determination based on machine vision was written by
Chen et al.
[15] that extracted feature points on a workpiece.
At least three points
were extracted from an image of the workpiece that was then rotated through a known
displacement with a second image being taken. Through stereo correspondence, the 3–
D locations of the points relative to the camera were calculated.
The transformation
matrix is then calculated from the matched points. Haralick and Joo [2] present robust
techniques for calculating the extrinsic pose parameters from a single image. Iterative
procedures with both robust and nonrobust methods for calculating a least squared error
solution are given. Experimental results and simulations show that the robust method has
considerable advantage when outliers are present in the data. It is stated that outliers are
11

generally due to incorrect correspondence of point pairs, an occurrence which is commonly
encountered.
Haralick et al. give closed-form iterative, least squared error solutions based on
point correspondences for four diﬀerent pose estimation problems [16]. These problems
are 2–D to 2–D estimation, 3–D to 3–D estimation, 2–D perspective to 3–D, and 2–D
perspective to 2–D perspective. A robust algorithm is also presented for each problem.
The eﬀects of varying amounts of noise on these solutions are also investigated. Both inde-
pendent additive Gaussian noise and independent additive noise from a slash distribution
are used in simulations to show the performance of each algorithm. The slash distribution
is used to simulate the presence of outliers in the data due primarily to correspondence
mismatches in the data. Experimental results show the advantage of the robust algorithm
for this noise distribution. The results also show that hundreds of corresponding points
must be used to achieve reasonable accuracies when the signal-to-noise ratio is less than
40 decibels.
An automatic spacecraft docking application described by Ho and McClamroch uses
computer vision and a rhombus target to determine relative pose [17]. The algorithm used
is point based where the four corners of the target are extracted. A nonlinear, iterative
least squared error solution is given. Focal length is assumed known. These values are fed
to observers that estimate the spacecraft rotational and translational velocities. Control
loops maintain position and attitude while keeping the rhombus mark in view of the
camera. Simulation results show that orientation accuracy is better than position accuracy
with the largest error in the range estimate. However, this error is proportional to the
range so that as range decreases during the docking maneuver the error also decreases to
an acceptable level.
Holt and Netravali give a theoretical discussion of the pose determination problem
given N corresponding points on an object whose coordinates are known in some 3–D
12

reference frame and the 2–D perspective projection onto an image plane [3]. If N is three,
it is shown that, in general, there can be up to four solutions unless the points are collinear;
in that case, there can be inﬁnitely many solutions. The case of four coplanar points is
examined for various combinations of object and image point collinearity conditions. Four
noncoplanar corresponding points are shown to have, at most, four solutions.
An analytic solution to the perspective four-point pose determination problem is
given by Horaud et al. [18]. The general case of noncoplanar is considered, with special
cases of four coplanar points and four points forming a right vertex. While a detailed
derivation is shown, no experimental results are provided. Howard and Book [19] describe
a video-based sensor intended for automated docking systems. This system was developed
for the National Aeronautics and Space Administration (NASA) with intended use on
Space Station Freedom and other satellite servicing vehicles. A solid state video camera,
digitizer, and special target form the basis of the system. The relative pose between the
camera and the target is determined. A three-point correspondence that has three circles
of retroreﬂective tape spaced in a line is determined from the target. The middle reﬂector
containing the other two circles is mounted on a pole spaced away from the target plane.
An algebraic method is used to calculate the pose from the image locations. No proof is
given that the pose calculated is unique since as many as four solutions are possible with
only three point correspondences.
A robot location determination method is given by Hung et al. [20] for use in mobile
navigation. Multiple marks are proposed in which a trapezium is used as a target along
with a mark identiﬁcation. A modiﬁcation of Fischler and Bolles’ method
[9] is used
to calculate the pose based on obtaining the corner points of the trapezium to subpixel
accuracy. Point correspondence and identiﬁcation are addressed in detail.
Hung et al. [21] propose a method for pose determination based on a planar quad-
rangle target. From knowledge of the dimensions of the target, the relative pose of the
13

camera coordinate system is found. The technique directly solves for the positions of the
vertices of the quadrangle relative to the camera perspective center. While the solution
is exact, the algorithm is sensitive to noise. To reduce noise eﬀects, a shape restoration
stage is given that attempts to restore the noise-perturbed shape of the quadrangle to its
original shape. This stage is a constrained optimization problem in which a suitable cost
function is deﬁned and then minimized iteratively. Results show relative errors of less
than 1% for a noise standard deviation of 0.5 pixel.
Another pose determination method using four coplanar points is given by Kamata
et al.
[22]. Intermediate transformations are deﬁned when relating world coordinates to
camera coordinates. These intermediate transformations simplify the coordinate systems
so that algebraic reasoning can be used to solve for the remaining transformation. Lens
focal length is required. The full solution is a direct closed-form method.
Kite and Magee describe an algorithm for pose determination of a camera relative
to a rectangular target of known size [23]. The 3–D problem is broken down into two 2–D
problems. In addition, only three corners of the target must be visible. A major restriction
in the use of this algorithm is that the optical axis of the camera must pass through the
centroid of the target. Noise eﬀects are not considered. An iterative algorithm for pose
determination of a rigid body from landmark points is given by Krishnan et al. [24]. These
points may be coplanar or noncoplanar. Any number of points greater than three may be
used. This method iteratively calculates the coordinates of the N landmark points relative
to the camera perspective center using least squared error for the overdetermined system.
The pose transformation algorithm of Veldpaus et al. [25] is used once the 3–D coordinates
of the individual points are known. Computer simulations were conducted using four to 12
points in coplanar arrangements with additive Gaussian noise having a standard deviation
of one pixel. Two metrics that permit comparison between results from diﬀerent methods
are deﬁned. These are the Range-Size Ratio (RSR) and the Focal-Length-to-Noise Ratio
14

(FNR). A signiﬁcant result is that an increased number of landmarks does not always
result in increased accuracy of pose parameters. Range errors were found to be less than
0.6% for planar targets in which the plane of the target was at least 20 degrees oﬀnormal
to the camera. Results obtained were compared with Hung et al., Yuan, and Abidi and
Chandra, all using four points
[21, 1, 4]. While the range and angle errors given were
lower than the other methods, the FNR of 800 was somewhat higher than that of Yuan
(735) and Abidi and Chandra (520), which would be expected to produce lower errors.
Mandel and Duﬃe address errors in mobile robot docking
[26].
A method is
proposed to compensate for the docking errors. Two methods are described for measuring
the error: (1) a touch trigger probe, or (2) a vision system.
The reference target for
the vision system is four points drawn on a cube with three points on one side and the
fourth on an adjacent side.
Four nonlinear equations are used to solve for the depth
of the points. The transformation matrix is then derived through a least squared error
approach by means of the singular value decomposition. This approach calculates a true
rigid body transformation of a rotation and a translation. Experimental results using real
hardware showed depth and angular errors of about 0.3%. Liu et al. describe methods
of pose determination using point correspondences and line correspondences [27]. Both
nonlinear and linear solutions are given in which the rotation and translation are calculated
separately.
Three line or point correspondences are needed for the nonlinear iterative
method while eight line or six point correspondences are needed for the linear technique.
Simulation results are given for each case. For line correspondence with the nonlinear
method and using four lines with a 5% level of uniformly distributed noise, the relative
error for rotation is 0.0086 and for translation is 0.0347. The relative errors for the linear
method are 0.2832 and 0.0992, respectively, which are considerably worse. The algorithm
for point correspondence forms lines from the points and uses the same algorithm as the
line matching method. A video approach to autonomous docking for spacecraft is given
15

by Tietz and Germann [28]. The paper describes a complete guidance and control system
with a TV camera on the chase vehicle. A target consisting of three lights arranged in a
“T” pattern with the middle light in front of the object is used. Flashing lights are used
to determine point correspondence. Equations are given for the pose calculation, but no
derivation is given nor is there a justiﬁcation for the use of only a three-point target so
that a unique solution may be achieved.
Jatko et al. [29] describe a method for autonomous docking in a battleﬁeld resupply
application. A target consisting of six LEDs arranged as two quadrangular faces is used
with a single camera for image acquisition. The LEDs ﬂash at a preset frequency with
temporal bandpass ﬁltering being performed on the images. A least squared error iterative
algorithm similar to Lowe was used for pose calculation [30]. The method was successfully
implemented and demonstrated on a full-scale robot arm. Robustness to missing features
as well as extraneous features is a key result. Goddard et al. [31], in a related application,
use the four coplanar point method of [8] to measure the pose of each face of the six-point
target. The point correspondence of the quadrilateral faces is used to determine which
face is being viewed. Experimental results are given for relative error in all six parameters.
Wolfe et al. provides an analysis of the constraints provided by point correspon-
dences for monocular pose determination
[32].
The constraints imposed by diﬀerent
numbers of corresponding points are examined for a perspective camera model. Coordi-
nate curves are used to trace the motion of a point from the camera reference as a position
parameter is varied. For the case of four points, it is shown that knowledge that the points
are in the shape of a rectangle is suﬃcient to recover the orientation without the known
length and width. Position is recoverable when these parameters are also known. Pinkney
et al. [33] and Pinkney and Perratt [34] describe a real-time photogrammetry system that
is a part of the remote manipulator system used on the NASA Space Shuttle. Based on a
TV image, the system calculates pose at frame rates through a point-based algorithm. A
16

least squared error ﬁt to the pose value is performed on the individual frame values every
n frames. The points are extracted from quadrangular or three-point targets.
Yuan presents a general method of pose determination or, as it is known in pho-
togrammetry, the exterior orientation calibration problem. The method is based on a
single image of known feature points
[1]. Instead of the collinearity constraint, the al-
gebraic structure of the transformation is used in deriving a nonlinear method for any
number of points. A proof of the existence of a solution for n points is given as well as
a proof of the uniqueness of a solution for the case of four noncollinear coplanar points.
Speciﬁc solutions are given for the cases of three, four coplanar, four noncoplanar, and
ﬁve noncoplanar corresponding points. Simulation results for coplanar and noncoplanar
four-point solutions show sensitivity to variations in range, sensitivity to image-size ratio,
and sensitivity to errors in image and feature point coordinates. Errors in position are
relatively independent of range while coplanar points tend to give higher orientation errors
than noncoplanar when the object plane is parallel to the image plane. A signiﬁcant result
is that noncoplanar targets give higher accuracies and have greater robustness than copla-
nar targets. No problems were encountered with multiple solutions for four noncoplanar
points. The conditions under which multiple solutions may arise are not given, however.
1.3.3
Pose Using Model-Based and Higher-Level Primitives Methods
Model-based methods of pose determination use an explicit model for the geometry
of the object in addition to its image in determining the pose. The object is modeled
in terms of points, lines, curves, planar surfaces, or quadric surfaces.
Some methods
obtain these features from computer aided design models
[35].
Other papers restrict
or simplify the model so that the object can be described easily and the computation
involved is reasonable
[36]. As in point-based methods, a perspective vision model is
generally assumed. Orthographic models with scaling that simplify the computation have
17

also been reported, but the accuracy is limited at closer ranges due to the perspective
approximation
[7]. For this class, methods have also been developed for single vision,
stereo, and range images. From the 2–D image in single-vision techniques, which is the
perspective projection of the known 3–D object, the problem is to determine the rotation
and translation of the object that would give rise to the given 2–D projection. Edges and
vertices must be recognized and matched with corresponding features in the object for
the pose to be calculated. One important example is the calculation of the pose from the
image of a circular feature (an ellipse) using a closed-form method [37]. An ellipse is ﬁt
to the feature edge through minimization of error criteria. Two solutions result from one
image, however, so that an additional feature or an additional image is required to derive
the correct pose.
Methods using range images are predominately model-based.
The range image
is 3–D rather than 2–D as with intensity images so that depth information is directly
available for matching features to the object. Algorithms, however, are generally iterative
and are based on nonlinear minimization of a cost function relating the image features
with the object features
[38].
Hypotheses and veriﬁcation may also be performed to
determine the correct pose
[39].
A method has also been developed based on neural
networks to solve the problem using Kohonen-Nets [40]. Stereo methods, by determining
the 3–D coordinates of features through matching, are similar to range image methods
[41]. Detailed summaries of methods in this class are given below. Chen describes a line-
to-plane pose determination method in which the image data are lines that correspond
to planar features on an object [42]. Point-to-point correspondence is not assumed. One
example of its application is the projection of a light stripe across a cube generating three
lines on three faces of the cube. The method shown is based on quaternions requiring
the solution of an eighth-degree polynomial. No initial guess is required. A minimum
of three lines is required, but the solution is not unique. Experience indicates that one
18

or two additional lines are required to determine a unique solution, but the uniqueness
of solutions is not addressed. Closed-form solutions are given for commonly encountered
special conﬁgurations. An analysis of the necessary and suﬃcient conditions for existence
is given. Noise sensitivity simulations that show the eﬀects on orientation were presented.
Signiﬁcant errors were seen through perturbations of the three line vectors. Choosing a
triplet of lines is recommended as the initial step in an iterative solution using additional
feature pairs and least squared error techniques. The author did not ﬁnd a direct solution
using additional feature pairs.
Chen and Tsai propose a solution for robot localization using an assumed shape
for a target object [36]. The object must have a polygon-shaped top and a lateral surface
perpendicular to the top with all dimensions of the polygon top and angles known. This
class of objects includes cubes, most buildings, and many machine parts.
Pencils are
deﬁned for a corner of the polygon from which a corresponding image pencil is extracted.
Experimental results show errors of 5% or less with sensitivity to critical orientation angles.
In comparison with the work by Hung et al. [21], the method has improved accuracy for
general orientations but is worse at the critical angles.
Three-dimensional planar objects are considered by Consales et al. for determina-
tion of spatial orientation
[43]. Assuming perspective projection, the method extracts
the image junctions of object vertices where three edges come together. The paper shows
that only two junction angles are required to calculate the orientation instead of three as
described in previous work.
A triplet of image lines is used by Dhome et al. for pose determination of 3–D
objects from a single image [44]. The lines are assumed to be the perspective projection
of three ridges on the object model with correct matching applied. A search is performed
for the model attitude consistent with image projections. The required transformations
are calculated from an eighth-degree polynomial. Since eight solutions are possible, logical
19

rules are given that reduce the number of solutions. In actual use, matching of the model
to the image lines must be performed in a general prediction-veriﬁcation procedure. Lowe’s
method [30] was used for matching while the proposed method is used at the beginning of
the procedure to reduce the searching required and to provide an initial value for Lowe’s
iterative procedure.
Ellis et al. gives a method for ﬁtting ellipses to edge-based image data [45]. The
ellipse parameters are used to determine the pose of the object. A particular emphasis is
the handling of uncertainty in which Kalman ﬁltering is used in extracting the ellipse data
and used in the model matching. The uncertainty is propagated throughout the matching
stages through a Mahalanobis distance measure and a Kalman ﬁlter. An orthographic
projection is used to simplify the calculations. Results show the detection and grouping
of ellipses in real images with many diﬀerent types of structures. No data is given on
the accuracy of ﬁt or on the accuracy of pose determination using this method. Ellipse
ﬁtting is used by Foster and Sanderson to determine object orientation [46]. This method
assumes objects with circular edges or surfaces and calculates the orientation based on
the ellipse equation. An error measure based on the minimization of distance that is used
to iteratively ﬁt edge points to the ellipse equation is deﬁned.
Safaee-Rad et al. describe a method to estimate the parameters of a quadratic curve
from a single image. The emphasis is on accurate estimation of ellipse parameters. These
ellipse parameters are then used for pose determination [47, 48, 49]. This work gives an
exact closed-form solution for calculating relative pose from circular object features. Since
the curve parameters are used, the method does not require individual point matching.
No restrictions are placed on the optical axis direction, and perspective projection is
assumed. The method is based on geometric reasoning and algebraic transformations to
ﬁrst calculate the relative orientation without knowledge of the radius. Lens focal length
is required. Since the circular feature has rotational symmetry, only two orientation angles
20

can be determined. The third is arbitrary and can be set to zero. If the radius is known,
the center coordinates can be determined directly. If the radius is unknown, two images
are required with prescribed viewpoint diﬀerences. As with any method using circles alone
as the determining feature, the mirror ambiguity gives two possible orientations for the
viewed circle. Also given is an extended method for 3–D quadratic features in which pose
is estimated for spherical features.
Simulated and real experimental results are given.
Combining the ellipse parameter ﬁnding method given in [48] and this method, the real
object tests show average orientation angle errors of 0.80 degrees or less and average
positional errors of 1.28 mm or less with the largest error in the depth axis.
Ferri et al. examine pose estimation through perspective inversion for four diﬀerent
problems [50]. These problems are: (1) four coplanar line segments; (2) three orthogonal
line segments; (3) a circular arc; and (4) a quadric of revolution. For case two, the method
gives a second-degree equation as the solution. Case three for elliptical images ﬁnds the
orientation of planes that intersect with a cone whose vertex is the focal point.
Two
possible solutions are possible for each image ellipse. When the circle radius is known, the
relative position of the center can be found. Experimental results are given for case three.
The ellipse parameters are derived from image edge points. Errors are given as 3.7% for
angles and 3.0% for center positions. The ellipse backprojection gave more robust results
than the surface of revolution method especially in the presence of occluding boundaries.
Glachet et al. provide a method to determine the pose of an object of revolution
from a single image and the model of the object
[51]. The method ﬁrst computes the
perspective projection of the axis of the object based on the geometric properties of the
object of revolution.
This is performed through a prediction-veriﬁcation scheme that
computes a virtual image in which the contours are symmetrical. Experimental results
show graphical images, but no error results are reported.
21

Goldberg and Lowe give a method for veriﬁcation of 3–D model parameters given a
single image [52]. This stage is performed after hypothesis generation in a model matching
method for pose determination. A nonlinear least squared error solution is presented that
minimizes the distance between the candidate model object with the projected pose and
the corresponding image features. New work includes the deﬁnition of a coordinate tree
data structure that consists of transformations between components of the model. This
tree can also consist of null components that represent transformations without bodies.
An advantage of this structure is the ease of calculating derivatives of points with respect
to variable parameterization. No experimental results are given. A simpliﬁed method is
given by Han and Rhee to calculate the six pose parameters given a special circular target
[53]. This target consists of a circle with a center dot and a dot near the inside edge of
the circle. A restrictive assumption is that the optical axis must past through the center
dot. Each pose parameter is then calculated separately. Experimental results using this
method as a camera calibration for two stereo cameras show average errors of less than
one percent. No explicit noise evaluation is given.
Haralick describes a pose determination method from the projection of a conic or
polygon of unknown size
[54]. The technique decomposes the problem into two three-
parameter problems if the shape and size of the curve are known. Exact point correspon-
dence is not required. Conics considered are the ellipse, parabola, and hyperbola. It is
assumed that the optical axis of the camera passes through the center of the image and
that the focal length is known. The orientation is ﬁrst determined as an optimization
problem with three unknowns. The translation is then calculated. The same approach
can be used for noncoplanar curves as long as the curve can be placed in parametric form.
In addition, a disadvantage of the iterative optimization is that initial values must be sup-
plied that may not converge. No experimental results are given. Haralick in [55] further
shows that the relative orientation may be calculated from the perspective projection of a
22

rectangular target whose size and position are unknown. Given the size, then the relative
position may be determined. The equations are relatively simple and are derived geomet-
rically to give a closed-form solution. An ambiguity is present in determining which side
of the rectangle the camera is on.
A standard pattern is proposed by Kabuka and Arenas that is used for mobile
robot position measurement using a single image [56]. This pattern, which is composed
of a circle and bar codes, when viewed from any position, will permit the extraction of the
relative pose from a projected image. The circle used in the target gives rise to an ellipse
in the image, and other marks determine the rotational ambiguity. Parameters of this
image ellipse are used to derive the pose parameters. A method is given for calculating
the size, position, and orientation of the ellipse based on position-invariant moments. The
derivation of the pose from these parameters is also shown.
Analysis of error sources
show that assumptions made in the derivation give rise to errors. One example is that
the optical axis passes through the center of the circle; another is that the swing angle is
zero. Experimental results are given with errors for the pan angle and the distance values.
As an extension of the above work, Hussain and Kabuka [57] further analyze the errors
associated with this method. The error sources considered are discretization of the image,
segmentation of pixels, and perspective distortion in which it is stated that the projection
of the circular target onto the image plane is not a true ellipse. Architectures for real-time
moment generation up to seventh order are also proposed.
Lowe gives a method for object pose determination through model matching with
hypothesis and veriﬁcation [30, 58]. Model features are initially matched to image features,
and an error function is deﬁned that computes the error for each image axis between the
projected image features and the observed image features. This function is the sum of
the products of the partial derivatives of the pose parameters times the pose correction
values.
Rotations are represented by quaternions to simplify the computations.
The
23

Newton-Raphson method is applied to the error function to iteratively solve for the pose
correction values. If convergence is not obtained, an incorrect match is indicated. This
method is extended to parametric models so that both pose and model parameters are
solved for during the iterative algorithm. Line-to-line correspondences may also be used
in addition to point-to-point. An advantage of this approach is that depth information is
not explicitly required. No quantitative results are given on the accuracies obtained with
this method.
Neira et al. outline a method for validation of geometric relations as part of an
overall object recognition scheme
[59]. Early estimation of object location is required
during the model matching. Uncertainty is incorporated in a symmetries and perturba-
tion model. Geometric features are classiﬁed into one of six transformation subgroups.
Data fusion of observations is performed through an IEKF. No details are given on this
integration method. Solutions to the validation of geometric relations are given for all
combinations of symmetries. A means for veriﬁcation is also given. The approach, “rec-
ognizing while locating,” enables unary and binary constraints to validate the minimum
number of observations that can give an object location hypothesis. Additional features
are then used to verify this hypothesis. The methodology for selecting the initial set of
observations is given. Kriegman et al. give constraints for recognizing and locating curved
3–D objects from single image features [60]. The features considered are those dependent
on the viewpoint of the curved object and are not intrinsic to the object model. The object
points are related to the image points by a series of n equations and n unknowns that
are the coordinates of the model points. An example is shown, but the general method is
compute intensive.
A method of determining pose based on real or virtual 3–D lines in a target scene
is described by Kumar
[61, 62]. Correspondence is achieved between the target lines
and the image lines without endpoint correspondence. The application is for a mobile
24

robot in an outdoor environment where the features are distant from the camera. An
uncertainty measure is developed relating the variance in the solution to the noise in the
input parameters. This measure assumes a perfect 3–D model with noise occurring only in
the image data. The method used is based on the constraints given by Liu et al. [27] with
a nonlinear optimization technique proposed by Horn [63] to solve the relative orientation
problem. While algorithms for line correspondences are given, point algorithms have also
been developed. Two basic methods are presented in which one ﬁrst calculates rotation
and then translation while the second calculates rotation and translation at the same time.
The simultaneous solution generally gives the best results. Closed-form solutions are given
for the variance of the rotation and translation as a function of the noise variance of the
image data. Simulated and real experiments were performed using the developed method.
Results were good, but possible improvements are noted along with a needed increase in
robustness due to mismatching of lines.
Linnainmaa et al. describe an approach for pose determination using triples of
points on an object matched to triples of points in an image [64]. These image triples
form a triangle that are junctions connected by edges or contours. A generalized Hough
transform is used to evaluate which object triangles correspond to the image triangles
and to evaluate relative pose. Further geometric constraints are applied to the clusters
to improve the matching. Multiple solutions from the three-point pose extraction are not
a problem since the clustering and subsequent constraints eliminate all but the correct
values. Experimental results are given that show good results based on large numbers of
triangle pairs.
A method that estimates pose through model feature matching is given by Ray
[65]. One or more monocular images are used to extract line features from an object that
are matched to model features through an interpretation tree. A cost function is deﬁned
for the interpretation tree branches that assumes that the approximate pose is known for
25

the object. After correspondence is established, an optimization function produces the
maximum likelihood pose estimate based on all feature matches. Experimental results
show higher accuracy using multiple views where position errors were about 0.1 inch and
3 degrees in rotation.
Roth et al. gives another model-based approach for pose correction of a mobile
robot
[66].
In combination with dead reckoning, this method matches model feature
points with image data points. A reverse projection of points to 3–D space is used to
ﬁnd the pose correction vector for rotation and translation. As a preprocessing step, the
orientation is ﬁrst estimated by means of a vanishing point technique. This measurement
is used to restrict the search space for feature matching. No experimental results are given.
Sheu and Bond describe a method to solve for the object pose from a single image
based on a hierarchy of features [67]. This method is an extension of previous work in
model matching. These features are primitives, generalized, and compound. Primitive
features include points, lines, and ellipses. A generalized feature is a grouping of primitive
features; a compound feature combines generalized features. Ellipse-to-ellipse correspon-
dence is a signiﬁcant contribution that permits a reduction in the complexity of the inverse
projection problem. The systematic handling of the feature matching is also signiﬁcant
and provides a robust method.
1.3.4
Methods Incorporating Noise Estimation and Uncertainty
The methods described in this section incorporate noise modeling or estimation as
an integral part of the location determination solution. Uncertainty is explicitly estimated
or predicted based on input noise statistics and on the given algorithm.
In presenting a new data fusion technique, Abidi [68] gives a one-dimensional ex-
ample of the fusion of stereo and ultrasonic range data. Stereo triangulation for position
measurement, while good for the transverse coordinates, x and y, is not as accurate for
26

depth measurement, z. An ultrasonic sensor, however, has good depth accuracy but poor
x, y resolution. A quadrangular target is used in which the four corner point coordinates
are found. For both the stereo images and the range data, probability distribution func-
tions are computed for the corner point locations in x, y, and z. Data from one coordinate
at a time is fused to provide the output results.
Ayache and Faugeras provide a method for representing uncertainty in measure-
ments of points, lines, and planes that are used in building visual maps of the environment
for a mobile robot [69]. Two images are taken at each robot location as a stereo pair.
The extended Kalman ﬁlter (EKF) is used as a model for estimating the position of these
primitives and the uncertainty through the covariance matrix. The feature location noise
from the image is modeled as zero mean Gaussian noise with a known variance. As the
robot moves, additional images are taken and are used to update the ﬁlter and to improve
the estimate of the 3–D locations. The solution for rotation and translation of relative
frames for images taken during movement is similarly modeled. Multiple primitives are
matched for both the ﬁrst and second positions of the robot. Each matched pair is used
as an input to the ﬁlter that provides an estimate of the relative displacement and the
error. Questions are raised as to whether the modeling is suﬃcient for lines and planes
and whether the assumption of Gaussian distributions is valid for actual situations. Pos-
terior marginal pose estimation is described by Wells in which a probabilistic model of
image features along with prior estimates of feature matches and pose is used to derive
a pose objective function
[70].
The pose is then found from the optimization of this
objective function. Parameters of the probabilistic model are obtained from the image.
Noise from image features is assumed to be independent with a Gaussian distribution. A
linear projection model is assumed. The objective function is shown to have a relatively
simple form under these assumptions. Experimental results are given for synthetic-range
27

imagery. The objective function has a sharp peak near the correct pose while local maxima
are also present.
Scene depth and image motion are computed by a method given by Singh that
measures image ﬂow from time-varying images [71]. Kalman ﬁltering is used to calculate
the depth using image ﬂow. An advantage is that the covariance matrices are available
that provide an estimate of the error as subsequent images are acquired. The ﬁltering
technique is based on one proposed by Matthies et al.
[72] that estimates depth and
depth uncertainty at each pixel as opposed to other techniques based on image features.
Correlation techniques are used to measure the ﬂow as well as to estimate the uncertainty.
A discussion and comparison of both the feature-based method and the proposed method
are given with quantitative measurements. The convergence rate and accuracies obtained
are similar. Experimental results are given for lateral motion of the camera.
Fischler and Bolles propose a method called random sample consensus (RANSAC)
for ﬁtting a model to experimental data in which the data may contain a signiﬁcant
percentage of gross errors
[9]. This method is applied to the perspective n point pose
determination problem which is also solved as a part of this paper. The minimum number
of points for a ﬁnite number of solutions is shown to be three, and a closed-form algorithm
is given based on geometric reasoning. Examples of multiple solutions are given for three,
four noncoplanar, and ﬁve noncoplanar point sets. Four coplanar points, and six points
in a general position, are shown to give a unique solution. The RANSAC procedure is
applied to a set of overconstrained points and initially starts with as small a data set as
possible. The set is expanded iteratively including additional points that do not vary more
than a threshold from an initial model. Points greater than the threshold are considered
outliers and are not used in the pose determination. Least squared error techniques may be
applied to the remaining points. Rationale for selecting the threshold and the maximum
number of iterations is given. Implementation details and experimental results are given
28

for simulated problems and for determining camera pose from an aerial image. The method
removed all gross errors from each trial.
1.3.5
Methods Using Kalman Filtering for Direct Pose and Motion Estima-
tion
Estimation methods and applications using Kalman ﬁltering are described in this
section. The problems being solved include not only pose estimation but also motion and
3–D structure of a rigid object. Some of these are “Structure from Motion” problems
where a priori knowledge of the object is not available. These methods use point features,
generally, and require a large number of features per image to solve for the many state
variables. Batch nonlinear optimization methods are also discussed [73] where estimates
are made by analyzing an entire sequence of images at once. For real-time applications such
as robotic control, visual servoing, etc., batch analysis is not a feasible option. Extended
Kalman ﬁltering is the most widely used method of recursive estimation.
A recursive method of estimating motion, structure, and focal length is given by
Azarbayejani and Pentland
[74]. A sequence of 2–D images from which feature points
are extracted is used as input. An EKF is used for estimation. The camera model uses
the image plane as the origin to aid in parameterizing the imaging geometry. A single
parameter, the depth, provides the estimate of the state for each structure point in contrast
to other approaches that use three parameters per point [75, 73]. This parameterization
is claimed to be more stable for recursive estimation than previous methods. However,
recovery of the 3–D x and y point coordinates is subject to the noise present in the image
features and could result in signiﬁcant errors. Quaternions are used to represent rotation
indirectly.
The three rotational velocities are used in the state vector while a global
rotation state is maintained between iterations but is not used in the EKF. Simulated,
as well as experiments on actual image sequences, are given along with the performance.
29

Broida et al.
in
[75] also describe a motion and structure estimation problem
and a solution based on the IEKF. Point-based image features are inputs with the 3–D
coordinates estimated using three state variables per point. The camera-referenced object
center x and y, translation velocity, and the 3–D point coordinates are state variables
normalized to the object center depth.
A rotational quaternion and angular velocity
complete the state estimate. A normalization step on the quaternion estimate is performed
immediately after the measurement update to constrain the extra degree of freedom. Initial
estimates for the ﬁlter are obtained from a batch method applied to a set of images.
Experimental results are given for both simulated as well as real image sequences. For the
simulated case, uniformly distributed noise (to mimic quantization error) is added to the
image points. Only four points from the vertices of a cube are processed. Convergence is
obtained with 10% noise and initial estimates within about 20% of the true values.
A Kalman ﬁlter approach is used by Lee and Kay for 3–D pose and motion es-
timation from stereo images
[76, 77]. Consecutive object frames are used as input to
estimate the orientation from which the position is calculated with object rotations and
translation velocities constant. The Kalman ﬁlter state is expressed as a rotation using
quaternions for linearization. Measurement noise is derived from 2–D images rather than
the 3–D feature points that give a more accurate noise estimate when the object distance
from the camera varies over a wide range. For each stereo pair, the orientation is ﬁrst es-
timated through a least squared error method at each measurement sample. The Kalman
ﬁlter is then applied over the time sequence of estimates. Simulation results considerably
improved estimates over the individual measurements.
Westmore and Wilson describe the use of an EKF to provide an estimate of the
position of a camera mounted on a robot end point [78]. This ﬁlter is implemented as part
of the measurement portion of a closed-loop position control for a six-degree-of-freedom
robot arm. Only three of the six pose parameters, however, are estimated due to the
30

constraint that the object motion be limited to a 2–D surface. Two feature points are
extracted from the image and used as inputs to the ﬁlter. An implementation and results
are given in which the update rate is 61 Hz. Wang and Wilson describe the estimation
of both relative position and orientation of a moving object using an EKF [79]. Feature
points from the object are extracted from the image and are provided to the Kalman
ﬁlter.
Using a minimum of three object points, an estimate for the six-element pose
vector is obtained. However, ﬁve noncoplanar points were found to give better estimates.
Motion is assumed smooth enough so that the ﬁrst derivatives of the pose parameters
are constant. Any deviation from this assumption appears as a disturbance to the state
model. Both simulation and experimental results are given. An implementation is also
described that is similar to that of Westmore and Wilson
[78]. Tracking errors under
simulation were found to be within 0.6 mm. in depth and within 0.4 degree for pitch
and yaw. The measurement error covariance matrix was initialized based on image point
location variance while the disturbance covariance matrix was initialized based on known
motion trajectories. The Kalman ﬁlter output estimate variances were a factor of three to
four less than the measurement noise variances, indicating the increased level of conﬁdence
in the estimates. For the real results, the tracking errors were not as good as the simulated
ones. However, error was attributed to the forward kinematic calculations for determining
the reference positions. The ﬁlter output error variance was much less than the forward
kinematics error variance.
An algorithm for estimating depth from known camera motion is given by Matthies
and Kanade
[72]. The algorithm, suitable for parallel processing, uses Horn’s optical
ﬂow method
[80] and a Kalman ﬁlter to estimate depth at each image pixel location.
Lateral translation is shown to give the highest sensitivity to the depth measurement. A
comparison is made to a Kalman ﬁlter approach using edge locations. Performance is
shown to be almost equal to the feature-based approach. Experimental results with real
31

images show that the algorithm performs well. Burl uses a parallel version of the EKF to
estimate depth from sequential high noise images containing a moving object [81]. The
ﬁlter is used to estimate Fourier coeﬃcients as well as velocity of the object. A reduced
order ﬁlter is obtained by truncating the Fourier coeﬃcients.
In the area of robot arm control using vision, Allen et al.
[82] describe a robotic
system for tracking and grasping a moving object. The imaging system consists of two
cameras and computes optical ﬂow using triangulation to determine the 3–D position.
A standard Kalman ﬁlter is used to predict the position from the 3–D measurements.
Experimental results are given showing the robot arm successfully picking up a moving
toy train. The train motion is approximately 2–D. Papanikolopoulos et al. also describe
a system for tracking a moving object [83] with a single camera mounted on a robot arm.
This system, similarly, uses optical ﬂow and a sum of squared diﬀerences for matching
between image frames. An object moving in a plane is tracked where depth to the object
is given. Various control techniques are also given including proportional-integral (PI),
pole assignment, and linear quadratic Gaussian. Both steady state, time invariant, and
time-varying Kalman ﬁlters are used to estimate the transformation state.
Wilson et al.
[84] provide a method for visual servoing from a robot end eﬀec-
tor. Point-based visual servoing is used along with an EKF to estimate the 3–D pose of
a workpiece object using a single camera. Knowledge of the object is required for the
estimate. The EKF estimates the six-element pose vector (three translations and three
angles) and the six ﬁrst derivatives of the pose vector. Manipulator control is achieved
through speciﬁcation of joint angle coordinates. The transformation from the end eﬀector
frame to the joint coordinate frame is given. A proportional-derivative control is used
in the actual implementation.
Experimental results are given for pose estimation and
for visual servoing. Accuracy is estimated by relative displacement calculated from the
kinematics solution. Robustness of the pose estimate is also shown by randomly inserting
32

missing features into the measurement. The missing features are handled in the measure-
ment equations by increasing the variance of the corresponding term of the measurement
error covariance matrix R. Only small perturbations were observed in the state estimate
when up to two features (out of a total of ﬁve) were occluded.
1.4
Synopsis of Dissertation
Chapter 2 of this dissertation provides background theory on elements needed for
the development of the proposed pose estimation theory.
These elements include the
basic theory of 3–D transformations, perspective projection, quaternions, dual numbers,
estimation theory, and extended Kalman ﬁltering. In Chapter 3, the theoretical approach
to the method is developed along with the new theory and analysis needed to fully explain
the operation. A rationale is given for the use of the IEKF as an estimator. This area
includes the projection of 3–D lines to the 2–D image plane, the formulation of the IEKF
with the dual quaternion transformation, and analysis of error estimates.
Chapter 4
provides an analysis of the noise statistics of the line-point parameters in comparison with
point noise. This includes derivation of the probability density function in terms of the
line end point density functions and the derivation of mean, variance, and covariance of
the line points. Chapter 5 gives the system implementation in a robotic arm application in
which a single camera is mounted on the end eﬀector of the robot. Both relative pose and
motion are estimated between the camera and a known object using the method described
in Chapter 3. Results are also given for structure estimation when the object geometry
is unknown. Experimental results are given for simulated testing as well as testing on
an actual robot arm. Simulation results are given for the errors obtained under diﬀerent
types of initial estimates. The method is compared to an IEKF estimation method based
on point features. Analysis of the advantages of the proposed method is given. Chapter 6
33

summarizes the results and presents the conclusions obtained from this work. Suggestions
for future directions are also given.
34

Chapter 2
Theoretical Foundation
This chapter develops the theoretical foundation for the pose estimation method
presented in Chapter 3. The areas covered include coordinate transformations, quater-
nions, dual numbers, estimation theory, and Kalman ﬁltering theory.
2.1
Coordinate Frame Transformations
Coordinate frame transformations between orthogonal 3-D references can be repre-
sented in several ways. The most common representation is the two-step process consisting
of a displacement of the origin of one reference frame with respect to the other followed
by a rotation around three angles. The rotation is given by a 3-by-3 orthonormal matrix
R while the displacement is a three-element vector ⃗d. The coordinate transformation is
then given by
⃗xc = R⃗xw + ⃗d.
(2.1)
⃗xc and ⃗xw are three-element vectors consisting of the 3–D point coordinates in the form
[x y z]T .
A homogeneous 4-by-4 matrix T, deﬁned in terms of R and ⃗d,
T =


R
⃗d
⃗0T
1

,
(2.2)
can be applied to the transformation as follows:
35

⃗yc = T⃗yw.
(2.3)
In this case, ⃗yc and ⃗yw are four-element vectors consisting of the 3-D point coordinates in
the form [x y z 1]T .
2.2
Quaternions
A quaternion is a four-component number consisting of a scalar part and three
orthogonal parts. Quaternions are an extension of complex numbers to R4 with three
imaginary parts. Formally, a quaternion q can be deﬁned as
q = q0 + q1⃗i + q2⃗j + q3⃗k
(2.4)
where each of the qi is a real number, and ⃗i, ⃗j, and ⃗k are orthogonal imaginary unit
vectors. From the above deﬁnition, the class of quaternions can be seen to include scalars,
ordinary complex numbers, and three-element spatial vectors. The unit vectors have the
following properties:
⃗i2 = −1,⃗j2 = −1,⃗k2 = −1; and
(2.5)
⃗i⃗j = ⃗k,⃗j⃗k =⃗i,⃗k⃗i = ⃗j.
(2.6)
A geometric interpretation of a quaternion is given below in Section 2.6. Addition
and subtraction of quaternions are straightforward and can be performed element by
element to form a result that is also a quaternion. These operations are associative and
commutative. Multiplication by a scalar real number is likewise closed. The quaternion of
all zeros is a zero element over addition. An additive inverse for each quaternion also exists.
36

Quaternions, therefore, are a group that forms a vector space over the real numbers. An
equivalent representation for quaternions is a column vector [q0 q1 q2 q3]T . Multiplication
of the two quaternions s and t is deﬁned as
st
=
(s0t0 −s1t1 −s2t2 −s3t3) +⃗i(s0t1 + s1t0 + s2t3 −s3t2)
+⃗j(s0t2 −s1t3 + s2t0 + s3t1) + ⃗k(s0t3 + s1t2 −s2t1 + s3t0).
(2.7)
Quaternion multiplication is associative and distributive although it is not commutative.
The unit element quaternion is 1+⃗i0+⃗j0+⃗k0. A multiplicative inverse element exists for
all quaternions except for the zero element so that the quaternion group forms a division
algebra. Matrix forms of quaternion multiplication are given below where quaternions are
expressed as four element vectors:
pq =


p0
−p1
−p2
−p3
p1
p0
−p3
p2
p2
p3
p0
−p1
p3
−p2
p1
p0


q =
+
Mp q
(2.8)
qp =


p0
−p1
−p2
−p3
p1
p0
p3
−p2
p2
−p3
p0
p1
p3
p2
−p1
p0


q =
−
Mp q.
(2.9)
The 4-by-4 matrices
+
Mp and
−
Mp are identical except that the lower right 3-by-3 submatrix
is transposed. The conjugate of a quaternion is
37

q∗= q0 −⃗iq1 −⃗jq2 −⃗kq3.
(2.10)
The square of the magnitude of a quaternion is then deﬁned as
∥q∥2 = qq∗= q2
0 + q2
1 + q2
2 + q2
3 = q · q.
(2.11)
The multiplicative inverse element of quaternion q is then
q−1 = (1/||q||2)q∗.
(2.12)
Consider the set of quaternions with unit magnitude. Unit quaternions form a sub-
group over multiplication where the inverse is simply the conjugate. The quaternion ma-
trices
+
Mp and
−
Mp are orthogonal when the quaternion is a unit quaternion. Quaternions
are useful in describing 3-D rotations as discussed below in Section 2.4. For additional
properties, see [63, 85].
2.3
Dual Numbers
A dual number is deﬁned as [11]
d = a + ϵb
(2.13)
where a, b are real numbers and ϵ is deﬁned as ϵ2 = 0. In the following, a will be referred
to as the real part and b as the dual part. Addition is deﬁned as
d1 + d2 = (a1 + eb1) + (a2 + ϵb2) = a1 + a2 + ϵ(b1 + b2),
(2.14)
and multiplication is deﬁned as
38

d1d2 = (a1 + ϵb1)(a2 + ϵb2) = a1a2 + ϵ(a1b2 + b1a2).
(2.15)
These operations are closed as well as associative and commutative. Multiplication, as
shown above, distributes over addition. Addition has an identity element, the number
zero. From these properties, it follows that dual numbers form an abelian group under
addition. A multiplicative identity exists, but not every dual number has a multiplicative
inverse; i.e., dual numbers with zero real parts. Thus, dual numbers form an abelian ring
under multiplication and division. The conjugate of a dual number is
d∗= a −ϵb.
(2.16)
The product of a dual number and its conjugate is
dd∗= a2
(2.17)
while the modulus of a dual number is simply the real part,
|d| = a.
(2.18)
Note that the modulus can be negative. The Taylor series expansion of a dual function
about its real part has a particularly simple form:
f(a + ϵb) = f(a) + ϵbf′.
(2.19)
This property is useful in the expansion of commonly used functions. For example,
sin(ˆθ) = sin(θ) + ϵd cos(θ)
(2.20)
39

where ˆθ = θ + ϵd.
Dual numbers were ﬁrst proposed by Cliﬀord [86] and developed by Study [87].
Study represented the dual angle of two skew lines in 3-D space as a dual number. The
dual angle is
ˆθ = θ + ϵd
(2.21)
where θ is the angle between the line vectors, and d is the shortest distance between the
lines.
2.4
Dual Number Quaternions
The dual number concept may be applied to quaternions as well as vectors and
matrices. As described below, a dual quaternion can be used to represent a general 3-D
transformation. The dual-number quaternion is deﬁned as
ˆq = r + ϵs
(2.22)
where r and s are each quaternions [11].
Addition and multiplication are deﬁned in
a similar manner to real quaternions.
Analogous to the real quaternion, dual-number
quaternions form a vector space over the dual numbers and likewise form an associative
algebra. The conjugate of a dual quaternion is deﬁned as
ˆq∗= r∗+ ϵs∗.
(2.23)
The squared magnitude of a dual quaternion is
∥ˆq∥2 = ˆqˆq∗= rr∗+ ϵ(rs∗+ sr∗).
(2.24)
40

The squared magnitude is a dual number with non-negative real part. If the real part is
not zero, then an inverse exists and is
ˆq−1 = ˆq∗/∥ˆq∥2.
(2.25)
A unit dual quaternion is deﬁned as one whose magnitude is 1 + ϵ0. Unit dual
quaternions form a subgroup over multiplication with the inverse equal to the conjugate.
The conditions for a unit dual quaternion are rr∗= 1 and rs∗+sr∗= 0. These conditions
imply that
⃗r · ⃗r = 1,⃗r · ⃗s = 0.
(2.26)
2.5
3-D to 2-D Perspective Projection
The problem of pose determination is shown in Figure 2.1 along with the separate
3-D reference frames for the target object and camera. A perspective projection model,
commonly referred to as the pinhole lens model, is used where the lens center is the camera
reference origin. The process of 3-D to 2-D projection is considered. The ﬁrst step is to
transform the object coordinates to the camera reference. Next, the x and y coordinates
of the projected object onto the image plane are found. These relations are, given image
coordinates (xi yi) and camera coordinates (xc yc zc),
xi = xc
λ
zc
, and
(2.27)
yi = yc
λ
zc
(2.28)
41

Target Object
Effective Focal Length, λ
z
x
y
XO
z
x
y
XC
Image Plane
(xc , yc , zc)
(xi , yi)
Center of
Projection
Fig. 2.1.
3–D to 2–D perspective projection model assumed in pose estimation problem.
The center of projection is the camera reference origin.
42

where λ is the eﬀective focal length [88]. As can be seen, the transformation is nonlinear
and noninvertible. That is, the relations are unique for 3-D to 2-D, but the 3-D coordinates
cannot be determined uniquely from the 2-D image coordinates.
From one intensity image of the target, the 2-D feature coordinates in the image
plane can be extracted. These coordinates, along with the geometric model of the target
and the intrinsic camera parameters, provide the necessary information for pose calcu-
lation. Camera calibration performed oﬀ-line prior to the pose measurement is used to
determine the intrinsic camera parameters such as pixel size, focal length, optical center,
and lens distortion. The relation between the 3-D camera coordinates and the 2-D image
is based on the assumed perspective projection vision model.
While the calculation of the image coordinates is straightforward given the 3-D
coordinates and the camera parameters, the inverse operation of determining the 3-D
coordinates from the 2-D image plane coordinates is more diﬃcult since the correspondence
of 2-D to 3-D coordinates is not unique.
2.6
Representation of 3-D Rotation by Quaternions
Quaternions can be used to represent 3-D rotations. The unit quaternion q = [q0 ⃗q]
may be placed in the form
q = cos θ
2 + (sin θ
2)⃗u
(2.29)
where cos θ
2 = q0, sin θ
2 = √⃗q · ⃗q, and ⃗u =
⃗q
√⃗q·⃗q when ⃗q · ⃗q is not zero [85]. This expression
describes the relationship between the quaternion elements and a rotation in 3-D. The
angle-axis representation of rotation is applicable here since an axis of rotation is described
along with the magnitude of rotation. In this case, θ represents the magnitude of rotation
about an axis ⃗u. The composite product of quaternions given by
43

r′ = qrq∗
(2.30)
describes the rotational transformation from r to r′.
r and r′ are quaternions whose
scalar element is zero and whose imaginary part is the 3-D coordinate of a point in the
two reference frames. q is a unit quaternion. The 3-by-3 rotation matrix R deﬁned in
terms of a unit quaternion is
R =


q2
0 + q2
1 −q2
2 −q2
3
2(q1q2 −q0q3)
2(q1q3 + q0q2)
2(q1q2 + q0q3)
q2
0 −q2
1 + q2
2 −q2
3
2(q2q3 −q0q1)
2(q1q3 −q0q2)
2(q2q3 + q0q1)
q2
0 −q2
1 −q2
2 + q2
3


.
(2.31)
Composition of rotations is easily realized through multiplication of quaternions as
shown by
r′ = (pq)r(pq)∗.
(2.32)
Horn [63] notes that fewer arithmetic operations are required to multiply two quaternions
than are required to multiply two 3-by-3 rotation matrices.
Matrix multiplication of
orthonormal matrices does not necessarily result in an orthonormal matrix due to ﬁnite
precision arithmetic. The quaternion resulting from a product of quaternions, however,
can easily be normalized to obtain a unit magnitude. It is not straightforward to ﬁnd the
nearest orthonormal matrix from the product of rotation matrices.
44

2.7
Representation of 3–D Rotation and Translation by Dual Number
Quaternions
Quaternions are limited in the sense that only rotation is represented in a full 3–D
transformation. Translation must be dealt with separately. Dual-number quaternions,
however, provide a framework that may be used to represent both rotation and transla-
tion. To describe the relationship of dual-number quaternions to the 3–D transformation,
an explanation of the screw representation of a transformation is required. The screw
transformation is shown pictorially in Figure 2.2.
The required parameters include the screw axis, the screw angle, and the screw
pitch. The screw axis is described by a line in 3–D space that has direction ⃗l passing
through point ⃗p. A 3–D line is deﬁned by the two three-element vectors ⃗l and ⃗m where
⃗m = ⃗p ×⃗l. While six parameters are used in this deﬁnition, only four degrees of freedom
are present due to the constraints ⃗l · ⃗m = 0 and ∥⃗l∥= 1 [89]. This particular formulation
leads to the dual quaternion representation of the line,
ˆ⃗l = ⃗l + ϵ⃗m.
(2.33)
The screw transformation then requires eight parameters for its representation. Walker
[11] and Daniilidis [89] have shown that a unit dual quaternion ˆq = r + ϵs representing
the 3-D transformation can be expressed as
ˆq =


cos(ˆθ/2)
sin(ˆθ/2)ˆ⃗l


(2.34)
where ˆ⃗l is a dual vector that represents the screw axis about which the coordinate system
has rotated and translated as given by ˆθ. An alternate form of a unit dual quaternion is
45

x
y
z
x
y
z
(Direction Vector)
θ
d
l
(Pitch)
Frame 1
Frame 2
(Screw Rotation)
p
(Point on Line)
8-tuple (l, p, θ, d)
Fig. 2.2.
Diagram showing screw form of 3–D transformation between two reference
frames. Eight parameters are needed to specify the complete transformation.
46

ˆq = q + ϵ t
2q.
(2.35)
q, in this form, is a real unit quaternion representing only rotation while t is a vector
quaternion representing translation.
The resulting 3-D transformation of a line can be expressed simply as
ˆl′ = ˆqˆlˆq∗.
(2.36)
The form of this equation is identical to that of Equation 2.30 for real quaternions. Com-
positions of tranformations are likewise similar,
ˆl′ = ˆpˆqˆlˆq∗ˆp∗.
(2.37)
An expression for the dual quaternion ˆq can also be represented in terms of a dual angle
that is similar to the angular form of the standard quaternion
ˆq =


cos( ˆθ
2)
sin( ˆθ
2)ˆ⃗n


(2.38)
where ˆ⃗n is a dual vector representation of a 3-D line and ˆθ is a dual angle.
The unit dual quaternion transforms lines from one coordinate frame to another.
In comparison, the real quaternion transforms points from one frame to another that diﬀer
by only a ﬁnite rotation.
2.8
Iterated Extended Kalman Filter
A dynamic process or system may be deﬁned by a set of state variables, some or all
of which cannot be measured directly. Available measurements are a function of the state
47

variables and may be noisy. The process itself may not be deterministic. The Kalman
ﬁlter, ﬁrst proposed by R.E. Kalman in 1960 [90], is the minimum variance Bayes’ estimate
of the state variables of a linear system model. It is the optimum in a least squares sense
of a linear system with additive Gaussian noise in both the process and the measurement.
In addition, it is the optimal linear ﬁlter for all other noise distributions [91, 92]. The
standard Kalman ﬁlter assumes a linear system model as well as a linear measurement
model. The linear system model is described by
⃗xk = Φk−1⃗xk−1 + ⃗wk−1
(2.39)
where ⃗xk is the state at sample k; ⃗xk−1 and ⃗wk−1, respectively, are the states and
process noise at sample k −1. The process noise, ⃗wk−1, is assumed to be white, zero-
mean Gaussian distribution with covariance matrix Qk at sequence number k. Φk−1 is
the state transition matrix at sample k. The linear measurement model is given by
⃗zk = Hk⃗xk + ⃗vk
(2.40)
where ⃗zk is the measurement vector at sample k.
⃗vk is the measurement noise, also
assumed white, zero-mean Gaussian with covariance matrix Rk. The ﬁlter provides es-
timates of the state and the error covariance matrix associated with the estimate. The
state estimate extrapolation from one sample to the next is
˜xk(−) = Φk−1˜xk−1(+).
(2.41)
˜xk is used to denote the estimate of xk. The (−) and (+) notations represent the estimate
before and after the measurement update, respectively. The Kalman ﬁlter also estimates
48

the state error through the error covariance matrix Pk. The error covariance extrapolation
is given by
Pk(−) = Φk−1Pk−1(+)ΦT
k−1 + Qk−1.
(2.42)
In addition to the extrapolation equations that incorporate the dynamic model, the ﬁlter
incorporates the measurement information through the use of the measurement update
equations. The state estimate update equation is
˜xk(+) = ˜xk(−) + Kk(⃗zk −Hk˜xk(−)).
(2.43)
Kk is called the ﬁlter gain and is given by
Kk = Pk(−)HT
k (HkPk(−)HT
k + Rk)−1.
(2.44)
To complete the Kalman ﬁlter deﬁnition, the error covariance update equation is
Pk(+) = (I −KkHk)Pk(−).
(2.45)
Initial estimates for the state and error covariances are ˜x0 = E[⃗x(0)] and P0 = E[(⃗x(0) −
˜x0)(⃗x(0) −˜x0)T ]. The measurement and process noises are assumed uncorrelated, i.e.,
E[⃗wT
i ⃗vj] = 0 for all i, j.
To apply the Kalman ﬁltering technique to nonlinear dynamic processes and to
nonlinear measurement models, extensions to the standard method have been developed.
The EKF and IEKF linearize the nonlinear function about the state estimate using the
linear terms of a Taylor series expansion about the current estimate. Considering the EKF
ﬁrst, the nonlinear system model has the form
49

⃗xk = φk−1(⃗xk−1) + ⃗wk−1.
(2.46)
The next state is assumed to be an explicit nonlinear function of the present state with
additive process noise. The nonlinear measurement model is
⃗zk = hk(⃗xk) + ⃗vk
(2.47)
with the measurements being an explicit nonlinear function of the present state and with
additive measurement noise. The linear term of the Taylor series expansion of the state
transition function is found from
Φk(˜xk(−)) = ∂φk(⃗x)
∂⃗x
⃗x=˜xk(−)
(2.48)
while the linear measurement expression is given by
Hk(˜xk(−)) = ∂h(⃗x)
∂⃗x
⃗x=˜xk(−).
(2.49)
The state estimate extrapolation uses the nonlinear function in predicting the next state,
˜xk(−) = φk−1(˜xk−1(+)),
(2.50)
and the nonlinear measurement function is used in calculating the state update,
˜xk(+) = ˜xk(−) + Kk(⃗zh −hk(˜xk(−))).
(2.51)
However, the computation of Kk requires the linearized term Hk in an equation that is
identical to the standard Kalman ﬁlter,
50

Kk = Pk(−)HT
k (HkPk(−)HT
k + Rk)−1.
(2.52)
The term is also used in the error covariance update equation
Pk(+) = (I −KkHk)Pk(−)
(2.53)
while the covariance extrapolation requires the linearized Φk that is also the same as the
regular Kalman ﬁlter,
Pk(−) = Φk−1Pk−1(+)ΦT
k−1 + Qk−1.
(2.54)
The initial conditions are the same as the standard ﬁlter. An implicit assumption
is that the functions φ and h are diﬀerentiable over all ⃗x. The transition matrix Φ and
the measurement matrix H are linearized about the latest state estimate. As a result,
Kk and Pk are random variables that depend on the sequence of state estimates, unlike
the standard ﬁlter for which these variables may be computed prior to any measurements.
Pk is, thus, not the true covariance matrix but is only an approximation.
Figure 2.3
shows, in block diagram form, the functionality of the EKF. The inputs consist of the
measurements, the measurement noise covariance, and the process noise covariance; the
outputs are the next state estimate and the error covariance matrix. The diagram shows
the ﬁlter operation as a feedback control loop where the summing junction minimizes the
error between the measurements and the estimated measurements.
In the IEKF, the updated state estimate ˜xk(+) is used repeatedly as a new lineariza-
tion point to obtain new estimates of ˜xk(+), Kk, and Pk(+). The iterative expressions
for these updated variables are
51

∂φ
∂
k
s s
s
s
k
−
=
+
−
1
1
( )
~
( )
K H P
k
k
k( )
−
hk(s)
φ k
s
−1( )
Transpose
Transpose
Transpose
Delay
Inverse
∂
∂
h s
s
k
s sk
( )
~ ( )
=
−
1
Pk( )
+
Φk−1
Pk( )
−
Hk
Qk
Rk
zk
K z
z
k
k
k
(
~ )
−
z
z
k
k
−~
)
(
~ +
ks
~
( )
sk−+
1
~ ( )
sk −
-
-
Kk
~zk
Delay
Φ
Φ
k
k
k
T
P
−
−
−
+
1
1
1
( )
H P
H
k
k
k
T
( )
−
Fig. 2.3.
Extended Kalman ﬁlter block diagram showing functionality along with the
inputs and outputs. The ﬁlter is seen to have a closed-loop feedback control where the
measurement error is minimized.
52

˜xk,i+1(+)
=
˜xk(−) + Kk,i(⃗zk −hk(˜xk,i(+)) −Hk(˜xk,i(+))(˜xk(−) −
−˜xk,i(+)))
(2.55)
Kk,i
=
Pk(−)HT
k (˜xk,i(+))(Hk(˜xk,i(+))Pk(−)HT
k (˜xk,i(+)) +
+Rk)−1
(2.56)
Pk,i+1(+)
=
(I −Kk,iHk(˜xk,i(+)))Pk(−)
(2.57)
˜xk,0(+)
=
˜xk(−)
(2.58)
where i denotes the iteration number.
The state estimate will converge after a ﬁnite
number of iterations. The result, in general, is a more accurate estimate of the state at
the expense of additional computational cost.
53

Chapter 3
Pose and Motion Estimation Method
This chapter develops the method for pose and motion estimation from monocular
camera images.
The perspective projection equations for points from 3–D to 2–D are
shown in Chapter 2 to be nonlinear. Calculation of relative position and orientation from
the 3–D coordinates of corresponding points in two reference frames as shown by [9],
[63], and [4] is also nonlinear. For the minimum three points needed to calculate a pose,
multiple solutions are possible. Solutions with four or more points are overconstrained
with a minimum error criterion used to determine the result. Motion estimation requires
a dynamic system model that, as will be shown, is nonlinear in the rotational dynamics.
Direct linear estimation techniques, such as the standard Kalman ﬁlter, are not applicable.
The problem examined in this chapter is: estimation of the 3–D pose and motion
of an object with respect to a camera. Two-dimensional images of the object are available
from the camera at regular time intervals.
The object and the camera, or both, may
be moving with respect to a ﬁxed reference frame. The object is well deﬁned, of known
geometry and size, and has a suﬃcient number of point and line features visible from a
particular view angle from which the pose estimation can be uniquely determined. The
approach to solving this estimation problem is given below. An extension to this theory is
also provided to estimate object structure where the object geometry is initially unknown.
54

3.1
Pose and Motion Estimation Using IEKF and Dual Number
Quaternions
The pose and motion estimation method is based on the IEKF and uses a dual
quaternion representation of a general 3-D transformation. The IEKF is applicable to this
problem since the dynamic state is highly nonlinear as is the measurement function. Use
of the dual quaternion representation provides a consistent, uniform treatment of both
rotational and translational states.
3.2
State Assignment
The state assignment for this problem provides an estimate of the pose or trans-
formation between the camera and the object reference frames and the ﬁrst derivatives of
this transformation. The assignment is based on the dual quaternion representation of the
3-D transformation as described in Chapter 2. Since the dual quaternion has a real part
and a dual part, both of which are quaternions, a direct implementation of this structure
would require eight variables with an additional eight for the ﬁrst derivatives. However,
since there are only six degrees of freedom in the 3-D transformation and since the ﬁlter
computation time is proportional to the number of state variables, fewer variables are
desirable. Based on the approach given by Broida [75], the state variable assignment is
⃗s =
h
tx ty tz q0 q1 q2 q3 vx vy vz ωx ωy ωz
iT .
(3.1)
Thirteen variables are used: ti and vi terms are the linear translation and linear
velocity, respectively; qi is the rotational quaternion in each axis; and ωi is the rotational
velocity in each axis. Translation, rather than the dual part of the dual quaternion, is part
55

of the state estimate since the dual part can readily be calculated from the translation
and the rotational real quaternion. This relation is given by
ˆq = q + ϵ t
2q
(3.2)
where t is a vector quaternion formed from the translation vector and by setting the scalar
part to zero. The ﬁrst derivative is
˙ˆq = ˙q + ϵ(
˙t
2q + t
2 ˙q).
(3.3)
Thus, the ﬁrst derivative of the dual quaternion ˙q can easily be computed from the linear
velocity and the other known state variables. Also, with the linear velocity as state vari-
ables, the state update for the linear translation becomes very simple. The ﬁrst derivative
of the translation is also continuous for all real dynamic translations.
Chou [85] gives the relation between quaternion angular velocity and the spatial
angular velocity,
Ω=


0
ωx
ωy
ωz


= 2 ˙qq∗.
(3.4)
Ωis a vector quaternion where the vector portion is the angular velocity about the axes.
Solving for ˙q gives
˙q = 1
2Ωq.
(3.5)
56

This equation shows that from an estimate Ωof angular velocity, the derivative of the
rotational quaternion may be easily determined. In a real physical system, the angular
velocity is continuous; from this equation, the quaternion derivative is also continuous.
While a minimal state variable representation of rotation using three rotation an-
gles instead of the four needed by a quaternion is possible, several diﬃculties arise. A ﬁnite
rotation range is used for each angle, generally [−π, π) or [0, 2π). A discontinuity exists at
the point where the wraparound occurs, i.e., the value becomes 0 at a 2π rotation angle.
The quaternion would also have to be calculated from the rotation angles. Additional
computation, including the evaluation of several trigonometric functions, negates the ad-
vantage of reducing the number of state variables by one. However, since the quaternion
has four parameters, an additional degree of freedom is present. As a result, normalization
of the quaternion to unit magnitude after each iteration is performed.
3.3
System Model
A functional block diagram of the overall system is shown in Figure 3.1.
This
diagram shows the basic operations involved in the pose estimation problem. A system
model will be deﬁned to ﬁt the discrete Kalman ﬁlter formulation. That is, a function φ
and additive noise ⃗w are required that can be used to calculate the next state in terms of
the present state as
⃗sk = φk−1(⃗sk−1) + ⃗wk−1.
(3.6)
⃗w is assumed to be additive Gaussian noise with zero mean and covariance matrix Qk−1.
The state transition function φ extrapolates from the state at time interval k−1 to the next
state at time interval k. The linear and angular velocities are assumed constant so that
57

Measurement
Noise
Process
Noise
State Estimate of
Pose and Velocity
Initial Error
Covariance
Model Geometry in
Object Reference Frame
Measurements
Measurements for
IEKF Estimation
IEKF for Pose and
Motion Estimation
Feature Extraction
and Model
Correspondence
2-D
Image of Object
2-D
Image of Object
Perspective
Projection to 2-D
3-D
Transformation to
Camera Reference
Frame
Object in
Reference Frame
Initial State
Fig. 3.1.
Functional block diagram of estimation problem showing operations involved in
pose estimation method from optical imaging to state estimate produced by the iterated
extended Kalman ﬁlter.
58

ωi(tk) = ωi(tk−1) and vi(tk) = vi(tk−1). Translation updates are also straightforward if
the velocity between time intervals is assumed constant.
The quaternion propagation in time must be derived. The diﬀerential equation is
given in Equation 3.5 where quaternion multiplication is applied. An alternate form where
Ωis expanded to its equivalent 4-by-4 matrix is
˙q = 1
2


0
−ωx
−ωy
−ωz
ωx
0
−ωz
ωy
ωy
ωz
0
−ωx
ωz
−ωy
ωx
0


q = ¯Ωq.
(3.7)
The solution when all ωi are constant is
q(t) = e¯Ω(t−tk)q(tk),
(3.8)
which is similar to that given in [75]. Since
2¯Ω
∥Ω∥is an orthogonal matrix and a skew-
symmetric matrix, the eigenvalues are ±i, ±i [93]. After solving for the closed form of
the matrix exponential [94], the solution, when simpliﬁed, is
q(t) =

cos(∥Ω∥(t −tk)
2
)I +
2
∥Ω∥sin(∥Ω∥(t −tk)
2
)¯Ω

q(tk).
(3.9)
The state transitions for each variable have now been deﬁned. The state transition
function is
59

φk(⃗s) =


tx + τvx
ty + τvy
tz + τvz
Qtranq(tk)
vx
vy
vz
ωx
ωy
ωz


(3.10)
where
Qtran =

cos(∥Ω∥(τ)
2
)I +
2
∥Ω∥sin(∥Ω∥τ)
2
)¯Ω

.
(3.11)
Qtran is the 4-by-4 quaternion transition matrix.
3.4
Measurement Model
The measurement update equation is
⃗zk = hk(⃗sk) + ⃗vk.
(3.12)
The measurement noise ⃗vk is a zero mean, Gaussian sequence, with covariance matrix
Rk. hk comprises the 3-D transformation and perspective projection functions shown in
Figure 3.1. Since the dual quaternion operation transforms lines to lines, the given model
features from the object are lines represented as dual vector quaternions
60

ˆli = li + ϵmi
(3.13)
where ⃗li is the unit direction vector of the line and ⃗mi is deﬁned as ⃗pi ×⃗li, with ⃗pi being
the coordinates of a point ⃗p on the line i.
The minimum number of model lines is four for coplanar lines and six for noncopla-
nar lines to calculate a consistent unique pose. The following equation is used to calculate
each transformed line:
ˆl′
i = ˆqˆliˆq∗.
(3.14)
Perspective projection is then applied to the transformed lines. The projected line lies in
a plane deﬁned by the 3-D line and the center of projection. This plane is described by
the equation
mxx + myy + mzz = 0
(3.15)
that intersects the image plane at z = −λ. The result is an equation of the projected line
in the z = −λ plane,
mxxi + myyi = mzλ,
(3.16)
where xi and yi are the image plane coordinates. Placing this equation in a normalized
form results in
mxxi
r
m2
x + m2
y
+
myyi
r
m2
x + m2
y
=
mzλ
r
m2
x + m2
y.
(3.17)
From this equation, the direction vector of the image line is
61

⃗li =


−
my
q
m2
x+m2
y
mx
q
m2
x+m2
y
0


.
(3.18)

mzλ
q
m2
x+m2
y

is the minimum distance from the origin to the line. Since the image plane is
located at z = −λ, the distance from the perspective center is
dpc =
v
u
u
u
u
u
t




mzλ
r
m2
x + m2
y




2
+ λ2 =
λ|⃗m|
r
m2
x + m2
y.
(3.19)
Since ⃗mi = ⃗pi ×⃗li where ⃗pi is a point on the image line and ⃗mi deﬁnes the same plane as
⃗m,
⃗mi = dpc
⃗m
|⃗m| =
λ
r
m2
x + m2
y
⃗m.
(3.20)
The projected image line is now deﬁned by a dual vector as
ˆ⃗li = ⃗li + ϵ⃗mi.
(3.21)
3.5
Representation of Lines in a Plane
A rationale for investigating lines as features as opposed to points or higher-order
curves is that planar surfaces are commonly found on manmade objects, particularly
in indoor environments. Intersection of these surfaces then results in 3-D lines. These
issues are discussed by Talluri in [95]. The image of an indoor environment containing
62

these surfaces results in linear features in the image plane that can be extracted in a
straightforward procedure.
The result of the perspective projection from the 3-D lines is a set of dual vec-
tor quaternion coplanar lines located in the image plane. Six parameters are required to
represent each of these lines. Since the lines are restricted to a known plane, however,
only two degrees of freedom are present. A format is also needed to compare these trans-
formed lines with line features measured from the acquired images. The format used for
representing these lines is an x, y point called the line point. A line point is deﬁned as
the intersection of the line feature with a line passing through the image origin that is
perpendicular to the line feature. Figure 3.2 illustrates the deﬁnition of the line point on
the image plane. The line point is unique for all lines except for those lines that pass
through the origin. For these cases, it is assumed the lines approach the origin with a
distance δ but do not actually pass through the origin. The line point has the advantage of
minimum state representation and a simple distance measure as well as being continuous
for all lines. Other line representations such as slope, intercept or ρ, θ have discontinuities
that cause considerable diﬃculties when partial derivatives are taken in calculating the
IEKF operating point.
The line point is calculated from the dual vector image line as
xlp = liymiz
(3.22)
ylp = −lixmiz.
(3.23)
In terms of the 3-D line dual vector components,
xlp = λ mxmz
m2
x + m2
y
(3.24)
63

.
Line  l + εm
(xlp,ylp)
X
Y
Line Point
ρ
θ
Fig. 3.2.
Line point for a line in 2-D image plane is deﬁned as the intersection point of
the line and a perpendicular line passing through the origin.
64

ylp = λ
mymz
m2
x + m2
y
.
(3.25)
Line features from the acquired image are likewise placed in this representation as mea-
sured variables.
3.6
Linearization of the State Transition Function
The linearized state transition matrix is computed as the partial derivative of the
state transition function with respect to each state variable and evaluated at time t = tk.
The time dependency requires that it be computed at each state update. Formally, Φk is
determined as
Φk(˜xk(−)) = ∂φk(⃗x)
∂⃗x
⃗x=˜xk(−).
(3.26)
Φk is a matrix of dimension n×n where n is the number of state variables. In the problem
being addressed here, n = 13. This matrix is
65

Φk =


1
0
0
· · ·
τ
0
0
0
0
0
0
1
0
· · ·
0
τ
0
0
0
0
0
0
1
· · ·
0
0
τ
0
0
0
0
0
0
0
0
0
...
Qtran
...
Qω
0
· · ·
1
0
0
0
0
0
0
· · ·
0
1
0
0
0
0
0
· · ·
0
0
1
0
0
0
0
· · ·
· · ·
1
0
0
0
· · ·
· · ·
0
1
0
0
· · ·
· · ·
0
0
1


.
(3.27)
Qtran is deﬁned above in Equation 3.11. Qω is deﬁned as
Qω = ∂Qtranq(tk)
∂⃗ω
⃗ω=⃗ωk
(3.28)
where ω =
h
ωx ωy ωz
iT . Qω is a 3-by-4 matrix. Each row may be computed separately
as given below. The partial derivative of Qtranq(tk) with respect to each ωi is
∂Qtranq(tk)
∂ωi
=
 
−ωiτ
2|ω| sin(|ω|τ
2 ) +
 
−ωiτ
|ω|2 cos(|ω|τ
2 ) −ωi
|ω|3 sin(|ω|τ
2 )
!
¯Ω+
+ 1
|ω| sin(|ω|τ
2 ) ∂¯Ω
∂ωi
!
q(tk).
(3.29)
The partial derivatives of ¯Ωwith respect to ωx, ωy, and ωz are
66

∂¯Ω
∂ωx
=


0
−1
0
0
1
0
0
0
0
0
0
−1
0
0
1
0


(3.30)
∂¯Ω
∂ωy
=


0
0
−1
0
0
0
0
1
1
0
0
0
0
−1
0
0


(3.31)
∂¯Ω
∂ωz
=


0
0
0
−1
0
0
−1
0
0
1
0
0
1
0
0
0


.
(3.32)
3.7
Linearization of the Measurement Function
The linearized measurement matrix is the partial derivative of the measurement
function with respect to each state variable and evaluated at time t = tk. Thus, it is time
dependent and must be calculated at each measurement update step. Formally, Hk is
determined as
Hk(˜sk(−)) = ∂h(⃗s)
∂⃗s
⃗s= ˜Sk(−).
(3.33)
Hk is a 2i-by-n matrix where i is the number of measured lines and n is the number of
states. For each measured line, the partial derivative of each line-point coordinate needs
to be computed
67

∂xlp
∂si
=
∂xlp
∂mx
∂mx
∂si
+
∂xlp
∂my
∂my
∂si
+
∂xlp
∂mz
∂mz
∂si
(3.34)
and
∂ylp
∂si
=
∂ylp
∂mx
∂mx
∂si
+
∂ylp
∂my
∂my
∂si
+
∂ylp
∂mz
∂mz
∂si
.
(3.35)
si above is a state variable.
The dual quaternion transformation repeated here is
ˆl = ˆqˆlmˆq∗.
(3.36)
Expanding each dual quaternion gives
l + ϵm = (r + ϵs)(lm + ϵmm)(r∗+ ϵs∗)
(3.37)
= rlmr∗+ ϵ(rlms∗+ rmmr∗+ slmr∗).
(3.38)
Let t = [0 ¯t]T be a quaternion where ¯t is the three-element translation vector. Then,
s = 1
2tr [11]. Substituting in the above equation and solving for m gives
m = rmmr∗+ 1
2rlmr∗t∗+ 1
2trlmr∗
(3.39)
using the relation s∗= 1
2r∗t∗.
m is in terms of the state variables.
The quaternion
multiplications may be replaced by the corresponding matrix forms to give
m =
+
Mr
−
M∗
r mm + 1
2
−
Mt∗
+
Mr
−
Mr∗lm + 1
2
+
Mt
+
Mr
−
Mr∗lm.
(3.40)
.
68

Factoring results in
m =
+
Mr
−
M∗
r mm + 1
2(
+
Mt +
−
Mt∗)
+
Mr
−
Mr∗lm.
(3.41)
Let R =
+
Mr
−
M∗
r and Mt+t∗=
+
Mt +
−
Mt∗. R has the standard 3-by-3 rotation matrix in
the lower right submatrix. The partial derivatives of m with respect to each state variable
can now be calculated
∂m
∂ti
= 1
2
∂Mt+t∗
∂ti
Rlm
(3.42)
where ti is one of the three translation variables;
∂m
∂qi
= ∂R
∂qi
mm + 1
2Mt+t∗∂R
∂qi
lm
(3.43)
where qi is one of the four rotational quaternion variables;
∂m
∂vi
= 0
(3.44)
where vi is one of the three linear velocity variables; and
∂m
∂ωi
= 0
(3.45)
where ωi is one of the three angular velocity variables. Mt+t∗also has a simpliﬁed form,
Mt+t∗=


0
0
0
0
0
0
−2t3
2t2
0
2t3
0
−2t1
0
−2t2
2t1
0


.
(3.46)
69

lm and mm are quaternions that compose the dual quaternion representation of the object
model and are known from the geometric description of the model. The matrix R is
R =


1
0
0
0
0
q2
0 + q2
1 −q2
2 −q2
3
2(q1q2 −q0q3)
2(q1q3 + q0q2)
0
2(q1q2 + q0q3)
q2
0 −q2
1 + q2
2 −q2
3
2(q2q3 −q0q1)
0
2(q1q3 −q0q2)
2(q2q3 + q0q1)
q2
0 −q2
1 −q2
2 + q2
3


.
(3.47)
The partial derivatives of R with respect to each rotational quaternion variable are
∂R
∂q0
=


0
0
0
0
0
2q0
−2q3
2q2
0
2q3
2q0
−2q1
0
−2q2
2q1
2q0


(3.48)
∂R
∂q1
=


0
0
0
0
0
2q1
2q2
2q3
0
2q2
−2q1
−2q0
0
2q3
2q0
−2q1


(3.49)
∂R
∂q2
=


0
0
0
0
0
−2q2
2q1
2q0
0
2q1
2q2
2q3
0
−2q0
2q3
−2q2


(3.50)
70

∂R
∂q3
=


0
0
0
0
0
−2q3
−2q0
2q1
0
2q0
−2q3
2q2
0
2q1
2q2
2q3


.
(3.51)
The partial derivatives of Mt+t∗with respect to each translational variable are
∂Mt+t∗
∂t0
=


0
0
0
0
0
0
0
0
0
0
0
−2
0
0
2
0


(3.52)
∂Mt+t∗
∂t1
=


0
0
0
0
0
0
0
2
0
0
0
0
0
−2
0
0


(3.53)
∂Mt+t∗
∂t2
=


0
0
0
0
0
0
−2
0
0
2
0
0
0
0
0
0


.
(3.54)
The partial derivatives of the image points xlp and ylp with respect to each element of ⃗m
are given below:
71

∂xlp
∂mx
=
λmz
(m2
x + m2
y)
−2
λm2
xmz
(m2
x + m2
y)2
(3.55)
∂xlp
∂my
=
−2
λmxmymz
(m2
x + m2
y)2
(3.56)
∂xlp
∂mz
=
λmx
(m2
x + m2
y)
(3.57)
∂ylp
∂mx
=
−2
λmxmymz
(m2
x + m2
y)2
(3.58)
∂ylp
∂my
=
λmz
(m2
x + m2
y)
−2
λm2
ymz
(m2
x + m2
y)2
(3.59)
∂ylp
∂mz
=
λmy
(m2
x + m2
y).
(3.60)
3.8
Iterated Extended Kalman Filter Representation
The required elements of the IEKF are given above. An initial estimate is required
based on prior knowledge. An initial error covariance matrix, P0, that is dependent on the
prior knowledge must be speciﬁed. Process and measurement noise covariance matrices,
Qk and Rk, respectively, must also be speciﬁed. After each iteration, the derived partial
derivatives with respect to each state variable are calculated to determine the linearized
equations for the Kalman ﬁlter. With the updated state estimate, a new linearization
is then performed about this state. Several iterations about the present state are then
performed converging to the most accurate estimate.
72

3.9
Extension of Iterated Extended Kalman Filter to Estimate Struc-
ture
When the object or scene geometry is unknown, the method described above is not
suﬃcient since the 3–D model parameters are assumed known. In this section, the estima-
tion method is extended to estimate structure in addition to the position and orientation.
The structure is estimated in terms of line features for consistency with the previously
developed theory. Both pose and structure are estimated simultaneously. However, with-
out prior information about an absolute dimension of the object, the structure and the
translation can only be estimated to within a scale factor due to perspective projection
[75]. A small object in close proximity to the camera will project the same image as a
large object far away. Rotation, on the other hand, can be determined absolutely. This
section gives the representation of the structure by lines and the extensions to the IEKF.
3.9.1
Structure Representation
Lines have been represented above by the dual vector ⃗l+ϵ⃗m where ⃗l is the direction
vector normalized to unit magnitude and ⃗m = ⃗p ×⃗l with ⃗p being a point on the line. The
magnitude of ⃗m is equal to the perpendicular or shortest distance from the origin to the
line. Also, since ⃗m is orthogonal to ⃗l, ⃗m ·⃗l = 0. Using this latter relation to estimate the
sixth, ﬁve state variables are estimated per line. These variables are:
h
mx
my
mz
lx
ly
i
.
(3.61)
lz can be derived from the dot product relation. The normalization relation for ⃗l is applied
after lz is calculated. This step is analogous to the quaternion normalization performed
after the measurement update. The state vector given by Equation 3.1 is extended by 5n
variables where n is the number of structure lines being estimated.
73

3.9.2
Extension to State Transition
Since the structure is assumed constant with time, the extension to the state tran-
sition function φk(sk) and the state transition matrix Φk(sk) is straightforward. For the
state transition function, additional elements corresponding to the number of structure
variables are added to the vector function to give an identity update. Additional columns
are appended to the state transition matrix with an identity update for the corresponding
state variable and zero in all other terms. Each structure line extends the matrix by ﬁve
columns.
3.9.3
Extension to Measurement Update
The measurement update requires the partial derivatives of the image plane pa-
rameters with respect to each state variable. As given previously, the relation between
the camera referenced line parameter vector ⃗m and the object referenced line is
⃗m = R⃗mm + Mt+t∗Rlm.
(3.62)
The partial derivatives of ⃗m with respect to each additional state variable have the form
for the vector ⃗mm,
∂⃗m
∂⃗mm
=


∂mx/∂mmx
∂my/∂mmx
∂mz/∂mmx
∂mx/∂mmy
∂my/∂mmy
∂mz/∂mmy
∂mx/∂mmz
∂my/∂mmz
∂mz/∂mmz


= RT ,
(3.63)
which is just the transpose of the rotation matrix. For the vector lm, the result is
∂⃗m
∂⃗lm
= (Mt+t∗R)T .
(3.64)
74

This expression is in terms of the translation and rotation and, as above, does not depend
on the particular state variable.
The ﬁrst two rows correspond to the state variables
lmx and lmy. The measurement update matrix is augmented with these values during
estimation with the IEKF.
75

Chapter 4
Noise Analysis
4.1
Analysis of Line Noise
In the simulation tests, noise is applied to the projected image points of the model.
The noise is assumed to be additive, independent, identically distributed Gaussian noise.
These noisy points are then used directly as measurements for the reference point method
against which the line method is compared. From these noisy points, lines are formed
by connecting pairs of points. These lines, as represented by line-point parameters given
in Chapter 3, are used as measurements for the line method.
Since the noise for the
points is independent with respect to x and y as well as each other, the measurement
error covariance matrix for the point method is a straightforward diagonal matrix whose
diagonal elements are the noise variances while the oﬀ-diagonal terms are zero. Figure
4.1 shows the noise added to the points as well as the variation of the line connecting
the points. The noise characterization of the line, however, is not as simple as that for
the points.
The line-point measurement error covariance matrix has signiﬁcant cross-
covariance terms and is dependent on the location of the points within the image as well
as the noise statistics. In this chapter, the derivation of the probability density function
(pdf) for the x and y line-point parameters is given along with the determination of the
covariance matrix.
An approximation to the covariance matrix that permits real-time
implementation is also shown.
76

p2 (mx2,my2)
σ
p1 (mx1,my1)
Fig. 4.1.
Diagram showing a point pair with a noise radius of one standard deviation
and the corresponding range of variation of the line connecting the points.
4.1.1
Probability Density Function of Line-Point Parameters
Consider two image points, p1 and p2, in an image where the x and y positions
of both points are independent, identically distributed Gaussian random variables. Con-
necting these points as shown in Figure 4.1 produces a line whose line-point values can be
expressed as a function of the end point x and y locations. The problem addressed here is
to calculate the pdf of the x and y line-point values given the pdf of the end points. The
end points are assumed to have a Gaussian density function. These pdf’s are given below:
px1(x1)
=
1
√
2πσ exp

−
(x1 −mx1)2
2σ2


px2(x2)
=
1
√
2πσ exp

−
(x2 −mx2)2
2σ2


py1(y1)
=
1
√
2πσ exp

−
(y1 −my1)2
2σ2


py2(y2)
=
1
√
2πσ exp

−
(y2 −my2)2
2σ2


(4.1)
77

where mi is the mean value of point i.
The standard deviation σ of the point noise is assumed to be the same for all points.
Mean values are the mean projections of the 3–D point on an object to the image plane.
To simplify the derivation, the following intermediate transformation is initially deﬁned:
xd
=
1
2(x2 −x1)
yd
=
1
2(y2 −y1)
xs
=
1
2(x2 + x1)
ys
=
1
2(y2 + y1).
(4.2)
The pdf of a variable y given y = f(x), and the pdf of x is [96]:
py(y) =
px(x)
|dy/dx|
x=f−1(y)
(4.3)
provided there is a one-one correspondence between x and y. Extending the pdf transfor-
mation to two variables results in
u
=
f1(x, y)
v
=
f2(x, y)
puv(u, v)
=
pxy(x, y)
J
u
x
v
y

(4.4)
where J is the Jacobean as deﬁned by the determinant
78

J =

∂u/∂x
∂u/∂y
∂v/∂x
∂v/∂y
.
(4.5)
It is assumed that J exists, is continuous over the domain, and is nonzero. This
ensures that the Jacobean has the same sign over the variable domain. In cases where the
Jacobean is continuous but passes through zero, it is possible to consider the regions of
same sign separately in evaluating the pdf [97].
Applying the bivarate pdf transformation to the speciﬁc case being examined here
results in the pdf of xs, ys, xd, and yd in terms of the pdf of the point variables x1, x2,
y1, and y2. First, the Jacobeans, Jxdxs and Jydys, are determined:
Jxdxs =

∂xd/∂x1
∂xd/∂x2
∂xs/∂x1
∂xs/∂x2

=

−1
2
1
2
1
2
1
2

= −1
2
(4.6)
Jydys =

∂yd/∂y1
∂yd/∂y2
∂ys/∂y1
∂ys/∂y2

=

−1
2
1
2
1
2
1
2

= −1
2.
(4.7)
Applying the transformations in Equation 4.2 and simplifying gives
pxdxs(xd, xs)
=
px1x2(x1, x2)
Jxdxs

= 2px1(x1)px2(x2)
pydys(yd, ys)
=
py1y2(y1, y2)
Jydys

= 2py1(y1)py2(y2).
(4.8)
Since the point random variables are statistically independent, the joint pdf’s are equal
to the product of the individual pdf’s. Substituting the Gaussian expressions for the pdf
of x1 and x2 in this equation and combining terms gives
79

pxdxs(xd, xs)
=
1
2πs2 exp

−
(xd −mxd)2
2s2

exp

−
(xs −mxs)2
2s2


pydys(yd, ys)
=
1
2πs2 exp

−
(yd −myd)2
2s2

exp

−
(ys −mys)2
2s2


(4.9)
where s = σ
√
2 is the standard deviation of the transformed variables xd, xs, yd, and ys.
Since pxdxs(xd, xs) and pydys(yd, ys) may be separated to give separate Gaussian
pdf’s, pxdpxs and pydpys, xs, xd, ys, and yd are statistically independent. As shown below,
this transformation simpliﬁes the calculation of the pdf of the line-point parameters, xlp
and ylp, given below. The line point, as given in Chapter 3, can be found from the line
direction and a point on the line. The direction vector l of the line formed by the two
points is


lx
ly

=


xd
q
x2
d+y2
d
yd
q
x2
d+y2
d


.
(4.10)
A point p on the line is (xs, ys), the line midpoint, and, since m = p × l, m is
m =


0
0
xsly −yslx


.
(4.11)
To recall the previously derived relation for xlp and ylp from Chapter 3,
xlp = lymz and ylp = −lxmz.
(4.12)
80

Substituting for lx, ly, and mz gives
xlp
=
xsy2
d −ysxdyd
x2
d + y2
d
ylp
=
ysx2
d −xsxdyd
x2
d + y2
d
.
(4.13)
Note that the xlp and ylp are duals in that the corresponding equations are the same
except that x is replaced by y to convert from one equation to the other. Any result for
one term may be easily found for the other.
Since xlp is a function of the derived variables, the pdf of xlp may be found from the
transformation given by the form of Equation 4.4 except that four variables are involved
in the transformation,
pxlpxdydys(xlp, xd, yd, ys) =
pxdydxsys(xd, yd, xs, ys)
Jxlp
xlpxdydys
xdydxsys

.
(4.14)
xs is eliminated in the transformation. Equation 4.13 is used to solve for xs in terms of
the remaining variables giving
xs =
(x2
d + y2
d)xlp + ysxdyd
y2
d
.
(4.15)
The Jacobean Jxlp is
81

Jxlp
=

∂xlp
∂xd
∂xlp
∂yd
∂xlp
∂xs
∂xlp
∂ys
1
0
0
0
0
1
0
0
0
0
0
1

(4.16)
=
y2
d
x2
d + y2
d
.
(4.17)
Jxlp here is positive and continuous and exists for all pairs of points that are physically
separate. Substituting this result in Equation 4.14 gives
pxlpxdydys(xlp, xd, yd, ys) =
x2
d + y2
d
y2
d
pxd(xd)pyd(yd)pxs(xs)pys(ys).
(4.18)
The products of the individual pdf’s are shown instead of the joint density function
since all variables are independent. Integration of this expression provides the sought after
pdf of xlp,
pxlp(xlp) =
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞

x2
d + y2
d
y2
d

pxd(xd)pyd(yd)pxs(xs)pys(ys)dxddyddys,
(4.19)
with Equation 4.15 substituted for xs. This result has no direct analytical solution but
may be solved numerically. Figures 4.2, 4.3, 4.4, and 4.5 show several examples of pxlp and
pylp calculated numerically.
The examples show two horizontal lines, a -1 slope line and
a +1 slope line. In addition, the best-ﬁt Gaussian is shown overlaid on the plot. For each
82

Probability Density Function for x  Line Point Given Horizontal Line End 
Points with Additive Gaussian Noise
0.00E+00
1.00E+00
2.00E+00
3.00E+00
4.00E+00
5.00E+00
6.00E+00
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
x Line Point (Image Plane Coordinates in mm.)
pdf
Gaussian
5.00E+00
5.10E+00
5.20E+00
5.30E+00
5.40E+00
5.50E+00
5.60E+00
5.70E+00
-0 .04
-0 .03
-0 .02
-0 .01
0
0.01
0.02
0.03
0.04
Fig. 4.2.
Probability density function of xlp for 1.25 mm. length horizontal line centered
about the y axis, ys= 3.125 mm., point noise 0.02 mm. standard deviation. A best-ﬁt
Gaussian pdf is also plotted, but the diﬀerences are seen to be small.
83

Probability Density Function for y  Line Point Given Horizontal Line End 
Points with Additive Gaussian Noise
0.00000
5.00000
10.00000
15.00000
20.00000
25.00000
30.00000
3.04
3.06
3.08
3.1
3.12
3.14
3.16
3.18
3.2
y Line Point (Image Plane Coordinates in mm.)
pdf
Gaussian
26.00000
26.20000
26.40000
26.60000
26.80000
27.00000
27.20000
27.40000
27.60000
27.80000
28.00000
3.118
3.12
3.122
3.124
3.126
3.128
3.13
Y lp  (Im ag e Plan e  Co o rd in ate s in  m m .)
Fig. 4.3.
Probability density function of ylp for 1.25 mm. length horizontal line centered
about the y axis, ys= 3.125 mm., point noise 0.02 mm. standard deviation. A best-ﬁt
Gaussian pdf is also plotted, but the diﬀerences are seen to be small.
84

Probability Density Function for x  Line Point Given Diagonal Line 
(-45 degrees) End Points with Additive Gaussian Noise
0.00E+00
5.00E-01
1.00E+00
1.50E+00
2.00E+00
2.50E+00
11
11.5
12
12.5
13
13.5
14
x Line Point (Image Plane Coordinates in mm.)
pdf
Gaussian
1.86E+00
1.88E+00
1.90E+00
1.92E+00
1.94E+00
1.96E+00
1.98E+00
2.00E+00
12.42
12.44
12.46
12.48
12.5
12.52
12.54
12.56
12.58
Fig. 4.4.
Probability density function of xlp for 1.25 mm. length downward diagonal
line at xs=12.5 mm., ys= 12.5 mm., point noise 0.02 mm. standard deviation. A best-ﬁt
Gaussian pdf is also plotted, but the diﬀerences are seen to be small.
85

Probability Density Function for x  Line Point Given Diagonal Line 
(45 degrees) End Points with Additive Gaussian Noise
0.00E+00
5.00E-01
1.00E+00
1.50E+00
2.00E+00
2.50E+00
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
x Line Point (Image Plane Coordinates in mm.)
pdf
Gaussian
1.86E+00
1.88E+00
1.90E+00
1.92E+00
1.94E+00
1.96E+00
1.98E+00
2.00E+00
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
Fig. 4.5.
Probability density function of xlp for 1.25 mm. length upward diagonal line
at xs=12.5 mm., ys= 12.5 mm., point noise 0.02 mm. standard deviation. A best-ﬁt
Gaussian pdf is also plotted, but the diﬀerences are seen to be small.
86

case, the Gaussian is seen to provide a good ﬁt to the data. These examples show that for
a wide range of point-pair combinations, the pdf of the line point may be approximated by
a Gaussian. The motivation here is to show that the line-point measurement input noise
to the Kalman ﬁlter is approximately Gaussian, a condition necessary to achieve optimum
performance. Note also that the variance of the density functions is not equal to the point
noise variance and varies depending on the line conﬁguration.
4.1.2
Covariance of xlp and ylp
In this section, the covariance expressions for the line-point parameters xlp and ylp
are derived. The expectation of the line-point parameter, xlp in terms of the derived point
parameters, is
xlp
=
fx(xd, yd, xs, ys)
=
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
fx(xd, yd, xs, ys)pxd(xd)pyd(yd)pxs(xs)pys(ys)
dxddyddxsdys;
(4.20)
and, for ylp, it is
ylp
=
fy(xd, yd, xs, ys)
=
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
fy(xd, yd, xs, ys)pxd(xd)pyd(yd)pxs(xs)pys(ys)
dxddyddxsdys.
(4.21)
The variance of xlp is
87

varxlp =
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
(xlp −xlp)2pxd(xd)pyd(yd)pxs(xs)pys(ys)dxddyddxsdys;
(4.22)
and, for ylp, it is
varylp =
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
(ylp −ylp)2pxd(xd)pyd(yd)pxs(xs)pys(ys)dxddyddxsdys
(4.23)
while the covariance of xlp and ylp is
(xlpylp)2
=
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
(xlp −xlp)(ylp −ylp)pxd(xd)pyd(yd)pxs(xs)pys(ys)
dxddyddxsdys.
(4.24)
These expressions are exact and may be solved numerically since no analytical
solution is known. However, a quadruple integration is computationally intensive. Simpli-
ﬁcations are possible to reduce the required computation with one approach given here.
Equation 4.13 may be expressed in the form
xlp
=
sin2 θxs −sin θ cos θys
ylp
=
cos2 θys −sin θ cos θxs
(4.25)
where
88

sin θ =
yd
q
x2
d + y2
d
and cos θ =
xd
q
x2
d + y2
d
.
(4.26)
θ is the angle of the line measured with respect to the x axis. The range is restricted to
between −π
2 and π
2 since the signs of xd and yd are arbitrary depending on which point
is chosen as 1 and which is chosen as 2. The expected value or mean for the line-point
parameters can then be expressed as
xlp
=
sin2 θmxs −sin θ cos θmys
ylp
=
cos2 θmys −sin θ cos θmxs
(4.27)
since θ and xs, ys are independent. This reduces the problem to calculating the means of
the trigonometric quantities
sin2 θ
=
Z ∞
−∞
Z ∞
−∞
y2
d
x2
d + y2
d
pxd(xd)pyd(yd)dxddyd
cos2 θ
=
Z ∞
−∞
Z ∞
−∞
x2
d
x2
d + y2
d
pxd(xd)pyd(yd)dxddyd
sin θ cos θ
=
Z ∞
−∞
Z ∞
−∞
xdyd
x2
d + y2
d
pxd(xd)pyd(yd)dxddyd.
(4.28)
The integral here is only a double integral requiring considerably less computation
than the triple integral of Equation 4.20. The expressions for the variances and covariance
can also be simpliﬁed. These quantities are given below in terms of expectation operators:
89

s2
xlp
=
x2
lp −xlp
2
s2
ylp
=
y2
lp −ylp
2
sxlpylp
=
xlpylp −xlpylp
(4.29)
where s2
xlp and s2
ylp are the variances of xlp and ylp, respectively, and sxlpylp is the
covariance of xlp, ylp. Substituting Equation 4.25 for xlp and ylp and simplifying gives
the following:
s2
xlp
=
sin4 θ(m2
xs + s2) −2cos θ sin3 θmxsmys + cos2 θ sin2 θ(m2
ys + s2) −x2
lp
s2
ylp
=
cos4 θ(m2
ys + s2) −2cos3 θ sin θmxsmys + cos2 θ sin2 θ(m2
xs + s2) −y2
lp
sxlpylp
=
2cos2 θ sin2 θmxsmys −cos3 θ sin θ(m2
ys + s2)
−cos θ sin3 θ(m2
xs + s2) −xlpylp
(4.30)
where s2 is the variance of xs and ys.
Each of the trigonometric expectations may be solved by double integration in
the form of Equation 4.28. These values are then used to calculate the variances and
covariances. Numerical integration has been used to calculate these covariances for several
cases of line orientation. Figure 4.6 shows the change in s2
xlp for a 1.25 mm. long horizontal
line centered about the y axis as ys increases from 0 to 12.5 mm. Figure 4.7 gives similar
results for a diagonal line at a 45 degree angle except that the large variability is in
s2
ylp.
While the covariance is negligible for horizontal or vertical lines, the covariance is
maximum for a 45 degree slope line. Another factor that inﬂuences the variance is the
90

Line Point Variance for Horizontal Line Centered about y  Axis
0.0000001
0.000001
0.00001
0.0001
0.001
0.01
0.1
1
0
2
4
6
8
10
12
14
ys Value (mm.)
Line Point Variance
xlp var.
point var.
ylp var.
Approx xlp
var.
Approx ylp
var.
Fig. 4.6.
Variance of line-point parameters for 1.25 mm. horizontal line centered on
the y axis as ys varies, point noise 0.02 mm. standard deviation, compared with point
noise variance. ylp variance is always less than the point variance while the xlp variance
becomes greater after only a small displacement.
91

Line Point Variance for Diagonal Line Centered about y  Axis
0.00001
0.0001
0.001
0.01
0.1
1
0
2
4
6
8
10
12
14
ys Value (mm.)
Line Point Variance
ylp var.
point var.
xlp var.
Approx xlp
var.
Approx ylp
var.
Fig. 4.7.
Variance of line-point parameters for 1.25 mm. diagonal line (45 degree angle)
centered on the y axis as ys varies, point noise 0.02 mm. standard deviation, compared
with point noise variance. xlp variance is always less than the point variance while the
ylp variance becomes greater after only a small displacement.
92

length of the line. As the line length increases, the variation in angle decreases resulting
in smaller variances.
This result is an important factor in the Kalman ﬁlter performance. The covari-
ances of the line points determine the values of the measurement covariance matrix in
the ﬁlter formulation. The magnitude of the covariances reﬂects the uncertainty in the
measurement. Where the covariance is low, the measured quantity has low uncertainty.
When the Kalman ﬁlter performs the measurement update, the input measurements are
weighted according to the covariance matrix values. In comparison with the point method,
the variances and covariances may be lower where the lines are longer and near the center
of the image. Short lines and lines farther away from the origin will have larger variances
than the points from which the lines are formed. The line method would be expected
to provide better estimates for objects in the center of the image than would the point
method. As the object moves signiﬁcantly away from the center or moves away from the
camera, the performance advantage of the line method would be expected to decrease
and, at some position, to drop below that of the point method. The implication here for
a moving object or scene is that the measurement covariance matrix must be updated
to provide the optimal weighing for the measurements at each sample instance. A ﬁxed
covariance matrix would be expected to give worse performance than an adaptive one for
the same measurements.
4.2
Real-Time Implementation
The evaluation of the covariance values depends on the numerical double integration
of several terms. The direct numerical calculation of these quantities is not feasible for a
real-time implementation. An approximation, however, has been developed that enables
its use in real-time applications. In the expressions for the variance calculations above,
the expected values of the trigonometric quantities depend on mxd and myd. For a scene
93

with moving objects that are being tracked, the line features that are projected to the
image plane thus change with time. At each iteration, the measurement covariance matrix
R needs to be updated based on the variance of the line-point parameters. The following
procedure gives the steps needed for the real-time estimation broken down into preliminary
calculations and then the calculations done during each iteration.
The preprocessing calculations are as follows:
Let mij = cosi θ sinj θ. Calculate m10, m01, m11, m20, m02, m40, m31, m22,
m13, and m04 for a table of angles from −π
2 to π
2 . The number of steps stored
in this table determines the resulting accuracy of the estimate. For example,
100 steps would resolve the angle to 1.8 degrees. The variance terms are given
by
vij = mij −
Y
i
m10
Y
j
m01.
(4.31)
vij is the variance of the corresponding sini θ cosj θ term with respect to xd and
yd. An initial variance and an initial line length are assumed. These form the
variance and line length squared ratio. During each iteration, an adjustment
is made to the variance vij based on the actual line length.
The run time processing calculations performed at each iteration are:
Calculate the angle and length squared of each feature line. The variance of
the xd and yd variables is assumed known. Calculate the ratio of the variance
and line length squared.
Then, calculate the ratio of this quantity to the
94

precalculated ratio. Index the moment and variance table according to the
nearest angle value. Then, calculate
s20 = m10m10 + ratio v20
(4.32)
s11 = m10m01 + ratio v11
(4.33)
and the remaining sij terms in a similar manner. These sij terms are the
approximate expected value terms to be substituted in Equation 4.30. The
ratio is used to correct the variances for diﬀerences in line length and point
noise standard deviation.
The variances calculated using this simpliﬁed method are shown in Figures 4.6 and
4.7. The real-time method is seen to give good results in comparison with the true values.
Simulation results in applying this adaptive method are given in Chapter 5.
95

Chapter 5
Experimental Results
The theoretical model for the image-based pose and motion estimation method has
been developed in Chapter 3. A suitable application is a dynamic system application with
a recursive solution that can be used in a real-time environment. This chapter provides an
evaluation for this model through both simulation and actual physical system testing in
this environment. The purpose is to demonstrate that the method works, that it performs
accurately and reliably, and that the relative performance in comparison with other prior
methods is good.
A dynamic vision application for the approach is deﬁned where a camera provides
the visual feedback for a robot performing a task. Simulation testing is presented ﬁrst.
This testing measures the accuracy of the estimation under an assumed noise distribution
and magnitude.
A target object with point features is used for the simulation.
The
results are compared with corresponding results from a point-based extended Kalman
ﬁltering method using an identical target to illustrate the performance diﬀerences. Speed
of convergence, mean square error, and stability are presented and analyzed. In the second
part of the chapter, actual test results measuring relative motion and position from a robot
arm are presented. Relative accuracy and noise characterization are given.
5.1
Simulation Results
Simulation testing has been performed to evaluate the performance of the pose
estimation method under a variety of conditions. The results are obtained by varying
noise levels, initial conditions, and object complexity to characterize errors, convergence,
96

and stability in the state estimate. The EKF calculates the state estimate that includes
the translation, the rotation quaternion, the linear velocity, and the angular velocity.
Mean square errors associated with each of these state variables are presented. Figure 5.1
shows the functional blocks associated with the estimation. The following dynamic model
assumptions given below were used for the simulation.
5.1.1
Dynamic Model
The object to be tracked is simulated, along with a camera located some distance
away viewing the object. The reference frame for the object is deﬁned in terms of the
object and therefore moves as the object moves relative to a base coordinate reference.
The camera and its corresponding reference frame are ﬁxed relative to this base frame.
A path for the object is deﬁned in terms of its position and orientation over time with
respect to the camera reference frame
⃗x(t) = ⃗f(t)
(5.1)
where ⃗x(t) is the position and orientation state vector and ⃗f(t) is, in general, a nonlinear
function of time. Smooth continuous motion is assumed so that all time derivatives of the
state exist. In addition, ⃗f(t) is assumed to be the solution of a linear diﬀerential vector
equation. With these assumptions, a state vector composed of ⃗x and a ﬁnite number of
derivatives with respect to time of ⃗x permits the expression of the path motion as a linear
system model,
⃗sk = Φtrue⃗sk−1,
(5.2)
where Φtrue is a constant coeﬃcient matrix of dimension n-by-n. n is the total number of
3-D parameters and derivatives required to represent the state sk. The simulated target
97

Measurement
Noise
Process
Noise
State Estimate of
Pose and Velocity
Initial Error
Covariance
Model Geometry in
Object Reference Frame
Measurements
Measurements for
IEKF Estimation
IEKF for Pose and
Motion Estimation
Feature Extraction
and Model
Correspondence
2-D
Image of Object
2-D
Image of Object
Perspective
Projection to 2-D
3-D
Transformation to
Camera Reference
Frame
Object in
Reference Frame
Initial State
Fig. 5.1.
Functional block diagram of the estimation problem showing the operations
involved in the pose estimation method from optical imaging to state estimate produced
by the iterated extended Kalman ﬁlter.
98

object starts at an initial position and orientation aligned with the camera reference and
separated by a distance of 1000 cm. in the z-axis direction. The position and orientation
are modeled by the set of variables
pose = {x y z θx θy θz}
(5.3)
where θx, θy, and θz are rotation angles about the x, y, and z axes, respectively. These
orthogonal rotation angles are commonly referred to as pitch, yaw, and roll. A trajectory
is deﬁned for the simulation that assumes constant velocity in all state variables.
5.1.2
Model Trajectory
For the given trajectory with constant velocities, each pose variable has the form
si = cit + di
(5.4)
with velocity ci and initial value di. In this formulation, the state vector is deﬁned as
⃗s = [x
y
z
θx
θy
θz
vx
vy
vz
ωx
ωy
ωz]T .
(5.5)
The value of the second derivative of the ith state variable is
¨si = wi
(5.6)
where wi is a white noise sequence from N(0, σ). Forming the transition matrix for the
state sequence gives
99

⃗sk = Φk−1⃗sk−1 =


1
0
0
0
0
0
c1τ
0
0
0
0
0
0
1
0
0
0
0
0
c2τ
0
0
0
0
0
0
1
0
0
0
0
0
c3τ
0
0
0
0
0
0
1
0
0
0
0
0
c4τ
0
0
0
0
0
0
1
0
0
0
0
0
c5τ
0
0
0
0
0
0
1
0
0
0
0
0
c6τ
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1


(5.7)
with time interval τ and initial state vector
⃗s0 = [d1 d2 d3 d4 d5 d6 c1 c2 c3 c4 c5 c6]T .
(5.8)
5.1.3
Camera Model
An ideal camera model is used in the simulation. Perspective projection is assumed
for the camera with a known eﬀective focal length. No lens distortion correction is applied.
Image acquisition and feature extraction are assumed performed so that the simulated
measurement data corresponds to the location of the feature in the image plane in actual
units. Noise of an assumed magnitude and distribution is added to the image feature
locations before processing.
100

5.1.4
Object Model
A target object is simulated with individual feature points. Pairs of these points,
when extracted from the image plane, are connected together to form lines. For these tests,
two objects were deﬁned. The ﬁrst has four coplanar points in a rectangular pattern. The
second target has six points with four being coplanar, as with the ﬁrst target, and two
located outside the plane. Figure 5.2(a) shows the dimensions and the shape of the four-
point target while Figure 5.2(b) is for the six-point target.
(a)
80 mm.
100 mm.
53.9 mm.
(b)
80 mm.
100 mm.
Fig. 5.2.
Target shapes and dimensions used in the simulation experiments: (a) four-point
coplanar target, and (b) six-point polyhedral 3-D target.
5.1.5
Estimation Model
The IEKF proposed here is used to estimate the position, orientation, and cor-
responding velocities of the object with respect to the camera. The noisy image plane
101

feature locations are used as inputs along with the a priori knowledge. As previously
given, the estimated state vector is
⃗s =
h
x y z q0 q1 q2 q3 vx vy vz ωx ωy ωz
iT .
(5.9)
5.1.6
Comparison Model
To evaluate the proposed method, a comparison model for estimation using an EKF
has been simulated based on previous work [75]. The state vector of estimated variables
is the same as Equation (5.9) above. Point features instead of line features are used for
input measurements. The frame transformation for a point ⃗x is modeled as
⃗x′ = R⃗x + ⃗t.
(5.10)
The same series of simulated tests were performed on this model. A comparison of results
that shows the performance diﬀerences obtained between the two methods is given.
5.1.7
Initial Conditions
Initial conditions requiring speciﬁcation include the initial state, ⃗s0, and the error
covariance matrix, P0. The state vector may be considered a collection of Gaussian random
variables with covariance P0. The initial state is a sample taken from each random variable.
Process noise given by the covariance matrix Q is also speciﬁed as an initial condition for
the simulations, remaining constant throughout. Similarly, measurement noise given by
the covariance matrix R is speciﬁed initially as a constant.
5.1.8
Measurement Noise Model
Optimal linear Kalman ﬁltering assumes that both measurement noise and process
noise are zero mean, Gaussian distributed. A Gaussian distribution is generally a good
102

approximation for phenomena that are the result of a large number of random ﬂuctuations.
However, in the case considered here, line or point features are extracted from a 2–D
digital image containing pixels quantized to discrete locations. The measurement noise
corresponds to the variation in location of the feature from successive image frames. As
given by
[98], the actual feature locations are not always closely approximated by a
true Gaussian distribution.
Since the pixels are quantized and since the features are
very localized within the image, a more exact noise model is used. This model is a zero
mean, truncated Gaussian distribution in which no noise samples are present exceeding
an integral number of standard deviations. In this section, the truncation factor is set at
two standard deviations of the normal Gaussian distribution.
5.1.9
Test Results
Simulation tests were performed using the four-point object model. These tests
give both a sample run and the experimentally determined RMS error over time for the
model trajectory. In addition, the point-based IEKF method was implemented and tested
under simulation as a basis for comparison. The results show data from both methods.
Table 5.1 gives the initial state conditions for the simulation using the trajectory
described above with a four-point geometric model. The initial states are the same for
both methods. Table 5.2 gives the assumed initial error covariance diagonal terms and
the process noise covariance diagonal terms for both the dual quaternion method and the
point method used for comparison.
Note that the process noise is diﬀerent for each
method. In all cases, the oﬀ-diagonal terms of the covariance matrices are zero. For these
tests, the simulated sample interval is 0.1 second. In these tests, the nonadaptive dual
quaternion measurement error method was used. The error values were held constant over
the entire measurement interval.
103

Table 5.1.
Actual and assumed initial states of the extended Kalman ﬁlter for the four-point simu-
lation tests. The initial states are the same for both the dual quaternion method and the
point method.
Translation
Quaternion
Linear
Rotational
State
Velocity
Velocity
x y z
q0 q1 q2 q3
x y z
x y z
(mm.)
(mm./sec.)
(rad./sec.)
True Initial State
10 10 1000
1 0 0 0
-5 2 -5
-0.03 0.05 -0.2
Initial State Estimate
0 0 990
0.9998 0.01 0.01 0.01
0 0 0
0 0 0
Table 5.2.
Dual quaternion method and point method initial error covariance matrix and process
noise matrix diagonal terms of the extended Kalman ﬁlter for the four-point simulation
tests.
Translation
Quaternion
Linear
Rotational
Noise Covariance
Velocity
Velocity
x y z
q0 q1 q2 q3
x y z
x y z
(mm.2)
(mm./sec.)2
(rad./sec.)2
Dual Quaternion
100
0.01
100
0.1
Error Covariance
Dual Quaternion
10−5
10−5
10−5
10−6
Process Noise
Point Method
100
0.01
10
0.1
Error Covariance
Point Method
10−4
10−6
10−4
10−5
Process Noise
104

The input noise is a truncated Gaussian with a standard deviation of 0.02 mm.
as described above for the measurement noise model.
This noise level corresponds to
approximately four percent of the image size in the image.
Based on this noise level,
the measurement error covariance matrix has diagonal elements of 0.0004 mm.2 for each
measured variable. The oﬀ-diagonal terms are all zero. Although the dynamic model is
the same for both the dual quaternion method tests and the comparison method tests,
tuning and stabilization adjustments were required for both methods. Nonzero process
noise quantities were needed even though the dynamic model assumed no process uncer-
tainty. The resulting process noise and initial error covariance diagonal are given in the
table. These values represent experimentally determined optimum values for the assumed
dynamic model. Stability was a signiﬁcant problem for the reference method with this
level of noise. Estimate errors occasionally became unbounded when no process noise was
used. Stability was not as great a concern for the dual quaternion method since the errors
did not become unbounded, reaching approximately ten percent error in the z-translation
parameter. Signiﬁcant improvements in errors were achieved, however, by adjustment of
these values. Error results are given later in this section.
Figures 5.3 through 5.15 show the results of a sample run with the four-point model
where the true path is shown along with the dual quaternion estimated values and the
comparison method estimation. The simulation was run for a time of 30 seconds with a
measurement interval of 0.1 second. As can be seen, the estimates from both methods
track the state variables reasonably well. However, the depth estimates, z and vz, and
angular velocities show larger diﬀerences, particularly as shown in the z axis, Figure 5.5;
linear z-axis velocity, Figure 5.12; and the angular y-axis velocity, Figure 5.14. The dual
quaternion method settles to the correct value faster than the point-based method for
these cases.
For linear y-axis velocity the point method has a larger overshoot before
settling. Further details are shown in Figures 5.16 through 5.28.
105

Pose Estimation Results for X Translation
-140
-120
-100
-80
-60
-40
-20
0
20
0
5
10
15
20
25
30
Time (sec.)
Translation (mm.)
Point estimate
True 
Line estimate
Fig. 5.3.
tx translation pose estimate simulation results with a four-point target showing
linear translation over time for the line method, the comparison point method, and the
true x translation value from a sample run. Both methods are seen to give good estimation
results for this axis.
Pose Estimation Results for Y Translation
0
10
20
30
40
50
60
70
0
5
10
15
20
25
30
Time (sec.)
Translation (mm.)
Point estimate
True 
Line estimate
Fig. 5.4.
ty translation pose estimate simulation results with a four-point target showing
linear translation over time for the line method, the comparison point method, and the
true y translation value from a sample run. Both methods are seen to give good estimation
results for this axis.
106

Pose Estimation Results for Z Translation
840
860
880
900
920
940
960
980
1000
1020
0
5
10
15
20
25
30
Time (sec.)
Translation (mm.)
Point estimate
True 
Line estimate
Fig. 5.5.
tz translation pose estimate simulation results with a four-point target showing
linear translation over time for the line method, the comparison point method, and the
true z translation value from a sample run. More deviation from the true value is seen for
this axis than in x and y although the errors are not large.
Pose Estimation Results for Quaternion Element q0
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
Time (sec.)
Quaternion Value
True 
Line estimate
Point estimate
Fig. 5.6.
q0 quaternion pose estimate simulation results with a four-point target showing
constant rotation over time for the line method, the comparison point method, and the true
q0 quaternion value from a sample run. Both methods are seen to give good estimation
results for this state variable.
107

Pose Estimation Results for Quaternion Element q1
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0
5
10
15
20
25
30
Time (sec.)
Quaternion Value
True 
Line estimate
Point estimate
Fig. 5.7.
q1 quaternion pose estimate simulation results with a four-point target showing
constant rotation over time for the line method, the comparison point method, and the
true q1 quaternion value from a sample run. More deviation from the true value is seen
for this axis than in the q0 estimate with signiﬁcant errors occurring at the beginning and
at the end.
Pose Estimation Results for Quaternion Element q2
-0.05
0
0.05
0.1
0.15
0.2
0.25
0
5
10
15
20
25
30
Time (sec.)
Quaternion Value
True 
Line estimate
Point estimate
Fig. 5.8.
q2 quaternion pose estimate simulation results with a four-point target showing
constant rotation over time for the line method, the comparison point method, and the
true q2 quaternion value from a sample run. More deviation from the true value is seen
for this axis than in the q0 estimate with signiﬁcant errors occurring at the beginning and
at the end.
108

Pose Estimation Results for Quaternion Element q3
-1
-0.9
-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
0
0
5
10
15
20
25
30
Time (sec.)
Quaternion Value
True 
Line estimate
Point estimate
Fig. 5.9.
q3 quaternion pose estimate simulation results with a four-point target showing
constant rotation over time for the line method, the comparison point method, and the true
q3 quaternion value from a sample run. Both methods are seen to give good estimation
results for this state variable.
Pose Estimation Results for X Linear Velocity
-6
-5
-4
-3
-2
-1
0
1
2
0
5
10
15
20
25
30
Time (sec.)
Linear Velocity (mm./sec.)
True 
Line estimate
Point estimate
Fig. 5.10.
vx translation velocity pose estimate simulation results with a four-point
target showing constant linear velocity over time for the line method, the comparison
point method, and the true vx velocity value of -5 mm./sec. from a sample run. Both
methods are seen to give good estimation results for this axis after the initial settling time.
109

Pose Estimation Results for Y Linear Velocity
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
0
5
10
15
20
25
30
Time (sec.)
Linear Velocity (mm./sec.)
True 
Line estimate
Point estimate
Fig. 5.11.
vy translation velocity pose estimate simulation results with a four-point
target showing constant linear velocity over time for the line method, the comparison
point method, and the true vy velocity value of 2 mm./sec. from a sample run. Both
methods are seen to give good estimation results for this axis after the initial settling
time.
Pose Estimation Results for Z Linear Velocity
-6
-5
-4
-3
-2
-1
0
1
2
3
4
0
5
10
15
20
25
30
Time (sec.)
Linear Velocity (mm./sec.)
True 
Line estimate
Point estimate
Fig. 5.12.
vz translation velocity pose estimate simulation results with a four-point
target showing constant linear velocity over time for the line method, the comparison
point method, and the true vz velocity value of -5 mm./sec. from a sample run. The line
method estimates are seen to settle faster to the true value and with less error than the
point method.
110

Pose Estimation Results for Angular Velocity about X Axis
-0.25
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity (rad./sec.)
True 
Line estimate
Point estimate
Fig. 5.13.
ωx angular velocity pose estimate simulation results with a four-point target
showing constant angular velocity over time for the line method, the comparison point
method, and the true ωx angular velocity value of -0.03 rad./sec. from a sample run. Both
methods are seen to give good estimation results for this rotation angle after the initial
settling time, although, higher divergence is present for the point method near the end of
the simulation.
Pose Estimation Results for Angular Velocity about Y Axis
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity (rad./sec.)
True 
Line estimate
Point estimate
Fig. 5.14.
ωy angular velocity pose estimate simulation results with a four-point target
showing constant angular velocity over time for the line method, the comparison point
method, and the true ωy angular velocity value of 0.05 rad./sec. from a sample run. Both
methods are seen to give good estimation results for this rotation angle after the initial
settling time, although, higher divergence is present for the point method near the end of
the simulation.
111

Pose Estimation Results for Angular Velocity about Z Axis
-0.25
-0.2
-0.15
-0.1
-0.05
0
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity (rad./sec.)
True 
Line estimate
Point estimate
Fig. 5.15.
ωz angular velocity pose estimate simulation results with a four-point target
showing constant angular velocity over time for the line method, the comparison point
method, and the true ωz angular velocity value of -0.2 rad./sec. from a sample run. Both
methods are seen to give good estimation results for this rotation angle after the initial
settling time.
Pose Estimation Error for x  Translation
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Line error
Point error
Fig. 5.16.
tx translation pose estimate simulation results for a four-point target showing
linear translation estimation error over time for the line method and the comparison point
method from a sample run. The deviation for both methods is seen to be small with
similar errors over the simulation time.
112

Pose Estimation Error for y  Translation
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Line error
Point error
Fig. 5.17.
ty translation pose estimate simulation results for a four-point target showing
linear translation estimation error over time for the line method and the comparison point
method from a sample run. The deviation for both methods is seen to be small with
similar errors over the simulation time.
Pose Estimation Error for z  Translation
-10
-8
-6
-4
-2
0
2
4
6
8
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Line error
Point error
Fig. 5.18.
tz translation pose estimate simulation results for a four-point target showing
linear translation estimation error over time for the line method and the comparison point
method from a sample run. The deviation for both methods is larger than for the x and
y translations. Response time is faster for the line method with somewhat smaller errors
over the simulation time.
113

Pose Estimation Error for Quaternion Element q 0
-0.006
-0.004
-0.002
0
0.002
0.004
0.006
0.008
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line error
Point error
Fig. 5.19.
q0 quaternion pose estimate simulation results for a four-point target showing
rotation estimation error over time for the line method and the comparison point method
from a sample run. The deviation for both methods is seen to be small with similar errors
over the simulation time.
Pose Estimation Error for Quaternion Element q 1
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line error
Point error
Fig. 5.20.
q1 quaternion pose estimate simulation results for a four-point target showing
rotation estimation error over time for the line method and the comparison point method
from a sample run. The deviation for both methods is seen to be small with similar errors
over the simulation time except at the end where the point method has somewhat larger
error.
114

Pose Estimation Error for Quaternion Element q 2
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line error
Point error
Fig. 5.21.
q2 quaternion pose estimate simulation results for a four-point target showing
rotation estimation error over time for the line method and the comparison point method
from a sample run. The deviation for both methods is seen to be small with similar errors
over the simulation time except at the end where the point method has somewhat larger
error.
Pose Estimation Error for Quaternion Element q 3
-0.008
-0.006
-0.004
-0.002
0
0.002
0.004
0.006
0.008
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line error
Point error
Fig. 5.22.
q3 quaternion pose estimate simulation results for a four-point target showing
rotation estimation error over time for the line method and the comparison point method
from a sample run. The deviation for both methods is seen to be small with similar errors
over the simulation time.
115

Pose Estimation Error for Linear x  Velocity
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
Time (sec.)
Linear Velocity Error (mm./sec.)
Line error
Point error
Fig. 5.23.
vx translation velocity pose estimate simulation results for a four-point target
showing linear translation velocity estimation error over time for the line method and the
comparison point method from a sample run. The deviation for both methods is seen to
be small; the line method errors are somewhat less over the simulation time.
Pose Estimation Error for Linear y  Velocity
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
Time (sec.)
Linear Velocity Error (mm./sec.)
Line error
Point error
Fig. 5.24.
vy translation velocity pose estimate simulation results for a four-point target
showing linear translation velocity estimation error over time for the line method and the
comparison point method from a sample run. The deviation for both methods is seen to
be small; the line method errors are somewhat less over the simulation time.
116

Pose Estimation Error for Linear z  Velocity
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
Time (sec.)
Linear Velocity Error (mm./sec.)
Line error
Point error
Fig. 5.25.
vz translation velocity pose estimate simulation results for a four-point target
showing linear translation velocity estimation error over time for the line method and the
comparison point method from a sample run. The deviation for both methods is seen to
be larger than for the x and y velocity estimates. The line method has faster settling time
with somewhat lower errors than the point method over the simulation time.
Pose Estimation Error for Angular Velocity about x  Axis
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity Error (rad./sec.)
Line error
Point error
Fig. 5.26.
ωx angular velocity pose estimate simulation results for a four-point target
showing constant angular velocity error over time for the line method and the comparison
point method from a sample run.
The deviation for both methods is relatively large
compared with the true velocity, although, the mean error is near zero.
117

Pose Estimation Error for Angular Velocity about y  Axis
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity Error (rad./sec.)
Line error
Point error
Fig. 5.27.
ωy angular velocity pose estimate simulation results for a four-point target
showing constant angular velocity error over time for the line method and the comparison
point method from a sample run.
The deviation for both methods is relatively large
compared with the true velocity, although, the mean error is near zero.
Pose Estimation Error for Angular Velocity about z  Axis
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity Error (rad./sec.)
Line error
Point error
Fig. 5.28.
ωz angular velocity pose estimate simulation results for a four-point target
showing constant angular velocity error over time for the line method and the comparison
point method from a sample run. The deviation for both methods is small compared with
the true velocity over the simulation time.
118

These ﬁgures give the sample estimation errors over the same period of time from
the same sample data. For the z-translation error, the initial convergence is seen to be
faster than the point-based method with somewhat less error over time. The velocity error
for linear z is signiﬁcantly higher for the point method. The above results are for a single
representative run and are not necessarily typical. A further measure of performance is the
error over a large number of trials. Using the same initial conditions given in Table 5.1, a
series of tests were performed to experimentally measure the error. Figures 5.29 through
5.41 show the calculated RMS error over 100 sample runs for both the dual quaternion
line method and the quaternion point method. The diﬀerence between the two methods is
shown more clearly here since in almost all state variables, the dual quaternion method has
lower RMS error over the test time interval. The square root of the corresponding mean
diagonal element from the calculated covariance error matrix is also shown in each ﬁgure.
This matrix in the linear Kalman ﬁlter is the predicted error covariance and depends
only on the initial conditions and noise models. In the nonlinear EKF, the covariance
matrix depends on the measurements and does not necessarily represent the actual error
covariance [92]. In these tests, the covariance matrix values correspond, generally, closely
to the actual error values for the dual quaternion method while large diﬀerences are present
for the point method. Speciﬁc anomalies are present in the quaternion variables. Near
the end of the time sequence, the dual quaternion RMS error shows a relatively large peak
not present in the previous data, which are slowly changing and have very low error.
119

RMS Error for x  Translation
0
0.5
1
1.5
2
2.5
3
3.5
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Line
Point
Cov. line
Cov. point
Fig. 5.29.
tx translation pose estimate simulation results for a four-point target showing
linear translation root mean square estimation error over time for the line method and
the comparison point method calculated from 100 sample runs. The square root of the
predicted error covariance from the Kalman ﬁlter for both methods is also shown. The
root mean square error is seen to be signiﬁcantly greater for the point method than for
the line method over most of the time interval. The Kalman ﬁlter predicted error is close
to the actual error for the line method.
RMS Error for y  Translation
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Line
Point
Cov. point
Cov. line
Fig. 5.30.
ty translation pose estimate simulation results for a four-point target showing
linear translation root mean square estimation error over time for the line method and
the comparison point method calculated from 100 sample runs. The square root of the
predicted error covariance from the Kalman ﬁlter for both methods is also shown. The
root mean square error is seen to be signiﬁcantly greater for the point method than for
the line method over most of the time period. The Kalman ﬁlter predicted error is close
to the actual error for the line method.
120

RMS Error for z  Translation
0
5
10
15
20
25
30
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Line
Point
Cov. line
Cov. point
Fig. 5.31.
tz translation pose estimate simulation results for a four-point target showing
linear translation root mean square estimation error over time for the line method and
the comparison point method calculated from 100 sample runs. The square root of the
predicted error covariance from the Kalman ﬁlter for both methods is also shown. The
root mean square error is seen to be signiﬁcantly greater for the point method than for the
line method over the last half of the simulated time period. The Kalman ﬁlter predicted
error is close to the actual error for the line method.
RMS Error for Quaternion Element q 0
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line
Point
Cov. line
Cov. point
Fig. 5.32.
q0 quaternion pose estimate simulation results for a four-point target showing
rotation root mean square estimation error over time for the line method and the com-
parison point method calculated from 100 sample runs. The square root of the predicted
error covariance from the Kalman ﬁlter for both methods is also shown. The root mean
square error is seen to be small for both methods over the simulated time period. The
Kalman ﬁlter predicted error for the line method greatly overestimates the actual error.
121

RMS Error for Quaternion Element q 1
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line
Point
Cov. line
Cov. point
Fig. 5.33.
q1 quaternion pose estimate simulation results for a four-point target showing
rotation root mean square estimation error over time for the line method and the com-
parison point method calculated from 100 sample runs. The square root of the predicted
error covariance from the Kalman ﬁlter for both methods is also shown. The root mean
square error is seen to be considerably larger for the point method than the line method
over the simulated time period. The Kalman ﬁlter predicted error for the point method
greatly underestimates the actual error while the predicted error for the line method is
close to the actual error except near the end of the time period.
122

RMS Error for Quaternion Element q 2  
0
0.05
0.1
0.15
0.2
0.25
0.3
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line
Point
Cov. line
Cov. point
Fig. 5.34.
q2 quaternion pose estimate simulation results for a four-point target showing
rotation root mean square estimation error over time for the line method and the com-
parison point method calculated from 100 sample runs. The square root of the predicted
error covariance from the Kalman ﬁlter for both methods is also shown. The root mean
square error is seen to be considerably larger for the point method than the line method
over the simulated time period. The Kalman ﬁlter predicted error for the point method
greatly underestimates the actual error while the predicted error for the line method is
close to the actual error except near the end of the time period.
123

RMS Error for Quaternion Element q 3
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0
5
10
15
20
25
30
Time (sec.)
Quaternion Error
Line
Point
Cov. line
Cov. point
Fig. 5.35.
q3 quaternion pose estimate simulation results for a four-point target showing
rotation root mean square estimation error over time for the line method and the com-
parison point method calculated from 100 sample runs. The square root of the predicted
error covariance from the Kalman ﬁlter for both methods is also shown. The root mean
square error is seen to be small for both methods over the simulated time period. The
Kalman ﬁlter predicted error for the line method greatly overestimates the actual error.
RMS Error for x  Translation Velocity
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
15
20
25
30
Time (sec.)
Velocity Error (mm./sec.)
Line
Point
Cov. line
Cov. point
Fig. 5.36.
vx translation velocity pose estimate simulation results for a four-point target
showing linear translation root mean square estimation velocity error over time for the
line method and the comparison point method calculated from 100 sample runs. The
square root of the predicted error covariance from the Kalman ﬁlter for both methods is
also shown. The root mean square error is seen to be signiﬁcantly greater for the point
method than for the line method over most of the time period. The Kalman ﬁlter predicted
error is close to the actual error for the line method.
124

RMS Error for y  Translation Velocity
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
15
20
25
30
Time (sec.)
Velocity Error (mm./sec.)
Line
Point
Cov. line
Cov. point
Fig. 5.37.
vy translation velocity pose estimate simulation results for a four-point target
showing linear translation root mean square estimation velocity error over time for the
line method and the comparison point method calculated from 100 sample runs. The
square root of the predicted error covariance from the Kalman ﬁlter for both methods is
also shown. The root mean square error is seen to be signiﬁcantly greater for the point
method than for the line method over most of the time period. The Kalman ﬁlter predicted
error is close to the actual error for the line method.
125

RMS Error for z  Translation Velocity
0
1
2
3
4
5
6
7
8
9
10
0
5
10
15
20
25
30
Time (sec.)
Velocity Error (mm./sec.)
Line
Point
Cov. line
Cov. point
Fig. 5.38.
vz translation velocity pose estimate simulation results for a four-point target
showing linear translation root mean square estimation velocity error over time for the
line method and the comparison point method calculated from 100 sample runs. The
square root of the predicted error covariance from the Kalman ﬁlter for both methods is
also shown. The root mean square error is seen to be signiﬁcantly greater for the point
method than for the line method, particularly during the initial settling time. The Kalman
ﬁlter predicted error is close to the actual error for the line method but considerably
underestimates the actual error for the point method.
126

RMS Error for Angular Velocity about x  Axis
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity Error (rad./sec.)
Line
Point
Cov. line
Cov. point
Fig. 5.39.
ωx angular velocity pose estimate simulation results for a four-point target
showing angular root mean square estimation velocity error over time for the line method
and the comparison point method calculated from 100 sample runs. The square root of
the predicted error covariance from the Kalman ﬁlter for both methods is also shown. The
root mean square error is seen to be signiﬁcantly greater for the point method than for the
line method over the latter portion of the simulation time. The Kalman ﬁlter predicted
error is close to the actual error for the line method but considerably underestimates the
actual error for the point method.
127

RMS Error for Angular Velocity about y  Axis
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity Error (rad./sec.)
Line
Point
Cov. line
Cov. point
Fig. 5.40.
ωy angular velocity pose estimate simulation results for a four-point target
showing angular root mean square estimation velocity error over time for the line method
and the comparison point method calculated from 100 sample runs. The square root of
the predicted error covariance from the Kalman ﬁlter for both methods is also shown. The
root mean square error is seen to be signiﬁcantly greater for the point method than for the
line method over the latter portion of the simulation time. The Kalman ﬁlter predicted
error is close to the actual error for the line method but considerably underestimates the
actual error for the point method.
128

RMS Error for Angular Velocity about z  Axis
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0
5
10
15
20
25
30
Time (sec.)
Angular Velocity Error (rad./sec.)
Line
Point
Cov. line
Cov. point
Fig. 5.41.
ωz angular velocity pose estimate simulation results for a four-point target
showing angular root mean square estimation velocity error over time for the line method
and the comparison point method calculated from 100 sample runs. The square root of
the predicted error covariance from the Kalman ﬁlter for both methods is also shown. The
root mean square error is seen to be signiﬁcantly greater for the point method than for the
line method over the latter portion of the simulation time. The Kalman ﬁlter predicted
error is close to the actual error for the line method and for the point method.
129

5.1.10
Simulation Using Adaptive Noise Estimation
A simulation has been performed to compare the results using the adaptive mea-
surement noise method in the IEKF with the nonadaptive version using a constant mea-
surement covariance matrix. This simulation used a four-point target with a large oﬀset
from the center of the image to show the eﬀects of larger noise covariance in the line
point measurements. The point method was also simulated to compare the results for the
oﬀ-center object.
Table 5.3 gives the initial state conditions for this adaptive method simulation
with a four-point geometric model. Initial states are the same for all methods. Table 5.4
gives the assumed initial error covariance diagonal terms and the process noise covariance
diagonal terms for both the adaptive and nonadaptive dual quaternion method and for the
point-based method used for comparison.
In all cases, the oﬀ-diagonal terms of the error
covariance matrices are zero. For these tests, a simulated sample interval of 0.1 second
is used.
Note that the process noise covariance is the same for both dual quaternion
methods and is diﬀerent for the point method. Both process noise and measurement noise
covariances were held constant for the nonadaptive dual quaternion method and for the
point method during the simulation while the measurement noise covariance terms were
varied based on the image line locations using the approximate real-time algorithm given
in Section 4.2.
The input noise is a truncated Gaussian with a standard deviation of 0.02 mm.
as described above for the measurement noise model.
This noise level corresponds to
approximately four percent of the image size in the image.
Based on this noise level,
the initial measurement error covariance matrix has diagonal elements of 0.0004 mm.2
for each measured variable. The oﬀ-diagonal terms are all zero. The dynamic model is
the same constant linear velocity and constant angular velocity for both the adaptive and
nonadaptive dual quaternion method tests as well as for the point comparison method
130

Table 5.3.
Actual and assumed initial states of the dual quaternion and point-based extended Kalman
ﬁlter for simulation comparison with the adaptive measurement noise line method, the
nonadaptive line method, and the comparison point method.
Translation
Quaternion
Linear
Rotational
State
Velocity
Velocity
x y z
q0 q1 q2 q3
x y z
x y z
(mm.)
(mm./sec.)
(rad./sec.)
True Initial State
400 400 1000
1 0 0 0
-5 2 -5
.01 -.02 -.1
Initial State Estimate
390 390 990
.9998 .01 .01 .01
0 0 0
0 0 0
Table 5.4.
Dual quaternion method and point method initial error covariance matrix and process
noise matrix diagonal terms of the extended Kalman ﬁlter for simulation comparison
with the adaptive measurement noise line method, the nonadaptive line method, and the
comparison point method.
Translation
Quaternion
Linear
Rotational
Noise Covariance
Velocity
Velocity
x y z
q0 q1 q2 q3
x y z
x y z
(mm.2)
(mm./sec.)2
(rad./sec.)2
Dual Quaternion
100
0.01
100
0.01
Error Covariance
Dual Quaternion
10−5
10−6
10−3
10−4
Process Noise
Point Method
100
0.01
100
0.01
Error Covariance
Point Method
0
0
10−2
10−3
Process Noise
131

tests. Tuning and stabilization adjustments were required for both methods and in some
cases are diﬀerent for each method. The resulting process noise matrix and initial error
covariance matrix diagonals are given in Table 5.4. These values represent experimentally
determined optimum values for the assumed dynamic model. Stability was a signiﬁcant
problem for all methods with this amount of x and y translation. Estimate errors occa-
sionally became unbounded when no process noise was used. Signiﬁcant improvements in
errors were achieved, however, by adjustment of these values. Figures 5.42, 5.43, and 5.44
show the RMS errors for the translation state variables tx, ty, and tz, respectively. The
RMS errors were calculated over 100 sample runs for each of the methods. The graphs
show the results from the three simulation runs of 30 seconds. These results for each
variable are similar and show that the point method gives the most accurate results over
most of the simulated time with much less initial overshoot. The adaptive method has
less error and less overshoot than the nonadaptive method and converges near the end of
the simulation to give the least error. The results are as expected from the analysis. The
large x and y translations give much higher noise variances for the line points than for
the individual points. The adaptive measurement method updates the measurement co-
variance matrix with the calculated variances to give better results than the nonadaptive
method.
5.2
Robot Arm Tests
Experimental testing has been performed using a robot arm equipped with a CCD
area camera and a computer system for processing images and controlling the arm. Static
tests using no relative motion are used to characterize the measurement noise as well
as variation of the estimates. Tests were also performed with controlled relative motion
between an object and the camera.
132

RMS Error for x  Translation
0
10
20
30
40
50
60
70
80
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Adaptive Line
Point
Fixed Line
Fig. 5.42.
tx translation pose estimate simulation results for a four-point target oﬀset from
the image center. The ﬁgure shows linear translation root mean square estimation error
over time for the adaptive line method, the nonadaptive line method and the comparison
point method calculated from 100 sample runs. x and y translations were both initially
400 mm. The RMS error is seen to be signiﬁcantly less for the point method than for
the adaptive and nonadaptive line methods over most of the time period. Initial error
overshoot is also much less with the point method. The adaptive line method shows less
error than the nonadaptive method over most of the simulation.
133

RMS Error for y  Translation
0
10
20
30
40
50
60
70
80
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Adaptive Line
Point
Fixed Line
Fig. 5.43.
ty translation pose estimate simulation results for a four-point target oﬀset from
the image center. The ﬁgure shows linear translation root mean square estimation error
over time for the adaptive line method, the nonadaptive line method and the comparison
point method calculated from 100 sample runs. x and y translations were both initially
400 mm. The root mean square error is seen to be signiﬁcantly less for the point method
than for the adaptive and nonadaptive line methods over most of the time period. Initial
error overshoot is also much less with the point method. The adaptive line method shows
less error than the nonadaptive method over most of the simulation.
134

RMS Error for z  Translation
0
20
40
60
80
100
120
140
160
0
5
10
15
20
25
30
Time (sec.)
Translation Error (mm.)
Adaptive Line
Point
Fixed Line
Fig. 5.44.
tz translation pose estimate simulation results for a four-point target oﬀset from
the image center. The ﬁgure shows linear translation root mean square estimation error
over time for the adaptive line method, the nonadaptive line method and the comparison
point method calculated from 100 sample runs. x and y translations were both initially
400 mm. The root mean square error is seen to be signiﬁcantly less for the point method
than for the adaptive and nonadaptive line methods over most of the time period. Initial
error overshoot is also much less with the point method. The adaptive line method shows
less error than the nonadaptive method over most of the simulation.
135

5.2.1
Experimental Setup
The experimental setup, a picture of which is shown in Figure 5.45, consists of a
Mitsubishi RV-E2 six-degree-of-freedom robot arm and a Cidtec CID2250D CID camera
used for image acquisition.
This camera is a progressive scan type, 30 frames per second. The image resolution
is 512-by-512 square pixels where each pixel is 0.015 mm. in width. The robot arm has
its own controller with a teach pendant for manually moving each of the six axes. The
picture also shows the object used for the initial tests, a rectangular ﬁgure from which the
edges are extracted. The pose estimation is performed on a 166 Mhz. Pentium PC. The
robot is controlled remotely via a serial interface to the robot controller. A block diagram
of the entire conﬁguration is shown in Figure 5.46. The remote robot control consists of
position, velocity, and acceleration commands. Low-level joint control is not accessible for
this arm. As a result, high speed, dynamic control of the arm cannot be demonstrated.
The functionality, however, does permit control at low update rates.
5.2.2
Dynamic Model
The system dynamic model is the same model described previously under the sim-
ulation section. Position, orientation, and their ﬁrst derivatives are state estimates.
5.2.3
Measurement Model
The measurement model for the experiment consists of the functions given in Figure
5.47.
First, the image of the target object is acquired by the camera and the frame
grabber. Then, the object edges are extracted and the contour formed. Straight lines
are calculated next from the contour with length ﬁltering performed to eliminate short
lines that could arise from extraneous features. Collinear lines are also combined into one
line. The resulting lines are used as inputs to the ﬁlter after conversion to the line point
136

Fig. 5.45.
Mitsubishi RV-E2 six-axis robot arm and the Cidtec camera used in the
experimental tests.
137

CIDTEC
Camera
Mitsubishi
RV-E2
Robot Arm
and Controller
Matrox
Frame Grabber
PC
Pentium 166
32 MB
Visual C++
Target
PCI Bus
RS232
Fig. 5.46.
Block diagram showing the system hardware used in the experimental setup.
The hardware includes a Cidtec camera, frame grabber, PC compatible computer, and
Mitsubishi robot arm and controller.
Calculate
Line Points
Fit Lines to
Contours
Form
Contours
Extract
Edges
Acquire
Image
Fig. 5.47.
Measurement functions applied to obtain line point measurements from the
camera image in robot arm experiments.
138

parameterization. Correspondence to the known geometric model is established at this
stage.
5.2.4
Initial Conditions
In the experimental tests, an initial estimate of the relative pose between the camera
and the object is needed.
This lack of knowledge is a signiﬁcant diﬀerence from the
simulation tests where the initial pose is known and is used as the basis from which the
initial estimate is formed. To overcome this diﬃculty, an alternate method is used to
provide an initial estimate of the pose. This method uses the four-point coplanar pose
calculation method of Abidi and Chandra [4]. The initial velocities are set to zero as in
the simulation tests.
5.2.5
Measurement Noise Results
Preliminary tests were performed to characterize the measurement noise present
in the features extracted from the acquired image. These measurements were taken by
repeatedly acquiring images of the target with no relative motion. Approximately 500
images were analyzed. The approximate distribution is given by the histograms shown in
Figures 5.48 and 5.49 for a typical line point x and y coordinate.
The distribution is seen to have a single peak with no values outside an interval
about the peak. The variances for all eight coordinates ranged from 2×10−6 to 1×10−5.
As expected from the results in Chapter 4, the noise approximates a Gaussian density
function.
5.2.6
Target Motion Results
The tests were performed with the target moving in a circular pattern around the z
axis at constant speed. A sample period is one second is used. The camera and robot arm
139

x 3 Noise Distribution
0
10
20
30
40
50
60
70
80
90
100
-1.76954
-1.768810909
-1.768081818
-1.767352727
-1.766623636
-1.765894545
-1.765165455
-1.764436364
-1.763707273
-1.762978182
-1.762249091
-1.76152
-1.760790909
-1.760061818
-1.759332727
-1.758603636
-1.757874545
-1.757145455
-1.756416364
-1.755687273
-1.754958182
-1.754229091
More
Bin
Frequency
Frequency
Fig. 5.48.
Histogram of the distribution of x line point measurement values from 500
camera images for a stationary target.
y 3  Noise Distribution
0
20
40
60
80
100
120
140
160
-0.262081
-0.259651545
-0.257222091
-0.254792636
-0.252363182
-0.249933727
-0.247504273
-0.245074818
-0.242645364
-0.240215909
-0.237786455
-0.235357
-0.232927545
-0.230498091
-0.228068636
-0.225639182
-0.223209727
-0.220780273
-0.218350818
-0.215921364
-0.213491909
-0.211062455
More
Bin
Frequency
Frequency
Fig. 5.49.
Histogram of the distribution of y line point measurement values from 500
camera images for a stationary target.
140

were directly above the target and stationary. The motion is conﬁned to the x-y plane.
Figures 5.50 through 5.62 show the estimated response for each state variable over time.
Experimental Results for x  Translation
-60
-40
-20
0
20
40
60
80
1
21
41
61
81
101
121
141
161
181
Sample
Translation (mm.)
Fig. 5.50.
tx translation pose estimate from experimental testing for constant target
motion about the z axis.
The estimate shows a sinusoidal variation corresponding to
the true x variation of the target. Note the step response at the beginning of the target
motion.
As expected, the x and y translations vary in a sinusoidal pattern consistent with
constant circular motion.
The z-axis position is seen to vary with a small sinusoidal
component over the test interval along with apparent noise. Although the motion was
nominally in the x-y plane of the target and parallel to the image plane of the camera, the
setup was not calibrated precisely. Any nonparallelism would result in a small sinusoidal z
variation with time. In this test, the z variation is seen to be about 5 mm. The quaternion
estimates are also consistent with the actual motion. For motion about the z axis, only q0
and q3 should vary signiﬁcantly in magnitude. The results demonstrate this since q1 and
141

Experimental Results for y  Translation
-80
-60
-40
-20
0
20
40
60
80
1
21
41
61
81
101
121
141
161
181
Sample
Translation (mm.)
Fig. 5.51.
ty translation pose estimate from experimental testing for constant target
motion about the z axis.
The estimate shows a sinusoidal variation corresponding to
the true y variation of the target. Note the step response at the beginning of the target
motion.
Experimental Results for z  Translation
505
510
515
520
525
530
1
21
41
61
81
101
121
141
161
181
Sample
Translation (mm.)
Fig. 5.52.
tz translation pose estimate from experimental testing for constant target
motion about z axis. The estimate shows a noisy variation about a mean z value with a
small sinusoidal component. The true target z value is nominally constant.
142

Experimental Results for Quaternion Element q 0
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1
21
41
61
81
101
121
141
161
181
Sample
Quaternion Value
Fig. 5.53.
q0 quaternion pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a full 360 degree sinusoidal variation corre-
sponding to the true q0 variation of the target as it moves through two complete revolu-
tions.
Experimental Results for Quaternion Element q 1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
1
21
41
61
81
101
121
141
161
181
Sample
Quaternion Value
Fig. 5.54.
q1 quaternion pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a relatively small noisy variation about zero
in comparison to a true q1 constant value of zero.
143

Experimental Results for Quaternion Element q 2
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
1
21
41
61
81
101
121
141
161
181
Sample
Quaternion Value
Fig. 5.55.
q2 quaternion pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a relatively small noisy variation about zero
in comparison to a true q2 constant value of zero.
Experimental Results for Quaternion Element q 3
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1
21
41
61
81
101
121
141
161
181
Sample
Quaternion Value
Fig. 5.56.
q3 quaternion pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a full 360 degree sinusoidal variation corre-
sponding to the true q3 variation of the target as it moves through two complete revolu-
tions.
144

Experimental Results for x  Linear Velocity
-5
-4
-3
-2
-1
0
1
2
3
4
1
21
41
61
81
101
121
141
161
181
Sample
Linear Velocity  (mm./sec.)
Fig. 5.57.
vx translation velocity pose estimate from experimental testing for constant
target motion about the z axis. The estimate shows a noisy sinusoidal variation compared
to the true x sinusoidal velocity variation of the target. Note the step response at the
beginning of the target motion.
Experimental Results for y  Linear Velocity
-4
-3
-2
-1
0
1
2
3
4
5
1
21
41
61
81
101
121
141
161
181
Sample
Linear Velocity  (mm./sec.)
Fig. 5.58.
vy translation velocity pose estimate from experimental testing for constant
target motion about the z axis. The estimate shows a noisy sinusoidal variation compared
to the true y sinusoidal velocity variation of the target. Note the step response at the
beginning of the target motion.
145

Experimental Results for z  Linear Velocity
-4
-3
-2
-1
0
1
2
3
4
5
1
21
41
61
81
101
121
141
161
181
Sample
Linear Velocity  (mm./sec.)
Fig. 5.59.
vz translation velocity pose estimate from experimental testing for constant
target motion about z axis. The estimate shows a noisy variation about a mean z velocity
value of zero. The true target vz velocity value is nominally zero.
Experimental Results for Angular Velocity about x  Axis
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
1
21
41
61
81
101
121
141
161
181
Sample
Angular Velocity  (rad./sec.)
Fig. 5.60.
ωx angular velocity pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a noisy variation about a mean ωx velocity
value of zero. The true target ωx velocity value is nominally zero.
146

Experimental Results for Angular Velocity about y  Axis
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
1
21
41
61
81
101
121
141
161
181
Sample
Angular Velocity  (rad./sec.)
Fig. 5.61.
ωy angular velocity pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a noisy variation about a mean ωy velocity
value of zero. The true target ωy velocity value is nominally zero.
Experimental Results for Angular Velocity about z  Axis
-0.09
-0.08
-0.07
-0.06
-0.05
-0.04
-0.03
-0.02
-0.01
0
0.01
1
21
41
61
81
101
121
141
161
181
Sample
Angular Velocity  (rad./sec.)
Fig. 5.62.
ωz angular velocity pose estimate from experimental testing for constant target
motion about the z axis. The estimate shows a noisy variation about a mean ωz velocity
value of -0.06 rad./sec. The true target ωz velocity value is nominally constant at -0.06
rad./sec.
147

q2 vary little with no trend discernable. Linear velocity estimates similarly illustrate the
sinusoidal velocity components in x and y. The z linear velocity estimate, however, shows
signiﬁcant noise present, masking the true velocity component which is near zero. Angular
velocity estimates show signiﬁcant noise in the x and y components although the mean
appears to remain near zero. The angular z velocity, however, gives an accurate estimate
of the true rotational speed. The 0.06 rad./sec. velocity corresponds to a revolution time
of 105 seconds. This time has been veriﬁed to within one second. The z angular velocity
results also show the fast transient response of the initial start-up from no motion to the
constant rotational speed.
5.3
Test Results From Unknown Object Geometry
Experimental results to demonstrate structure and pose estimation are given in
this section. A more realistic object than the rectangular target used in Section 5.2 was
assembled from sections of PVC pipe. Figure 5.63 is a picture of the pipe object. Four
sections of straight pipe were connected using joints to form the object. No dimensional
information was known about the pipe prior to the experiment. The CIDTEC camera
mounted on the Mitsubishi robot arm and the computer system given in Figure 5.46 were
used for these tests.
5.3.1
Structure Estimation
The structure estimation is performed for linear robot arm motion and for rotational
motion. For comparison, a reference structure is calculated in terms of the line parameters
⃗l and ⃗m given in Chapter 3. This reference method uses a stereo technique to calculate
the true dimensions of the pipe assembly. The details involved in calculating the true
3–D line parameters from the set of stereo images obtained with the robot arm are given
148

Fig. 5.63.
Pipe assembly used in the structure estimation robot arm experiments. The
picture shows the four segments of white PVC pipe joined together.
149

below. During operation, the structure estimates which are referenced to a local reference
frame are transformed to the calculated reference frame and then used for comparison.
5.3.1.1
Calculation of Actual Structure for Reference
The procedure used to calculate the actual structure is to move the robot arm over
the pipe, stop the motion, and then acquire 100 frames and extract the line parameters
of the pipe features for each frame. The arm is then moved 50 mm. in the positive x
direction; 100 frames were again taken with the line parameters calculated. The average
of the line parameters from each of the left and right positions is then calculated. Each
3–D pipe line feature is computed using the following relations. First, the normalized −→
mn
values are calculated from the image line points from each set of left and right points
−→
mn =


λxlp
ρ
q
λ2+ρ2
λylp
ρ
q
λ2+ρ2
ρ
q
λ2+ρ2


(5.11)
where λ is the focal length and ρ =
r
x2
lp + y2
lp.
The −→
mn’s give the normal to the plane containing the object line, the image line,
and the optical center. The direction vector ⃗l is found from the cross product
⃗l = −→
mnleft × −→
mnright.
(5.12)
The magnitudes of the ⃗m left and right vectors are still unknown. These can be expressed
in terms of the transformation between the left and right reference frames,
kleft−→
mnleft = kright−→
mnright + (⃗t ×⃗l),
(5.13)
150

where in this case ⃗t is the translation and is equal to [50 0 0]T . An assumption is also
implicitly made that no rotation occurs, so that the direction vector is the same for both
the left and the right sides. Since only x translation is involved, the y and z components
of the above equation can be used to solve for the scale factor kright:
kright =
mnleftz(⃗t ×⃗l)y −mnleft(⃗t ×⃗l)z
mnleftymnrightz −mnrightymnleftz
.
(5.14)
An additional translation of 500 mm. in the positive z direction is then applied to place
the reference origin close to the object. The resulting structure data for the pipe is given
in Table 5.5 below.
Table 5.5.
Reference pipe line data as calculated by two image views separated by an x translation
of 50 mm. The data show the ⃗l and ⃗m components of the 3–D lines.
Line
lx
ly
lz
mx
my
mz
0
-0.512
0.859
0
47.5
28.4
-30.2
1
0.846
0.453
0.281
-6.67
-40.9
85.7
2
-0.543
0.838
0.0482
0.332
-11.1
196.5
3
-0.981
0.194
-0.0171
-4.74
-4.68
218.384
5.3.1.2
Initial State Estimate
The structure is estimated relative to a ﬁxed reference frame set during algorithm
initialization that depends on the relative position between the camera and the object.
Since the scale for translation and structure is arbitrary, a normalization is applied to
ensure that the scale is ﬁxed during state and measurement updates. This ﬁxed frame is
151

set by acquiring an initial image of the object before estimation begins. The state pose
variables are set to the following:
tx
=
ty = 0
q0
=
1
q1
=
q2 = q3 = 0.
(5.15)
This pose setting aligns the object reference frame’s x and y axes with the camera reference
frame’s axes. With no relative rotation, the z-axis translation is calculated from the ﬁrst
image line parameters:
tz = λ
ρ
(5.16)
with ρ =
r
x2
lp + y2
lp.
Structure line initial values are normalized to the ﬁrst −→
m0 line vector. These vectors
have the initial values
⃗m
=

0
0
−ρtz
λ
T
(5.17)
⃗l
=
−ylp
ρ
xlp
ρ
0
T
.
The direction vectors, ⃗l, are assumed to be parallel to the image plane.
152

5.3.1.3
Transformation of Structure Estimate Frame to Reference Frame
The object reference frame is dependent on the initial relative pose between the
camera and the object and varies with each start-up. A transformation is required to
compare the estimation results between diﬀerent tests or to a known reference. For com-
parison to the reference structure obtained through the stereo technique given in Section
5.3.1.1, the estimated structure must be transformed to the base reference frame. This
relationship between the estimated frame and the reference frame is given for each object
line by
⃗lref
=
R⃗lest
(5.18)
⃗mref
=
sR⃗mest + ⃗t ×⃗lref
with rotation matrix R, translation vector ⃗t, and scale s to be determined.
R for the ﬁrst equation can be found by application of the method given by Horn
for calculating the transformation between two 3–D reference frames [63]. For the second
equation, the error squared sum is
n−1
X
i=0
|e|2 =
n−1
X
i=0
|⃗mrefi −sRmesti −Lrefi⃗t|2.
(5.19)
This expression is minimized with respect to s and ⃗t by computing the partial derivatives
and setting them equal to zero. Solving for ⃗t results in
⃗t
=

I3 −
(Pn−1
i=0 LT
refiLrefi)−1(Pn−1
i=0 LT
refiR⃗mest)(Pn−1
i=0 (R⃗mesti)T Lrefi)
Pn−1
i=0 |R⃗mesti|2


−1
153



n−1
X
i=0
LT
refiLrefi)−1




(
n−1
X
i=0
LT
refi ⃗mrefi) −
(Pn−1
i=0 LT
refiR⃗mest)(Pn−1
i=0 (R⃗mesti)T ⃗mrefi)
Pn−1
i=0 |R⃗mesti|2



(5.20)
and for s results in
s =
Pn−1
i=0 (R⃗mesti)T ⃗mrefi −Pn−1
i=0 (R⃗mesti)T Lrefi⃗t
Pn−1
i=0 |R⃗mesti|2
.
(5.21)
These expressions provide the least squared error transformation between the two frames.
This transformation is then applied to the estimated structure values to transform them
to the baseline reference frame. The reference and estimated values can, as a result, be
directly compared.
5.3.2
Response to Step Change in Motion
The structure estimates are obtained by introducing a step change in motion to
the robot arm holding the camera. The axes changed were the x axis, z axis, and the
roll axis about the z axis. In each case, only a single axis was moved at a time. Figures
5.64 through 5.71 give the results of the structure estimates when the z axis is moved.
These ﬁgures show the ⃗l and ⃗m components of the object lines. Also shown are the
true structure values. The estimates have been transformed to the reference frame using
the method in Section 5.3.1.3 above.
Generally, good tracking of the structure values
is obtained. As seen, the ⃗l estimates give somewhat more accurate results than the ⃗m
estimates. Results from moving the other axes were similar.
154

Structure Estimation for Line L vector
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude 
L0x
L0y
L0z
Ref L0x
Ref L0y
Ref L0z
Fig. 5.64.
Structure estimate of the three ⃗l line vector components for line zero with
motion along the z axis. The true structure values are also shown for comparison. The x
and y components are close to the true values while the z component has a constant oﬀset
error.
Structure Estimation for Line M Vector
-40
-30
-20
-10
0
10
20
30
40
50
60
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude (mm.)
M0x
M0y
M0z
Ref M0x
Ref M0y
Ref M0z
Fig. 5.65.
Structure estimate of the three ⃗m line vector components for line zero with
motion along the z axis. The true structure values are also shown for comparison. The y
and z components are accurately estimated while the x component has signiﬁcant error.
155

Structure Estimation for Line L Vector
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude
L1x
L1y
L1z
Ref L1x
Ref L1y
Ref L1z
Fig. 5.66.
Structure estimate of the three ⃗l line vector components for line one with
motion along the z axis. The true structure values are also shown for comparison. The
x and y components have small errors in the estimate while the z component is seen to
converge slowly to the true value.
Structure Estimation for Line M Vector
-80
-60
-40
-20
0
20
40
60
80
100
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude (mm.)
M1x
M1y
M1z
Ref M1x
Ref M1y
Ref M1z
Fig. 5.67.
Structure estimate of the three ⃗m line vector components for line one with
motion along the z axis. The true structure values are also shown for comparison. The
z component is estimated accurately while the x component is seen to converge to the
correct value. Error in y, though initially small, increases with time.
156

Structure Estimation for Line L Vector
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude
L2x
L2y
L2z
Ref L2x
Ref L2y
Ref L2z
Fig. 5.68.
Structure estimate of the three ⃗l line vector components for line two with
motion along the z axis. The true structure values are also shown for comparison. Accurate
estimates are obtained for all three components.
Structure Estimation for Line M Vector
-50
0
50
100
150
200
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude (mm.)
M2x
M2y
M2z
Ref M2x
Ref M2y
Ref M2z
Fig. 5.69.
Structure estimate of the three ⃗m line vector components for line two with
motion along the z axis. The true structure values are also shown for comparison. Accurate
estimates are obtained for all three components.
157

Structure Estimation for Line L Vector
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude
L3x
L3y
L3z
Ref L3x
Ref L3y
Ref L3z
Fig. 5.70.
Structure estimate of the three ⃗l line vector components for line four with
motion along the z axis. The true structure values are also shown for comparison. The x
and y components are close to the true values while the z component has a constant oﬀset
error.
Structure Estimation for Line M Vector
-50
0
50
100
150
200
250
0
20
40
60
80
100
120
140
160
180
200
Iteration
Magnitude (mm.)
M3x
M3y
M3z
Ref M3x
Ref M3y
Ref M3z
Fig. 5.71.
Structure estimate of the three ⃗m line vector components for line four with
motion along the z axis. The true structure values are also shown for comparison. The x
and z components have relatively small errors while a constant oﬀset error is present for
the y component.
158

Chapter 6
Summary and Conclusions
6.1
Summary
A new pose and motion estimation method has been developed and described in this
dissertation. This method uses an imaging technique with a single low-cost CCD camera
along with a reference object or scene to calculate estimates for relative six-degree-of-
freedom position and orientation as well as the associated velocity estimates. Intended
for real-time use, this method can be applied to robot vision, control, and assembly tasks.
For an object with unknown geometry, the method is also able to estimate the scaled 3–D
object coordinates. The system model for the method is based on line features, a dual
quaternion parameterization for the 3-D transformation, and the IEKF.
The IEKF is
used as an estimator for the highly nonlinear imaging model. Development of the theory
and the overall approach is described in detail.
This approach provides two solutions for the 3–D estimation problem. For a rigid
object or environment whose dimensions are unknown, the problem is to ﬁnd the object
structure in terms of the line feature coordinates in the local 3-D reference frame from
the motion of the robot. Once the geometric target dimensions are estimated, the basic
method may then be applied to provide the pose and motion estimation. The full method
to estimate structure as well as pose and motion requires the estimation of many more
state variables than when the object dimensions are known. As a result, computational
requirements are also increased signiﬁcantly and real-time application may not be feasible.
159

A potential use for this method is on a mobile robot that is attempting to navigate
through a room with various kinds of obstructions such as pipes, columns, and other
equipment. The experimental testing has shown the feasibility of using this method in a
realistic application where objects with line features are present. The testing demonstrated
the image acquisition, feature extraction, matching and feature correspondence to the
model, and then the pose and motion estimation as the robot moves within the scene.
Another potential application is in visual servoing whereby a camera-mounted robot arm
maintains a ﬁxed position and orientation with respect to a moving object within a scene.
An analysis of the relationship of the measurement noise in the projected 2–D
image line parameters to the noise in associated point features has also been presented.
Experimental testing to demonstrate and verify the performance of the method from both
simulations and actual robot arm tests have been performed and presented as well. The
results from these tests have been compared to the performance of a point-based method.
6.2
Conclusions and Contributions
The newly developed solution given here is unique among previous methods of
real-time pose and motion estimation. This method is based on line features, 3–D trans-
formations of lines, and extended Kalman ﬁltering. While others have previously used
extended Kalman ﬁltering for 3–D image-based pose estimation, this previous work has
used point features for the 3–D and imaging models. This new method describes the ﬁrst
known eﬀort that combines line features and extended Kalman ﬁltering to estimate pose
and motion as well as 3–D structure. The EKF is particularly appropriate for this appli-
cation because it permits the estimation of unknown quantities along with the derivatives
of these quantities to any desired order in a format suitable for real-time application since
a new estimate is produced at each iteration. The standard linear Kalman ﬁlter is also
the optimal estimator where the measurement and process noise statistics are Gaussian.
160

Extending the ﬁlter to nonlinear applications results in a suboptimal estimator. The use
of a suboptimal estimator here is justiﬁed, however, since optimal estimation of nonlinear
systems is not generally amenable to analysis. No analysis pertaining to optimal esti-
mation for the pose estimation problem was found in the literature. An analysis of the
problem was examined during this work and is not believed to be analytically solvable.
Good estimation results have been obtained in the past with the IEKF in other appli-
cations [99] as well as in pose estimation using point features. When the system model
and measurement model satisfy the standard Kalman ﬁlter conditions, better estimation
will result from the EKF. In Chapter 4, it was shown that the line-point parameters have
near-Gaussian density functions when the end points have Gaussian distributions. For
image feature points, a Gaussian distribution is both a common assumption and one that
corresponds to actual data. This satisﬁes the standard Kalman ﬁlter assumption that the
measurements have a Gaussian distribution.
The EKF, while well suited as an estimator for this application, requires setup and
tuning to ensure stability over the expected range of values. In particular, the initial error
covariance, Pk, should reasonably reﬂect the expected initial errors in the state variables.
The ﬁlter can become unstable for error covariance values too high, while values too low
will result in poor settling time or divergence from the true value. The process noise,
Qk, is also critical in obtaining good results. If the imaging model were perfect and the
relative camera-to-object motion were perfectly uniform, the process noise would be zero.
However, in the physical world, perspective projection is not exactly obtained due to lens
aberrations and distortions as well as errors in the image sensing by a camera and the
acquisition of the data. Neither is motion constant since an object may accelerate or be
perturbed by external inﬂuences. The process noise must be determined empirically either
through simulation or through actual testing. Even in simulations where the assumed
model is correct to double precision ﬂoating point accuracy, process noise may be required
161

simply due to the nonlinearity of the model and the errors due to the linearization based
on the truncated Taylor series expansion of the deﬁning equations. Incorrect values, here
again, may result in an unstable ﬁlter.
For a real-time application, computational requirements of an estimation method
are highly important. A position control feedback control loop may require updates on the
order of tens or even hundreds of times per second. The principal factors directly aﬀecting
the computation speed for the EKF used in this method are the number of states and the
number of measurements. The number of computations is on the order of the square of
these values. Consequently, structure estimation, since it requires many more states, is
not as suitable for real-time implementation as estimation of only the pose variables. The
number of measurements depends on the number of lines in the scene being used for the
estimation. Accuracy improves with an increase in the number of lines but at the expense
of increased computation. As the results from this method have shown, good accuracy
can be obtained even with a minimal set of four feature lines. Six lines give even higher
accuracy with improved stability but can still be considered within the real-time range.
For example, the computation time for the EKF estimation method, written in C++ on
a 166 Mhz. Pentium PC, was 100 ms. using a six-line object.
The experimental results obtained from simulation and actual robot arm testing
have shown that this method estimates 3–D pose parameters more accurately than previ-
ous point-based extended Kalman ﬁltering within the central region of a camera view. In
particular, the range and range motion estimation has been shown to be measured more
accurately with faster settling times than with other methods of real-time estimation.
Noise analysis has shown that these results are, in part, due to the noise statistics of the
line measurements. For line features around the center of the image, the noise variance is
less than the corresponding noise from points used as features. Thus, the value can be es-
timated more accurately in this region. However, as an object moves a signiﬁcant distance
162

from the center, the variance increases, reaching a point where the resulting estimate will
be worse than the corresponding point-based estimate. Quantifying this region has not
proved successful since no one distance will result in all pose variables being estimated
more accurately. The behavior is that some variables will become more accurate while
others will not. Tuning by way of the process noise will also aﬀect the point at which one
method will show better results.
This work has produced a number of new developments in the area of pose esti-
mation as a result of the analysis and experimental testing. The primary contributions
resulting from this work include the application of the EKF to pose, motion, and structure
estimation based on line features from a scene or object. The combining of line features
with the EKF has not been previously seen. The pose estimation method has been devel-
oped and implemented in C++ software and demonstrates the method is more accurate
and has faster response when compared to point-based methods over the central region of
an image. Parameterization of the line transformations in terms of dual quaternions as an
aid in understanding and developing the Kalman ﬁlter measurement model is also a new
contribution. While others have used dual quaternions to perform 3–D transformations of
lines, this approach applies dual quaternions within the Kalman ﬁlter measurement model
to transform from the object reference frame to the camera reference frame. Also new is
a two-parameter 2–D line representation that is continuously diﬀerentiable in the image
plane with respect to the image plane x and y axes. The partial derivatives of the parame-
ters with respect to each state variable are also continuous. Analysis of the noise statistics
of the 2–D image line-point parameters in relation to point feature noise statistics was also
performed. The pdf of the line point as a function of the end point density functions was
derived, requiring numerical integration for solution. As part of this analysis, the variance
and covariance of the line point values were also derived in terms of the end points. Since
163

the exact solution also requires numerical integration, an approximate solution was devel-
oped suitable for real-time implementation. These estimated covariances are applied to
the measurement covariance matrix of the EKF at each iteration so that during relative
motion, the measurement covariances accurately reﬂect the statistics of the measurement
data processed by the ﬁlter. The result is improved state estimates with less error. An-
other signiﬁcant contribution is the method developed to compare structure estimates.
This method calculates the least squared error transformation of 3–D lines from a local
reference frame to a base reference frame. Software for the pose estimation method has
been developed and fully implemented in C++. It has been integrated on a computer
system with a robot arm and camera and successfully demonstrated.
6.3
Future Work
While the basic method has been developed and successfully tested, much addi-
tional work remains in terms of analysis and extension of the concepts. One application
of the method is for visual servoing of a robot arm. This requires implementation of a
feedback control loop for moving the arm in relation to scene motion. The system imple-
mented and demonstrated for this work had iteration times on the order of one second
and a robot command interface that permitted only point-to-point movements. While
the basic operation could be observed, a better implementation on fast hardware would
be a control loop executing ten or more times per second and directly controlling the
joint angles of the arm. Another area is to extend the physical model to estimate higher
order derivatives (acceleration, etc.) and provide an evaluation of performance in com-
parison to this method. Also of interest would be an analysis of the iteration count for
the IEKF. During experimentation, it was found that iteration counts of three or more
sometimes gave results with less error than an iteration count of one while under diﬀer-
ent conditions the reverse was true. Usually a higher iteration count improved estimates
164

from the point method while the line method gave the best estimates with an iteration
count of one. Other research areas include the analysis of the EKF in terms of stability
and optimum performance. Analyzing the eﬀects of and aids to assist in setting the ﬁlter
parameters for near optimum performance would be of great beneﬁt. Another research
area would be to analyze the computational requirements of this method in comparison
to the computational requirements of the point method.
165

REFERENCES

REFERENCES
[1] J. S.-C. Yuan, “A general photogrammetric method for determining object position
and orientation,” IEEE Transactions on Robotics and Automation, vol. 5, no. 2, pp.
129–142, April 1989.
[2] R. M. Haralick and H. Joo, “2d-3d pose estimation,” in Proceedings of the Interna-
tional Conference on Pattern Recognition, 1988, pp. 385–391.
[3] R. J. Holt and A. N. Netravali, “Camera calibration problem: Some new results,”
CVGIP: Image Understanding, vol. 54, no. 3, pp. 368–383, November 1991.
[4] M. A. Abidi and T. Chandra, “Pose estimation for camera calibration and landmark
tracking,”
in Proceedings of the IEEE International Conference on Robotics and
Automation, May 1990, pp. 420–426.
[5] R. Y. Tsai, “A versatile camera calibration technique for high-accuracy 3d machine
vision metrology using oﬀ-the-shelf tv cameras and lenses,” IEEE Journal of Robotics
and Automation, vol. RA-3, no. 4, pp. 323–344, August 1987.
[6] V. Torre, A. Verri, and A. Fiumicelli, “Stereo accuracy for robotics,” in Robotics
Research, The Third International Symposium, G. Giralt O. D. Faugeras, Ed., pp.
5–9. MIT Press, 1986.
[7] D. F. DeMenthon and L. S. Davis, “Model-based object pose in 25 lines of code,” in
Computer Vision– ECCV ’92, G. Sandini, Ed., pp. 335–343. Springer-Verlag, 1992.
[8] M. A. Abidi and T. Chandra, “A new eﬃcient and direct solution for pose estima-
tion using quadrangular targets: Algorithm and evaluation,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 17, no. 5, pp. 534–538, May 1995.
167

[9] M. A. Fischler and R. C. Bolles, “Random sample consensus: A paradigm for model
ﬁtting with applications to image analysis and automated cartography,” Communi-
cations of the ACM, vol. 24, no. 6, pp. 381–395, June 1981.
[10] L. G. Trabasso and C. Zielinski, “Semi-automatic calibration procedure for the vision-
robot interface applied to scale model decoration,” Robotica, vol. 10, pp. 303–308,
1992.
[11] M. W. Walker, L. Shao, and R. A. Volz, “Estimating 3-d location parameters using
dual number quaternions,” CVGIP: Image Understanding, vol. 54, no. 3, pp. 358–367,
November 1991.
[12] T. Q. Phong, R. Horaud, A. Yassine, and D. T. Pham,
“Optimal estimation of
object pose from a single perspective view,” in International Conference on Computer
Vision, February 1993, pp. 534–539.
[13] T. Phong, R. Horaud, A. Yassine, and D. Pham, “Object pose from 2-d to 3-d point
and line correspondences,” International Journal of Computer Vision, vol. 15, pp.
225–243, 1995.
[14] M. A. Abidi and R. C. Gonzalez, “The use of multisensor data for robotic applica-
tions,” IEEE Transactions on Robotics and Automation, vol. 6, no. 2, pp. 159–177,
April 1990.
[15] N. Chen, J. R. Birk, and R. B. Kelley, “Estimating workpiece pose using the feature
points method,” IEEE Transactions on Automatic Control, vol. AC-25, no. 6, pp.
1027–1041, December 1980.
[16] R. M. Haralick, “Pose estimation from corresponding point data,” IEEE Transactions
on Systems, Man, and Cybernetics, vol. 19, no. 6, pp. 1426–1446, November 1989.
168

[17] C. J. Ho and N. H. McClamroch, “Automatic spacecraft docking using computer
vision-based guidance and control techniques,” Journal of Guidance, Control, and
Dynamics, vol. 16, no. 2, pp. 281–288, March 1993.
[18] R. Horaud, B. Conio, and O. Leboulleux, “An analytic solution for the perspective
4-point problem,” Computer Vision, Graphics, and Image Processing, vol. 47, pp.
33–44, 1989.
[19] R. T. Howard and M. L. Book, “Video guidance sensor for autonomous capture,” in
NASA Automated Rendezvous and Capture Review, November 1991.
[20] K. Hung, C. Shyi, J. Lee, and T. Lee, “Robot location determination in a complex
environment by multiple marks,” Pattern Recognition, vol. 21, no. 6, pp. 567–580,
1988.
[21] Y. Hung, P. Yeh, and D. Harwood, “Passive ranging to known planar point sets,”
Tech. Rep. CAR-TR-65, Center for Automation Research, University of Maryland,
June 1984.
[22] S. Kamata, R. O. Eason, M. Tsuji, and E. Kawaguchi, “A camera calibration using
4 point-targets,”
in Proceedings 11th IAPR International Conference on Pattern
Recognition, August 1992, vol. 1, pp. 550–553.
[23] D. H. Kite and M. Magee, “Reasoning about camera positioning relative to rectan-
gular patterns,” in Proceedings SPIE Sensor Fusion: Spatial Reasoning and Scene
Interpretation, November 1988, vol. 1003, pp. 222–233.
[24] R. Krishnan, H. J. Sommer III, and P. D. Spidaliere, “Monocular pose of a rigid body
using point landmarks,” Computer Vision, Graphics, and Image Processing, vol. 55,
no. 3, pp. 307–316, May 1992.
169

[25] F. E. Veldpaus, “An analytic solution for the perspective 4-point problem,” CVGIP,
vol. 47, pp. 33–44, 1989.
[26] K. Mandel and N. A. Duﬃe, “On-line compensation of mobile robot docking errors,”
IEEE Journal of Robotics and Automation, vol. RA-3, no. 6, pp. 591–598, December
1987.
[27] Y. Liu, T. S. Huang, and O. D. Faugeras, “Determination of camera location from
2-d to 3-d line and point correspondences,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 12, no. 1, pp. 28–37, January 1990.
[28] J. C. Tietz and L. M. Germann, “Autonomous rendezvous and docking,” in Proceed-
ings of the 1982 American Control Conference, 1982, vol. 2, pp. 460–465.
[29] W. B. Jatko, J. S. Goddard, R. K. Ferrell, S. S. Gleason, J. S. Hicks, and V. K. Varma,
“Crusader automated docking system phase iii report,” Tech. Rep. ORNL/TM-13177,
Oak Ridge National Laboratory, March 1996.
[30] D. G. Lowe, Perceptual Organization and Visual Recognition, Kluwer, Boston, MA,
1985.
[31] J. S. Goddard, W. B. Jatko, R. K. Ferrell, and S. S. Gleason, “Robust pose deter-
mination for autonomous docking,” in ANS Sixth Topical Meeting on Robotics and
Remote Systems, February 1995, pp. 767–774.
[32] W. J. Wolfe, G. K. White, and L. J. Pinson, “A multisensor robotic locating system
and the camera calibration problem,”
in SPIE Intelligent Robots and Computer
Vision, September 1985, vol. 579, pp. 420–431.
[33] H. F. L. Pinkney, C. I. Perratt, and S. G. MacLean, “Canex-2 space vision system
experiments for shuttle ﬂight sts-54,” in SPIE Close-Range Photogrammetry Meets
Machine Vision, 1990, vol. 1395, pp. 374–381.
170

[34] H. F. L. Pinkney and C. I. Perratt, “A ﬂexible machine vision guidance system for
3-dimensional control tasks,” in Proceedings of the International Society for Pho-
togrammetry and Remote Sensing, Commission V Symposium, June 1986, pp. 401–
411.
[35] J. Ponce, A. Hoogs, and D. J. Kreigman, “On using cad models to compute the pose
of curved 3d objects,” CVGIP: Image Understanding, vol. 55, no. 2, pp. 184–197,
March 1992.
[36] S. Chen and W. Tsai, “Determination of robot locations by common object shapes,”
IEEE Transactions on Robotics and Automation, vol. 7, no. 1, pp. 149–156, February
1991.
[37] R. Safaee-Rad, I. Tchoukanov, B. Benhabib, and K. C. Smith, “Accurate parameter
estimation of quadratic curves from grey-level images,” CVGIP: Image Understand-
ing, vol. 54, no. 2, pp. 259–274, September 1991.
[38] Y. Han, W. E. Snyder, and G. L. Bilbro, “Pose determination using tree annealing,”
in Proceedings IEEE International Conference on Robotics and Automation, May
1990, pp. 427–432.
[39] J. Ohya, L. Davis, and D. DeMenthon,
“Recognizing 3-d objects from range im-
ages and ﬁnding their six pose parameters,” Tech. Rep. CAR-TR-538, Center for
Automation Research, University of Maryland, February 1991.
[40] B. Wirtz and C. Maggioni, “3-d pose estimation by an improved kohonen-net,” in
Visual Form: Analysis and Recognition, C. Arcelli, Ed., pp. 593–602. Plenum Press,
New York, 1992.
171

[41] S. Ma, S. Si, and Z. Chen, “Quadric curve based stereo,” in Proceedings 11th Inter-
national Conference on Pattern Recognition, The Hague, The Netherlands, August
1992, vol. 1, pp. 1–4.
[42] H. H. Chen,
“Pose determination from line-to-plane correspondences: Existence
condition and closed-form solutions,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 13, no. 6, pp. 530–541, June 1991.
[43] R. Consales, A. D. Bimbo, and P. Nesi, “On the perspective projection of 3-d planar-
faced junctions,” in Proceedings 11th International Conference on Pattern Recogni-
tion, The Hague, The Netherlands, August 1992, vol. 1, pp. 616–619.
[44] M. Dhome, M. Richetin, J. LaPreste, and G. Rives, “Determination of the attitude of
3-d objects from a single perspective view,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 11, no. 12, pp. 1265–1278, December 1989.
[45] T. Ellis, A. Abbood, and B. Brillaut, “Ellipse detection and matching with uncer-
tainty,” Image and Vision Computing, vol. 10, no. 5, pp. 271–276, June 1992.
[46] N. J. Foster and A. C. Sanderson,
“Determining object orientation using ellipse
ﬁtting,” in SPIE Intelligent Robots and Computer Vision, 1984, vol. 521, pp. 33–43.
[47] R. Safaee-Rad, I. Tchoukanov, B. Benhabib, and K. C. Smith, “3d pose estimation
from a quadratic-curved feature in two perspective views,” in Proceedings 11th IAPR
International Conference on Pattern Recognition, The Hague, The Netherlands, Au-
gust 1992, vol. 1, pp. 341–344.
[48] R. Safaee-Rad, I. Tchoukanov, B. Benhabib, and K. C. Smith, “Accurate parameter
estimation of quadratic curves from grey-level images,” CVGIP: Image Understand-
ing, vol. 54, no. 2, pp. 259–274, September 1991.
172

[49] R. Safaee-Rad, I. Tchoukanov, K. C. Smith, and B. Benhabib, “Three-dimensional
location estimation of circular features for machine vision,” IEEE Transactions on
Robotics and Automation, vol. 8, no. 5, pp. 624–640, October 1992.
[50] M. Ferri, F. Mangili, and G. Viano,
“Projective pose estimation of linear and
quadratic primitives in monocular computer vision,”
CVGIP: Image Understand-
ing, vol. 58, no. 1, pp. 66–84, July 1993.
[51] R. Glachet, M. Dhome, and J. Lapreste, “Finding the perspective projection of an
axis of revolution,” Pattern Recognition Letters, vol. 12, pp. 693–700, November 1991.
[52] R. R. Goldberg and D. G. Lowe,
“Hessian methods for veriﬁcation of 3d model
parameters from 2d image data,” in Proceedings of SPIE: Sensor Fusion: Spatial
Reasoning and Scene Interpretation Conference, 1989, vol. 1003, pp. 63–67.
[53] M. Han and S. Rhee,
“Camera calibration for three-dimensional measurement,”
Pattern Recognition, vol. 25, no. 2, pp. 155–164, 1992.
[54] R. M. Haralick and Y. H. Chu, “Solving camera parameters from the perspective
projection of a parameterized curve,” Pattern Recognition, vol. 17, no. 6, pp. 637–
645, 1984.
[55] R. M. Haralick, “Determining camera parameters from the perspective projection of
a rectangle,” Pattern Recognition, vol. 22, no. 3, pp. 225–230, 1989.
[56] M. R. Kabuka and A. E. Arenas,
“Position veriﬁcation of a mobile robot using
standard pattern,” IEEE Journal of Robotics and Automation, vol. RA-3, no. 6, pp.
505–516, December 1987.
[57] B. Hussain and M. R. Kabuka, “Real-time system for accurate three-dimensional
position determination and veriﬁcation,” IEEE Transactions on Robotics and Au-
tomation, vol. 6, no. 1, pp. 31–43, February 1990.
173

[58] D. G. Lowe, “Three-dimensional object recognition from single two-dimensional im-
ages,” Artiﬁcial Intelligence, vol. 31, pp. 355–393, 1987.
[59] J. Neira, L. Montano, and J. D. Tardos,
“Constraint-based object recognition in
multisensor systems,”
in Proceedings IEEE International Conference on Robotics
and Automation, May 1993, vol. 3, pp. 135–142.
[60] D. Kriegman, B. Vijayakumar, and J. Ponce, “Constraints for recognizing and locat-
ing curved 3d objects from monocular image features,” in Computer Vision–ECCV
’92, G. Sandini, Ed., pp. 829–833. Springer-Verlag, 1992.
[61] R. Kumar,
“Determination of camera location and orientation,”
in Proceedings:
Image Understanding Workshop, May 1989, pp. 870–879.
[62] R. Kumar and A. R. Hanson, “Robust methods for estimating pose and a sensitivity
analysis,” CVGIP: Image Understanding, vol. 60, no. 3, pp. 313–342, November 1994.
[63] B. K. P. Horn, “Closed-form solution of absolute orientation using unit quaternions,”
Journal of the Optical Society of America, vol. 4, no. 4, pp. 629–642, April 1987.
[64] S. Linnainmaa, D. Harwood, and L. S. Davis,
“Pose determination of a three-
dimensional object using triangle pairs,”
IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 10, no. 5, pp. 634–647, September 1988.
[65] L. P. Ray, “Estimation of modeled object pose from monocular images,” in Pro-
ceedings IEEE International Conference on Robotics and Automation, May 1990, pp.
408–413.
[66] Y. Roth, A. S. Wu, R. H. Arpaci, T. Weymouth, and R. Jain, “Model-driven pose cor-
rection,” in Proceedings IEEE International Conference on Robotics and Automation,
May 1992, pp. 2625–2630.
174

[67] D. D. Sheu and A. H. Bond, “A generalized method for 3d object location from single
2d images,” Pattern Recognition, vol. 25, no. 8, pp. 771–786, 1992.
[68] M. A. Abidi, “A regularized multi-dimensional data fusion technique,” in Proceedings
IEEE International Conference on Robotics and Automation, April 1991, pp. 2738–
2744.
[69] N. Ayache and O. Faugeras, “Building, registrating, and fusing noisy visual maps,”
International Journal of Robotics Research, vol. 7, no. 6, pp. 45–65, December 1988.
[70] W. M. Wells III, “Posterior marginal pose estimation,” in Proceedings DARPA Image
Understanding Workshop, January 1992, pp. 745–751.
[71] A. Singh, “Robust computation of image-motion and scene-depth,” in Proceedings
IEEE International Conference on Robotics and Automation, April 1991, pp. 2370–
2737.
[72] L. Matthies, T. Kanade, and R. Szeliski, “Kalman ﬁlter-based algorithms for esti-
mating depth from image sequences,” International Journal of Computer Vision, vol.
3, no. 8, pp. 209–236, 1989.
[73] J. Weng, P. Cohen, and M. Herniou, “Camera calibration with distortion models
and accuracy evaluation,”
IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 14, no. 10, pp. 965–980, October 1992.
[74] A. Azarbayejani and A. P. Pentland, “Recursive estimation of motion, structure, and
focal length,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
17, no. 6, pp. 562–575, June 1995.
[75] T. J. Broida, S. Chandrashekhar, and R. Chellappa, “Recursive 3-d motion estimation
from a monocular image sequence,” IEEE Transactions on Aerospace and Electronic
Systems, vol. 26, no. 4, pp. 639–656, July 1990.
175

[76] S. Lee and Y. Kay, “A kalman ﬁlter approach for accurate 3-d motion estimation
from a sequence of stereo images,” CVGIP: Image Understanding, vol. 54, no. 2, pp.
244–258, September 1991.
[77] S. Lee and Y. Kay,
“An accurate estimation of 3-d position and orientation of a
moving object for robot stereo vision: Kalman ﬁlter approach,” in Proceedings IEEE
International Conference on Robotics and Automation, May 1990, pp. 414–419.
[78] D. B. Westmore and W. J. Wilson, “Direct dynamic control of a robot using an end-
point mounted camera and kalman ﬁlter position estimation,” in Proceedings IEEE
International Conference on Robotics and Automation, April 1991, pp. 2376–2384.
[79] J. Wang and W. J. Wilson, “3d relative position and orientation estimation using
kalman ﬁlter for robot control,” in Proceedings IEEE International Conference on
Robotics and Automation, May 1992, pp. 2638–2645.
[80] B. K. P. Horn, Robot Vision, MIT Press, Cambridge, MA, 1986.
[81] J. B. Burl, “A reduced order extended kalman ﬁlter for sequential images containing
a moving object,” IEEE Transactions on Image Processing, vol. 2, no. 3, pp. 285–295,
July 1993.
[82] P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman, “Automated tracking and
grasping of a moving object with a robotic hand-eye system,” Tech. Rep. CUCS-034-
91, Department of Computer Science, Columbia University, November 1991.
[83] N. P. Papanikolopoulos, P. K. Khosla, and T. Kanade, “Visual tracking of a moving
target by a camera mounted on a robot: A combination of control and vision,” IEEE
Transactions on Robotics and Automation, vol. 9, no. 1, pp. 14–35, February 1993.
176

[84] W. J. Wilson, C. C. W. Hulls, and G. S. Bell,
“Relative end-eﬀector control us-
ing cartesian position based visual servoing,” IEEE Transactions on Robotics and
Automation, vol. 12, no. 5, pp. 684–696, October 1996.
[85] J. C. K. Chou, “Quaternion kinematic and dynamic diﬀerential equations,” IEEE
Transactions on Robotics and Automation, vol. 8, no. 1, pp. 53–64, February 1992.
[86] W. K. Cliﬀord, “Preliminary sketch of bi-quaternions,” Proceedings of the London
Mathematical Society, vol. 4, pp. 381–395, 1873.
[87] E. Study, Geometrie der Dynamen, Leipzig, 1903.
[88] R. C. Gonzalez and P. Wintz, Digital Image Processing, Addison-Wesley, Reading,
MA, 1977.
[89] K. Daniilidis and E. Bayro-Corrochano, “The dual quaternion approach to hand-eye
calibration,” in Proceedings IEEE International Conference on Pattern Recognition,
1996, p. A7E.6.
[90] R. E. Kalman, “A new approach to linear ﬁltering and prediction problems,” Journal
of Basic Engineering (ASME), vol. 82D, pp. 35–45, March 1960.
[91] A. Gelb, Ed.,
Applied Optimal Estimation,
The MIT Press, Cambridge, Mas-
sachusetts, 1974.
[92] Y. Bar-Shalom and T. E. Fortmann, Tracking and Data Association, Academic Press
Inc., Orlando, Florida, 1988.
[93] E. Kreyszig, Advanced Engineering Mathematics, John Wiley and Sons, New York,
1983.
[94] W. L. Brogan, Modern Control Theory, Prentice-Hall, Englewood Cliﬀs, New Jersey,
1982.
177

[95] R. Talluri and J. K. Aggarwal, “Mobile robot self-location using model-image feature
correspondence,” IEEE Transactions on Robotics and Automation, vol. 12, no. 1, pp.
63–77, February 1996.
[96] P. Z. Peebles Jr.,
Probability, Random Variables, and Random Signal Principles,
McGraw-Hill, New York, NY, 1980.
[97] J. B. Thomas, An Introduction to Applied Probability and Random Processes, John
Wiley and Sons, New York, NY, 1971.
[98] C. C. W. Hulls,
Dynamic Real-Time Multisensor Fusion Using an Object Model
Reference Approach, Ph.D. thesis, University of Waterloo, Waterloo, Ontario, 1996.
[99] R. G. Brown and P. Y. C. Hwang, Introduction to random signals and applied Kalman
ﬁltering, John Wiley and Sons, New York, second edition, 1986.
178

VITA
James Samuel Goddard, Jr., was born in Sparta, Tennessee, on November 13, 1949.
After attending public schools in Tennessee, he graduated from White County High School,
Sparta, Tennessee, in June 1967. He received a Bachelor’s degree in Electrical Engineering
from Georgia Institute of Technology, Atlanta, Georgia, in June 1971. After graduation,
he held an engineering position with the National Aeronautics and Space Administration,
Goddard Space Flight Center, Greenbelt, Maryland, from 1971 to 1976.
During this
period, he also attended graduate school at the University of Maryland, College Park,
Maryland, receiving a Master of Science Degree with a major in Electrical Engineering in
December 1974. Since 1976, he has held several engineering positions at the Department
of Energy facilities in Oak Ridge, Tennessee. For the past seven years, he has been a
development engineer in the Image Science and Machine Vision group at the Oak Ridge
National Laboratory.
179

