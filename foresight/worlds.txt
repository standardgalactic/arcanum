Worlds, Models, and Descriptions
John F. Sowa
Abstract.  Since the pioneering work by Kripke and Montague, the term possible world has appeared in most 
theories of formal semantics for modal logics, natural languages, and knowledge-based systems.  Yet that term 
obscures many questions about the relationships between the real world, various models of the world, and 
descriptions of those models in either formal languages or natural languages.  Each step in that progression is 
an abstraction from the overwhelming complexity of the world.  At the end, nothing is left but a colorful 
metaphor for an undefined element of a set W called worlds, which are related by an undefined and undefinable 
primitive relation R called accessibility.  For some purposes, the resulting abstraction has proved to be useful, 
but as a general theory of meaning, the abstraction omits too many significant features.  So much information 
has been lost at each step that many philosophers, linguists, and psychologists have dismissed model-theoretic 
semantics as irrelevant to the study of meaning.  This article examines the steps in the process of extracting the 
pair (W,R) from the world and the way people talk about the world.  It shows that the Kripke worlds can be 
reinterpreted as part of a Peircean semiotic theory, which can also include contributions from many other studies 
in cognitive science.  Among them are Dunn's semantics based on laws and facts, the lexical semantics preferred 
by many linguists, psychological models of how the world is perceived, and philosophies of science that relate 
theories to the world.  A full integration of all those sources is far beyond the scope of this article, but an outline 
of the approach suggests that Peirce's vision is capable of relating and reconciling the competing sources.
Published in Studia Logica, Special Issue Ways of Worlds II, 84:2, 2006, pp. 323-360.  
1. Models for Modality
As a de facto standard for modal logic, Kripke semantics is usually adopted for any kind of intensional 
logic, and logicians rarely ask fundamental questions about its suitability:  Although a Tarski-style 
model is a set-theoretic construction whose elements correspond to things that are assumed to exist, 
what features of a Kripke-style model enable it to cross the boundary between what is and what must 
be, ought to be, or is believed to be?  Since a Kripke model derives modality from a set of possible 
worlds and an accessibility relation among worlds, doesn't it presuppose a more primitive notion of 
modality that lies hidden in the -ible suffixes of possible and accessible?  If not, by what magic can a 
Kripke model derive ought from is?  If so, what makes possible worlds possible or accessible?  Could 
that more primitive notion of modality be formalized?  Can the modal axioms be derived from a more 
primitive notion that avoids the possible worlds and the accessibility relation?  What formal 
mechanisms could support such a derivation?  Would it be as powerful and flexible as Kripke 
semantics?  Or more so?  Could it explain the bewildering variety of axioms for modality?  Could it 
enable different modes of modality to be expressed in the same sentence?  Could it support multimodal 
reasoning?  How?  
An alternate semantics proposed by Hintikka (1961, 1963) and extended by Dunn (1973) may answer 
or at least clarify these questions. Instead of possible worlds, Hintikka assumed maximally consistent 
sets of propositions called model sets with an alternativity relation between model sets that is 
equivalent to Kripke's accessibility relation between worlds.  Informally, Hintikka's model sets could 
be considered descriptions of Kripke's worlds. Formally, model sets have more structure that permits a 
wider range of operations. Dunn took advantage of that structure to distinguish a subset of each model 
set called laws: 

1. Each model set Mi is considered the facts that describe the corresponding world wi. 
2. For each world wi, Dunn assumed a subset Li of Mi called the laws of wi. 
3. Then Dunn defined accessibility from the world wi to the world wj to mean that the laws Li of 
the first world are a subset of the facts Mj of the second world: 
R(wi,wj) ≡ Li⊂Mj. 
According to this definition, the laws of the first world wi remain true in the second world wj, 
but they may be demoted from the status of laws to merely contingent facts. 
4. Finally, Dunn showed that this construction is a conservative refinement of Kripke's theory, 
since any Kripke model structure (W,R,Φ) can be converted to one of Dunn's model structures 
by replacing every world w in W with the set M of propositions that are true in w and the set L 
of propositions that are necessarily true in w: 
M = {p | Φ(p,w) = true}. 
L = {p | (∀u  
∈W)(R(w,u)  Φ(
⊃
p,u) = true}. 
This construction generates a model in Dunn's sense for every model in Kripke's sense. It holds for the 
normal models (Kripke 1963), in which every law is also a fact, and for the non-normal models (Kripke 
1965), in which some laws might not be facts. In deontic logic, for example, the laws are obligatory, 
but sinners might violate them. For such models, R is not reflexive because a non-normal world is not 
accessible from itself. (This, by the way, is one more example of the nonintuitive nature of Kripke 
models:  it seems odd to say that any world could be inaccessible from itself, especially the real world 
because it happens to have sinners.) 
With Dunn's construction, the accessibility relation is no longer primitive, and modal semantics does 
not depend on imaginary worlds. Instead, modality depends on the choice of laws, which could be laws 
of nature or merely human rules and regulations. Every theorem proved with Kripke's models is also 
true of Dunn's models, but the additional structure available with model sets enables new operations, 
theories, and applications to be explored. Furthermore, vague questions about the meaning of modality 
can be stated more precisely as "Who or what determines the laws?" 
Any mention of a lawgiver threatens to invoke the twin demons of subjectivism and psychologism, 
which Kripke's sets seem to avoid. Montague (1967) explicitly stated his goal of replacing messy 
human attitudes with sets:  "It has for fifteen years been possible for at least one philosopher (myself) 
to maintain that philosophy, at this stage in history, has as its proper theoretical framework set theory 
with individuals and the possible addition of empirical predicates." With sets as the ultimate ontology, 
everything is reducible to constructions built from a single primitive, the empty set:  { }. The next step 
is to construct the set whose only member is the the empty set, {{ }}, or iterations of such emptiness 
ad infinitum:  {{ }, {{ }}, {{{ }}}, ...}. 
Like Montague, Quine preferred to reduce everything to sets, but one of his former students, who later 
spent many years working as an assistant to Gödel, came to deplore the emptiness of such construc-
tions. Wang (1986) called Quine's critical, but reductionist approach logical negativism: 
Quine merrily reduces mind to body, physical objects to (some of) the place-times, place-
times to sets of sets of numbers, and numbers to sets. Hence, we arrive at a purified 
ontology which consists of sets only.... I believe I am not alone in feeling uncomfortable 

about these reductions. What common and garden consequences can we draw from such 
grand reductions? What hitherto concealed information do we get from them? Rather than 
being overwhelmed by the result, one is inclined to question the significance of the 
enterprise itself. (p. 146) 
In support of this view, Wang quoted a personal letter from C. I. Lewis about the state of philosophy in 
1960: 
It is so easy... to get impressive 'results' by replacing the vaguer concepts which convey real 
meaning by virtue of common usage by pseudo precise concepts which are manipulable by 
'exact' methods — the trouble being that nobody any longer knows whether anything actual 
or of practical import is being discussed. (p. 116) 
As this discussion indicates, many questions about the meaning of modality are not adequately 
answered by Kripke semantics. In fact, the possible worlds themselves raise serious ontological and 
epistemological questions. Their contribution to the study of meaning is a mathematical structure 
whose relevance to language, communication, and the intentions of the people who communicate is 
certainly not obvious. The five sections of this article explore these issues and their relationship to 
possible worlds and to the laws and facts that describe them: 
1. This first section has presented the one-to-one mapping between possible worlds and sets of 
laws and facts.  That mapping preserves all the axioms and theorems of Kripke semantics, and it 
makes the laws that determine modality accessible to critical analysis. 
2. The next section discusses philosophical issues about the derivation of laws and facts from the 
analysis and observation of worlds or situations, either real or imaginary. The conclusion is that 
observation alone is insufficient to derive the laws or principles that determine modal and 
intentional properties. 
3. A more promising source of laws is Peirce's theory of signs, which covers the full range of 
semiotic issues from epistemology and philosophy of science to formal ontology with its 
applications to linguistics and computer science and finally to methods of reasoning, 
communication, and purposive action in and on the world. 
4. Since Dunn's semantics shifts the focus of modal semantics from possible worlds to 
propositions, Section 4 replaces the definition of proposition as a set of possible worlds with a 
more fine-grained definition as an equivalence class of sentences. 
5. The concluding section discusses new representations in linguistics and new methods of 
reasoning in artificial intelligence that become available when the formalism is expressed in 
terms of laws and facts. 
An earlier article (Sowa 2003) presented nested graph models as a family of general structures for 
Dunn's semantics that could be specialized to versions of Kripke models, Montague models, Barcan 
models, and models for various temporal, modal, and intentional logics. This article explores the 
technical advantages and the methodological clarifications that become possible with that approach. 
2. Attempts to Derive Modal Notions
Tarski's semantics derives the truth or falsity of statements in first-order logic from models consisting 
of a set of objects that are assumed to exist and relations that are true of those objects. But the 
impossibility of deriving ethical or aesthetic judgments from observations of existing objects and 

relations has been recognized for centuries. Yet philosophers have continued to search for methods of 
deriving closely related modal notions, propositional attitudes, and social relationships from abstract 
sets or observable data. Despite his fondness for sets as a basis for ontology, Quine (1972) recognized 
that Kripke's sets failed to capture the intuitive meaning of modality: 
The notion of possible world did indeed contribute to the semantics of modal logic, and it 
behooves us to recognize the nature of its contribution: it led to Kripke's precocious and 
significant theory of models of modal logic. Models afford consistency proofs; also they 
have heuristic value; but they do not constitute explication. Models, however clear they be 
in themselves, may leave us at a loss for the primary, intended interpretation. 
Finding a model for a set of axioms can, by itself, prove only that the axioms are consistent relative to 
the axioms of the system used in constructing the model. Whatever modality is embodied in a Kripke 
model structure (W,R,Φ) is ultimately derived from some choice of a particular set W and relation R. 
The effect of the formal machinery is to expose the modality that lurks behind the chosen W and R; it 
does not explain the reason for the choice. 
The metaphor of calling an element of W a world conjures up images of the Starship Enterprise visiting 
worlds with physical laws similar to our own, but with very different flora, fauna, and cultures.  But 
Kripke's original worlds were nothing but unstructured points, and all the modal information lay buried 
in the primitive and unexplained accessibility relation R. Montague added more detail by assuming 
that a noun like cow would correspond to a function that would map any world to the set of its cows. 
By using the syntax of a sentence as a blueprint for composing such functions, Montague showed how 
a function could be constructed for the sentence that would map possible worlds to truth values. 
Montague's brilliant mathematical innovations distracted attention from the fact that the worlds were 
still undefined primitives, and there were no guidelines for defining the functions assumed for each 
word in the lexicon or actually applying them to any world. Barbara Partee, one of the pioneers of 
formal semantics, made the following observation about the hidden assumptions: 
In Montague's formal semantics the simple predicates of the language of intensional logic 
(IL), like love, like, kiss, see, etc., are regarded as symbols (similar to the "labels" of 
[predicate calculus]) which could have many possible interpretations in many different 
models, their "real meanings" being regarded as their interpretations in the "intended 
model". Formal semantics does not pretend to give a complete characterization of this 
"intended model", neither in terms of the model structure representing the "worlds" nor in 
terms of the assignments of interpretations to the lexical constants.  The present formaliza-
tions of model-theoretic semantics are undoubtedly still rather primitive compared to what 
is needed to capture many important semantic properties of natural languages, including, 
for example, spatial and other perceptual representations which play an important role in 
many aspects of linguistic structure. The logical structure of language is a real and 
important part of natural language and we have fairly well-developed tools for describing it. 
There are other approaches to semantics that are concerned with other aspects of natural 
language, perhaps even cognitively "deeper" in some sense, but which we presently lack 
the tools to adequately formalize. (2005, Lecture 4) 
As in Quine's observation about Kripke's models, Montague's models demonstrate consistency, but 
"leave us at a loss for the primary, intended interpretation."  A lexicographer, however, would expect 
to find the intended interpretation in a dictionary entry. 
To avoid the worlds of Kripke and Montague, Barwise and Perry (1983) proposed situation semantics 
as a theory that relates meaning to much smaller situations.  They defined a proposition as a pair {σ;T} 

consisting of a situation σ and a situation type T, which expresses a pattern of relationships that is true 
of σ. According to their original definition, a situation is a region of space-time, but they found it 
difficult to specify the criteria for determining what region is relevant to any particular case.  One of 
their examples was the situation of a college lecture, which illustrates the difficulty: 
1. A lecture could be considered a situation bounded by a fifty-minute time period in a spatial 
region enclosed by the walls of a classroom. But if the time were shifted by thirty minutes, the 
region would include the ending of one lecture and the beginning of another. That time shift 
would create an unnatural "situation." 
2. If the space were shifted by half the width of a classroom, it would include part of one class 
listening to one professor, part of another class listening to another professor speaking on a 
different topic, and a wall between the two lectures. That would create an even more unnatural 
situation than the time shift. 
3. A third option would fix the coordinate system relative to the sun instead of the earth. Then the 
region that included the class at the beginning of the lecture would stay behind as the earth 
moved. Within a few minutes, it would be in deep space, containing nothing but an occasional 
hydrogen atom. 
Even more complex situations would be needed for the referents of Greek mythology or the U.S. legal 
history. The first is fictional, and the second is intertwined with all the major events that happened in 
the United States from 1776 to the present. The space-time region for a fictional situation does not 
exist, and the space-time region for the U.S. legal history cannot be separated from the region of its 
political, economic, or cultural history. 
In discussing the development of situation theory, Devlin (1991) observed that the definitions were 
stretched to the point where situations "include, but are not equal to any of simply connected regions 
of space-time, highly disconnected space-time regions, contexts of utterance (whatever that turns out 
to mean in precise terms), collections of background conditions for a constraint, and so on."  After 
further discussion, Devlin admitted that they cannot be defined: "Situations are just that: situations. 
They are abstract objects introduced so that we can handle issues of context, background, and so on." 
In short, situations determine meaning, but there are no criteria for distinguishing a meaningful 
situation from an arbitrary chunk of space-time.  The purpose of situation semantics is to derive the 
meaning of propositional attitudes from situations, but the choice of situation depends critically on 
the meaning that is supposed to be derived. 
The difficulty of deriving meaning from observable data was illustrated by the shortcomings of one of 
the most ambitious attempts:  Der logische Aufbau der Welt by Carnap (1928).  As he explained in the 
preface to the second edition (1961), "The main problem concerns the possibility of the rational 
reconstruction of the concepts of all fields of knowledge on the basis of concepts that refer to the 
immediately given."  To demonstrate possibility, the book begins with an analysis of elementary 
experiences (Elementarerlebnisse) and "outlines" the methods for defining concepts at three levels: 
1. Lower levels, autopsychological objects:  Detailed logical formulas for representing 
similarity, quality classes, part identity, part similarity, similarity between qualities, sensations, 
the divisions of an elementary experience, the visual field, colors, and the preliminary time 
order. 
2. Intermediate levels, physical objects:  No formalism, but an extended discussion "merely in a 
loose paraphrase" (§123) of the geometric principles involved in the derivation of "My Body" 
and its relationships to physical objects in a Euclidean space. 
3. Upper levels, heteropsychological objects:  A vague discussion of "other persons" with 

frequent apologies for the vagueness and some hopes that behaviorism or neural science might 
someday fill the gaps. 
For Carnap, psychological "objects" were physical objects that have a spatiotemporal location within 
an individual "human object" (§18). He "clarified" the notion of "intention relation" by claiming it is 
"nothing but" a relation between a psychological object and some other object (§164). He recognized 
the importance of the "sign relation", but admitted that "The construction of this relation is more 
difficult than any of the other relations which we have hitherto undertaken" (§141). In the 1961 
preface, Carnap devoted several pages to discussing his later ideas about the lower and intermediate 
levels, but he said nothing about the vague upper levels, which include intentions, signs, language, 
communication, and social relations. 
Among the more recent attempts to reduce meaning and intentionality to observable terms, Smith 
(1995, 1998, 1999) tried to define all aspects of human life in terms of mereological sums of physical 
objects and processes. His resulting ontology has two basic categories, continuants (physical objects) 
and occurrents (processes), which he combines by the partOf relation to form complex entities called 
physical-behavioral units and social objects: 
•
Examples of physical-behavioral units include "Wendy's Friday afternoon class, Jim's meeting 
with his teacher, your Thursday lunch, Frank's early morning swim." These units are similar to 
Barwise and Perry's situations, but the hyphenated adjective makes them sound more observable 
and therefore more objective. 
•
Examples of social objects include legal entities such as "juries, courts, contracts, lawsuits", 
cultural entities such as "works of music and literature", and human social groups such as 
"families and tribes, nations and empires, but also orchestras and chess clubs, battalions and 
football teams, as well as those more or less short-lived social groupings, which arise when 
strangers are formally introduced, or pair up on the dance floor." In trying to make social 
objects purely physical, Smith defined them as far-flung conglomerations of objects and 
processes. A contract, for example, would include not only the signed piece of paper, but also 
the people who signed it, the act of signing, and all the objects and processes involved in 
fulfilling the contract throughout its duration. What is missing from Smith's definition are the 
only things that could make it meaningful:  the intentions of the people who sign the contract 
and carry out its provisions. 
Although Smith claimed objectivity for his definitions, they violate the requirements for an effective 
operational test. His social objects, for example, include so many physical entities scattered over long 
intervals of time that itemizing them on paper is difficult and observing them in action is impossible. 
When arguing about a contract in a court of law, lawyers do not consult the mereological sum of 
physical actions, but sentences written on paper or spoken by witnesses. In Smith's examples, the 
fundamental requirement that determines the nature and extent of the entities involved is some sign or 
signs spoken by humans (e.g., "Meet in my office at 2 pm.") or recorded on paper (e.g., a deed, receipt, 
or contract). In every case, signs are fundamental not only to the definition of the social objects, but to 
their actual existence.
 
These examples illustrate the difficulty, if not impossibility of deriving modality or propositional 
attitudes from any set or structure — concrete, abstract, or imaginary — unless it had been selected or 
constructed on the basis of some prior intentionality. This point is independent of any position for or 
against modal realism, as formulated by David Lewis (1986):  "The thesis that the world we are part of 
is but one of a plurality of worlds" (p. vii). Although he believed in the reality of possible worlds, 
Lewis admitted that there is no way to gain any knowledge of them by observation or communication. 

Instead, he maintained that knowledge of possible worlds is obtained in the same way as mathematical 
knowledge:  "we come by our opinions largely by reasoning from general principles that we already 
accept; sometimes in a precise and rigorous way, sometimes in a more informal fashion, as when we 
reject arbitrary-seeming limits on the plenitude of the mathematical universe" (p. 113). In addition to 
those general principles, Lewis proposed "a principle of recombination" for assembling elements of the 
actual world with imaginary variations. In other words, any possible worlds that are knowable or 
imaginable are the product of human reasoning, imagination, and selection of principles. Therefore, 
any modality or intentionality derived by any formalism based on them must be a transformation of 
whatever intentions motivated their selection or construction. 
3. Semiotic Foundations
Tarski semantics for first-order logic depends on a dyadic correspondence between some pattern of 
symbols stated in a formal language and some pattern of objects and relations that exist in a model. 
Such correspondences can answer several important kinds of questions about an object, an event, or a 
situation:  What is it?  What are its properties?  What is it made of?  And how is it related to other 
things?  But there's one kind of question that no amount of observation can answer with certainty:  
those that begin with the word why. For modal logic, no observations can explain why some pattern in 
the world might be necessary, possible, or impossible. For the behavior of humans and other animals, 
no observation can explain why they happen to perform one action rather than another. A thoughtful 
observer might be able to guess the reasons for their actions based on an analogy with his or her own 
experience, but the intentions themselves are not directly observable. Since humans can talk, the best 
way to determine their intentions is not to observe and classify worlds or situations, but just to ask 
them. But to take that option is an admission that Carnap's attempt to reduce mental phenomena to 
observable data about behavior has failed. 
Among the philosophers who believe that Carnap's approach was a dead end, Searle (1983) claimed 
that the semantics of natural language, at least for language about anything dealing with intentionality, 
depends critically on the nature of the mind: 
The capacity of speech acts to represent objects and states of affairs in the world is an 
extension of the more biologically fundamental capacities of the mind (or brain) to relate 
the organism to the world by way of such mental states as belief and desire, and especially 
through action and perception. Since speech acts are a type of human action, and since the 
capacity of speech to represent objects and states of affairs is part of a more general 
capacity of the mind to relate the organism to the world, any complete account of speech 
and language requires an account of how the mind/brain relates the organism to reality. 
(p. vii) 
In the concluding chapter, Searle claimed "there really are such things as intrinsic mental phenomena 
which cannot be reduced to something else or be eliminated by some kind of re-definition. There really 
are pains, tickles, and itches, beliefs, fears, hopes, desires, perceptual experiences, experiences of 
acting, thoughts, feelings, and all the rest" (p. 262). In the middle of the book, Searle formalized some 
of the discussion in formulas of the following kind, where p represents a proposition that describes 
some action or state (p. 32): 
Sorry(p)  →  Bel(p)  &  Des(~p). 
This formula says that if some person x is sorry for the state of affairs described by p, then x believes p 

and x desires that p not be true. Since Searle claimed that mental attitudes such as sorrow, belief, and 
desire actually exist, each of his monadic predicates could be expanded to a triadic relation named 
Experience, which explicitly relates the agent to the attitude and the proposition: 
Sorry(p)  →  (∃x:Person)(∃y:Sorrow)Experience(x,y,p). 
Bel(p)  →  (∃x:Person)(∃y:Belief)Experience(x,y,p). 
Des(p)  →  (∃x:Person)(∃y:Desire)Experience(x,y,p). 
The first formula says that the predicate Sorry about p implies the existence of a person x and an 
instance of sorrow y, which x experiences about p.  Similarly, the second and third formulas relate the 
predicates Bel and Des to instances of belief and desire, which x experiences about p.  In a later book, 
Searle (1995) was more explicit in using triadic relations to describe social relations.  All his triads had 
the form 
X counts as Y in context C. 
Searle's implicit or explicit triads for describing intentions are incompatible with Smith's attempt to 
avoid any commitment to mental phenomena.  In a public debate (Smith & Searle 2001), Smith tried to 
interpret Searle's constructions in terms of his own ontology, which does not admit the existence of 
intentions. Searle replied 
I think in the end he makes many useful points, but I also believe that he misunderstands 
me in certain very profound ways. I believe his misunderstandings derive from the fact that 
he approaches this topic with a set of concerns that are fundamentally different from mine, 
and in consequence, he tends to take my views as attempts to answer his questions rather 
than attempts to answer my questions. 
Searle recognized that relations that refer to intentions have greater power and flexibility than Smith's 
partOf relation. Smith criticized that flexibility as too loose and imprecise and noted that a context 
itself is a social object that requires some independent definition. Both authors made valid points:  
Searle's book demonstrates the fundamental role of intentions in creating and sustaining social 
relationships, but Smith's criticism shows the need for clear distinctions and greater precision. 
Philosophers since Aristotle have recognized the sign relation as triadic. The Scholastic logicians used 
the Latin terms signum for the sign, significatio for its significance or sense, and suppositio for the 
object it refers to. Saussure (1916) is a notable exception, who proposed a dyadic signifier to signified 
relation, which corresponds to the signum-significatio side of the meaning triangle, but omits Tarski's 
signum-suppositio or sign-object side. The dyadic versions by Saussure and Tarski have complemen-
tary weaknesses:  Tarski had a clear criterion for truth, but no recognition of intention; Saussure's 
failure to recognize the object led theorists such as Derrida to endless levels of interpretation with no 
criteria for convergence. 
In his study of Frege and Husserl, Mohanty (1982) noted that both of them had similar triadic 
definitions of signs, but with different labels. Frege's labels, Zeichen, Sinn, and Bedeutung, usually 
translated sign, sense, and reference, correspond to the Latin signum, significatio, and suppositio. 
Husserl's terms were Bedeutung for sense and Gegenstand (object) for reference. Yet Mohanty 
discussed their difficulties in dealing with context-dependent indexicals, such as I, you, this, or that: 
Husserl refers to indexicals and their like as threatening "to plunge all our hard-won 
distinctions back into confusion." These are what he calls "essentially occasional" 

expressions, in their case "it is essential to orient actual meaning to the occasion, the 
speaker, and the situation." (p. 57) 
Although both Husserl and Frege had triadic sign relations, they recognized only one type of sign, 
which Peirce called a symbol.  That triad was adequate for interpreting most content words in language, 
but not indexicals and other function words.  They did not even consider extralinguistic signs, such as 
a pointing finger or smoke as a sign of fire.  They faced the same dilemma as Searle:  either allow 
vagueness by loosening the definitions of the three terms in the triad or introduce new triads to 
accommodate different types of signs.  The latter option, however, would be ad hoc, unless some 
principled method could be found for generating new triads. 
Peirce discovered a metalevel principle for generating triads during the 1860s, when he and his father, 
the mathematician Benjamin Peirce, worked their way through Kant's Kritik der reinen Vernunft.  To 
derive his twelve categories, Kant started with four major groups — Quantity, Quality, Relation, and 
Modality — and divided each group into triads. As an example, Kant divided the Relation group into 
three categories named Inherence, Causality, and Community. While searching for a deeper principle 
underlying Kant's categories, Peirce noticed that Inherence could be defined by a monadic predicate 
that characterizes something by what it has in itself, independent of anything else; Causality would 
require a dyadic predicate that characterizes some reaction between two entities; and Community 
would depend on a triadic relation that establishes new connections among the members of the 
community. Following is one of Peirce's widely quoted definitions of the triad: 
First is the conception of being or existing independent of anything else. Second is the 
conception of being relative to, the conception of reaction with, something else. Third is the 
conception of mediation, whereby a first and a second are brought into relation. (CP 6.32) 
Unlike Aristotle's categories, which represent types and disjoint subtypes of existing things, Peirce's 
are phenomenological categories that represent the multiple ways of perceiving, conceiving, or 
describing anything.  They cannot be disjoint because the same thing can be described in many 
different ways.  Furthermore, the trichotomy is not just a single triad, but a metalevel principle that 
can be applied repeatedly to any phenomena to generate new categories.  His first application was to 
analyze Kant's method of subdividing the four major groups, but Peirce later applied it to any method 
of conceiving anything. 
Many logicians failed to appreciate the force of Peirce's distinction because it seemed "obvious" to 
them that any triadic relation could be defined as a combination of dyads. For example, the sentence 
Sue gives a child a book could be represented with a triadic relation gives or with three dyadic relations 
that represent the case relations or thematic roles of agent, recipient, and theme: 
(∃x:Person)(∃y:Child)(∃z:Book) 
      (nameOf(x,Sue)  gives(
∧
x,y,z)). 
(∃x:Person)(∃y:Child)(∃z:Book)(∃w:Give) 
      (nameOf(x,Sue)  agnt(
∧
w,x) 
 rcpt(
∧
w,y)  thme(
∧
w,z)). 
The first formula introduces three entities of types Person, Child, and Book, states that the name of the 
person is Sue, and relates all three by a single triadic relation.  The second formula reifies the relation 
by introducing the variable w, which refers to an instance of type Give.  This method of reification, 
which Peirce called hypostasis, is often called event semantics and attributed to Davidson (1967). 
Peirce, however, not only recognized the importance of hypostasis or reification of events and 
attributes, he also observed that it does not change the triadic connectivity of the formula. Connectivity 

is not obvious when a formula is written in predicate calculus, but it is much clearer when the formula 
is translated to a graph.  Figure 1 shows the two conceptual graphs that are derived from each of the 
two formulas in predicate calculus. 
 
Figure 1:  Two conceptual graphs for Sue gives a child a book 
The graph on the left of Figure 1 has a single relation node of type Gives linked to three concept nodes 
for each of the three participants.  The graph on the right replaces the relation node with a concept node 
of type Give and three dyadic relations of type Agnt, Rcpt, and Thme.  The resulting graph has more 
nodes, but exactly the same connectivity:  instead of three arcs attached to a relation node, it has three 
arcs attached to a concept node.  Peirce observed that any representation of intentionality of any kind 
must include an implicit or explicit triadic connection, which cannot be eliminated by hypostasis.  If it's 
not a triadic relation, it will include a triadic connection to some reified version of the intention. 
Peirce viewed semiotics or as he spelled it, semeiotic, as the unifying theme that relates naturally 
occurring signs and conventional symbols to concepts and reality.  Every sign is an example of 
Thirdness because it relates a perceptible mark (1) to some concept or other mental sign called its 
interpretant (2), which determines its referent or object (3). Although a sign requires an interpreter, 
Peirce claimed "Thought is not necessarily connected with a brain", but every thought is a sign, and 
every sign depends on some mind or "quasi-mind" (CP 4.551, 1906). The following definition 
emphasizes the independence of signs on any particular implementation: 
I define a sign as something, A, which brings something, B, its interpretant, into the same 
sort of correspondence with something, C, its object, as that in which it itself stands to C. In 
this definition I make no more reference to anything like the human mind than I do when I 
define a line as the place within which a particle lies during a lapse of time. (1902, p. 235) 
Modern research has built on that definition to study zoösemiotics in animals and even phytosemiotics 
in plants (Deely 2003). 
On the surface, Peirce's triadic definitions seem similar to Aristotle's, Frege's, or Husserl's.  The 
difference, however, is that any of the three terms in the triad — the sign, the interpretant, or the object 
— could be further subdivided by his metalevel principle.  By analyzing the method by which the 
interpretant relates the sign to the object, Peirce obtained the triad of icon, index and symbol:  an icon 
represents an object by its inherent form, which resembles the intended object; an index represents its 
object by some physical relationship, such as a pointing finger or a dial on a meter; and a symbol 
represents its object by some convention, which may correspond to a concept or just a habit.  The 
referent for an icon is a perceptible pattern; the referent for an index is a physical link; and the referent 
for a symbol is a convention represented by a concept, a principle, or a proposition.  The indexicals that 
caused so much trouble for Frege and Husserl were a solved problem for Peirce; in fact, he had coined 
the word indexical for the special class of indexes that are expressed by the deictic words of natural 
languages. 
By using the trichotomy repeatedly, Peirce derived a systematic classification of all aspects of signs 
and the ways they may be used.  A weather vane on a steeple is, first of all, a sign of its own existence 

as a physical object.  It serves as an index when it is used to indicate the direction of the wind.  An icon 
of a rooster on the weather vane is a sign of the bird it resembles, but it might also be interpreted as an 
icon of a bird facing into the wind because its streamlined shape reduces the resistance from that 
direction.  A rooster is also used as a symbol of early morning because its habitual crowing makes it an 
index of dawn.  A closer inspection of the weather vane could reveal many other signs: a trademark as 
a symbol of the manufacturer; the composition of the metal as an index of the manufacturing process 
and even the source of the ore; and the design as an index of the style or the period when it was made. 
Finally, a weather vane on a steeple, even when made in the 20th century, could be used as a symbol of 
a rural 19th-century village with its close ties to the natural forces of wind and weather. 
The interpretant of a sign may be a percept, a concept, or an arbitrarily complex proposition. If the sign 
is a sentence, the interpretant is a proposition, whose truth is determined by testing its implications 
against perceptions.  In the following paragraph, Peirce related a Tarski-style evaluation function to the 
methods of perception and to the feelings associated with perception: 
A sign is only a sign in actu by virtue of its receiving an interpretation, that is, by virtue of 
its determining another sign of the same object. This is as true of mental judgments as it is 
of external signs. To say that a proposition is true is to say that every interpretation of it is 
true. Two propositions are equivalent when either might have been an interpretant of the 
other. This equivalence, like others, is by an act of abstraction (in the sense in which 
forming an abstract noun is abstraction) conceived as identity. And we speak of believing 
in a proposition, having in mind an entire collection of equivalent propositions with their 
partial interpretants. Thus, two persons are said to have the same proposition in mind. The 
interpretant of a proposition is itself a proposition. Any necessary inference from a 
proposition is an interpretant of it. When we speak of truth and falsity, we refer to the 
possibility of the proposition being refuted; and this refutation (roughly speaking) takes 
place in but one way. Namely, an interpretant of the proposition would, if believed, produce 
the expectation of a certain description of percept on a certain occasion. The occasion 
arrives: the percept forced upon us is different. This constitutes the falsity of every 
proposition of which the disappointing prediction was the interpretant. Thus, a false 
proposition is a proposition of which some interpretant represents that, on an occasion 
which it indicates, a percept will have a certain character, while the immediate perceptual 
judgment on that occasion is that the percept has not that character. A true proposition is a 
proposition belief in which would never lead to such disappointment so long as the 
proposition is not understood otherwise than it was intended. (CP 5.569) 
Note the words expectation, disappointment, and intended.  In various writings, Peirce observed that 
perception is always accompanied by some feelings — positive, negative, or neutral — toward the 
object of perception. Deely (2003) noted that Peirce's usage anticipates the psychoanalytic notion of 
cathexis, which binds a cognition with an emotion or affect in a unitary experience. Value judgments 
can be derived from perception, but from the emotional side of the cathexis, not its cognitive side. 
Besides using the trichotomies to classify signs, Peirce also used them to classify mathematics, 
philosophy, science, and the study of signs themselves.  He subdivided the field of semiotics into three 
subfields according to which aspect of signs each addresses: 
1. Universal grammar is first because it studies the structure of signs independent of their use. 
The syntax of a sentence, for example, can be analyzed without considering its meaning, 
reference, truth, or purpose within a larger context. In its full generality, universal grammar 
defines the types of signs and patterns of signs at every level of complexity in every sensory 
modality. 

2. Critical logic, which Peirce defined as "the formal science of the conditions of the truth of 
representations" (CP 2.229), is second because truth depends on a dyadic correspondence 
between a representation and its object. In further analyzing the branches of logic, he observed 
that induction exemplifies Secondness because it depends on a dyadic relation between 
propositions and reality. Deduction exemplifies Thirdness because it is determined by mediating 
laws that relate premises to conclusions. In looking for the missing Firstness, he discovered the 
principle of abduction, which generates new hypotheses, which are further tested by the 
methods of deduction and induction. 
3. Methodeutic or philosophical rhetoric is third because it studies the principles that relate signs 
to each other and to the world: "Its task is to ascertain the laws by which in every scientific 
intelligence one sign gives birth to another, and especially one thought brings forth another" 
(CP 2.229). By "scientific intelligence," Peirce meant any intellect capable of learning from 
experience, among which he included dogs and parrots. Pietarinen (2004) pointed out that 
Peirce had anticipated much of the modern work on speech acts, relevance, and conversational 
implicatures; although he hadn't listed the principles as conveniently as Grice (1975), he 
discussed and analyzed versions of them in many of his writings. 
Charles Morris (1938) popularized this triad under the headings syntax, semantics, and pragmatics. 
Unfortunately, Morris's choice of words has led to confusion, especially over the word semantics, 
which logicians use in much same sense as Peirce's term critical logic, but linguists use in a much 
broader sense. As an alternative, linguists have coined the term lexical semantics, which includes 
aspects of all three branches of semiotics. 
When applied to the modes of existence, Peirce's trichotomy generates three fundamental ontological 
categories, which correspond to Kant's triad under Modality:  Possibility, Actuality, and Necessity. As 
Peirce said (1908, EP 2.478), "I recognize three Universes, which are distinguished by three Modalities 
of Being": 
1. Ideas or possibilia.  This universe, which has a strong resemblance to Plato's forms or 
Whitehead's eternal objects, "embraces whatever has its being in itself alone, except that 
whatever is in this Universe must be present to one consciousness or be capable of being so 
present in its entire Being.  It follows that a member of this universe need not be subject to any 
law, not even to the principle of contradiction."  The ideas include all the possible worlds that 
David Lewis imagined and much more, since they need not be organized in worlds and they 
need not be actualizable.  They even include Meinong's round squares and other ideas that are 
so vague, so contradictory, or so immense that they could never be actualized. All mathematical 
objects belong to this universe, including Cantor's infinities and even multitudes beyond 
anything that Cantor imagined. Any idea that any intelligent being of any species could ever 
conceive is in this universe. 
2. Existents.  This universe includes "Objects whose Being consists in their Brute reactions, and 
of, second, the Facts (reactions, events, qualities, etc.) concerning those Objects.... Every 
member of this Universe is either a Single Object subject, alike to the Principles of 
Contradiction and to that of Excluded Middle, or it is expressible by a proposition having such 
a singular subject." 
3. Necessitants.  "The third Universe consists of the co-being of whatever is in its Nature 
necessitant, that is, is a Habit, a law, or something expressible in a universal proposition. 
Especially, continua are of this nature.... It includes whatever we can know by logically valid 
reasoning."  All mathematical theorems are in this universe, but the objects they characterize 
are either in the second universe (Existents) if they are actualized or in the first (Ideas) if they 

are not.  In Lewis's terms, the necessitants are the general principles and the principles of 
recombination for creating, organizing, interpreting, and understanding worlds, either actual or 
merely possible. 
These three categories clarify the issues discussed by the physicist Eugene Wigner (1960) in his classic 
paper "On the unreasonable effectiveness of mathematics in the natural sciences."  Wigner marveled at 
the success of mathematics in describing the universe and predicting the outcome of experiments 
before they had been carried out.  The reason for that success can be expressed in Peirce's terms:  
everything existent can be described by mathematics because the infinities in the universe of ideas 
include more than enough options to describe the finite universe; mathematical theories in the universe 
of necessitants can predict the future because the infinity of all possible theories includes the 
formulations of every conceivable law of nature. Science is the project that searches the infinity of 
theories to find those that best characterize what exists and how it operates.  What is truly marvelous 
is not the ability of mathematics to express accurate theories about the universe, but the ability of the 
human mind to discover some of those theories or at least useful approximations to them. 
Peirce's representations of modality evolved with his development of logic.  In the same paper where 
he presented his algebraic notation for predicate calculus, Peirce (1885) explained the semantics of 
counterfactual or hypothetical propositions by quantification over "states of things": 
Now, the peculiarity of the hypothetical proposition is that it goes out beyond the actual 
state of things and declares what would happen were things other than they are or may be. 
The utility of this is that it puts us in possession of a rule, say that "if A is true, B is true," 
such that should we hereafter learn something of which we are now ignorant, namely that A 
is true, then, by virtue of this rule, we shall find that we know something else, namely, that 
B is true. There can be no doubt that the Possible, in its primary meaning, is that which may 
be true for aught we know, that whose falsity we do not know. The purpose is subserved, 
then, if, throughout the whole range of possibility, in every state of things in which A is 
true, B is true too. 
In various writings, Peirce represented necessity by a universal quantifier Πω and possibility by an 
existential quantifier Σω that range over states of things indexed by the variable ω.  In some writings, 
he read "under all circumstances" for the quantifier Πω. 
In analyzing the development of Peirce's views of modality, Lane (forthcoming) observed that a major 
shift occurred in 1896, when Peirce moved from a position of weak modal realism to strong modal 
realism. Before then, he appeared to be satisfied with the interpretation of modality as quantification 
over "states of things" or "states of information." Afterwards, he considered that view "too 
nominalistic" and "superficial" because it did not recognize laws as active general principles that are 
"really operative in nature" (CP 5.100, EP 2.183). In his later writings, Peirce occasionally used 
quantification over states of information because it is "extremely helpful up to a certain point", but he 
insisted that the ultimate source of modality is the laws or general principles that determine what states 
are necessary or possible. 
Lane's study suggests that Peirce would have preferred Dunn's semantics of laws and facts to Kripke's 
semantics of possible worlds, which is basically a refinement of Peirce's method of quantifying over 
states. Dunn's models focus on the laws that are the ultimate source of modality. They are, in fact, the 
necessitants of the third universe. In his later work on logic, Peirce developed existential graphs, which 
are just as expressive as his algebraic notation for predicate calculus (Peirce 1885, 1909). In adding 
modality to the graphs, Peirce experimented with quantification over states of information, but he 
preferred to use colored areas, which indicate "the nature of the universe or the universes of discourse 

(for several may be referred to in a single assertion)" (quoted by Roberts 1973, p. 102). With the option 
of allowing multiple universes in the same assertion, Peirce began to explore multimodal reasoning. 
Those explorations were fragmentary and incomplete, but modern systems of multimodal reasoning 
have not made significantly more progress. 
As this survey indicates, semiotics is a rich subject that has a great deal of power for analyzing 
interrelationships among signs, the objects they refer to, and the people who use them for interpreting 
the world, acting upon it, and communicating with other people. If possible worlds have any reality, as 
David Lewis claimed, it could best be interpreted as the reality of sign types, such as Peirce's universe 
of ideas or possibilia. The facts of the actual world would be in the universe of existents, and the facts 
of other worlds would be in the universe of ideas. All the variations of laws that govern all the worlds, 
actual or not, would be in the universe of necessitants. These three kinds of elements furnish the raw 
material for defining meanings and relating them to the actual world. To illustrate how other theories of 
meaning could be interpreted as special cases of Peirce's semiotics, consider the definition of 
proposition in situation semantics:  a pair {σ;T} consisting of a situation σ and a sign type T, which 
Barwise and Perry called a situation type. If T accurately describes σ and σ happens to be in the 
universe of existents, the proposition is true in the ordinary sense. But the proposition could also be 
true in some hypothetical, future, or intentional sense if σ is a situation of the proper sort in the 
universe of ideas. The laws, rules, principles, and habits in the universe of necessitants determine how 
the objects, processes, and intelligent beings in the other two universes behave and interact. This 
approach will be developed further in Section 5. 
4. Propositions as Equivalence Classes
Any definition of modality in terms of propositions requires a precise definition of proposition. 
Informally, a proposition should represent the language-independent meaning of a sentence, but 
"language independence" and "meaning" are two notoriously difficult notions to formalize. Stalnaker 
(1976) proposed that the proposition expressed by a sentence be defined as the set of possible worlds in 
which the sentence is true. That definition, however, is too coarse grained:  it causes all mathematical 
theorems to collapse into a single proposition. Yet Fermat's last theorem, for example, doesn't mean the 
same as 2+2=4, and it's much harder to prove. For a theory of modality based on Dunn's semantics, 
Stalnaker's definition cannot discriminate laws or facts that happen to be true in exactly the same 
worlds. With the axiom system S5, all worlds have the same laws, and they all degenerate to a single 
proposition. A suitable definition of proposition must make finer distinctions. 
As Peirce said, a proposition corresponds to "an entire collection of equivalent propositions with their 
partial interpretants" (CP 5.569). To formalize this notion, a proposition may be defined as an 
equivalence class of sentences in some formal language L under some meaning-preserving translation 
(MPT) defined over the sentences of L. Meaning-preserving translations can be defined for any kind of 
language, but for simplicity, this article will restrict the definition to first-order logic. Formally, a 
meaning-preserving translation from the sentences of a first-order language L1 to the sentences of a 
first-order language L2 is any function f that satisfies the following four constraints: 
1. Invertible.  The MPT f must have an inverse g, which maps sentences from L2 back to L1.  For 
any sentence s in L1, f(s) is a sentence in L2, and g(f(s)) is a sentence in L1 but not necessarily s 
itself. To ensure that f is defined for all sentences in L1, the language L2 must be at least as 
expressive as L1. If L2 is more expressive than L1, then the inverse g might be undefined for 

some sentences in L2. In that case, the language L2 would express a superset of the propositions 
of L1. 
2. Truth preserving.  Although the sentences s and g(f(s)) might not be identical, both must have 
exactly the same truth conditions:  for any model M of the language L1, M entails s (i.e., M s) 
if and only if M
g(f(s)). Preserving truth is necessary for meaning preservation, but it is a weak 
condition that groups too many sentences in the same equivalence classes. Ideally, the test to 
determine whether two sentences "mean the same" should be "obvious." Formally, it should be 
computable by an efficient algorithm — one whose computation time is linearly or at worst 
polynomially proportional to the length of the sentence. 
3. Vocabulary preserving.  When s is translated from L1 to L2 and back to g(f(s)), the syntax 
might be rearranged, but the words or symbols that represent proper names and ontology must 
appear in both sentences s and g(f(s)). For example, the sentence Every cat is a cat should not 
be considered to have the "same meaning" as Every dog is a dog or Every unicorn is a unicorn, 
since the first is about cats, the second is about dogs, and the the third is about nonexistent 
things. This criterion could be relaxed to allow terms to be replaced by synonyms or definitions, 
but arbitrary content words or predicates must not be added or deleted by the translations. An 
admissible MPT might replace the word cat with domestic feline, but it should not replace the 
word cat with dog or unicorn. 
4. Structure preserving.  When s and g(f(s)) are mapped to a canonical form that uses only 
negation ~, conjunction , and the existential quantifier  as the logical operators, they must
∧
∃
 
contain computationally equivalent patterns of negations and quantifiers:  i.e., the decidability, 
provability, and computational complexity of both sentences must be the same. (Formally, the 
functions f1 through f4, which are defined below, specify what patterns are considered 
computationally equivalent.) The short justification for this approach is that conjunction is the 
simplest and least controversial of all the Boolean operators, while negation introduces serious 
philosophical and computational problems, which are inherited by other operators that require 
a negation in their definition.  Intuitionists, for example, deny that ~~p should be considered 
provably equivalent to p. Computationally, ~~p and p have different effects on the binding of 
values to variables in Prolog, SQL, and other systems. For relevance logic, Anderson and 
Belnap (1975) disallowed the disjunctive syllogism, which is based on  and ~, because it can
∨
 
introduce extraneous information into a proof. 
The first two conditions, by themselves, specify an MPT whose propositions would be identical to 
Stalnaker's definition:  each equivalence class would consist of all the sentences that happen to be true 
in a particular set of possible worlds.  (Such an MPT, however, would not be efficiently computable 
because it would require a complete first-order theorem prover to verify whether two sentences express 
the same proposition.)  By imposing stronger constraints, conditions 3 and 4 not only make the MPT 
more efficient, they also reduce the size of the equivalence classes and thereby preserve finer 
distinctions:  they ensure that the content words or predicates remain identical or synonymous, they 
preserve the logical structure, and they prevent irrelevant content from being inserted.  If s is the 
sentence Every farmer who owns a donkey beats it, then the sentence g(f(s)) might be If a farmer x 
owns a donkey y, then x beats y.  Those sentences use different logical and syntactical symbols, but 
they have the same truth conditions, they have the same content words, and they have the same 
structure when expressed with only , ~, and . There are several reasons for choosing those three
∧
∃
 
operators for the canonical form: 
1. They are the three operators that Peirce chose to use for existential graphs, for reasons similar 

to the following. 
2. When sentences in natural languages are translated to FOL, those are the three most frequently 
occurring operators in the result. 
3. They are also the natural choice for database query languages, such as SQL, because they have 
the least computational complexity. 
4. Statements that use only conjunction and the existential quantifier are the simplest to verify by 
direct observation. 
5. Negation adds some complexity, especially when multiple negations occur in the same 
sentence, but it is necessary to express full FOL. 
6. Statements that contain any of the other common logical operators — disjunction, implication, 
and the universal quantifier — cannot be verified by direct observation. 
In general, an MPT is defined as a function that translates sentences from one language to another, but 
for many applications L1 and L2 may be the same language. Any MPT f defined over a single language 
L partitions the sentences of L into a set of equivalence classes called propositions.  A series of six 
MPTs f0, ..., f5 are defined below, which have the following properties: 
•
Each MPT fi is its own inverse:  gi = fi. 
•
Every equivalence class p in the partition of L determined by fi contains exactly one canonical 
sentence c. 
•
For every sentence s in p, fi(s) = c and gi(fi(s)) = c. 
•
Any two sentences s1 and s2 of L are contained in the same equivalence class if and only if fi 
maps both of them to the same canonical sentence c:  fi(s1) = c and fi(s2) = c. 
A sentence s is said to state the proposition p if and only if s is contained in the equivalence class p. 
For any language L, the simplest MPT f0 is the identity function:  for every sentence s in L, f0(s)=s. 
This translation satisfies all four criteria for an MPT, since f0 is its own inverse, it preserves truth, 
vocabulary, and structure, and it is very easy to compute. Its primary drawback is that it fails to group 
sentences into useful classes:  every sentence in L is the canonical sentence of a proposition in which 
there are no other sentences. 
In order to group sentences into larger equivalence classes, the MPT f1 translates every sentence to a 
canonical sentence that uses only the three logical symbols , ~, and .  The inverse of 
∧
∃
f1 is f0 itself. 
The details for defining f1 will vary with the syntax of every version of FOL, but to illustrate the 
method, let L be a conventional infix notation for FOL with the two quantifiers represented by the 
symbols  and 
, with negation represented by ~, and with conjunction, disjunction, material
∃
∀
 
implication, and equivalence represented by the symbols , , , and ≡.  The translation of any
∧∨⊃
 
sentence or subsentence s of L to its canonical form f1(s) is defined recursively: 
1. If s is an atom (i.e., any relation symbol applied to its arguments), the result is s unchanged: 
f1(s)  ⇒  s 
2. If s consists of a prefix operator (negation or quantifier) applied to a subsentence u, the result is 

the transformed operator applied to the translation of u: 
f1(~u)  ⇒  ~f1(u) 
f1((∃x)u)  ⇒  (∃x)f1(u) 
f1((∀x)u)  ⇒  ~(∃x)~f1(u) 
3. If s consists of a dyadic operator ( , , , or ≡) applied to two subsentences 
∧∨⊃
u and v, the result 
is a transformation of the operator applied to the translations of u and v: 
f1(u  
∧v)  ⇒  f1(u) 
 
∧f1(v) 
f1(u  
∨v)  ⇒  ~(~f1(u) 
 ~
∧f1(v)) 
f1(u  
⊃v)  ⇒  ~(f1(u) 
 ~
∧f1(v)) 
f1(u ≡ v)  ⇒  (~(f1(u) 
 ~
∧f1(v)))  (~(
∧
f1(v) 
 ~
∧f1(u))) 
The function f1 satisfies the criteria for an MPT:  it has an inverse f1; it preserves truth because every 
step of the translation expands one operator according to its definition in terms of , ~, and ; it leaves
∧
∃
 
the content words and symbols unchanged; and it preserves the structure of a sentence when mapped to 
, ~, and . For any sentence 
∧
∃
s, f1(s) is a unique canonical sentence of the equivalence class defined by 
f1:  every sentence in L that is mapped to that canonical sentence is said to state the same proposition. 
For all logical operators except ≡, the time to convert any sentence to the canonical sentence is linear. 
If a sentence contains a large number of equivalences, the translation time may be exponentially 
proportional to the number of equivalences.  Such sentences, however, are extremely rare; when people 
write p≡q≡r, they usually intend it as the conjunction p≡q and q≡r, which merely doubles the 
translation time. 
The function f1 would classify sentences of the form p∧q and q∧p in different equivalence classes 
because its definition does not reorder any of the subsentences.  To group such sentences in the same 
equivalence class, the function f2 makes the same symbol replacements as f1, but it also sorts 
conjunctions in lexical order according to some encoding, such as Unicode.  As a result, arbitrary 
permutations of conjunctions map to the same canonical sentence and are grouped in the same 
equivalence class.  Since disjunctions are defined in terms of of conjunctions, the sentences p∨q and 
q∨p would also be in the same class.  Since a list of N terms can be sorted in time proportional to 
(N log N), the function f2 would take just slightly longer than linear time.
 
Ideally, sentences that differ only in the names assigned to their bound variables, such as (∃x)P(x) and 
(∃y)P(y), should also be grouped together.  Therefore, the function f3 should rename the variables to a 
fixed sequence, such as x1, x2 , .... That renaming must be done before conjunctions are sorted; the 
simplest method would be to move all quantifiers to the front by mapping every sentence to prenex 
normal form and then naming the variables in the order in which they appear. The function f3 would 
first convert sentences to prenex normal form and rename variables, then it would perform the 

translation method of f1 and sort conjunctions according to the method of f3. For most sentences, the 
renaming process is very fast, but it can slow down for sentences with large numbers of variables in 
complex patterns; such sentences, however, are usually rather rare. 
The function f3 is not yet an ideal MPT because it would assign sentences of the form u, u∧u, and 
u∨u to different equivalence classes.  To group such sentences together, the function f4 would perform 
exactly the same translations as f3 followed by one additional step:  deleting duplicate conjunctions. 
Since f3 has already sorted conjunctions in lexical order, the deletion can be performed in linear time 
just by checking whether any conjunct is identical to the one before it; if so, it would be deleted from 
the result.  This process would also delete duplicate disjunctions since the method of f1 would have 
already mapped disjunctions to combinations of conjunctions and negations. 
For most purposes, the function f4 is a good MPT for grouping sentences that "say the same thing." 
As an example, consider the following sentence, which Leibniz called the Praeclarum Theorema 
(splendid theorem): 
((p  
⊃r)  (
∧q 
 
⊃s)) 
 ((
⊃
p  
∧q) 
 (
⊃r 
 
∧s)) 
The function f3 would translate it to the following canonical sentence: 
~((~(p  ~
∧r)  ~(
∧
q 
 ~
∧s)) 
 ~(~(
∧
p  
∧q) 
 ~(
∧
r 
 
∧s)) ) 
This sentence is not as readable as the original, but it serves as the canonical representative of an 
equivalence class of f3 that contains 864 different, but highly similar sentences.  The function f4, 
which deletes duplicate conjuncts, can relate infinitely many sentences to the same form.  Such 
transformations factor out accidental differences caused by the choice of symbols or syntax. 
To account for synonyms and definitions, another function f5 could be used to replace a word or 
relation such as cat with its definition as domestic feline.  If recursions are allowed, the replacements 
and expansions would be equivalent in computing power to a Turing machine; they could take 
exponential amounts of time or even be undecidable.  Therefore, f5 should only expand definitions 
without recursions, direct or indirect.  Definitions of this form are common in database systems, in 
which a fixed set of relations are privileged, and virtual relations may be defined in terms of the 
privileged relations.  With such restrictions, any sentence that uses virtual relations could always be 
expanded to a form that only uses relations in the privileged set. Therefore, the function f5 would first 
expand all virtual relations to the privileged set before performing the translations defined for f4. 
Since no recursion is involved, the expansions would take at most polynomial time. 
In summary, an open-ended number of meaning-preserving translations could be defined, any of which 
would be suitable for supporting Dunn's semantics. For ease of computing, the availability of efficient 
algorithms for computing a unique canonical form for any proposition should be considered in the 
choice of MPT. If no definitional mechanisms are available in the given language L, MPT f4 would be 
a good choice. If the language supports nonrecursive definitions, f5 could be used. For languages that 
go beyond FOL, an extended version of f4 or f5 could be defined to accommodate the additional 
operators and structure. Much more important than choosing any particular version of MPT is the 
decision to define propositions as equivalence classes of sentences. 

5. Making Possible Worlds Meaningful
One of the oldest controversies about Aristotle's categories was whether they represent the kinds of 
things that exist or the way people perceive, think, and talk about things that exist. Theophrastus, 
Aristotle's successor as head of the Lyceum, said that the categories were intended in all those ways — 
in modern terms, ontological, epistemological, and lexical. Yet the fragmented methodologies of those 
subjects are scattered across the fields of philosophy, linguistics, and artificial intelligence, in each of 
which the researchers who work on formal semantics or lexical semantics are disjoint sets.  For 
linguistics, Partee (2005) hoped that "these different approaches can be seen as complementary and 
not necessarily antagonistic." 
One reason for hoping that Peirce's semiotics can help relate the fragmented subfields is that the scope 
of his research was as broad as Aristotle's.  In addition to his research on mathematics, physics, and 
logic, he had been an associate editor of the Century Dictionary, for which he wrote, revised, or edited 
over 16,000 definitions.  The combined influence of logic and lexicography is apparent in a letter he 
wrote to to B. E. Smith, the editor of that dictionary: 
The task of classifying all the words of language, or what's the same thing, all the ideas that 
seek expression, is the most stupendous of logical tasks.  Anybody but the most accomp-
lished logician must break down in it utterly; and even for the strongest man, it is the 
severest possible tax on the logical equipment and faculty. 
In this remark, Peirce equated the lexicon with the set of expressible ideas and declared logic the 
primary means of analysis.  Unlike Frege, Husserl, and Russell, he did not avoid the challenge of 
characterizing the language people actually use by escaping to some purified realm of formal semantics 
or ontology. 
Semiotics is a unified subject that addresses all possible uses of signs by all possible species. Various 
students of the subject may prefer to analyze different aspects or to adopt a philosophical, mathema-
tical, or applied approach, but academic compartmentalization should not create artificial barriers. 
Following are some of the issues: 
1. Dunn's semantics of laws and facts is an important part of the synthesis because it enables any 
formal method based on possible worlds to be mapped to an equivalent method based on laws 
and facts, which are easier to incorporate into a semiotic theory or a computational system. In 
fact, most AI systems that implement some version of possible worlds already represent them as 
sets of statements that describe those worlds. 
2. Peirce's views on the reality of possibilia are compatible with David Lewis's views on the reality 
of possible worlds.  Furthermore, Peirce's emphasis on the reality of generals, which are of the 
same nature as laws, accommodates an integrated combination of Dunn's laws with Lewis's 
worlds. 
3. Vogt (2002) claimed that the so-called symbol grounding problem, as formulated by Harnad 
(1990) had already been solved by Peirce. By relating the sign to both its object and its 
interpretant, signs are grounded, directly or indirectly, in physical reality.  Signs, however, 
presuppose some version of epistemology or philosophy of science for characterizing exactly 
how the triadic relation is determined in any particular instance. 
4. Peirce did have a detailed theory of epistemology for everyday knowledge and for the most 
advanced science.  For the first, he called the methods coenoscopy (from the Greek koinos 
[common]) and for the second idioscopy, which depends on sophisticated instruments whose 

measurements are related via mathematical models to scientific theories.  The idioscopic 
methods of science cut through Kant's veil of phenomena to reach the noumena or things in 
themselves.  But Peirce's admission of finite fallibility is the claim that there is no way of 
knowing which, if any, of the idioscopic measurements characterize physical reality or just one 
more veil between coenoscopic perception and whatever happens to be independent of the 
senses, the minds, and the technology of any species. 
5. The emotions that accompany any perception or action are the ultimate source of the aesthetic, 
ethical, and semiotic value judgments associated with the cognitive content. This binding of 
cognition with emotion — in modern terms, cathexis — is the foundation for any aspect of 
meaning that involves purpose, belief, desire, hope, fear, love, hate, or any other form of 
intentionality. To the extent that different individuals have similar emotional bindings to similar 
cognitions, the derived value judgments can have some degree of objectivity. 
6. A dozen years after his pioneering work on predicate calculus (Peirce 1885), he switched to 
existential graphs as his preferred version of logic. Although the graphs are equivalent to 
predicate calculus in expressive power, they exhibit the relational structure and the nests of 
contexts more clearly. In fact, the nested contexts are isomorphic to both conceptual graphs, 
which Sowa (1984) explicitly based on Peirce's graphs, and discourse representation structures, 
which Kamp (1981) invented independently. 
7. During the last decade of his life, Peirce developed his tinctured existential graphs, which use 
twelve colors for the different modes of being or "universes of discourse":  four colors for 
modes of actuality; four for modes of possibility; and four for modes of intentionality (Roberts 
1973). Unlike the modern theories that treat only one modality, Peirce allowed multiple colors 
to be mixed in the same graph in order to relate "multiple universes of discourse." 
These seven points are a rough grouping of ideas that Peirce developed in many thousands of pages 
that have been published and many more thousands of pages of still unpublished manuscripts. 
One controversy that semiotics clarifies are the many options for describing things and events from 
various perspectives.  As examples, Gangemi et al. (2003) maintain that the terms vase and lump of 
clay have different identity criteria; therefore, they imply two distinct objects that happen to occupy 
the same location.  Others maintain that the distribution of matter takes precedence over any method 
of describing it:  if two descriptions characterize the same matter, they must describe the same object. 
Peirce, however, would say that anything can be described in any number of ways at either a 
coenoscopic or an idioscopic level. Any choice of words or other signs depends on the intentions of 
some viewer who might choose one perspective rather than another.  That choice is not purely 
subjective, since there are objective, but species-specific criteria for preferring one to another (Deely 
2003). A bee, for example, might ignore the vase and focus on the flowers in the vase, while a dog 
might push the flowers aside and drink the water that some human had put there for a very different 
purpose.  Each perspective depends on the intentions of some individual of some species, and any 
question about the priority of one perspective over another cannot be answered without considering 
the intentions of the questioner. 
Peirce's phenomenological trichotomy supports a methodology for classifying and relating different 
perspectives.  For example, an object may be recognized as a dog, cat, man, or woman by directly 
observable properties of the individual (Firstness), but the individual cannot be recognized as a pet, 
stray, owner, or employee without some evidence of an external relationship (Secondness).  The 
corresponding Thirdness is expressed by nouns such as ownership or employment, which involve 
intentions marked by signs, such as a contract, a handshake, or a habitual pattern of behavior by pet 
and owner.  Verbs can be classified by the same principles as nouns:  whether they signify some 

directly observable event (Firstness); some indirectly related effects (Secondness); or the mediating 
intentions (Thirdness).  The next three sentences describe the same act in each of those ways: 
1. Brutus stabbed Caesar. 
2. Brutus killed Caesar. 
3. Brutus murdered Caesar. 
An act of stabbing can be recognized by objective criteria at the instant it happens. That is a classifica-
tion by Firstness, since no other entities, events, or mental attitudes are involved. But an act of stabbing 
cannot be identified as killing unless a second event of dying occurs.  Caesar had time to ask "Et tu, 
Brute?" before the stabbing could be interpreted as a killing (Secondness).  Murder is Thirdness that 
depends on the intention of the agent.  Determining whether an act of stabbing that resulted in killing 
should be considered a murder may depend on subtle clues, whose interpretation may require a judge, 
a jury, and a lengthy trial.  For lexical semantics, these examples show the potential of semiotics for 
classifying and defining the words that describe any perspective on any world. 
For formal semantics, graph logic facilitates a shift from a functional representation, as in Montague 
grammar, to a relational representation, as in conceptual graphs or Kamp's DRS. Formally, every 
function is a relation, but not every relation is a function:  a functional notation, such as y = f(x1,...,xn), 
can always be converted to an equivalent relational form, such as f(x1,...,xn,y), with the added constraint 
that there is a unique y for every combination of x1,...,xn; but if that constraint is not true, the inverse 
conversion is not possible.  In both linguistics and programming languages, a functional representation 
has the advantage of supporting compositional semantics:  the derivation of the semantic interpretation 
of a sentence as a function of the interpretation of its phrases and subphrases.  Unfortunately, context-
dependent ambiguities can destroy functionality. 
To preserve functionality, Montague (1970) removed the ambiguities from his "fragment" of English 
by inventing an elegant technique of underspecification:  he left quantified noun phrases in an 
unresolved state between an intensional or an extensional interpretation until the verb type could 
determine one or the other.  The many versions of unification grammars leave the representation 
underspecified by providing slots for variables whose values may be determined later.  Those 
techniques, however, are defeated by the enormous flexibility of natural languages, which have 
ambiguities that may require structural changes that are far more complex than inserting values in slots. 
Even worse than ambiguity is ellipsis, which might omit any part of a sentence that a native speaker 
could infer or at least guess.  Finally, the compositional methods, which are designed for languages 
with a fixed word order such as English, break down when applied to languages with free word order, 
such as classical Latin or modern Russian. 
All these difficulties cause the process of interpreting language to resemble puzzle solving:  the words 
and background information are like pieces in a jigsaw puzzle that are scrambled in an unpredictable 
order instead of a strictly regimented sequence. Puzzle solving requires constraint satisfaction methods, 
which are more readily accommodated by relational representations and are just as logical as functional 
composition. Conceptual graphs have been used as the semantic representation with both compositional 
and puzzle-solving methods. With any method, the semantic patterns associated with each word sense 
are represented by canonical conceptual graphs (Sowa 1976, 1992), which serve as the pieces of the 
semantic puzzle.  The structure of the lexicon, as described by Sowa (1984), is illustrated in Figure 2.
 

 
Figure 2:  words → senses → canonical graphs → theories 
The dotted arrows in Figure 2 link the word types of any language to an open-ended number of word 
senses or concept types, each of which is linked to one or more canonical graphs, which are puzzle 
pieces of possibly different shapes.  At the right is a lattice of all possible theories, each of which 
represents a collection of axioms, theorems, and facts about some domain or subdomain in one of 
Peirce's universes:  ideas, existents, or necessitants.  Each canonical graph maps to one or more theories 
in which its logical pattern is used. Following are the four kinds of entities shown in Figure 2: 
1. Words.  Every natural language consists of meaningful units called words or morphemes.  In 
some languages, every morpheme is written as a separate word, but other languages group 
multiple morphemes into a single word. 
2. Senses.  Each word or morpheme has one or more possible meanings, called senses.  Most 
dictionaries list a few senses for each word, but Cruse (2000) maintained that any word can 
have an open-ended number of highly domain-dependent microsenses.  In conceptual graphs, 
the senses or microsenses are represented by concept or relation types, which are organized in 
a hierarchy (partial ordering) by the operations of generalization and specialization.  Some types 
may be expressed by word senses in several different natural languages, but others might not be 
expressible by a single word in any language.  Way (1991) studied the use of metaphor as a 
means of extending the type hierarchy dynamically, and Sowa (2000) treated metaphor as one 
of the operators for navigating the lattice of theories, either in human thought or in computer 
implementations. 
3. Canonical graphs.  Each word sense or concept type has one or more canonical graphs, which 
represent the typical patterns in which that concept type occurs, either in natural language 
expressions or in the axioms of some theory.  Canonical graphs represent the selectional 
constraints on the word senses or concept types that may be interconnected in any pattern, but 
they can never rule out new options that may result from changes in the world or innovations 
in language use. 
4. Theories.  The logical pattern expressed by any canonical graph may occur in zero or more 
theories, which characterize mathematical structures, domains of knowledge, or just 
fragmentary thoughts or ideas that might someday evolve into more complex hypotheses. 
The lattice shows how each theory is related to every other:  as a generalization, specialization, 

sibling, or distant cousin.  New theories can be added at the top, bottom, or middle levels of the 
lattice at any time without affecting any reasoning that involves other theories in the lattice. 
The complete lattice of all possible theories is infinite, but only a finite subset could ever be 
conceived in any brain or be represented in any computer system. 
The puzzle pieces may be expressed in a logical notation, but the labels on the nodes of a graph or 
the predicates of a formula cannot be limited to a fixed lexicon or a predefined formal ontology.  The 
labels are signs, whose relations to any world, real or imaginary, must be determined by the semiotic 
processes. Different theories may use the same labels, but theories on different branches of the lattice 
may define the labels in incompatible ways. The partial ordering of theories (T1≤T2) could be specified 
by model-theoretic entailment (T1 T2) or by provability (T1
T2).  For simplicity, the theories could 
be restricted to some version of first-order logic, or they might be extended to higher-order logic.  In 
any case, no modal operators should appear in any theory, since modality can be handled by Dunn's 
techniques. 
When Tarski (1933, 1936) wrote his famous papers on the concept of truth in formalized languages, 
his models were mathematical structures that entailed propositions stated in first-order logic.  But 
when formal semanticists write w
p or σ
p and claim that w is a world, σ is a situation, and p is a 
proposition expressed in a natural language, they are ignoring Kant's struggle with the problems of 
phenomena and noumena and the two centuries of phenomenology, epistemology, and philosophy of 
science written in response to Kant.  Figure 3 illustrates the issues:  on the right are some axioms for a 
theory stated in FOL, and in the middle is a Tarski-style model consisting of a set of nodes that 
represent entities and a set of arcs that represent relationships among those entities.  On the left is the 
world with all its complexities of noumena.  The evaluation function Φ determines the truth value of a 
proposition in the theory on the right in terms of the model in the middle.  That model is, at best, an 
approximation to some aspect of the world selected for some purpose by some human.
 
 
Figure 3:  The world, a model, and a theory 
The triad of theory, model, and world in Figure 3 is an example of a Peircean sign relation:  the theory 
is a complex sign, the model in the middle is one of many possible interpretants of the theory, and the 
referent is some aspect of the world that resembles the model to some degree of approximation. That 
diagram suggests a way of relating Peirce's semiotics to the ontology of processes by Whitehead (1929) 

and the language games of Wittgenstein (1953): 
1. The ultimate nature of the physical world is process-like, and what people call objects are 
projections of semiotic structures (e.g., mental models or mathematical constructions such as 
the digram at the center of Figure 3) onto what Whitehead called repeated occurrences of 
similar event types. 
2. What Wittgenstein called a language game is a set of conventions, such as Dunn's laws or 
Peirce's necessitants, that serve as the axioms of some theory that characterizes a family of 
possible models of the world. They may be mental models, models represented on paper, or 
models consisting of data structures and programs in a computer system. The semiotic system, 
neural or artificial, maps those models to the vocabulary and syntax of either a natural language 
or some other symbolic system; e.g., a computer language; a game such as chess, bridge, or 
baseball; or any other conventional behavior pattern in society, e.g., traffic conventions 
supported by a system of roads, signal lights, intersections, lane markers, and barriers. 
3. A proposition expressed in some semiotic system such as a natural language is true if its 
denotation in terms of the model is true and if the model corresponds to some aspect of the 
world that is adequate for its intended purpose. The degree of precision or vagueness of a true 
proposition depends on the adequacy of the correspondence between the model and the world. 
Tarski's logical models can never be vague, but they are always approximations. As the 
engineer and statistician George Box (2005) said, "All models are wrong; some models are 
useful." 
As this article has shown, semiotics goes beyond relationships among signs to relationships of signs to 
the world and to the agents who observe the world, interpret it in signs, and use the signs to plan further 
actions upon the world.  For artificial intelligence, this connection of signs to perception, action, and 
language supports the symbol grounding necessary for making the language processing by AI systems 
and the actions by AI robots meaningful in the same sense that the words and actions of humans are 
meaningful.  The combination of semiotics with Dunn's semantics of laws and facts provides a 
theoretical foundation for modality and intentionality that captures more of the intended interpretation 
than a undefinable relation R over an undefined set W.  An important promise of this combination is 
the ability to support multimodal reasoning as a kind of metalevel reasoning about the source of the 
laws and facts.  Instead of complex axioms for each mode with even more complex interactions 
between modes, it enables the laws to be partitioned in a hierarchy that represents grades of necessity 
or levels of entrenchment:  logical, physical, economic, legal, social, cultural, or personal (Sowa 2003). 
Exploring the full implications of Peirce's semiotics is far beyond the scope of this article, but the 
outline presented here suggests a wealth of resources waiting to be developed. 
References
Anderson, Alan Ross, & Nuel D. Belnap, Jr. (1975) Entailment: The Logic of Relevance and Necessity, Princeton 
University Press, Princeton. 
Barwise, Jon, & John Perry (1983) Situations and Attitudes, MIT Press, Cambridge, MA. 
Box, George E. P., J. Stuart Hunter, & William G. Hunter (2005) Statistics for Experimenters: Design, Innovation, and 
Discovery, 2nd Edition, Wiley-Interscience, New York.
 Cruse, D. Alan (2000) "Aspects of the micro-structure of word meanings," in Y. Ravin & C. Leacock, eds., Polysemy: 
Theoretical and Computational Approaches, Oxford University Press, Oxford, pp. 30-51. 
Davidson, Donald (1967) "The logical form of action sentences," reprinted in D. Davidson (1980) Essays on Actions and 
Events, Clarendon Press, Oxford, pp. 105-148. 

Deely, John (2003) The Impact on Philosophy of Semiotics, St. Augustine's Press, South Bend, IN. 
Devlin, Keith (1991) "Situations as mathematical abstractions," in J. Barwise, J. M. Mark Gawron, G. Plotkin, & S. Tutiya, 
eds., Situation Theory and its Applications, CSLI, Stanford, CA, pp. 25-39. 
Dunn, J. Michael (1973) "A truth value semantics for modal logic," in H. Leblanc, ed., Truth, Syntax and Modality, North-
Holland, Amsterdam, pp. 87-100. 
Gangemi, A., N. Guarino, C. Masolo, & A. Oltramari (2003) "Sweeting WordNet with DOLCE," AI Magazine 24:3. 
Grice, H. Paul (1975) "Logic and conversation," in P. Cole & J. Morgan, eds., Syntax and Semantics 3: Speech Acts, 
Academic Press, New York, pp. 41-58. 
Harnad, Stevan (1990) "The symbol grounding problem," Physica D 42, 335-346. 
Hintikka, Jaakko (1961) "Modality and quantification," Theoria 27, 110-128. 
Hintikka, Jaakko (1963) "The modes of modality," Acta Philosophica Fennica, Modal and Many-valued Logics, pp. 65-81. 
Kamp, Hans (1981) "A theory of truth and semantic representation," in Formal Methods in the Study of Language, ed. by J. 
A. G. Groenendijk, T. M. V. Janssen, & M. B. J. Stokhof, Mathematical Centre Tracts, Amsterdam, 277-322. 
Kant, Immanuel (1787) Kritik der reinen Vernunft, translated by N. Kemp Smith as Critique of Pure Reason, St. Martin's 
Press, New York. 
Kripke, Saul A. (1963) "Semantical analysis of modal logic I," Zeitschrift für mathematische Logik und Grundlagen der 
Mathematik 9, 67-96. 
Kripke, Saul A. (1965) "Semantical analysis of modal logic II: Non-normal modal propositional calculi," in J. W. Addison, 
Leon Henkin, & Alfred Tarski (1965) The Theory of Models, North-Holland Publishing Co., Amsterdam, pp. 206-220. 
Lane, Robert (forthcoming) "Peirce's Modal Shift: From Set Theory to Pragmaticism." 
Lewis, David K. (1986) On the Plurality of Worlds, Basil Blackwell, Oxford. 
Mohanty, J. N. (1982) Husserl and Frege, Indiana University Press, Bloomington. 
Montague, Richard (1967) "On the nature of certain philosophical entities," revised version in Montague (1974) pp. 148-
187. 
Montague, Richard (1970) "The proper treatment of quantification in ordinary English," reprinted in Montague (1974), pp. 
247-270. 
Montague, Richard (1974) Formal Philosophy, Yale University Press, New Haven. 
Morris, Charles W. (1938) Foundations of the Theory of Signs, Chicago University Press, Chicago. 
Partee, Barbara H. (2005) "Formal Semantics," Lectures at a workshop in Moscow. 
http://people.umass.edu/partee/RGGU_2005/RGGU05_formal_semantics.htm 
Peirce, Charles Sanders (1885) "On the algebra of logic," American Journal of Mathematics 7, 180-202. 
Peirce, Charles Sanders (1902) Logic, Considered as Semeiotic, MS L75, edited by Joseph Ransdell, 
http://members.door.net/arisbe/menu/LIBRARY/bycsp/L75/ver1/l75v1-01.htm 
Peirce, Charles Sanders (1909) Manuscript 514, with commentary by J. F. Sowa, available at 
http://www.jfsowa.com/peirce/ms514.htm 
Peirce, Charles Sanders (CP) Collected Papers of C. S. Peirce, ed. by C. Hartshorne, P. Weiss, & A. Burks, 8 vols., Harvard 
University Press, Cambridge, MA, 1931-1958. 
Peirce, Charles Sanders (EP) The Essential Peirce, ed. by N. Houser, C. Kloesel, and members of the Peirce Edition Project, 
2 vols., Indiana University Press, Bloomington, 1991-1998. 
Pietarinen, Ahti-Veikko (2004) "Grice in the wake of Peirce," Pragmatics and Cognition 12:2, pp. 295-315. University of 
Helsinki 
Quine, Willard Van Orman (1972) "Responding to Saul Kripke," reprinted in Quine, Theories and Things, Harvard 
University Press, 
Roberts, Don D. (1973) The Existential Graphs of Charles S. Peirce, Mouton, The Hague. 

Saussure, Ferdinand de (1916) Cours de Linguistique Générale, translated by W. Baskin as Course in General Linguistics, 
Philosophical Library, New York, 1959. 
Searle, John R. (1969), Speech Acts. An Essay in the Philosophy of Language, Cambridge University Press, Cambridge. 
Searle, John R. (1983), Intentionality. An Essay in the Philosophy of Mind, Cambridge University Press, Cambridge. 
Searle, John R. (1995), The Construction of Social Reality, Free Press, New York. 
Smith, Barry (1995) "Formal ontology, common sense, and cognitive science," International J. of Human-Computer 
Studies 43, 641-668. 
Smith, Barry (1998) "Basic Concepts of Formal Ontology," in Guarino (1995) pp. 19-28. 
Smith, Barry (1999) "Les objets sociaux," Philosophiques 26:2, 315-347. English version at 
http://wings.buffalo.edu/philosophy/ontology/socobj.htm 
Smith, Barry, & John Searle (2001) "The construction of social reality: an exchange," American Journal of Economics and 
Sociology 60. http://wings.buffalo.edu/philosophy/faculty/smith/articles/dksearle.htm 
Sowa, John F. (1976) "Conceptual graphs for a database interface," IBM Journal of Research and Development 20:4, 336-
357. 
Sowa, John F. (1984) Conceptual Structures: Information Processing in Mind and Machine, Addison-Wesley, Reading, 
MA. 
Sowa, John F. (1992) "Logical structures in the lexicon," in Lexical Semantics and Commonsense Reasoning, edited by 
James Pustejovsky and Sabine Bergler, LNAI 627, Springer-Verlag, Berlin, pp. 39-60. 
http://acl.ldc.upenn.edu/W/W91/W91-0205.pdf 
Sowa, John F. (1999) "Relating templates to logic and language," in Information Extraction: Towards Scalable, Adaptable 
Systems, ed. by M. T. Pazienza, LNAI 1714, Springer-Verlag, pp. 76-94. http://www.jfsowa.com/pubs/template.htm 
Sowa, John F. (2000) Knowledge Representation: Logical, Philosophical, and Computational Foundations, Brooks/Cole 
Publishing Co., Pacific Grove, CA. 
Sowa, John F. (2003) "Laws, facts, and contexts: Foundations for multimodal reasoning," in Knowledge Contributors, 
edited by V. F. Hendricks, K. F. Jørgensen, and S. A. Pedersen, Kluwer Academic Publishers, Dordrecht, pp. 145-184. 
Stalnaker, Robert (1976) "Propositions," in A. MacKay & D. Merrill, eds., Issues in the Philosophy of Language, Yale 
University Press, New Haven, CT, pp. 79-91. 
Tarski, Alfred (1933) "Pojecie prawdy w jezykach nauk dedukcynych," German trans. as "Der Wahrheitsbegriff in den 
formalisierten Sprachen," English trans. as "The concept of truth in formalized languages," in Tarski (1982) pp. 152-278. 
Tarski, Alfred (1936) "Über den Begriff der logischen Folgerung," translated as "On the concept of logical consequence" in 
Tarski (1982) pp. 409-420. 
Tarski, Alfred (1982) Logic, Semantics, Metamathematics, Second edition, Hackett Publishing Co., Indianapolis. 
Vogt, Paul (2002) "The physical symbol grounding problem," Cognitive Systems Research 3:3, 429-457. 
Wang, Hao (1986) Beyond Analytic Philosophy: Doing Justice to What We Know, MIT Press, Cambridge, MA. 
Way, Eileen C. (1991) Knowledge Representation and Metaphor, Kluwer Academic Publishers, Dordrecht. 
Whitehead, Alfred North (1929) Process and Reality: An Essay in Cosmology, corrected edition edited by D. R. Griffin & 
D. W. Sherburne, Free Press, New York, 1978. 
Wigner, Eugene (1960) "The unreasonable effectiveness of mathematics in the natural sciences", Communications in Pure 
and Applied Mathematics 13:1. 
Wittgenstein, Ludwig (1953) Philosophical Investigations, Basil Blackwell, Oxford. 

