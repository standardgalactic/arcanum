




Contents

Cover Page
Title
Copyright
Contents
Preface
Part I: More on Propositional and First-Order Logic

Chapter 1. More on Propositional Logic

I. Propositional Logic and the Boolean Algebra of Sets
II. An Algebraic Approach
III. Another Completeness Proof
IV. Fidelity to Modus Ponens

Chapter 2. More on First-Order Logic

I. Magic Sets
II. Gentzen Sequents and Some Variants
III. Craig’s Lemma and an Application
IV. A Unification
V. A Henkin-Style Completeness Proof

Part II: Recursion Theory and Metamathematics

Chapter 3. Some Special Topics

I. A Decision Machine
II. Variations on a Theme of Gödel
III. R-Systems
IV. A Synthesis

Chapter 4. Elementary Formal Systems and Recursive Enumerability

I. More on Elementary Formal Systems
II. Recursive Enumerability
III. A Universal System

Chapter 5. Some Recursion Theory

I. Enumeration and Iteration Theorems
II. Recursion Theorems

Chapter 6. Doubling Up
Chapter 7. Metamathematical Applications

I. Simple Systems
II. Standard Simple Systems

Part III: Elements of Combinatory Logic

Chapter 8. Beginning Combinatory Logic
Chapter 9. Combinatorics Galore

I. The B-Combinators
II. The Permuting Combinators
III. The Q-Family and the Goldfinch, G
IV. Combinators Derivable from B,T,M and I (λ-I Combinators)

Chapter 10. Sages, Oracles and Doublets
Chapter 11. Complete and Partial Systems

I. The Complete System
II. Partial Systems of Combinatory Logic

Chapter 12. Combinators, Recursion and the Undecidable

I. Preparation for the Finale
II. The Grand Problem

Afterword. Where to Go from Here
References
Index



Pagebreaks of the print version

i
iii
iv
v
vii
ix
1
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
69
70
71
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
89
90
91
92
93
94
95
96
97
98
99
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
205
206
207
208
209
210
211
212
213
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
253
254
255
256
257
258
259
260
261
262
263
265
266
267
269
270
271
272
273
274
275
276
277


















A BEGINNER’S FURTHER GUIDETO MATHEMATICAL LOGIC








A BEGINNER’S FURTHER GUIDETO MATHEMATICAL LOGIC
Raymond Smullyan








 
Published by
World Scientific Publishing Co. Pte. Ltd.
5 Toh Tuck Link, Singapore 596224
USA office: 27 Warren Street, Suite 401-402, Hackensack, NJ 07601
UK office: 57 Shelton Street, Covent Garden, London WC2H 9HE
Library of Congress Cataloging-in-Publication Data
Names: Smullyan, Raymond M., author.
Title: A beginner’s further guide to mathematical logic / Raymond Smullyan.
Description: New Jersey : World Scientific, 2016. | Includes bibliographical references and index.
Identifiers: LCCN 2015033651 | ISBN 9789814730990 (hardcover : alk. paper) | ISBN 9789814725729 (pbk : alk. paper)
Subjects: LCSH: Logic, Symbolic and mathematical.
Classification: LCC QA9.A1 S619 2016 | DDC 511.3--dc23
LC record available at http://lccn.loc.gov/2015033651
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
On the cover, the three photos from left to right are the logicians
Emil Post, Alan Turing, and Ernst Zermelo.
Copyright © 2017 by World Scientific Publishing Co. Pte. Ltd.
All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means, electronic or mechanical, including photocopying, recording or any information storage and retrieval system now known or to be invented, without written permission from the publisher.
For photocopying of material in this volume, please pay a copying fee through the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to photocopy is not required from the publisher.
Typeset by Stallion Press
Email: enquiries@stallionpress.com
Printed in Singapore







Contents
Preface
Part I: More on Propositional and First-Order Logic
Chapter 1. More on Propositional Logic
I.Propositional Logic and the Boolean Algebra of Sets
II.An Algebraic Approach
III.Another Completeness Proof
IV.Fidelity to Modus Ponens
Chapter 2. More on First-Order Logic
I.Magic Sets
II.Gentzen Sequents and Some Variants
III.Craig’s Lemma and an Application
IV.A Unification
V.A Henkin-Style Completeness Proof
Part II: Recursion Theory and Metamathematics
Chapter 3. Some Special Topics
I.A Decision Machine
II.Variations on a Theme of Gödel
III.R-Systems
IV.A Synthesis
Chapter 4. Elementary Formal Systems and Recursive Enumerability
I.More on Elementary Formal Systems
II.Recursive Enumerability
III.A Universal System
Chapter 5. Some Recursion Theory
I.Enumeration and Iteration Theorems
II.Recursion Theorems
Chapter 6. Doubling Up
Chapter 7. Metamathematical Applications
I.Simple Systems
II.Standard Simple Systems
Part III: Elements of Combinatory Logic
Chapter 8. Beginning Combinatory Logic
Chapter 9. Combinatorics Galore
I.The B-Combinators
II.The Permuting Combinators
III.The Q-Family and the Goldfinch, G
IV.Combinators Derivable from B,T,M and I (λ-I Combinators)
Chapter 10. Sages, Oracles and Doublets
Chapter 11. Complete and Partial Systems
I.The Complete System
II.Partial Systems of Combinatory Logic
Chapter 12. Combinators, Recursion and the Undecidable
I.Preparation for the Finale
II.The Grand Problem
Afterword. Where to Go from Here
References
Index







Preface
This book is a sequel to my Beginner’s Guide to Mathematical Logic [Smullyan, 2014]. I originally intended both volumes to be a single volume, but I felt that at my age (now 96), I could pass away at any time, and I wanted to be sure that I would at least get the basic material out.
The previous volume deals with elements of propositional and first-order logic, contains a bit on formal systems and recursion, and concludes with chapters on Gödel’s famous incompleteness theorem, along with related results.
The present volume begins with a bit more on propositional and first order logic, followed by what I would call a “fein” chapter, which simultaneously generalizes some results from recursion theory, first-order arithmetic systems, and what I dub a “decision machine.” Then come four chapters on formal systems, recursion theory and metamathematical applications in a general setting. The concluding five chapters are on the beautiful subject of combinatory logic, which is not only intriguing in its own right, but has important applications to computer science. Argonne National Laboratory is especially involved in these applications, and I am proud to say that its members have found use for some of my results in combinatory logic.
This book does not cover such important subjects as set theory, model theory, proof theory, and modern developments in recursion theory, but the reader, after studying this volume, will be amply prepared for the study of these more advanced topics.
Although this book is written for beginners, there are two chapters – namely 3 and 8 – that I believe would also be of interest to the expert.
For brevity, all references to the first volume, The Beginner’s Guide to Mathematical Logic, of this two-volume introduction to mathematical logic will be given in the remainder of this volume as The Beginner’s Guide [Smullyan, 2014].
Elka Park
November 2016







Part I
More on Propositional and First-Order Logic







Chapter 1
More on Propositional Logic
I.Propositional Logic and the Boolean Algebra of Sets
Many of you have probably noticed the similarity of the logical connectives to the Boolean operations on sets. Indeed, for any two sets A and B and any element x, the element x belongs to the intersection A ∩ B if and only if x is in A and x is in B. Thus x ∈ (A∩B) iff (x ∈ A) Λ (x ∈ B). Thus the logical connective Λ (conjunction) corresponds to the Boolean operation ∩ (intersection). Likewise the logical connective ∨ (disjunction) corresponds to the Boolean operation ∪ (union), since x ∈ (A ∪ B) iff (x ∈ A) ∨ (x ∈ B) Also, x ∈  (x is in the complement of A) if and only if ∼(x ∈ A) (x is not in A), so that the logical connective negation corresponds to Boolean operation of complementation.
Note: As in The Beginner’s Guide [Smullyan, 2014], I will often abbreviate the phrase “if and only if” by “iff”, following the valuable suggestion of Paul Halmos.
We saw in Chapter 1 of The Beginner’s Guide how to verify the correctness of a Boolean equation by the method of what I called “indexing”. However, due to the correspondence between the logical connectives and the Boolean operations on sets, one can also verify Boolean equations by truth tables. The following single example will illustrate the general idea. Consider the Boolean equation  This equation is valid iff for every element x, the element x is in  iff x is in  Thus the equation  is to the effect that for all x,

Thus the proposition x ∈  iff x ∈  is equivalent to

And that formula is a tautology, for it is an instance of ∼ (p Λ q) ≡ ∼ p ∨∼ q, as can be seen by replacing (x ∈ A) by p and replacing (x ∈ B) by q.
Similarly, the Boolean equation A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ C) holds since pΛ (q ν r) ≡ (p Λ q) ν (p Λ r) is a tautology.
In general, given a Boolean equation, define its counterpart to be the result of substituting Λ for ∩ and ν for ∪; ∼t for  (where t is any set variable or Boolean term), ≡ for =, and propositional variables p, q, r, etc., for set variables A, B, C, etc. For example, the counterpart of the Boolean equation  is the propositional formula ∼p ∨ q ≡ ∼(p Λ ∼q).
Well, a Boolean equation is valid if and only if its counterpart is a tautology of propositional logic. And since the formula ∼p ∨ q ≡ ∼(p Λ ∼q) is again easily seen to be a tautology, the Boolean equation  is seen to be valid.
Thus, truth tables can be used to test Boolean equations. Going in the other direction, the method of “indexing,” described in Chapter 1 of The Beginner’s Guide [Smullyan, 2014] can be used to verify a statement of propositional logic, after making the appropriate Boolean equation counterpart to the propositional statement (found by reversing the substitutions mentioned above).
II.An Algebraic Approach
This approach to propositional logic is particularly interesting and leads to the subject known as Boolean rings.
For any two natural numbers x and y, if x and y are both even, so is their product x × y. Thus an even times an even is even. An even times an odd is even. An odd times an even is even, but an odd times an odd is odd. Let us summarize this in the following table.







x
y
x × y


E
E
E


E
O
E


O
E
E


O
O
O


Note the similarity of this table to the truth table for disjunction. Indeed, if we replace E by T, O by F, and × by ∨, we get the truth table for disjunction:







x
y
x ∨ y


T
T
T


T
F
T


F
T
T


F
F
F


This is not surprising, since x times y is even if and only if x is even OR y is even.
However, this might well suggest (as it did to me, many years ago!) that we might transfer the problem of whether a given formula is a tautology, to the question of whether or not a certain algebraic expression in variables for natural numbers is always even (for all possible values of the variables). So let us now do this, i.e. define a transformation of a formula of propositional logic into an arithmetic expression (using only + and × and variables for natural numbers) such that the propositional formula will be a tautology iff every number corresponding to the numerical value of an instance of the arithmetic expression (i.e. an instance in which natural numbers replace the variables) is even.
We let even correspond to truth, odd to falsehood, and multiplication to disjunction; then we take the letters p, q, r, … both as variables for propositions in logical expressions and as variables for natural numbers in the corresponding arithmetic expressions. Thus the logical expression p ∨ q is now transformed to the arithmetic expression p × q, or more simply, pq.
What should we take for negation? Well, just as ∼ transforms truth into falsehood, and falsehood into truth, we want an operation that transforms an even number to an odd one, and an odd one to an even one. The obvious choice is to add 1. Thus we let the logical expression ∼p correspond with the numerical expression p + 1.
As was seen in Chapter 5 of The Beginner’s Guide [Smullyan, 2014], once we have ⋁ and ∼, we can obtain all the other logical connectives. We take p ⊃ q (implication) to be ∼ p ∨ q, which transforms to (p + 1)q, which is pq + q.
What about conjunction? Well, we take p ⋀ q to be ∼(∼p ∨ ∼q), and so in the numerical interpretation we take p ⋀ q to be ((p + 1)(q + 1)) + 1, which is (pq +p+q +1)+1, which is pq +p+q +2. But 2 is an even number, which means that for any number z, the number z + 2 has the same parity as z (i.e. z + 2 is even if and only if z is even). And so we can drop the final 2, and take p ⋀ q to be pq + p + q.
We could take p ≡ q to be (p ⊃ q) ⋀ (q ⊃ p) or we could take p ≡ q to be ((p ⋀ q) ⋁ (~p ⋀ ~q)). But it is much simpler to do the following: Now, p ≡ q is true iff p and q are either both true or both false. Thus in our numerical interpretation, we want the number p ≡ q to be even if and only if p and q are either both even or both odd. And the simple operation that does the trick is addition: we thus take p ≡ q to be p + q.
We note that p ∨ p is equivalent to p and thus p × p has the same parity as p. So we can replace p × p (or pp) by p. Also, for any number p, p + p is always even, and so we can replace p + p with 0 (which is always even). Moreover, we can eliminate any even number that occurs as an addend, because eliminating an even addend does not change the parity of a numerical expression: e.g. p + 0 and p + 4 is of the same parity as p, so can be replaced by p.
Under the transformations just described, a formula is a tautology in the usual propositional interpretation, if and only if it is always even in its numerical interpretation.
Let us consider the tautology (p ⊃ q) ≡∼ (p ⋀ ∼q). Well, p ⊃ q corresponds to pq + q. Now, p ⋀ ∼q transforms to p(q + 1) + p + (q + 1), which simplifies to pq+p+p+q+1. We can drop p+p, which is always even, and so the transformation of p ⋀ ∼q reduces to pq + q + 1. Thus ∼(p ⋀ ∼q) reduces to pq + q + 1 + 1, but 1 + 1 is even, and can be dropped, so that ∼(p ⋀ ∼q) reduces to pq + q, which is the same thing that p ⊃ q reduced to. Thus (p ⊃ q) ≡ ∼(pLet us consider the tautology ⋀ ∼q) reduces to (pq + q) + (pq + q), which expression can be reduced to 0 (since, as mentioned above, the sum of the same two equal numbers is always even). We have thus shown that (p ⊃ q) ≡ ∼(p ⋀ ∼q) is a tautology by showing that the corresponding numerical expression it reduces to (in this case 0) is always even.
Note: Readers familiar with modern higher algebra know that a Boolean ring is a ring with the additional properties that x + x = 0 and x2 = x for any element x of the ring.
III.Another Completeness Proof
We now turn to a completeness proof for propositional logic of an entirely different nature, and one which has a particularly interesting extension to first-order logic, which we consider in the next chapter. It is based on the notion of maximal consistency.
We will consider axiom systems using ∼, ⋀, ∨, and ⊃ as undefined connectives, and in which modus ponens (from X and X ⊃ Y, to infer Y) is the only rule of inference. We will say that a Boolean truth set is a set S of formulas such that for any pair of formulas X and Y, the following conditions hold:
(1) (X ⋀ Y) ∈ S iff X ∈ S and Y ∈ S.
(2) (X ∨ Y) ∈ S iff either X ∈ S or Y ∈ S.
(3) (X ⊃ Y) ∈ S iff either X ∉ S or Y ∈ S.
(4) (∼X) ∈ S iff ∉ S (thus, one and only one of X, ∼X is in S).
Notes: [a] Whenever necessary to avoid a possible misreading, given a formula X and a set S, I will abbreviate the statement that ∼X is a member of S by (∼X) ∈ S instead of by ∼X ∈ S, as the latter could be read as “It is not the case that X is in S.” Similarly, I write (∼X) ∉ S rather than ∼X ∉ S to mean that ∼X is not in S, for ∼X ∉ S might be read as ∼(X ∉ S), i.e. that it is not the case that X is not a member of S (so that X is in S). Outer parentheses around formulas are often omitted for easier reading, but may be included for greater clarity of meaning; for instance, parentheses are used to distinguish between (X ⋀ Y) ∈ S from X ⋀ (Y ∈ S), two possible readings of X ⋀ Y ∈ S (where X and Y are variables for sentences, and S is a variable representing a set of sentences). [b] The reader should also realize that for any statement of the form “X iff Y ”, the statement is equivalent to its negated form “∼X iff ∼Y ”, by the very meaning of the term “iff” (which is an abbreviation for “if and only if”). Thus,

If I refer to an assumption or condition in a proof or a problem solution, but want the reader to take it in its equivalent “negated” form, I often indicate this fact.
We also recall from The Beginner’s Guide [Smullyan, 2014] the unifying α, β notation in propositional logic for unsigned formulas:







α
α1
α2


X ∧ Y
X
Y


∼(X ∨ Y)
∼X
∼Y


∼(X ⊃ Y)
X
∼Y


∼∼X
X
X









β
β1
β2


∼(X ∧ Y)
∼X
∼Y


X ∨ Y
X
Y


X ⊃ Y
∼X
Y


Problem 1. S is a Boolean truth set if and only if for every α and every formula X the following conditions hold:
(1) α ∈ S iff α1 ∈ S and α2 ∈ S.
(2) Either X ∈ S or (∼X) ∈ S, but not both.
Exercise 1. Prove that S is a Boolean truth set iff for every β and every formula X the following conditions hold:
(1) β ∈ S iff either β 1 ∈ S or β 2 ∈ S.
(2) Either X ∈ S or (∼X) ∈ S, but not both.
The definition of a Boolean truth set here can be shown to be equivalent to the one given for a truth set in Chapter 6 (Propositional Tableaux) of The Beginner’s Guide [Smullyan, 2014], where we called a set S of propositional formulas a truth set if it was the set of all formulas true under some interpretation I of the propositional variables. We also showed there that the definition was equivalent to the set S satisfying the following three conditions: for any signed formula X and any α and β :
T0: Either X or its conjugate  is in S, but not both.
T1: α is in S if and only if α1 and α2 are both in S.
T2: β is in S if and only if either β1 is in S or β2 is in S.
With these facts in mind, the reader should easily see that the definitions of (Boolean) truth set in The Beginner’s Guide and this work are equivalent. Note that once one sees the equivalence between the definitions in the two books, it is obvious that every Boolean truth set for propositional logic is satisfiable (since the characterization in The Beginner’s Guide made a truth set satisfiable by definition).
Indeed, to go a little further (to make connections between very similar concepts a little clearer), we often define a Boolean valuation as follows. First, by a valuation (in the context of propositional logic) is meant an assignment of truth values t and f to all formulas. A valuation ∨ is called a Boolean valuation if for all formulas X and Y, the following conditions hold:
B1: ∼X receives the value t under v if and only if v(X) = f [i.e. when X is false under v]; and accordingly v(∼X) = f iff v(X) = t.
B2: X ⋀ Y receives the value t under v iff v(X) = v(Y) = t [i.e. when both X and Y are true under the valuation v].
B3: X ∨ Y receives the value t under v iff v(X) = t or v(Y) = t [i.e. when either X is true under v or Y is true under v, or both are true under v].
B4: X ⊃ Y receives the value t under v iff v(X) = f or v(Y) = t [i.e. when either X is false under v or Y is true under v].
The interpretations that we used in The Beginner’s Guide in terms of assigning truth values to all formulas by assigning truth values to the propositional variables, and then working up through the structure of the formulas, using the meanings of the connectives, to obtain a truth value for every formula, are closely related to the truth values of formulas given through a Boolean valuation. In fact a given Boolean valuation defines a unique interpretation yielding the same truth value on all formulas in the interpretation as in the Boolean valuation: we obtain the interpretation by looking at the truth values the Boolean valuation assigns to the propositional variables. And if one is given an interpretation of the propositional variables, it is clear that the interpretation defines a Boolean valuation on the formulas, just by the way the interpretation assigns truth values to all formulas, starting out only with truth values on the propositional variables.
Because of this tight equivalence between Boolean valuations and interpretations in propositional logic, we will use the words “valuation” and “interpretation” interchangeably (and will do so later in first-order logic as well, where a similar tight relationship holds, this being something that was already discussed in The Beginner’s Guide [Smullyan, 2014]).
Consequence Relations
We consider a relation S ⊢ X between a set S of formulas and a formula X. We read S ⊢ X as “X is a consequence of S,” or as “S yields X”.
We call the relation ⊢ a consequence relation if for all sets S, S1 and S2 and all formulas X and Y, the following conditions hold:
C1: If X ∈ S, then S ⊢ X.
C2: If S1 ⊢ X and S1 ⊆ S2 then S2 ⊢ X.
C3: If S ⊢ X and S, X ⊢ Y, then S ⊢ Y.
C4: If S ⊢ X, then F ⊢ X for some finite subset F of S.
In what follows, we assume that ⊢ is a consequence relation. We write ⊢X to mean  ⊢ X, where  is the empty set. For any n, we write X1, …, Xn ⊢ Y to mean {X1, …, Xn} ⊢ Y, and for any set S, we write S, X1, …, Xn to mean S ∪ {X1, …, Xn}.
An important example of a consequence relation is this. For any axiom system , take S ⊢ X to mean that X is derivable from the elements of S together with the axioms of  by use of the inference rules of the system; in other words, if we take the elements of S and add them as new axioms of , then X becomes provable in that enlarged system. Note that under this relation ⊢, the statement ⊢ X simply says that X is provable in the system A. It is obvious that this relation ⊢ satisfies conditions C1, C2, and C3. As for C4, if X is derivable from S, any derivation uses only finitely many elements of S.
We now return to consequence relations in general.
Problem 2. Prove that if S ⊢ X and S ⊢ Y, and X, Y ⊢ Z, then S ⊢ Z.
Consistency
In what follows, “set” shall mean a set of formulas.
We shall call a set S inconsistent (with respect to a consequence relationship ⊢) if S ⊢ Z for every formula Z; otherwise, we shall call S consistent.
Problem 3. Show that if S is consistent, and S ⊢ X, then S, X is consistent.
Problem 4. Suppose S is consistent. Show the following:

(a)If S ⊢ X and X ⊢ Y, then S, Y is consistent.
(b)If S ⊢ X and S ⊢Y and X, Y ⊢ Z, then S, Z is consistent.

Maximal Consistency
We say that S is maximally consistent (with respect to ⊢) if no proper superset of S is consistent. [By a proper superset of S is meant a set S′ such that S⊆ S′ and S ′ contains one or more elements not in S.]
Problem 5. Show that if M is maximally consistent, then for all formulas X, Y and Z:

(a)If X ∈ M and X ⊢ Y, then Y ∈ M.
(b)If X ∈ M and Y ∈ M and X, Y ⊢ Z then Z ∈ M.

Boolean Consequence Relations
We shall define a consequence relation ⊢ to be a Boolean consequence relation if it satisfies the following additional conditions (for all formulas α, X and Y):
C5: (a) α ⊢ α1; (b) α ⊢ α2.
C6: α1, α2 ⊢ α.
C7: X, ∼X ⊢ Y.
C8: If S,∼X ⊢ X, then S ⊢ X.
We say that an interpretation satisfies a set S if all elements of S are true under the interpretation. We shall say that X is a tautological consequence of S if X is true under all interpretations that satisfy S.
Exercise 2. Define S╞ X to mean that X is a tautological consequence of S. Prove that this relation ╞ is a Boolean consequence relation. (Hint: For C4, use the Corollary to the Compactness Theorem for Propositional Logic on p. 95 in The Beginner’s Guide [Smullyan, 2014].)
We say a consequence relation ⊢ is tautologically complete if, for every set S and formula X, whenever X is a tautological consequence of S, then S ⊢ X.
Problem 6. Suppose ⊢ is tautologically complete. Does it necessarily follow that  ⊢ X for every tautology X? [We recall that  is the empty set.]
The following, which we aim to prove, will be our main result:
Theorem 1. Every Boolean consequence relation is tautologically complete. Thus, if ⊢ is a Boolean consequence relation, then:

(1)If X is a tautological consequence of S, then S ⊢ X.
(2)If X is a tautology, then ⊢ X (i.e.  ⊢ X). This means that every system of propositional logic based on a Boolean consequence relation is also complete (i.e. that all tautologies are provable within the system).

To prove Theorem 1, we will need the following lemma:
Lemma 1. [Key Lemma] If ⊢ is a Boolean consequence relation and M is maximally consistent with respect to ⊢, then M is a Boolean truth set.
Problem 7. Prove Lemma 1.
Compactness
We recall from Chapter 4 of The Beginner’s Guide [Smullyan, 2014] that a property P of sets is called compact if for any set S, S has the property P if and only if all finite subsets of S have property P. Thus, to say that consistency is compact is to say that a set S is consistent if and only if all finite subsets of S are consistent.
Problem 8. [A key fact!] Show that for a Boolean consequence relation ⊢, consistency (with respect to ⊢) is compact.
Let us also recall the denumerable compactness theorem of Chapter 4 of The Beginner’s Guide, which says that for any compact property P of subsets of a denumerable set A, any subset of A having property P can be extended to (i.e. is a subset of) a maximal subset of A having property P. We have just shown that for a Boolean consequence relation, consistency is a compact property. Hence, any consistent set is a subset of a maximally consistent set. This fact, together with the Key Lemma, easily yields Theorem 1.
Problem 9. Now prove Theorem 1.
IV.Fidelity to Modus Ponens
We shall say that a consequence relation ⊢ is faithful to modus ponens if

holds for all X and Y.
In what follows, we shall assume that ⊢ is faithful to modus ponens. We shall say that a formula X is provable with respect to ⊢ when ⊢ X holds (i.e. if  ⊢ X). Until further notice, “provable” will mean provable with respect to ⊢.
Problem 10. Show that if S ⊢ X and S ⊢ X ⊃ Y, then S ⊢ Y.
Problem 11. Show the following:

(a)If X ⊃ Y is provable, and S ⊢ X, then S ⊢ Y.
(b)If X ⊃ (Y ⊃ Z) is provable, and S ⊢ X and S ⊢ Y, then S ⊢ Z.
(c)If X ⊃ Y is provable, then X ⊢ Y.
(d)If X ⊃ (Y ⊃ Z) is provable, then X, Y ⊢ Z.

We now consider an axiom system  for propositional logic in which modus ponens is the only inference rule. Such a system I will call a standard system. By a deduction from a set S of formulas is meant a finite sequence X1, …, Xn of formulas, such that for each term Xi of the sequence, Xi is either an axiom (of ), or a member of S, or is derivable from two earlier terms by one application of modus ponens. We call a deduction X1,…, Xn from S a deduction of X from S if Xn = X. We say that X is deducible from S to mean that there is a deduction of X from S. We now take S ⊢ X to be the relation that X is deducible from S. This relation is indeed a consequence relation.
Problem 12. Prove that ⊢ is a consequence relation.
It is obvious that this relation ⊢ is faithful to modus ponens, since the sequence X, X ⊃ Y, Y is a deduction of Y from {X, X ⊃ Y }.
We say that the axiom system  has the deduction property if for all sets S and formulas X and Y, if S, X ⊢ Y, then S ⊢ X ⊃ Y.
Theorem 2. [The Deduction Theorem] A sufficient condition for an axiom system  to have the deduction property is that the following are provable in  (for all X, Y and Z):
A1: X ⊃ (Y ⊃ X).
A2: (X ⊃(Y ⊃ Z)) ⊃((X ⊃ Y) ⊃ (X ⊃ Z)).
For this, we need:
Lemma 2. If A1 and A2 hold then X ⊃ X is provable.
Until further notice, we assume that A1 and A2 hold.
Problem 13. To prove the above lemma, show that for any formulas X and Y, the formula (X ⊃ Y) ⊃ (X ⊃ X) is provable. Then, taking Y ⊃ X for Y in the formula just proved, conclude the proof of Lemma 2.
Problem 14. In preparation for the proof of the Deduction Theorem, show the following:

(a)If S ⊢ Y, then S ⊢ X ⊃ Y.
(b)If S ⊢ X ⊃ (Z ⊃ Y) and S ⊢ X ⊃ Z, then S ⊢ X ⊃ Y.

Problem 15. Prove the Deduction Theorem as follows: Suppose S, X ⊢ Y. Then there is a deduction Y1, …, Yn from S ∪ {X} in which Y =Yn. Now consider the sequence X ⊃ Y1, …, X ⊃ Yn. Show that for all i ≤ n, if S ⊢ X ⊃ Yj for all j < i then S ⊢ X ⊃ Yi. [Hint: Break the proof up into the following four cases: Yi is an axiom of ; Yi ∈ S; Yi = X; Yi comes from two terms of the sequence Y1, …, Yi −1 by modus ponens.]
It then follows by complete mathematical induction that S ⊢ X ⊃ Yi for all i ≤ n, and so in particular, S ⊢ X ⊃ Yn, and thus S ⊢ X ⊃ Y. [Hint: In the original sequence Y1,…, Yn, the term Yi is either an axiom of , or is X itself, or comes from two earlier terms by modus ponens. Handle each case separately.]
We now have all the parts necessary to prove:
Theorem 3. A sufficient condition for a standard axiom system for propositional logic  to be tautologically complete is that for all X, Y, Z and α, the following are provable:
A1: X ⊃ (Y ⊃ X)
A2: (X ⊃ (Y ⊃ Z)) ⊃ ((X ⊃ Y) ⊃ (X ⊃ Z))
A3: (a) α ⊃ α1 and (b) α ⊃ α2
A4: α1 ⊃ (α2 ⊃ α)
A5: X ⊃ (∼X ⊃ Y)
A6: (∼X ⊃ X) ⊃ X
Problem 16. Prove Theorem 3 by showing that under the conditions given in Theorem 3, the relation ⊢ is a Boolean consequence relation and thus tautologically complete.
One can easily verify that A1–A6 are all provable in the axiom system S of Chapter 7 of The Beginner’s Guide [Smullyan, 2014]. [One can make a tableau proof for each of these formulas and then convert them to proofs in S by the method explained in that chapter. The completeness proof for tableaux is not needed for this.] We thus have a second proof of the completeness of S based on maximal consistency, and thus the theorem:
Theorem 4. The axiom system S of Chapter 7 of The Beginner’s Guide is tautologically complete.
Maximal consistency and Lindenbaum’s Lemma also play a key role in alternative completeness proofs for first-order logic, to which we turn in the next chapter.
Solutions to the Problems of Chapter 1

1.We are to show that S is a Boolean truth set iff the two conditions of Problem 1 hold. The second condition of Problem 1 is the same as the fourth condition in the definition of the Boolean truth set, so we need only consider the first condition of Problem 1. (Remember, when thinking about “negated” conditions or assumptions, that ∼(X Λ Y) is equivalent to ∼X ∨∼ Y and ∼(X ∨Y) is equivalent to ∼X Λ∼Y.)
(a)Suppose S is a Boolean truth set. We must show that in all cases for α, the first condition of Problem 1 holds, namely that α∈ S iff α1∈ S and α2 ∈ S.
Case 1. α is of the form X Λ Y. This is immediate.
Case 2. α is of the form ∼(X ∨ Y). Then α1 = ∼ X and α2 = ∼Y. Thus we are to show that (∼(X ∨ Y))∈ S iff (∼X) ∈ S and (∼Y) ∈ S. Well,
•(∼(X ∨ Y)) ∈ S is true iff (X ∨ Y) ∉ S [by (4) in the definition of a Boolean truth set].

•Then (X ∨ Y) ∉ S is true iff X ∉ S and Y ∉ S [by the negated form of (2) in the definition of a Boolean truth set].
•And X ∉ S and Y ∉ S is true iff (∼ X) ∉ S and (∼ Y) ∈ S [by (4) in the definition of a Boolean truth set].
Case 3. α is of the form ∼(X ⊃ Y). Then α1 = X and α2 = ∼Y. We are to show that (∼(X ⊃ Y)) ∈ S iff X ∈ S and (∼ Y) ∈ S. Well,
•(∼(X ⊃ Y)) ∈ S is true iff X ⊃ Y∉ S [by (4) in the definition of a Boolean truth set].
•Then X ⊃ Y ∉ S is true iff X ∈ S and Y ∉ S [by the negated form of (3) in the definition of a Boolean truth set].
•And X ∈ S and Y ∉ S is true iff X ∈ S and (∼ Y) ∈ S [by (4) in the definition of a Boolean truth set].
Case 4. α is of the form ∼∼X. Here both α1 and α2 are X, so we must show that (∼∼ X)∈ S iff X ∈ S. Well, (∼ ∼ X)∈ S iff(∼ X) ∉ S [by (4) in the definition of a Boolean truth set], which is true iff X ∈ S[again by (4) in the definition of a Boolean truth set].
(b)Going in the other direction, suppose the conditions of Problem 1 hold: (i) α ∈ S iff α1 ∈ S and α2 ∈ S, and (ii) for any X, either X ∈ S or ∼X ∈ S, but not both. We are to show that S is a Boolean truth set.
Case 1. We are to show that X Λ Y ∈ S iff X ∈ S and Y ∈ S: Well X Λ Y is an α, where X = α1 and Y = α2 and α ∈ S iff α1 ∈ S and α2 ∈ S [by assumption (i) above]. Thus it is immediate that X Λ Y ∈ S iff X ∈ S and Y ∈ S.
Case 2. We are to show that X ∨ Y ∈ S iff X ∈ S or Y ∈ S: Note that here ∼(X ∨ Y) is an α, with α1 = ∼X and α2 = ∼Y. Well,
•X ∨ Y ∈ S iff ∼ (X ∨ Y) ∉ S [by assumption (ii) above].
•∼(X ∨ Y)∉ S is true iff (∼ X) ∉ S or (∼ Y) ∉ S [by the negated form of assumption (i)].
•Then (∼X) ∉ S iff X ∈ S and (∼ Y) ∉ S is true Y ∈ S [by assumption (ii) above].
Case 3. We are to show that (X ⊃ Y) ∈ S iff X∉ S or Y ∈ S. Note that here ∼ (X ⊃ Y) is an α, with α1 = X and α2 = ∼ Y. Well,
•(X ⊃ Y)∈ S iff ∼(X ⊃ Y) ∉ S [by assumption (ii)].
•Then ∼(X ⊃ Y) ∉ S is true iff X ∉ S or (∼Y)∉ S [by the negated form of assumption (i) above].
•Since it is true that (∼Y) ∉ S iff Y ∈ S [by assumption (ii) above], we now have X ∉ S or Y ∈ S, which is what was to be shown.
Case 4. To show that (∼X) ∈ S iff X ∉ S: As noted in the definition of a Boolean truth set, condition (4) of a truth set and condition (2) of Problem 1 are equivalent.
2.Suppose S ⊢ X and S ⊢ Y and X, Y ⊢ Z. Since X, Y ⊢ Z, then S, X, Y ⊢ Z [by C2, since { X, Y } ⊆ S ∪ { X, Y }]. Since S ⊢ Y, then S, X ⊢ Y [again by C2]. Thus S, X ⊢ Y and S, X, Y ⊢ Z. Hence [by C3], S, X ⊢ Z. [To see this more easily, let W = S ∪ { X}.] Then W ⊢ Y(since this is the same as S, X ⊢ Y). And W, Y ⊢ Z (since this is the same as S, X, Y ⊢ Z). Hence W ⊢ Z, by C3, taking W for S. Then, since S, X ⊢ Z and S ⊢ X (given), then S ⊢ Z [again by C3].
3.Suppose S ⊢ X. We are to show that if S is consistent, so is S, X (i.e. so is the set S ∪ {X}). We will show the equivalent fact that if S, X is inconsistent, so is S. Well, suppose S, X is inconsistent. Thus S, X ⊢ Z for every Z. Since also S ⊢ X (which is given), then S ⊢ Z [by C3], and thus S is also inconsistent.
4.Suppose S is consistent.
(a)Assume S ⊢ X and X ⊢ Y. We are to show that S, Y is consistent. Well, since X ⊢ Y, then S, X ⊢ Y [by C2]. Since also S ⊢ X, we have S ⊢ Y [by C3]. Since S is consistent and S ⊢ Y, then S, Y is consistent [by Problem 3].
(b)Assume S ⊢ X and S ⊢ Y and X, Y ⊢ Z. We are to show that S, Z is consistent. Well, since S ⊢ X and S ⊢ Y and X, Y ⊢ Z, it must also be true that S ⊢ Z [by Problem 2]. Then, since S is consistent and S ⊢ Z, we have that S, Z is consistent [by Problem 3].
5.Suppose M is maximally consistent. We first note that if M ⊢ X, then X ∈ M. For suppose M ⊢ X. Since M is consistent, then M, X is consistent [by Problem 3]. Consequently M, X cannot be a proper superset of M (no consistent set is). Thus X ∈ M. Now we consider the two parts of the problem.
(a)Suppose X ∈ M and X ⊢ Y. Since X ∈ M, then M ⊢ X [by C1]. Since X ⊢ Y and M ⊢ X, we obtain M ⊢ Y [by C2]. Hence, as just noted, Y ∈ M.
(b)Suppose X ∈ M and Y ∈ M and X, Y ⊢ Z. Then M ⊢ X, M ⊢ Y [by C1] and X, Y ⊢ Z. Thus M ⊢ Z [by Problem 2]. Hence Z ∈ M, as noted above.
6.Yes, if ⊢ is tautologically complete, then  ⊢ X for every tautology X. Here is the proof. Since there are no elements in the empty set  it is vacuously true that every element of the empty set is true under all interpretations. (If you don’t believe this, just try to find an element of the empty set which is not true under all interpretations!) Thus every interpretation satisfies both  and X, so X is a tautological consequence of . Then, since ⊢ is tautologically complete, it follows from the definition of a consequence relation being tautologically complete that  ⊢ X (in fact S ⊢ X holds for every set S [by C2]).
7.We are given that ⊢ is a Boolean consequence relation and that M is maximally consistent with respect to ⊢. By virtue of Problem 1, to show that M is a Boolean truth set, it suffices to show that the following two things hold for every α and every X:
(1) α ∈ M if and only if α1 ∈ M and α2 ∈ M.
(2) Either X ∈ M or (∼ X) ∈ M, but not both.
Regarding (1): Suppose α ∈ M. Since α ⊢ α1 [by C5(a)], then α1 ∈ M[by Problem 5(a)]. Thus if α ∈ M, then α1 ∈ M. Similarly, if α ∈ M, then α2 ∈ M [by C5(b)]. Thus if α ∈ M, then α1 ∈ M and α2 ∈ M.
Conversely, suppose α1 ∈ M and α2 ∈ M. We know that α1, α2 ⊢ α [by C6]. Hence α ∈ M [by Problem 5(b)].
Thus α ∈ M if and only if α1 ∈ M and α2 ∈ M.
Regarding (2): Since M is consistent, it cannot be that X and ∼ X are both in M, for if they were, then we would have both M ⊢ X and M ⊢ ∼ X, and since X, ∼ X ⊢ Z for every Z [by C7], we would have M ⊢ Z [by Problem 2], which would mean that M is inconsistent. Thus, since M is consistent, it cannot be that X ∈ M and ∼ X ∈ M. It remains to show that either X ∈ M or (∼ X) ∈ M. Let me show that if ∼ X is not in M, then X ∈ M.
Well, suppose that ∼ X is not in M. Then M, ∼ X is not consistent (since M is maximally consistent). Hence M, ∼ X ⊢ X (since M, ∼ X ⊢ Z for every Z), and therefore M ⊢ X [by C8]. Thus X ∈ M (since M is maximally consistent).
This completes the proof.
8.To show that if all finite subsets of a set S are consistent, then so is S, it suffices to show that if S is inconsistent, then some finite subset of S is inconsistent. Well, suppose S is inconsistent. Then for any formula X, we have both S ⊢ X and S ⊢ ∼ X (since S ⊢ Z for every Z). Since S ⊢ X, then F1 ⊢ X for some finite subset F1 of S [by C4]. Similarly, F2 ⊢ ∼ X for some finite subset F2 of S. Let F = F1 ∪ F2. Then F ⊢ X and F ⊢ ∼ X [by C2], since F1 F and F2 F). Also, X, ∼ X ⊢ Z for all Z [by C7]. Thus, for all Z, F ⊢ Z [by Problem 2], and we have shown that F is inconsistent.
9.We wish to show that every Boolean consequence relation is tautologically complete. By Problem 8 we know that for any Boolean consequence relation, consistency with respect to that relation is a compact property, and so [by the denumerable compactness theorem] every consistent set is a subset of a maximally consistent set, which must be a Boolean truth set [by the Key Lemma]. Thus every consistent set is satisfiable (by the discussion after the definition of a Boolean truth set), and hence every unsatisfiable set is inconsistent. Now, suppose X is a logical consequence of S. Then S, ∼ X is unsatisfiable, hence inconsistent. Therefore S, ∼ X ⊢ X (since S, ∼ X ⊢ Z for every Z), and so S ⊢ X [by C8]. This concludes the proof.
10.Suppose S ⊢ X and S ⊢ X ⊃ Y. It is also true that X, X ⊃ Y ⊢ Y, since we assume ⊢ is faithful to modus ponens. Hence S ⊢ Y [by Problem 2].
11.Proofs of the four statements:
(a)Suppose X ⊃ Y is provable and S ⊢ X. Since X ⊃ Y is provable ( ⊢ X ⊃ Y), then S ⊢ X ⊃ Y [by C2]. Thus S ⊢ X and S ⊢ X ⊃ Y ; hence S ⊢ Y [by Problem 10].
(b)Suppose X ⊃ (Y ⊃ Z) is provable, and S ⊢ X and S ⊢ Y. Since S ⊢ X and S ⊢ X ⊃ (Y ⊃ Z) [by C2], then S ⊢ Y ⊃ Z [by Problem 10]. Since also S ⊢ Y, then S ⊢ Z [again by Problem 10].
(c)In 11(a), we take the unit set { X} for S, and thus if X ⊃ Y is provable and X ⊢ X, then X ⊢ Y. But X ⊢ X does hold, so that if X ⊃ Y is provable, then X ⊢ Y.
(d)In 11(b), we take the set { X, Y } for S, and thus if X ⊃ (Y ⊃ Z) is provable, and X, Y ⊢ X, and X, Y ⊢ Y, then X, Y ⊢ Z. But X, Y ⊢ X and X, Y ⊢ Y both hold, and so if X ⊃ (Y ⊃ Z) is provable, then X, Y ⊢ Z.
12.We must verify that conditions C1 − C4 hold.
C1:If X ∈ S, then the unit sequence X (i.e. the sequence whose only term is X) is a deduction of X from S.
C2:Suppose S1 ⊆ S2. Then any deduction Y1, …, Yn from S1 is also a deduction from S2.
C3:If Y1, …, Yn is a deduction of X from S, and Z1, …, Zk is a deduction of Y from S, X, then the sequence Y1, …, Yn, Z1, …, Zk is a deduction of Y from S (as is easily verified).
C4:Any deduction from S uses only finitely many elements X1, …, Xn of S, hence is also a deduction from the set X1, …, Xn.
13.(a) We assume A1 and A2 hold. In A2;we take X for Z, and obtain

But by A1, we have ⊢ X ⊃ (Y ⊃ X). Hence, by modus ponens, we have

(b)In (X ⊃ Y) ⊃ (X ⊃ X), taking (Y ⊃ X) for Y, we have

But by A1 we have that X ⊃ (Y ⊃ X) is provable. So, by modus ponens, we also have ⊢ X ⊃ X.
14.(a) Suppose S ⊢ Y. Now, by (a) of Problem 11, for any formulas W1 and W2, if S ⊢ W1 and W1 ⊃ W2 is provable, then S ⊢ W2. Let us take Y for W1 and X ⊃ Y for W2. When we do so, we see that if S ⊢ Y and Y ⊃ (X ⊃ Y) is provable, then S ⊢ X ⊃ Y. However, S ⊢ Y [by assumption] and Y ⊃ (X ⊃ Y) is provable [by A1]; hence S ⊢, X ⊃ Y.
(b) By Problem 11(b), for any formulas W1, W2, and W3, if W1 ⊃ (W2 ⊃ W3) is provable, then if S ⊢ W1 and S ⊢ W2, it follows that S ⊢ W3. We now take X ⊃ (Z ⊃ Y) for W1, X ⊃ Z for W2, and X ⊃ Y for W3. Well, the formula W1 ⊃ (W2 ⊃ W3) is provable [by A2; it is the formula (X ⊃ (Z ⊃ Y)) ⊃ ((X ⊃ Z) ⊃ (X ⊃ Y))]. Hence, if S ⊢ W1 and S ⊢ W2, then S ⊢ W3, i.e. we have that if S ⊢ X ⊃ (Z ⊃ Y) and S ⊢ X ⊃ Z, then S ⊢ X ⊃ Y, which is what was to be proved.
15.We assume that for all j < i, S ⊢ X ⊃ Yj, and we are to prove that S ⊢ X ⊃ Yi.
Cases 1 and 2. Suppose that either Yi is an axiom of , or that Yi ∈ S. If the former, then Yi is provable, so that ⊢ Yi, and so S ⊢ Yi [by C2]. If Yi ∈ S, then again S ⊢ Yi [by C1]. Thus, in either case, S ⊢ Yi. Hence S ⊢ X ⊃ Yi, [by Problem 14(a)].
Case 3. Yi = X. Since X ⊃ X is provable [by Lemma 2], then S ⊢ X ⊃ X [by C2], and since Yi = X, then X ⊃ Yi is the formula X ⊃ X, and so S ⊢ X ⊃ Yi.
Case 4. This is the case where in the original sequence Y1, … Yn, the formula Yi came from terms earlier than Yi by modus ponens. Thus there is a formula Z such that Z and Z ⊃ Yi are earlier terms than Yi. Hence in the new sequence X ⊃ Y1, …, X ⊃ Yn the formulas X ⊃ Z and X ⊃ (Z ⊃ Yi) are earlier terms than X ⊃ Yi, and so, by the inductive hypothesi, S ⊢ X ⊃ Z and S ⊢ X ⊃ (Z ⊃ Yi). Then by Problem 14(b), S ⊢ X ⊃ Yi. This completes the proof.

16.We already know that conditions C1−C4 hold, and so it remains to verify conditions C5−C8.

Re C5: (a) Since α ⊃ α 1 is provable [by A3(a)], then α ⊢ α1 [by Problem 11(c)].
(b) Since α ⊃ α2 is provable [by A3(b)], then α ⊢ α2 [by Problem 11(c)].
Re C6: Since α1 ⊃ (α2 ⊃ α) is provable [by A4], then α1, α2 ⊢ α [by Problem 11(d)].
Re C7: Since X ⊃ (∼X ⊃ Y) is provable [by A5], then X, ∼X ⊢ Y [by Problem 11(d)].
Re C8: [This is the only part that needs the Deduction Theorem!] Suppos that S, ∼X ⊢ X Then, by the deduction theorem, S ⊢ ∼X ⊃ X. But also (∼X ⊃ X) ⊃ X is provable [by A6]. Therefore S ⊢ X [by Problem 11(a), taking ∼X ⊃ X for X and X for Y ].
This concludes the proof that ⊢ is a Boolean consequence relation, and so by Theorem 1, it follows that all tautologies are provable in the system (and more generally, that if X is a logical consequence of S, then S ⊢ X holds).







Chapter 2
More on First-Order Logic
I.Magic Sets
There is another approach to Completeness Theorems, the Skolem–Löwenheim Theorem and the Regularity Theorem that has an almost magic-like simplicity. I accordingly dub the central player a magic set.
As in propositional logic, in the context of first-order logic a valuation is an assignment of truth values t and f to all sentences (with or without parameters). And a valuation v is called a Boolean valuation if for all sentences (closed formulas) X and Y, the following conditions hold:
B1:The sentence ∼X receives the value t under v if and only if v(X) = f [i.e. when X is false under v]; and accordingly v(∼X) = f iff v(X) = t.
B2:The sentence X ∧ Y receives the value t under v iff v(X) = v(Y) = t [i.e. when both X and Y are true under the valuation v].
B3:The sentence X ∨ Y receives the value t under v iff v(X) = t or v(Y) = t [i.e. when either X is true under v or Y is true under v].
B4:The sentence X ⊃ Y receives the value t under v iff v(X) = f or v (Y) = t [i.e. when either X is false under v or Y is true under v].
By a first-order valuation (in the denumerable domain of the parameters) is meant a Boolean valuation satisfying the following two additional conditions:
F1:The sentence ∀xφ(x) is true under v if and only if for every parameter a, the sentence φ(a) is true under v.
F2:The sentence ∃xφ(x) is true under v if and only if the sentence φ(a) is true under v for at least one parameter a.
Let S be a set of sentences of first-order logic. We say that S is truth-functionally satisfiable, if there is a Boolean valuation in which every sentence in S is true. Similarly, we say that S is first-order satisfiable (in first-order contexts, just satisfiable), if there is a first-order valuation in which every sentence in S is true.
Note: Recall that it was noted in The Beginner’s Guide [Smullyan, 2014] that if we consider the set of all true sentences under a first-order valuation v, and look at the universe in which the individuals to which predicates apply as consisting of all the parameters, we can define an interpretation on that universe in which each n-ary predicate is true precisely on the n-tuples of parameters on which the valuation v says it is true (and false on the rest of n-tuples of parameters). This is called an interpretation in the domain of the parameters, and because of this we can say a set of sentences S is satisfiable in the denumerable domain of the parameters whenever there is a first-order valuation that satisfies S. Although here we are not using the concept of an interpretation of first-order logic as defined in The Beginner’s Guide, it is easy to see that any interpretation of first-order logic as defined there simultaneously defines a first-order valuation as defined here.
We leave it to the reader to verify the following:
Lemma. From F1 and F2 it follows that under any first-order valuation,

(1)∼∃xφ(x) is true iff ∼φ(a) is true for every parameter a, or, equivalently, that ∼∃xφ(x) and ∀x∼φ(x) are logically equivalent (i.e. always have the same truth value);
(2)∼∀xφ(x) is true iff ∼φ(a) is true for at least one parameter a, or, equivalently, that ∼∀xφ(x) and ∃x∼φ(x) are logically equivalent.

Now, let us again recall the uniform notation of The Beginner’s Guide, in which we let γ be any formula of the form ∀xφ(x) or ∽∃xφ(x), and by γ(a) we meant φ(a) or ∽φ(a) respectively. Similarly, we let δ be any formula of the form ∃xφ(x) or ∼∀xφ(x), and by δ(a) we respectively mean φ(a), ∽φ(a). Thus, in uniform notation, under any first-order valuation v:
: The sentence γ is true under v if and only if for every parameter a, the sentence γ(a) is true under v.
: The sentence δ is true under v if and only if the sentence δ(a) is true under v for at least one parameter a.
The following proposition will be helpful:
Proposition 1. Suppose that v is a Boolean valuation such that the following two conditions hold (for all sentences γ and δ):

(1)if γ is true under v, then for every parameter a, the sentence γ(a) is true under v, and
(2)if δ is true under v, then γ(a) is true under v for at least one parameter a.

Then v is a first-order valuation.
Problem 1. Prove Proposition 1.
We say that a set S of sentences of first-order logic is a Boolean truth set if there is a Boolean valuation v such that S is the set of all sentences that are true under v. This is equivalent to saying that for all sentences X and Y, (i) (∼X) ∈ S iff X ∉ S; (ii) α ∈ S iff α1 ∈ S and α2 ∈ S; (iii)β ∈ S iff β1 ∈ S or β2 ∈S.
We call a set of sentences of first-order logic S a first-order truth set if and only if it is a Boolean truth set and also, for all γ and δ:

(i)γ ∈ S iff γ(a) ∈ S for every parameter a; and
(ii)δ∈ S iff δ(a) ∈ S for at least one parameter a.

Alternatively, we could say that a set of sentences S is a first-order truth set if and only if there is a first-order valuation v such that S is the set of all sentences that are true under v.
Accordingly, Proposition 1 could be restated as follows:
Proposition 1′. Suppose S is a Boolean truth set satisfying the following two conditions (for all sentences γ and δ):

(1)if γ ∈ S, then for every parameter a, the sentence γ(a) ∈ S, and
(2)if δ ∈ S, then δ(a) ∈ S for at least one parameter a.

Then S is a first-order truth set.
Recall that we say that a (first-order) valuation v satisfies a set of sentences S if all elements of S are true under v. Also, a pure sentence is a sentence without parameters.
Now for the definition of a magic set. By a magic set M we mean a set of sentences that satisfies the following two conditions:
M1: Every Boolean valuation that satisfies M is also a first-order valuation.
M2: For every finite set S0 of pure sentences, and every finite subset M0 of M, if S0 is first-order satisfiable, so is S0 ∪ M0.
We shall soon prove that magic sets exist (which the reader might find surprising!). But first you will see that one of the important features of magic sets emerges from the following considerations:
We say that a sentence X is tautologically implied by a set of sentences S when X is true under all Boolean valuations that satisfy S. We shall call S a truth-functional basis for first-order logic if for any pure sentence X, the sentence X is valid if and only if X is tautologically implied by some finite subset S0 of S (which is equivalent to saying that there are finitely many elements X1, …, Xn of S such that the sentence (X1∧ … ∧ Xn) ⊃ X is a tautology).
Note: “Truth-functionally imply” and “tautologically imply” have the same meaning.
Theorem 1. Every magic set is a truth-functional basis for first-order logic.
Problem 2. Prove Theorem 1. (Hint: Use a fact proved in The Beginner’s Guide [Smullyan, 2014], Chapter 6, Problem 8, namely that if X is tautologically implied by S, then it is tautologically implied by some finite subset of S. This fact is an easy corollary of the compactness theorem for propositional logic, namely that if every finite subset of a denumerable set S is satisfied by some Boolean valuation, then S is satisfied by some Boolean valuation.)
Now we turn to the proof that magic sets exist. In Chapter 9 of The Beginner’s Guide [Smullyan, 2014] we defined a finite set of sentences to be regular if its elements are each of the form γ ⊃ γ (a) or δ ⊃ δ(a), and the elements can be placed in a sequence such that for each term of the sequence, if it is of the form δ ⊃ δ(a), then a does not occur in δ or in any earlier term of the sequence. We showed that for any pure sentence X, if X is valid, then there exists a regular set {X1, …, Xn} such that (X1∧ … ∧ Xn) ⊃ X is a tautology. We also showed that for any regular set R and any set S of pure sentences, if S is (first-order) satisfiable, so is R ∪ S.
We now define a denumerably infinite set S to be regular if every finite subset of S is regular. An infinite regular set M will be called complete if for every γ, the sentence γ ⊃ γ(a) is in M for every parameter a, and if for every δ, the sentence δ ⊃ δ(a) is in M for at least one parameter a.
Theorem 2. Every complete regular set is a magic set.
Problem 3. Prove Theorem 2. [Hint: Proposition 1 will now be helpful.]
Finally, we show that there exists a complete regular set (which completes the proof that there exists a magic set): We enumerate all δ-sentences in some denumerable sequence δ1, δ2, …, δn, …. We then consider all our parameters in some enumeration b1, b2, …, bn, …. We now let a1 be the first parameter in the enumeration of parameters that does not occur in δ1, we let a2 be the first parameter of the enumeration of parameters that occurs neither in δ1 nor in δ2 and so forth, i.e. for each n we let an be the first parameter of the enumeration of parameters that occurs in none of the terms δ1, δ2, …, δn. We let R1 be the set of all sentences δn ⊃ δn(an). We let R2 be the set of all γ ⊃ γ(a), for all γ and every a. Then R1 ∪ R2 is a complete regular set.
Applications of Magic Sets
The mere existence of magic sets (even those not necessarily regular) yields particularly simple and elegant proofs of the Skolem–Löwenheim Theorem and of the first-order compactness theorem as a consequence of the compactness theorem for propositional logic. We will see these proofs in a moment.
Note: But first a reminder of something from The Beginner’s Guide [Smullyan, 2014]. It was noted there that if we consider the set of all true sentences under a first-order valuation v, and look at the universe in which the individuals to which predicates apply as consisting of all the parameters, we can define an interpretation in which each n-ary predicate is true precisely on the n-tuples of parameters on which the valuation v says it is true (and false on the rest of n-tuples of parameters). This is called a (first-order) interpretation in the (denumerable) domain of the parameters, and because of this we can say a set of sentences S is first-order satisfiable in the domain of the parameters whenever there is a first-order valuation that satisfies S. Although we are not using the concept of an interpretation of first-order logic at the moment here, it is easy to see that any interpretation of first-order logic simultaneously defines a first-order valuation over the denumerable domain of the parameters.
Now suppose S is a denumerable set of pure sentences (no free variables, no parameters) such that every finite subset of S is first-order satisfiable. Here is an alternative proof that S is first-order satisfiable in the denumerable domain of the parameters (which simultaneously yields the compactness theorem for first-order logic, and the Skolem–Löwenheim Theorem, since if S is first-order satisfiable, then obviously so is every finite subset of S). The proof we now give uses the compactness theorem for propositional logic.
Let S be as above and let M be any magic set. Let K be any finite subset of S ∪ M. We first show that K is first-order satisfiable. Well, K is the union S0 ∪ M0 of some finite subset S0 of S with some finite subset M0 of M. Since S0 is first-order satisfiable, so is S0 ∪ M0 [by property M2 of a magic set]. Since S0 ∪ M0 is first-order satisfiable, by definition there is a first-order valuation v that satisfies S0 ∪ M0, and v is obviously also a Boolean valuation [since every first-order valuation is]. This proves that every finite subset K of S∪M is truth-functionally satisfiable. Hence, by the compactness theorem for propositional logic, the entire set S∪M is truth-functionally satisfiable. Thus there is a Boolean valuation v that satisfies S∪M, and since v satisfies M, it must be a first-order valuation [by property M1 of a magic set]. Thus, by what we said above about the relationship between the first-order interpretations of The Beginner’s Guide [Smullyan, 2014] and first-order valuations, all sentences of S are true (simultaneously satisfiable) in an interpretation over the denumerable domain of the parameters.
We recall the Regularity Theorem, which is that for every valid pure sentence X, there is a (finite) regular set R which truth-functionally implies X. (This is a somewhat simplified version of what we proved in The Beginner’s Guide. We will return to the full version of the Regularity Theorem later in this chapter.) We proved this using the completeness theorem for tableaux, namely we showed how, from a closed tableau for ∼X, such a regular set could be found (we took R to be the set of all γ ⊃ γ(a) such that γ (a) was inferred from γ by Rule C, together with all sentences δ ⊃ δ (a) such that δ(a) was inferred from δ by Rule D). A much simpler proof of the Regularity Theorem is easily obtained from the existence of a complete regular set (which is also a magic set).
Problem 4. How?
II. Gentzen Sequents and Some Variants
In Part III of this chapter, we will state and prove an important result known as Craig’s Interpolation Lemma, which has significant applications that we will also consider. Craig’s original proof of this result was quite complicated, and many simpler proofs of this have subsequently been given. One of these which I will give uses a variant of some axiom systems due to Gerhard Gentzen [1934, 1935], to which we now turn.
Background
We first need to speak of subformulas. In propositional logic, the notion of immediate subformula is given explicitly by the following conditions:
I0: Propositional variables have no immediate subformulas.
I1: ∼X has X as its only immediate subformula.
I2: X ∧ Y, X ∨ Y, X ⊃ Y have X and Y as immediate subformulas, and no other immediate subformulas.
For first-order logic, the notion of immediate subformula is given explicitly by the following conditions (we recall that an atomic formula of first-order logic is a formula which contains no logical connectives or quantifiers):
I′0: Atomic formulas have no immediate subformulas.
I′1: Same as I1.
I′2: Same as I2.
I′3: For any parameter a and variable x, φ (a) is an immediate subformula of ∀ xφ (x) and of ∃xφ (x).
Both in propositional logic and first-order logic, the notion of subformula is implicitly defined by the following conditions:
S1: If Y is an immediate subformula of X, or is identical to X, then Y is a subformula of X.
S2: If Y is an immediate subformula of X, and Z is a subformula of Y, then Z is a subformula of X.
The above implicit definition can be made explicit as follows: Y is a subformula of X if and only if either Y = X or there is a finite sequence of formulas whose first term is X and whose last term is Y and is such that each term of the sequence other than the first is an immediate subformula of the preceding term.
Now, G. Gentzen was out to find finitary proofs of the consistency of various systems (finitary in the sense that the proofs did not involve the use of infinite sets). For this purpose, he needed a proof procedure that required that any proof of a formula X used only subformulas of X. Such a proof procedure is said to obey the subformula principle. Of course, tableaux using signed formulas obey the subformula principle (and these are closely related to the Gentzen systems, as we will see). Tableaux using unsigned formulas, almost, but not quite, obey this principle — they use either subformulas or negations of them. [For example ∼X is not necessarily a subformula of ∼(X ∨ Y), but is the negation of one.]
Gentzen took a new symbol “→” and defined a sequent to be an expression of the form θ → Γ, where each of θ, Γ is a finite, possibly empty, sequence of formulas. Such an expression is interpreted to mean that if all terms of are true, then at least one term of Γ is true. That is, under any valuation v, the sequent θ → Γ is true under v iff it is the case that if all terms of θ are true, then at least one term of is Γ true. A sequent is called a tautology if it is true under all Boolean valuations, and valid if it is true under all first-order valuations. If θ is a non-empty sequence X1, …, Xn and Γ is a non-empty sequence Y1, …, Yk, then the sequent X1, …, Xn → Y1, …, Yk is logically equivalent to the formula (X1 ∧ … ∧Xn) ⊃ (Y1∨ … ∨Yk). If θ is the empty sequence, then θ → Γ is written as , where  is the empty set, or more simply → Γ.
Now, if Γ is a non-empty sequence, then the sequent → Γ (which is ) is read “if all elements of the empty set are true, then at least one element of Γ is true.” Well, all elements of the empty set are true (since there are no elements of the empty set), and so the sequent → Γ is logically equivalent to the proposition that at least one element of Γ is true. Thus a sequent →Y1, …, Yk is logically equivalent to the formula Y1 ∨ … ∨Yk.
Now, suppose that the right side Γ of the sequent θ → Γ is empty. The sequent is then more simply written as θ →. It is interpreted to mean that if all the terms of the sequence θ are true, then at least one element of the empty set  is true. Well, it is obviously not the case that some element of the empty set is true, so that θ → expresses the proposition that it is not the case that all terms of the sequence θ are true. If θ is a non-empty sequence X1, …, Xn, then the sequent X1, …, Xn → is logically equivalent to the formula ∼ (X1 ∧ … ∧Xn).
Finally, the → written alone is a sequent, and abbreviates  → , where  is the empty set. What does → by itself mean? It means that if all elements of the empty set are true, then at least one element of the empty set is true. Well, all elements of the empty set are vacuously true, but is it obvious that it is not the case that at least one element set if true. Thus the sequent → is simply false (under all valuations).
At this point, I must tell you an amusing incident: I was once giving a lecture on Gentzen sequents to a mathematics group. I started with Gentzen sequents in which both sides of the arrow were non-empty, and after having erased, first the left side, and then the right side, I was left with just the arrow alone on the board. After explaining that it meant falsehood, the logician William Craig (the inventor of Craig’s Interpolation Lemma) was in the audience, and with his typical sense of humor, raised his hand and asked, “And what does it mean if you write nothing on the board?”
In more modern Gentzen type systems, the sequents are of the form U → V, where U and V are finite sets of formulas, instead of sequences of formulas, and this is the course we will take.
Gentzen Type Axiom Systems
We shall first consider a Gentzen type axiom system G0 for propositional logic. For any finite set S of formulas and any formulas X1, …, Xn, we abbreviate S ∪ {X1, …, Xn} by S, X1, …, Xn. Thus,

abbreviates

The system G0 for propositional logic has only one axiom scheme and the eight inference rules below. Here X and Y are formulas of propositional logic, and U and V are sets of formulas of propositional logic.
Axiom Scheme of the system G0

Rules of the system G0
Conjunction

Disjunction

Implication

Negation

The inference rules introduce the logical connectives. There are two rules for each connective — one to introduce the connective on the left side of the arrow, the other to introduce the connective on the right side of the arrow. As an example of how the rules are applied, the conjunction rule C1 is that from the sequent U, X, Y → V one can infer the sequent U, X, ∧ Y → V. As for Rule C2, it says that from the two sequents U → V, X and U → V, Y one can infer the sequent U → V, X ∧ Y.
It should be obvious that the axioms of G0 are tautologies, and we leave it to the reader to verify that the inference rules preserve tautologies, in other words, that in any application of an inference rule, the conclusion is truth-functionally implied by the premises. It therefore follows (by mathematical induction) that we have: All provable sequents of G0 are tautologies; thus the system G0 is correct.
We will soon prove that the system G0 is complete, i.e. that all tautologies are provable in G0.
Problem 5. Show that the sequent U, X → V is logically equivalent to U → V, ∼ X and that the sequent U → V, X is logically equivalent to U, ∼ X → V.
The System G0 in Uniform Notation
Let S be a non-empty set {TX1, …, TXn, FY1, …, FYk} of signed formulas. By |S| we shall mean the sequent X1, …, Xn → Y1, …, Yk. We call |S| the corresponding sequent of S. We leave it to the reader to verify the important fact that S is unsatisfiable if and only if its corresponding sequent |S| is a tautology (and, for later use when we get to Gentzen systems for first-order logic), that if the elements of S are signed formulas of first-order logic, then S is unsatisfiable iff |S| is valid.
Of course, if S is a set {FY1, …, FYk} in which there are no formulas signed T, we take |S| to be the sequent → Y1, …, Yk, and if S is a set {TX1, …, TXn} in which there are no formulas signed F, we take |S| to be the sequent X1, …, Xn →.
More generally, for any set of signed formulas, if U is the set of formulas X such that TX ∈ S, and V is the set of formulas Y such that FY ∈ S, then |S| is the sequent U → V.
The system G0 in uniform notation is the following:
Axioms
(where S is a set of signed formulas)

Inference Rules

A: (α,α1, and α2 are signed formulas)
B: (β, β1, and β2 are signed formulas)

Problem 6. Verify that the above uniform system really is the system G0.
Proofs in Gentzen type systems are usually in tree form, unlike the linear form of proofs in the axiom systems previously considered. Only now, unlike the trees of tableaux, the tree must grow upwards if the user wishes to prove a statement by analyzing it from the outside in (as must take place by the tableau rules): the origin is at the bottom and the end points are at the top. The origin is the sequent to be proved, and the end points are the axioms used in the proof. As an example, here is a proof in tree form of the sequent p ⊃ q → ∼q ⊃ ∼p.

Completeness of G0
We now wish to show that all tautologies are provable in the Gentzen system G0 for propositional logic.
By a tableau proof of a sequent X1, …, Xn → Y1, …, Yk is meant a closed tableau proof showing that the set {TX1, …, TXn, FY1, …, FYk} is unsatisfiable (if the tableau is started with all these formulas, the tableau can be extended by the tableau rules for propositional logic in such a way that every branch closes). We will now show how, from a tableau proof of a sequent, we can obtain a proof of the sequent in the Gentzen system G0. To this end, it will be useful to first introduce, as an intermediary, another type of tableau that I will now call a block tableau (this was called a modified block tableau in my book First-Order Logic [Smullyan, 1968, 1995]). The analytic tableaux, which we have studied so far, are variants of the tableaux of Evert Beth [1959], whereas the block tableaux, to which we now turn, are variants of the tableaux of J. Hintikka [1955]. Like the tableaux of Hintikka, the points of the tree in a block tableau proof (this time again with the origin at the top, as in analytic tableaux) are not single formulas, but are finite sets of formulas, which we call the blocks of the tableaux. And what we do at any stage of the construction depends solely on the end points of the tree (rather than on the branch as a whole, as in analytic tableaux).
We will first consider block tableaux for propositional logic. By a block tableau for a finite set K of signed formulas of propositional logic, we mean a tree constructed by placing the set K at the origin, and then continuing according to the following rules (here, for a set S and formulas X, Y, we abbreviate the sets S ∪ {X} and S ∪ {X} ∪ {Y} by {S, X} or {S, X, Y} respectively):
Block Tableau Inference Rules for Propositional Logic
A′:

B′:

In words, these rules are:
A′: To any end point {S,α} on the tree, we subjoin {S,α1, α2} as sole successor.
B′: To any end point {S,β} on the tree, we simultaneously subjoin {S,β1} as left successor and {S,β2} as right successor.
We call a block tableau closed if each end point contains some element and its conjugate.
Changing from an analytic tableau to a block tableau is a relatively simple matter. To begin with, in constructing an analytic tableau, when using Rule A, and inferring α1 from α, subjoin α2 immediately after it. That is, use Rule A in the form:

Now, to start a block tableau for a sequent X1, …, Xn → Y1, …, Yk, put the set {TX1, …, TXn, FY1, …, FYk} at the origin, for we wish to prove that set of formulas is unsatisfiable. This concludes stage 0 of the construction of the block tableau corresponding to the analytic tableau that begins as follows:

Then, for each n, after completion of stage n for the two tableaux, when we use Rule A (Rule B) in the analytic tableau, we use Rule A′ (Rule B′, respectively) in the block tableau, and that concludes stage n + 1 of the constructions. We continue this until both tableaux close (which must happen at some stage, if the set of formulas {TX1, …, TXn, FY1, …, FYk} is unsatisfiable, i.e. the sequent is a tautology). Since we know that the tableau method for propositional logic is complete, this construction shows that the block tableau method for propositional logic is also complete.
As an example, here is a closed analytic tableau for the sequent p ⊃ q, q ⊃ r → p ⊃ r:

Here is the corresponding block tableau:

To convert a block tableau to a proof in the Gentzen system G0, just replace each block

by the sequent X1, …, Xn → Y1, …, Yk, and the resulting tree, when turned upside down, is a proof in the Gentzen system.
For example, from the above closed block tableau, we get the following proof in the Gentzen system:

Remarks. It is not surprising that this conversion of a block tableau to a proof in the Gentzen system works, because the block tableau rules are essentially the rules of the Gentzen system G0 turned upside down. That is, in Rule A′ of block tableaux

if we replace the premise and conclusion by the corresponding sequents, we get the rule

which is Rule A of the system G0 turned upside down. Likewise, if in Rule B′ of block tableaux

we replace the premise and conclusions by the corresponding sequents, we get the rule

which is Rule B of the system G0 turned upside down.
A Gentzen Type System G1 for First-Order Logic
For the Gentzen system G1, we take the axioms and inference rules of the system G0 (only now applied to formulas of first-order logic instead of formulas of propositional logic), and add the following inference rules:


One proves the completeness of G1 from the completeness of first-order block tableau in essentially the same manner as in propositional logic. One first makes a block tableau, whose rules are those of G0 (with sequents replaced by sets of signed formulas and turned upside down) together with:

One then converts a closed block tableau into a proof in the Gentzen system G1 as before, i.e. one replaces each block {S} by the sequent |S| and turns the resulting tree upside down.
The System GG
In some of the inference rules of the systems G0 and G1 (specifically the negation and implication rules), one transfers a formula from one side of the arrow to the other (or at least incorporates it into some formula on the other side). We need a modification of the system in which this does not happen. This modification, which we will call GG, has a certain symmetry with respect to the two sides of the arrow, and was called a symmetric Gentzen system in Smullyan [1968, 1995].
Here is the system GG, which is admirably suited to uniform notation (since we are dealing with sequents here, all formulas refer to unsigned sentences):

(A)
(B)
(C)
(D)
The Completeness of GG
We need another variant of analytic tableaux to prove the completeness of GG. In some of the rules of analytic tableaux, a conclusion of a rule has a different sign than that of the premise (which reflects the fact that in the corresponding Gentzen system, a sentence might change from one side of the arrow to the other). We now need an analytic tableau system in which this does not happen.
By an altered tableau we mean one constructed according to the following rules:



In the quantification rules (above and below), the asterisk “*” means that the parameter in the conclusion must not appear in the premise.
Here are the rules of the system altered tableaux in uniform notation (where the α, β, γ and δ are of course unsigned formulas):

For altered tableaux, a set of signed formulas is called closed if it contains either some TX and FX or some TX and T∼X or some FX and F∼X. A branch is called closed if the set of sentences on the branch is a closed set, and an altered tableau is called closed if every branch is closed.
We will need to know that the altered tableau method is both correct and complete, i.e. that there is a closed altered tableau for a set S if and only if S is unsatisfiable. To this end, we can save half the labor by the following notation, which might aptly be dubbed a super-unifying notation.
We let A be any signed sentence of the form Tα or Fβ. If it is the former, we let A1 = Tα1 and let A2 = Tα2; if the latter, we let A1 = Fβ1 and let A2 = Fβ2. We let B be any sentence of the form Tβ or Fα. If it is the former, we let B1, B2 be Tβ1 and Tβ2 respectively; if the latter, we let B1, B2 be Fα1 and Fα2, respectively. We let C be any sentence of the form Tγ or Fδ, and we take C(a) to be Tγ(a), Fδ(a), respectively. We let D be any sentence of the form Tδ or Fγ, and we take D(a) to be Tδ(a), Fγ(a), respectively. With this notation, the altered tableau rules for first-order logic are now most succinctly expressed as follows:

Problem 7. Prove that the altered tableau system is correct and complete, and then finish the proof of the completeness of the Gentzen system GG.
III.Craig’s Lemma and an Application
Craig’s Interpolation Lemma
A sentence Z of first-order logic is called an interpolant for a sentence X ⊃ Y if X ⊃ Z and Z ⊃ Y are both valid and every predicate and parameter of Z occurs in both X and Y. Craig’s celebrated Interpolation Lemma (often simply called Craig’s Lemma) is that if X ⊃ Y is valid, then there is an interpolant for it, provided that Y alone is not valid and X alone is not unsatisfiable [Craig, 1957].
If we allow t’s and f’s to be part of our formal language, and define “formula” accordingly, then the proviso above is unnecessary, because if X ⊃ Y is valid and Y alone is valid, then t is an interpolant for X ⊃ Y, or if X alone is unsatisfiable, then f is an interpolant for X ⊃ Y (as the reader can verify). This is the course we shall take.
Formulas that involve t or f will temporarily be called non-standard formulas. As shown in The Beginner’s Guide [Smullyan, 2014], any non-standard formula is logically equivalent to either a standard formula or to t or to f.
There is a corresponding interpolation lemma for propositional logic. For propositional implication X ⊃ Y, a propositional formula Z is called an interpolant for X ⊃ Y if X ⊃ Z and Z ⊃ Y, and if every propositional variable in Z occurs in both X and Y. Craig’s Lemma for proposition logic is that if X ⊃ Y is a tautology, it has an interpolant.
Returning to first-order logic, a formula Z is called an interpolant for a sequent U → V, if U → Z and Z → V are both valid, and if every predicate and parameter of Z occurs in at least one element of U and at least one element of V. Obviously, Z is an interpolant for a sequent

if and only if Z is an interpolant for the formula

so that the existence of interpolants for all valid sequents is equivalent to the existence of interpolants for all valid sentences X ⊃ Y. Thus we will consider Craig’s Lemma for First-Order Logic in the equivalent form that there exist interpolants for all valid sequents.
We will prove Craig’s Lemma for the Gentzen system GG.
Proof of Craig’s Interpolation Lemma for the Gentzen System GG
It suffices to show that there is an interpolant for each of the axioms of GG, and that for each of the inference rules of GG, if there is an interpolant for the premise, or interpolants for the two premises, then there in an interpolant for the conclusion.
Exercise. Prove this for the axioms and for Rules A and B (which don’t involve the quantifiers).
Proving the claim of the exercise really proves Craig’s Interpolation Lemma for propositional logic. Now let’s turn to Rules C and D, which do involve the quantifiers.
The proof for Rule D is relatively simple. Suppose that X is an interpolant for U, δ(a) → V, and that a does not occur in U, δ or V. Then X must also be an interpolant for U, δ → V for the following reasons:
Since X is an interpolant of U, δ(a) → V, then every parameter of X occurs in V (as well as in U, δ(a)). But since a does not occur in V (by hypothesis), then a cannot occur in X. From this, it follows that all parameters of X occur in U, δ (as well as in V).
To see this, let b be any parameter that occurs in X. We know that b ≠ a. We also know that b occurs in V (all parameters of X do). It remains to show that b occurs in U, δ. If b occurs in U, we are done. If not, then b must occur in δ(a) (since b occurs in U, δ(a)). But the parameters of δ are those of δ(a) other than a. Since b occurs in δ(a) and b ≠ a, then b must occur in δ. This proves that all parameters of X occur in U, δ (as well as in V). Also all predicates of X occur in U, δ and in V since they all occur in U, δ(a) and in V, and the predicates of δ are the same as those of δ(a).
Finally, since U, δ(a) → X is valid (as well as X → V), and a does not occur in U, δ or in X, then U,δ → X is valid (in fact it follows from U, δ(a) → X by Rule D). Thus X is an interpolant for the sequent U, δ → V.
The proof that if X is an interpolant for U → V, γ(a), and a does not occur in U or in V, γ (a), then X is an interpolant for U → V, γ is similar to the preceding proof, and is left to the reader. This takes care of Rule D.
The proof for Rule C is much trickier! Suppose that X is an interpolant for U, γ(a) → V. Then U, γ(a) → X and X → V are of course valid, but X may fail to be an interpolant for U, γ → V because the parameter a might occur in X, but not in U, γ. If a does not occur in X, or if a does occur in U, γ, there is no problem. The critical case is one in which a does occur in X, but not in U, γ. For this case, we find another interpolant for U, γ → V as follows:
We take some variable x that does not occur in X and we let φ(x) be the result of replacing every occurrence of a in X by x. We note that φ(a) is X itself. We will show that ∀ xφ(x) is an interpolant for U, γ → V.
Let γ′ = ∀xφ(x). Then γ′(a) = φ(a), which is X. We already know that U, γ → γ′(a) is valid (because it is U, γ → X), and since a does not occur in U, γ, and obviously not in γ′ (which is ∀xφ(x)), then U, γ → γ′ is valid (it is in fact derivable from U, γ → γ′(a) by Rule D of the system GG). Also γ′(a) → V (which is X → V) is valid by hypothesis. Hence γ′ → V is valid (by Rule C, taking U to be the empty set). Thus U, γ → γ′ and γ′ → V are both valid, and all predicates and parameters of γ′ are both in U, γ and in V (since a does not occur in γ′). Thus ∀xφ(x), which is γ′, is an interpolant for U, γ → V. This completes the proof for the γ rule of Rule C.
As to the δ rule of Rule C, a similar proof, which we leave to the reader, reveals the following: Suppose X is an interpolant for U → V, δ (a). Then if a does not occur in X, or a does occur in V, δ, then X is an interpolant for U → V, δ. But if a does occur in X but not in V, δ, then an interpolant for U → V, δ is ∃xφ(x), where φ(x) is the result of replacing every occurrence of a in X by a variable x that does not occur in X.
This concludes the proof of Craig’s Interpolation Lemma.
Beth’s Definability Theorem
One important application of Craig’s Interpolation Lemma is that it provides a neat proof of an important result of the Dutch logician E. Beth [1959]. While discussing Beth’s Definability Theorem, I will switch to the language of first-order interpretations that we used frequently in The Beginner’s Guide [Smullyan, 2014] rather than that of first-order valuations, with which we have spent more time here.
We consider a finite set S of sentences without parameters and involving only predicates P, P1, … Pn of degree 1. For any sentence X whose predicates are among P, P1, …, Pn, as usual we write S ⊢ X to mean that X is a logical consequence of S (i.e. is true in all interpretations that satisfy S). We say that P is explicitly definable from P1, …, Pn with respect to S iff there is a formula φ(x) whose predicates are among P1, …, Pn (and hence do not involve P) such that S ⊢ ∀x(P x ≡ φ(X)). Such a formula φ(x) is called an explicit definition of P from P1, …, Pn with respect to S.
There is another type of definability called implicit definability, which consists of the following: We take a new predicate P′, distinct from any of P, P1, …, Pn and which does not occur in any element of S, and we let S′ be the result of replacing P by P′ in every sentence in S. Then P is said to be implicitly definable from P1, …, Pn with respect to S iff

When this is true, we also say that the sentences of S implicitly define the predicate P.
We remark that this condition is equivalent to the condition that for any two interpretations that satisfy S, if they agree on each of P1, …, Pn, then they agree on P. (Two interpretations I1 and I2 are said to agree on a predicate Q iff they assign the same value to Q.) This equivalence is not necessary for the proof of Beth’s theorem, but is of independent interest. A proof of this can be found in my book Logical Labyrinths [Smullyan, 2008, 2009, pp. 294–295].
It is relatively easy to show that if P is explicitly definable from P1, …, Pn with respect to S, then it is implicitly so definable.
Problem 8. Show that if P is explicitly definable from P1, …, Pn with respect to S, then it is implicitly so definable.
Theorem B. [Beth’s Definability Theorem] If P is implicitly definable from P1, …, Pn with respect to S, then it is explicitly so definable.
This theorem is far from obvious, but Craig’s Lemma provides a neat and elegant proof of it.
Suppose that P is implicitly definable from P1, …, Pn with respect to S. Let P′ and S′ be as previously defined, and assume we have

Let X be the conjunction of the elements of S (the order does not matter), and let X′ be the conjunction of the elements of S′. We thus have

Since the above sentence is valid, then for any parameter a the sentence (X ∧ X′) ⊃ (Pa ≡ P′a) is valid. [We recall that S is a set of sentences without parameters because that is one of the conditions on both explicit and implicit definitions. Thus the sentence X ∧ X′ has no parameters.] Hence by propositional logic, the sentence (X ∧ X′)⊃ (Pa ⊃ P′a), is valid. Thus, again by propositional logic, so is the following sentence:

In this sentence, P does not occur in X′ ⊃ P′a and P ′ does not occur in X ∧ Pa. By Craig’s Lemma, there is an interpolant Z for the sentence (X ∧ Pa)⊃ (X′ ⊃ P′a). All predicates and parameters of Z occur in both X ∧ Pa and in X′ ⊃ P′a. Hence neither P nor P′ can occur in Z; in fact all the predicates of Z are among P1, …, Pn. Also,Z contains no parameters other than a, since X and X′ contain no parameters, as noted above. Let x be a variable that does not occur in Z, and let φ(x) be the result of substituting x for all occurrences of a in Z. But φ(a) = Z, so φ(a) is an interpolant for (X ∧ Pa)⊃ (X′ ⊃ P′a). Hence the following two sentences are valid:
(1) (X ∧ Pa) ⊃ φ(a).
(2) φ(a) ⊃(X′ ⊃ P′a).
From (1), we get the valid sentence:
(1′) X ⊃ (Pa ⊃ φ(a)).
From (2), we get the valid sentence:
(2′) X′ ⊃ (φ(a) ⊃ P′(a)).
Since (2′) is valid, so is its notational variant:
(2″) X ⊃ (φ(a) ⊃ P(a)).
[If a sentence is valid, it remains valid if we replace any predicate by a new predicate.]
By (1′) and (2″) we get the valid sentence:

Since a does not occur in X, the sentence  is valid. Since X is the conjunction of the elements of S, it then follows that  and so the formula φ(x) explicitly defines P from P1, …, Pn with respect to S.
This concludes the proof of Beth’s Definability Theorem.
Note: The fact that S was assumed finite was not really necessary for the proof. A compactness argument can be used to modify the proof for an infinite set.
IV.A Unification
Let us review some of the main results that we have proved about first-order logic in The Beginner’s Guide [Smullyan, 2014] and in this chapter.
T1: The Completeness Theorem for Analytic Tableaux. Every valid formula is provable by the tableau method.
T2: The Skolem–Löwenheim Theorem. For any set S of formulas, if S is satisfiable, then it is satisfiable in a denumerable domain.
T3: The Compactness Theorem. For any infinite set S of formulas, if every finite subset of S is satisfiable, so is S.
T4: The Completeness of a First-Order Axiom System.
T5: The Regularity Theorem. For every valid pure sentence X, there is a (finite) regular set R which truth-functionally implies X.
T6: Craig’s Interpolation Lemma. If the sentence X ⊃ Y (of propositional or first-order logic) is valid, there is a sentence Z such that X ⊃ Z and Z ⊃ Y are both valid and, for propositional logic, every propositional variable occurring in Z occurs in both X and Y, while, for first-order logic, every predicate and parameter of Z occurs in both X and Y.
In my paper “A Unifying Principle in Quantification Theory” [Smullyan, 1963], I stated and proved a result that yields all of T1 − T6 as special cases. To this we now turn. What follows is slightly different from the original formulation of 1963.
Consider any property ϕ of sentences. A set of sentences that has property ϕ will be called ϕ-consistent (i.e. “consistent” with having the property ϕ).
We are now going to be especially interested in certain sets of sentences that have what we will call an analytic consistency property, and we will use the Greek symbol Γ to signal that a property of sets is an analytic consistency property. Thus we define a set S of sentences to be Γ-consistent — which means the same as saying that the set S has the analytic consistency property Γ — if the following conditions hold:
Γ0: S contains no element and its negation (or no element and its conjugate, if we are considering signed formulas).
Γ1: For any α in S, the sets S ∪ {α1} and S ∪ { α2} are both Γ-consistent (and hence so is S ∪{υ1, υ2}).
Γ2: For any β in S, at least one of the sets S ∪ {β1} and S ∪ {β2}is Γ-consistent.
Γ3: For any γ in S and any parameter a, the set S Γ {γ(a)} is Γ-consistent.
Γ4: For any δ in S, the set S ∪ {δ(a)} is Γ-consistent for any parameter a that does not occur in any element of S.
Theorem U. [The Unification Theorem] For any analytic consistency property Γ, if S is Γ-consistent, then S is satisfiable in the domain of the parameters.
One proof of Theorem U is quite similar to the completeness proof for tableaux: Given a Γ-consistent set S, it easily follows that no tableau for S can close (verify this!). Hence a systematic tableau for S will contain an open branch that includes all elements of S, and is a Hintikka set, and thus satisfiable in the domain of the parameters.
Discussion. We proved Theorem U using analytic tableaux, although the theorem makes no reference to tableaux. My 1963 proof used neither tableaux nor König’s lemma, and is as follows: We shall consider now only the case that S is denumerable (the construction for a finite set S is even simpler, and the modification is left to the reader). We shall also assume that only finitely many parameters are found in the formulas of S (if not, we can always introduce a new denumerable group of parameters, and use them along with the parameters in the formulas of S, in the construction ahead).
In what follows, consistent will mean Γ-consistent. Given a consistent denumerable set of sentences S, the idea is to define an infinite sequence of formulas whose set of terms is a Hintikka set that contains all the elements of S. The sequence is defined in stages: at each stage we have a finite segment (X1, …, Xk) of our desired infinite sequence, and at the next stage, we extend that sequence to a larger finite sequence (X1, …, Xk, …, Xk+m).
First some definitions and notations: Let θ be a finite sequence (X1, …, Xk). We shall say that θ is consistent with S when the set S ∪{X1, …, Xk} is consistent. For any formulas Y1, …, Ym, by the expression θ, Y1, …, Ym is meant the sequence (X1, …, Xk, Y1, …, Ym).
Given a consistent denumerable set S, let s1, …, sn, … be an enumeration of S. We let θ1 be the unit sequence (s1). This sequence is obviously consistent with S, since s1 ∊ S. This concludes the first stage.
Now suppose we have completed the nth stage and have on hand a sequence (X1, …, Xk), where k ≥ n. We call this sequence θn, and we assume it to be consistent with S. We then extend θn to a finite sequence θn+1 in a manner depending on the nature of Xn:
(a)If Xn is an α, we take θn+1 to be θn, α1, α2, sn+1.
(b)If Xn is an β, then either θn, β1 or θn, β2 is consistent with S. If the former we take βn+1 to be θn, β1, sn+1; if the latter, we take θn+1 to be θn, β2, sn+1.
(c)If Xn is a γ, we let a be the first parameter for which γ(a) is not a term of θn, and we take θn+1 to be θn,γ(a), γ, sn+1.
(d)If Xn is a δ, we take θn+1 to be θn, δ(a), sn+1, where a is a parameter new to S and new to θn.
This concludes stage n + 1 of the sequence.
Since for each n the sequence θn is consistent with S, then no formula and its conjugate (or negation, if we are dealing with unsigned formulas) are terms of θn. Hence no formula and its conjugate (or negation) are terms of θ. Also it is obvious from the construction that if α is a term of θ, so are α1 and α2; and if β is a term of θ, so is either β1 or β2; and if γ is a term of θ, then so is γ(a) for every parameter a (since we encounter γ denumerably many times); and if δ is a term of θ, then so is δ(a) for some parameter a. Thus the set of terms of θ is a Hintikka set and includes all elements of S, and so S is denumerably satisfiable in the domain of the parameters.
This construction makes no explicit reference to trees, although θ is in fact the leftmost open branch of a systematic tableau for S. This proof does not use König’s Lemma.
Applications: We will provide a sketch of the application of Γ-consistency in proofs of the important results listed above. The details will be left to the reader.
T1: (Completeness of the tableau method).
Call a set S tableau-consistent if there exists no closed tableau for S. It is easily seen that tableau-consistency is an analytic consistency property. Hence by the Unification Theorem, every tableau-consistent set is satisfiable. Therefore, if a set S is unsatisfiable, there must be a closed tableau for S. Now, if a formula X is valid, then the set {∼X} is unsatisfiable, so that there is a closed tableau for ∼X, which means that X is tableau provable.
T2: (The Skolem-Löwenheim Theorem).
Satisfiability itself is easily seen to be an analytic consistency property. Hence by Theorem U, every satisfiable set is satisfiable in the denumerable domain of the parameters.
T3: (The Compactness Theorem).
Call a set S F-consistent if all finite subsets of S are satisfiable. F-consistency is easily seen to be an analytic consistency property. Hence, by Theorem U, if all finite subsets of S are satisfiable, then S is satisfiable.
T4: (Completeness of an axiom system for first-order logic).
Give an axiom system for first-order logic, call a set S consistent (with respect to the system) if no element of S is refutable, and no finite set {X1, …, Xn} of elements of S are such that the conjunction X1∧ … ∧ Xn is refutable. For the axiom system given in The Beginner’s Guide [Smullyan, 2014], or for any other standard system, this consistency is easily shown to be an analytic consistency property. Hence every consistent set is satisfiable and every unsatisfiable set is inconsistent. If now X is valid, then ∽X is unsatisfiable, and hence inconsistent. This means that ∽X is refutable, i.e. (∽∽X) is provable, and so is X. Thus every valid formula is provable, and the system is complete.
T5: (The Regularity Theorem).
Let us review some definitions and facts: We recall that a (finite) regular set R means a set whose members can be arranged in a sequence

where for each i ≤ n, Qi is either a γ or a δ, and if Qi is a δ, the parameter ai does not occur in δ, nor in any earlier term of the sequence. Such a parameter (occurring in a Qi(ai) where Qi is a δ in a regular set R) is called a critical parameter of R.
We recall that a set S is said to tautologically imply a formula X if X is true in all Boolean valuations that satisfy S. For S a finite set {X1, …, Xn}, this condition is equivalent to the condition that the sentence (X1 ∧ …∧ Xn) ⊃ X is a tautology (or, in the case that n = 1, that X1 ⊃ X is a tautology). A finite set {X1, …, Xn} is said to be truth-functionally unsatisfiable if the sentence ∽(X1 ∧ … ∧ Xn) is a tautology.
The Regularity Theorem is that for every valid sentence X, there is a regular set R that tautologically implies X, and which is such that no critical parameter of R occurs in X.
For any finite set S (of sentences), an associate of S is a regular set R such that R ∪ S is truth-functionally unsatisfiable, and no critical parameter of R occurs in any element of S. Now the Regularity Theorem is equivalent to the proposition that every unsatisfiable (finite) set S has an associate.
Problem 9. Show that the Regularity Theorem is equivalent to the proposition that every unsatisfiable (finite) set S has an associate.
Let us now call a finite set S A-consistent if S has no associate. It can be shown that A-consistency is an analytic consistency property. Hence by Theorem U, for any finite set S, if S has no associate, then S is satisfiable. Hence if S is unsatisfiable, then S has an associate, which proves the Regularity Theorem [by Problem 8].
T6: (Craig’s Interpolation Lemma).
By a partition S1|S2 of a set S we mean a pair of subsets S1 and S2 of S such that every element of S is in either S1 or S2, but not in both; in other words S1 ∪ S2 = S and 
By a partition interpolant between S1 and S2 we mean a sentence Z such that all predicates and parameters of Z occur in both S1 and S2, and S1 ∪ {∽Z} and S2 ∪ {Z} are both unsatisfiable. Now, call a set S Craig-consistent if there is a partition S1|S2 of S such that there is no partition interpolant between S1 and S2. Well, Craig-consistency can be shown to be an analytic consistency property. Hence by Theorem U, every Craig-consistent set is satisfiable. Therefore if S is unsatisfiable, then for every partition S1|S2 of S, there is a partition interpolant between S1 and S2.
Now, suppose X ⊃ Y is valid. Then the set {X,∽ Y} is unsatisfiable. Consider the partition {X}|{∽Y}. Then there is a partition interpolant Z between {X} and {∽Y }. Thus {X, ∽Z} (which is {X} ∪ {∽Z}) and {Z, ∽Y } (which is {∽Y } ∪ {Z}) are both unsatisfiable, hence X ⊃ Z and Z ⊃ Y are both valid, and of course every predicate and parameter of Z are in both X and Y. Thus Z is an interpolant of X ⊃ Y.
Grand Exercise. Verify that the six properties which I claimed to be analytic consistency properties really are so.
V.A Henkin-Style Completeness Proof
Adolf Lindenbaum (a Polish logician born in 1904 and killed by the Nazis in 1941), proved in the 1920s that every consistent set could be extended to a maximally consistent set, a result that afterwards became known as Lindenbaum’s Lemma. Leon Henkin [1949] published a proof of the completeness of various axiom systems for first-order logic by a method which is completely different from the ones we have so far considered, and which incorporates the method Lindenbaum used to prove his lemma.
Henkin’s method combines maximal consistency with another condition: A set S of sentences is called E-complete (existentially complete) if for any element δ in S, there is at least one parameter a such that δ(a) ∊ S. Henkin showed that every consistent set S can be extended to a set M that is both maximally consistent and E-complete, and that every such set M is a (first-order) truth set. We will see now that Henkin’s method applies to a more general situation.
Recall from Section IV that we are using the phrase “the set S of sentences is Γ-consistent” to mean that S has the property Γ and that Γ is an analytic consistency property. We define a property Δ of sentences to be a synthetic consistency property, if it is an analytic consistency property and satisfies the additional condition that for any set S having property Δ, and any sentence X, either S ∪ {X} has the property Δ, or S ∪ {∽X} has the property Δ. We will call a set Δ-consistent if it has a particular synthetic consistency property Δ. So in what follows “ Γ ” will be used to designate any analytic consistency property, and “Δ” will be used to designate any synthetic consistency property (but remember that any synthetic consistency property is also an analytic consistency property).
A property P of sets is said to be of finite character if for any set S, S has the property P if and only if all finite subsets of S have property P.
Henkin’s completeness proof for standard axiom systems for first-order logic goes through for any synthetic consistency property Δ of finite character. In this more general form its statement is the following: for any synthetic consistency property Δ of finite character, any Δ-consistent set S can be extended to a maximally Δ-consistent set M, and this can be done in such a way that the set M will be a first-order truth set.
We will show a suitable modification of Henkin’s proof for Δ-consistent properties (synthetic consistency properties) of finite character yields the following Henkin-style proof for Γ-consistent properties of finite character, i.e. for any analytic consistency property of finite character).
So, consider any analytic consistency property Γ that is also of finite character. We will show that if S is Γ-consistent, then S can be extended to a set M which is both maximally Γ-consistent and E-complete, and that the set M obtained in this way is a Hintikka set, and hence satisfiable (so that M ’s subset S is also satisfiable).
Problem 10. Show that if M is both maximally Γ-consistent and E-complete, then M is a Hintikka set.
Note: As previously remarked, for any synthetic consistency property Δ of finite character, if M is both maximally Δ-consistent and E-complete, then M is not only a Hintikka set, but even a truth set. Indeed, more generally, any Hintikka set S having the property that for every X, either X ∊ S or (∽ X) ∊ S, must be a truth set. We leave the proof of this as an exercise for the reader.
Now, given an analytic consistency property Γ of finite character, how do we actually go about extending a given Γ-consistent set S of finite character to a set M which is both maximally Γ-consistent and E-complete? (As in our proof of Theorem U above, we will assume that the number of parameters in S is finite, or that we have an additional denumerable number of parameters available to us, none of which occur in any formula of S, i.e. parameters that we can just add to the parameters in S if necessary.)
Here is Lindenbaum’s method of extending a consistent set S to a maximally consistent set M (assuming consistency to be of finite character), which we will extend from the conventional meaning of consistent to our meaning of Γ-consistent (i.e. analytically consistent): We first arrange all the sentences of first-order logic in some infinite sequence X1, X2 …, Xn, Xn+1, …. Now consider a Γ-consistent set S. We construct an infinite sequence of sets S0, S1, …, Sn, Sn+1, … as follows: We take S0 to be S. Then, for every positive n, assuming Sn already defined, we take Sn+1 to be Sn if Sn ∪ {Xn} is not consistent; while if Sn ∪ {Xn} is consistent, and if Xn is not some δ, then we take Sn+1 to be Sn ∪ {Xn}. But if Sn ∪ {Xn} is consistent and Xn is some δ, then Sn ∪ { δ, δ(a)} is also consistent for any parameter a new to Sn ∪ {δ} (since we are dealing with an analytic consistency property). In this case we take Sn+1 to be Sn ∪ {δ, δ(a)}, where a is a parameter new to Sn and to δ. The set M that results from taking the union of all the sets S0, S1, …, Sn, Sn+1, … is both maximally consistent and E-complete. For it is easily seen to be E-complete by construction (realize that at every stage of the construction, there will always be a new parameter available). And the union M of all the consistent sets Sn is consistent because we are assuming our consistency property to be of finite character. And it is maximally consistent because any formula not already in it was kept out of M because adding it would have made a finite subset of M inconsistent.
Immediately after proving that every consistent set S can be extended to a set M that is both maximally consistent and E-complete, Henkin stated as a corollary the completeness of first-order logic, that is, that every valid formula was provable. But he did use a well-known property of the axiom systems he was considering that does not apply to the slightly different formalization of first-order logic in this volume (his systems included the constants t and f, which we did discuss in the context of propositional logic in The Beginner’s Guide [Smullyan, 2014], and which can also be employed in first-order logic).
Now, we might try to show the completeness of first-order logic similarly to the way Henkin did by first showing that ordinary consistency is an analytic consistency property (a common definition of an individual sentence X being consistent would be that there exists a formula Z such that X ⊃ Z is not provable). But far easier for us is to just remind ourselves that in Section IV of this chapter (the Unification section), we showed that both the first-order tableau method of proof and the standard first-order axiomatic systems of proof are complete by defining the appropriate analytic consistency property for each of them.
We now see that there are basically two types of completeness proofs directly applicable to the usual formulations of First-Order Logic. One is along the lines of Lindenbaum and Henkin, which extends a consistent set directly to a truth set, and for this we need the full force of synthetic consistency properties. The second type of completeness proof (which is along the lines of Gödel, Herbrand, Gentzen, Beth, and Hintikka) extends a consistent set, not directly to a truth set, but to a Hintikka set, which can be further extended to a truth set. And for the second type of proof, we do not need synthetic consistency, but only analytic consistency.
Solutions to the Problems of Chapter 2

1.We are given that v is a Boolean valuation.
(1)We are given that if γ is true under v, then so is γ(a) for every parameter a. In addition, we must prove the converse, i.e. that if γ(a) is true under v for every parameter a, then γ is true under v. We shall prove the equivalent proposition that if γ is false under v, then so is γ(a) for at least one parameter a.
We first consider the case that γ is of the form . Now suppose that  is false under v. Then  is true under v [since v is a Boolean valuation]. Hence by (2) of the lemma, there is at least one parameter a such that ∼φ(a) is true under v. Hence φ(a) is false under v, which is what was to be shown [since if γ is , γ(a) is φ(a)].
Now consider the case that γ is of the form ∼∃xφ(x). Well, suppose that ∼∃xφ(x) is false under v. This means that ∃xφ(x) is true under v [since v is a Boolean valuation]. Then by F2, φ(a) is true for at least one parameter a. But then ∼φ(a) is false for that parameter. And that is what we needed to prove, since when the γ formula is of the form ∼∃xφ(x), γ(a) is ∼φ(a).
(2)We are given that if δ is true under v, then so is δ(a) for at least one parameter a. Again we must prove the converse, i.e. that if δ(a) is true for at least one parameter a, so is δ. We will prove the equivalent proposition that if δ is false, then so is δ(a) for every parameter a.
We first consider the case that δ is of the form ∃xφ(x). So suppose ∃xφ(x) is false. Then ∼∃xφ(x) is true under v [since v is a Boolean valuation]. Then by (1) of the Lemma, ∼φ(a) is true for every parameter a. Hence φ(a) is false for every parameter a, which was what had to be proved [since φ(a) is δ(a) when δ is of the form suppose ∃xφ(x)].
Now consider the case that δ is of the form . Well, suppose  is false. Then  is true [since v is a Boolean valuation]. Then by F1, φ(a) is true for every parameter a. This means that, ∼φ(a) is false for every parameter a [since v is a Boolean valuation], which is what had to be proved [since when δ is of the form , δ (a) is of the form ∼φ(a)].
2.We must show that every magic set M is a truth-functional basis for first-order logic. Thus we must show that if M is a magic set and X is a pure sentence, then X is valid if and only if X is tautologically implied by some finite subset M0 of M. We assume M is a magic set.
(a)Suppose that X is a valid pure sentence. Then X is true under all first-order valuations, and thus also true under all Boolean valuations that satisfy M (because all such valuations are also first-order valuations by condition M1 in the definition of a magic set). Thus X is tautologically implied by M [by the definition of tautological implication], and hence by some finite subset M0 of M [by Problem 8 of Chapter 6 of The Beginner’s Guide [Smullyan, 2014]].
(b)Conversely, suppose X is a pure sentence that is tautologically implied by some finite subset M0 of M. Then the set M0∪{∼X} is not truth-functionally satisfiable (i.e. not satisfied by any Boolean valuation), so that M0∪{∼X} is not first-order satisfiable (since every first-order valuation is also a Boolean valuation). If ∼X were first-order satisfiable, then M0 ∪ {∼X} would also be first-order satisfiable [by M2], but since it isn’t, then ∼X is not first-order satisfiable, which means that X is valid.

This completes the proof.
3.We must show that if M is a complete regular set of sentences, then M is a magic set. So let M be a complete regular set of sentences.
Proof of M1: We must show that any Boolean valuation that satisfies M is also a first-order valuation. So suppose v is a Boolean valuation that satisfies M. By Proposition 1, to show that v is a first-order valuation, it suffices to show that if γ is true under v, so is γ(a) for every parameter a, and that if δ is true under v, then so is δ(a) for at least one parameter a. Well, suppose γ is true under v. Since M is complete, γ ⊃ γ(a) is in M for every parameter a. Thus γ ⊃ γ (a) is true under v (since every member of M is true under v). Then γ (a) must also be true under v (since v is a Boolean valuation).
Now suppose δ is true under v. Since M is complete, δ ⊃ δ (a) is in M for some parameter a. Thus δ ⊃ δ (a) if true under v (since every member of M is true under v). Then δ (a) must also be true under v (since v is a Boolean valuation).
Proof of M2: We must show that for every finite set S of pure sentences, and every finite subset M0 of M, if S is first-order satisfiable, so is S ∪ M0.
We first show that for any finite set S of sentences (possibly with parameters), if S is (first-order) satisfiable, then:

(1)S ∪ {γ ⊃ γ(a)} is satisfiable, for any γ and any a;
(2)S ∪ {δ ⊃ δ (a)} is satisfiable, for any δ and any a that does not occur in δ or in any element of S.

Now (1) is obvious, since γ ⊃ γ(a) is valid, hence true under all interpretations.
As for (2), let I be an interpretation of all predicates and parameters of S in some non-empty universe U which is such that all elements of S are true under I. Extend I to an interpretation I1 by assigning values in U to all predicates and parameters of δ which do not occur in S. Now let a be any parameter which occurs neither in δ nor in any element of S. We now wish to extend I1 to an interpretation I2 in which δ (a) has a truth value — that is, we must assign a value k of U to the parameter a and we wish to do so in a way that makes δ ⊃ δ (a) true. Well, if δ is false under I1, then any k in U will do, since δ ⊃ δ (a) will automatically be true. On the other hand, if δ is true under I1, then there must be at least one element k in U such that δ (a) is true when k is assigned to a. Thus, we assign such an element k to a, and then, since this assignment makes δ ⊃ δ (a) true, δ ⊃ (a) must also be true.
This proves (1) and (2). Finally, consider a finite subset M0 of M. Then M0 is a finite regular set. Arrange M0 in some regular sequence Q1 ⊃ Q1(a1), …, Qn ⊃ Qn(an), where for each i ≤ n, Qi is either a γ or a δ, and if Qi is a δ, the parameter ai does not occur in δ, nor in any earlier term of the sequence.
Now let S be any satisfiable finite set of pure sentences. Let

And for each i < n, let

By (1) and (2) above, S1 is satisfiable, and for each i < n, if Si is satisfiable, so is Si+1. It then follows by mathematical induction that each of the sets S1, S2, …, Sn is satisfiable. Thus S ∪ M0, which is Sn, is satisfiable.
4.Let M be a complete regular set of sentences. By Theorem 2, it is also a magic set, so that by Theorem 1, it is a truth-functional basis for first-order logic. Thus, for any valid pure sentence X there is a finite subset R of M which truth-functionally implies X. The set R is obviously regular.
5.For any set S of formulas, let Con S be the conjunction of the elements of S (the order does not matter) if S is non-empty, or t if S is empty (in which case all elements of S are true). In either case, under any interpretation, Con S is true if and only if all elements of S are true. Let Dis S be the disjunction of all the elements of S if S is non-empty, or f if S is empty. In either case, under any interpretation, Dis S is true iff at least one element of S is true. A sequent U → V is logically equivalent to the simple formula Con U ⊃ Dis V.
We are to show that U, X → V is logically equivalent to U → V, ∼X, or what is the same thing, that Con (U,X) ⊃ Dis V is equivalent to Con U ⊃ Dis (V, ∼X). [The proof that U → V, X is logically equivalent to U, ∼X → V is similar.]
Well, under any interpretation (Boolean valuation), X is either true or false.
Case 1. Let X be true. Then Con (U, X) is logically equivalent to Con (U), so that Con (U, X) ⊃ Dis V is logically equivalent to Con (U) ⊃ Dis V. But also, since X is true, ∼X is false, so that Dis (V, ∼X) is logically equivalent to Dis (V). Thus Con U ⊃ Dis (V, ∼X) is also equivalent to Con U ⊃ Dis (V). So in this case, the sequents U, X → V and U → V, ∼X are both equivalent to U → V, hence equivalent to each other.
Case 2. Let X be false. Then Con (U, X) is false, hence Con (U, X) ⊃ Dis V is true. But also, since X is false, then ∼X is true, so that Dis (V, ∼X) is true, and therefore Con U ⊃ Dis (V, ∼X) is true. Thus in this case the sequents U, X → V and U → V, ∼X are both true, hence equivalent to each other.
6.S is a set T X1, …, TXn, FY1, …, FYk of signed formulas. We let U be the set X1, …, Xn and let V be the set {Y1, …, Yk}.
First, let us consider the axioms {S, T X, F X} of the uniform system. The set {S, T X, F X} is the set

and so |S, T X, F X| is the sequent

This is U, X → V, X, the axiom of G0.
As for the inference rules, we must consider the various α and β cases separately. For example, consider the case that α is of the form F X ⊃ Y. Then α1 = TX and α2 = FY. Then |S,α1,α2| is |TX1, …, TXn, FY1, …, FYk, TX, FY|, which is the sequent

which is U, X → V, Y, and so for the case α = F X ⊃ Y, Rule A is Rule I1 of the system G0.
We leave it to the reader to verify the other cases, specifically that if α is T X ∧ Y, F X ∨ Y, T∼X, F∼X, then Rule A is respectively Rule C1, D1, N2, N1 of the system G0, and if β is F X∧Y, T X∨Y, T X ⊃ Y, then Rule B is respectively Rule C2, D2, I2.
7.First for the correctness of the altered tableau method. We are to show that if there is a closed altered tableau for a set, then the set is really unsatisfiable.
For any satisfiable finite set S of signed formulas, the following facts hold:

(1)If A ∊ S then S ∪ {A1} and S ∪ {A2} are both satisfiable.
(2)If B ∊ S then either S ∪ {B1} or S ∪ {B2} is satisfiable.
(3)If C ∊ S then for every parameter a the set S ∪{C(a)} is satisfiable.
(4)If D ∊ S then for every parameter a new to S, the set S ∪ {D(a)} is satisfiable.

These facts can be proved in the same manner as in The Beginner’s Guide [Smullyan, 2014], replacing α,β,γ,δ by A,B,C,D respectively. It then follows that if S is satisfiable, at no stage can an altered tableau for S close. Hence if there is a closed altered tableau for S, then S is unsatisfiable. Thus the altered tableau method is correct.
As for completeness, one can make a systematic altered tableau for a set in essentially the same manner as for analytic tableaux (in the instructions given in The Beginner’s Guide [Smullyan, 2014], just replace α,β,γ,δ by A,B,C,D respectively). If the systematic altered tableau runs on forever without closing, then it contains at least one infinite open branch, and the set S of the sentences on the branch satisfies the following conditions:

(0)For no X in S is it the case that both T X and F X are both in S, or that T X and T ∼X are both in S or that F X and F ∼X are both in S.
(1)If A ∊ S, so are A1 and A2.
(2)If B ∊ S, then B1 ∊ S or B2 ∊ S.
(3)If C ∊ S, then so is C(a) for every parameter a.
(4)If D ∊ S, then so is D(a) for at least one parameter a.

Such a set S is very much like a Hintikka set, and the proof of its satisfiability is very close to that for Hintikka set. About the only significant difference is in the assignment of truth values to the atomic sentences. Now, for any atomic unsigned sentence X, if T X ∊ S or F ∼X ∊ S, give X the value truth (in which case T X and F ∼X are then true). If either T ∼X ∊ S or F X ∊ S, then give X the value false (in which case T ∼X and F X are then true). [No ambiguity can result, because if T X ∊ S or F ∼X ∊ S, then neither T ∼X ∊ S nor F X ∊ S can hold, and if T ∼X ∊ S or F X ∊ S, then neither T X ∊ S nor F ∼X ∊ S can hold.] If none of T X, T ∼X, FX, F ∼X occur in S, then give X the value truth or falsity at will — say truth. Under this valuation, all elements of S can be shown to be true by mathematical induction on degrees, much as in the proof of Hintikka’s Lemma. Thus S is satisfiable.
We now see that if the systematic altered tableau for S does not close, then S is satisfiable, hence if S is not satisfiable, then any systematic altered tableau for S must close. This completes the completeness proof for the altered tableau method.
To complete the proof of the completeness of the Gentzen system GG, we now need altered block tableaux for first-order logic, whose rules in uniform notation are the following:

We recall that if S is the set {TX1, …, TXn, FY1, …, FYk}, by |S| we mean the sequent X1, …, Xn → Y1, …, Yk.
For any block tableau rule of the form

by the counterpart of the rule we mean

(i.e. “from the sequent |S2|, to infer the sequent |S1|”). For any block tableau rule of the form

by its counterpart is meant the rule

(i.e. “from the sequents |S2| and |S3|, to infer the sequent |S1|”). Well, the counterparts of the eight block tableau rules are precisely the eight rules of the system GG. To see this, let U be the set of unsigned sentences X such that T X ∊ S and let V be the set of sentences Y such that F Y ∊ S. Consider, for example, the block tableau rule

The counterpart of this rule is

but this is just

which is a rule of GG.
Another example: The counterpart of the rule is

which is

which is a rule of GG.
The reader can verify the cases of the other six rules.
Now if  is a closed altered block tableau for a set of signed formulas whose counterpart is the sequent U → V, each end point S is closed, and the sequence |S| is an axiom of GG:

(i)|S, T X, F X| is U, X → V, X;
(ii)|S, T X, T ∼X| is U, X, ∼X → V ;
(iii)|S, F X, F ∼X| is U → V, X, ∼X.

And since the counterparts of the block rules are rules of GG, it follows that if we replace each point S of the tree  by the sequent |S| and turn the tree upside down, we have a proof of the sequent U → V in the system GG. This concludes the proof of the completeness of GG.
8.It is understood that P′ is a predicate distinct from each of P, P1, …, Pn, and that P′ occurs in no element of S, and that S′ is the result of substituting P′ for P in every element of S.
Now suppose that P is explicitly definable from P1, …, Pn with respect to S. Let φ(x) be a formula whose predicates are among P1, …, Pn but do not involve P and which is such that

Then also, of course, . Hence,

which implies

Therefore  is logically valid, as the reader can verify (say, with a tableau). Thus P′ is implicitly definable from P1, …, Pn with respect to S.

9.(a) Suppose every unsatisfiable set has an associate. Consider any valid sentence X. Then the unit set {∼X} is unsatisfiable, hence has an associate R. Since R ∪ {∼X} is not truth-functionaly satisfiable, then R tautologically implies X [since X must be clearly true in any Boolean valuation satisfying R]. Moreover, X contains no critical parameter of R, which proves the Regularity Theorem.
(b)Conversely, assume the Regularity Theorem. Let S be a finite set {X1, …, Xn} of sentences which is unsatisfiable. Then the sentence ∼(X1 Λ … Λ Xn) is valid. Hence by the assumed Regularity Theorem, there is a regular set R which tautologically implies ∼(X1 Λ … Λ Xn) and which is such that no critical parameter of R occurs in any element of ∼(X1 Λ … Λ Xn), and thus no critical parameter of R occurs in any element of S. Since R tautologically implies ∼(X1 Λ … Λ Xn), then R ∪ {X1, …, Xn} is truth-functionally unsatisfiable, i.e. R ∪ S is truth-functionally unsatisfiable. Thus R is an associate of S.
10.We are given that M is maximally Γ-consistent and E-complete, and we are to show that M is a Hintikka set. In what follows, “consistent” will mean Γ-consistent. We shall say that a sentence X is consistent with M if the set M ∪ {X} is consistent. Since M is maximally consistent, then if X is consistent with M, then X must be a member of M (otherwise M would be only a proper subset of the larger consistent set M ∪ {X}, contrary to M’s maximality).
H0:No formula and its conjugate (or formula and its negation, if unsigned formulas are in questions) can be in the set M, because M is consistent, and that is one of the conditions on (Γ) consistency.
H1:Suppose α ∊ M. Then of course is consistent with M. Thus α1 and α2 are both consistent with M (since Γ is an analytic consistency property). Therefore α1 and α2 are members of M. This proves that if α ∊ M, then α1 ∊ M and α2 ∊ M.
H2:Suppose β ∊ M. Then β is consistent with M and thus either β1 is consistent with M or β2 is consistent with M, and therefore either β1 ∊ M or β2 ∊ M.
H3:Suppose γ ∊ M. Then γ is consistent with M, so that γ(a) is consistent with M for every parameter a. Therefore γ(a) ∊ M for every parameter a.
H4:Suppose δ ∈ M. Then δ(a) ∈ M for at least one parameter a by the assumption that M is E-complete.
By H1–H4, M is a Hintikka set.








Part II
Recursion Theory and Metamathematics







Chapter 3
Some Special Topics
The items of this chapter represent reconstructions and generalizations of various results in recursion theory, undecidability and incompleteness.
First for a preliminary matter, which I should have addressed much earlier: By a pair (x, y) [more often called an ordered pair], is meant a set whose elements are x and y, together with a designation which specifies that x is the first element and y the second. In contrast, the unordered pair {x, y} [note the curly brackets instead of parentheses] is just the set whose members are x and y, without any specification that one of the two elements comes first and the other second. The set {x, y} is the same as the set {y, x}, but the pair (x, y) is different from the pair (y, x) [except in the case when x = y].
Throughout this chapter, the word number shall mean positive integer, and we will consider an operation that assigns to each pair (x, y) of numbers a number denoted xy, or sometimes x * y. We will assume the operator is such that xy (or x * y) uniquely determines the x and the y — that is, the only way that x * y can be the same as z * v is when x = z and y = v. And we assume that every number n is assigned to some pair x * y, for some x and some y. We generally assume also that since the function * is 1-1 and onto from the set of ordered pairs of positive integers to the set of positive integers, we can use its inverse to go from a given a number n to the pair (x, y) such that n = x * y.
I.A Decision Machine
We consider a calculating machine with infinitely many registers R1, R2,…,Rn,…. We call n the index of Rn. Each register is, so to speak, in charge of a certain property of numbers. To find out whether a given number n has a certain property, one goes to the register in charge of that property and feeds in the number. The machine then goes into operation, and one of three things happens:

(1)The machine eventually halts and flashes a signal — say a green light — signifying that the number does have the property, in which case we say that the register affirms n.
(2)The machine eventually halts and flashes a different signal — say a red light — to indicate that the number doesn’t have the property, in which case we say that the register denies n.
(3)The machine runs on forever, without ever being able to determine whether or not the number does have the property, in which case we say that n stumps the register, or that the register is stumped by n.

We assume we are given a machine M that satisfies the following two conditions:
M1: With each register R in the calculating machine is associated another register R′ in the calculating machine called the opposer of R, which affirms those numbers which R denies, and denies those numbers which R affirms (and hence is stumped by those and only those numbers which stump R).
M2: With each register R in the calculating machine is associated another register R# in the calculating machine called the diagonalizer of R, such that for any number x, the register R# affirms x iff R affirms xx and denies x iff R denies xx. (I.e. R# affirms x iff R affirms x * x and R# denies x iff R denies x * x, for our special function * which has the properties listed above for it; we will write xy for x * y from now on in this chapter.)
Recall our denumerable list of registers R1, R2,…, Rn,…. For any number n, we let n′ be the index of (Rn)′, and let n# be the index of (Rn)#. Thus Rn′ = (Rn)′ and Rn# = (Rn)#.
We will say that two registers Ra and Rb behave the same way towards a number n when they either both affirm n, or both deny n or are both stumped by n. We call two registers similar if they behave the same way towards every number. It is easy to see that for any register R, the registers R′# and R#′ are similar, for each denies x iff R affirms xx, and each affirms x iff R denies xx. (R#′ means the same as (R′)# and R#′ means the same as (R#)′, but we will always eliminate the parentheses if the resulting meaning is clear.)
Universal Registers
We shall call a register U universal if, for all numbers x and y, the register U affirms xy iff Rx affirms y.
The propositions that follow will be proved later on in a more general framework. Meanwhile, some of you might like to try proving them on your own.
Proposition 1.1. Any universal register can be stumped (by some number or other).
Proposition 1.2. If at least one of the registers is universal, then some register is stumped by its own index (i.e. for some number n, the register Rn is stumped by n).
Contra-Universal Registers
We shall call a register V contra-universal if for all numbers x and y, the register V affirms xy iff Rx denies y.
Proposition 1.3. Any contra-universal register V can be stumped.
Proposition 1.4. If some register is universal, then some register is stumped by the index of its opposer. If some register is contra-universal, then some register is stumped by its own index.
Creative Registers
We shall call a register C creative if, for every register R, there is at least one number n such that C affirms n iff R affirms n.
Proposition 1.5. Any creative register can be stumped.
Proposition 1.6. Every universal register is creative, and every contra-universal register is creative.
Note: Propositions 1.5 and 1.6 provide additional and particularly simple proofs of Propositions 1.1 and 1.3.
For the rest of this section, we let A be the set of all numbers xy such that Rx affirms y, and let B be the set of all numbers xy such that Rx denies y.
Fixed Points
We call a number xy a fixed point of a register R if R affirms xy iff xy ∊ A (i.e. Rx affirms y) and R denies xy iff xy ∊ B (i.e. Rx denies y).
Proposition 1.7. Every register has a fixed point.
Proposition 1.8. Suppose that U is universal and V is contra-universal. Then:

(a)Any fixed point of U′ will stump U.
(b)Any fixed point of V will stump V.

Proposition 1.9. Suppose R is a register that affirms all numbers in A and doesn’t affirm any number in B. Then R can be stumped. [This proposition is particularly interesting.]
A Halting Problem
Let us say that a register R halts at a number n if R either affirms n or denies n. Call a register R a stump detector if R halts at those and only those numbers xy such that Rx is stumped by y.
Proposition 1.10. No register is a stump detector.
Domination
We say that n dominates m if Rn affirms all numbers affirmed by Rm.
Proposition 1.11. Suppose U is universal and V is contra-universal and that R affirms all numbers affirmed by U and denies all numbers affirmed by V. Then R can be stumped.
This proposition can be restated in terms of domination as follows: Suppose U is universal and V is contra-universal and there is a register R such that R dominates U and R′ dominates V. Then there exists an x such that R is stumped by x.
Affirmation Sets
By the affirmation set of a register R we shall mean the set of all numbers affirmed by R. We shall call a set of numbers S an affirmation set if it is the affirmation set of some register.
Proposition 1.12. If one of the registers is universal, then there exists an affirmation set whose complement is not an affirmation set.
II.Variations on a Theme of Gödel
Some of the items here are repetitions of results in The Beginner’s Guide [Smullyan, 2014]. The reason for these repetitions will be apparent later on. Again, the propositions that follow will be proved later, in a more general setting.
We consider a mathematical system  in which we have an infinite sequence H1, H2, …, Hk, …, of expressions called predicates, and to each predicate H and each number n is assigned an expression denoted H(n), called a sentence. [Informally, we think of H as the name of some property of numbers, and of the sentence H(n) as expressing the proposition that the number n has the property named by H.]
We assume that every sentence is H(n) for some H and some n. We arrange all the sentences in a sequence S1, S2, …, Sk, … in such a manner that for numbers x and y the sentence Sx * y is the sentence Hx(y). We call n the index of the predicate Hn, and we call k the index of the sentence Sk. Thus x * y is the index of the sentence Hx(y).
Provable, Refutable and Undecidable Sentences
Some of the sentences are called provable and some are called refutable, and we assume the system is consistent, by which we mean that no sentence is both provable and refutable. A sentence is called decidable if it is either provable or refutable, and it is called undecidable if it is neither provable nor refutable. The system is called complete if every sentence is decidable, and is called incomplete otherwise. We let P be the set of all provable sentences and R be the set of all refutable sentences.
For any set W of sentences, we let W0 be the set of all indices of the elements of W. Then P0 is the set of all n such that Sn is provable, and R0 is the set of all n such that Sn is refutable.
We shall call two sentences X and Y equivalent if when one is provable, so is the other, and when one is refutable, so is the other (and hence, when one is undecidable, so is the other); in other words, they are either both provable, both refutable, or both undecidable.
We are given that the system  obeys two conditions:

(1)With each predicate H is associated a predicate H′ called the negation of H such that for every number n the sentence H′(n) is provable iff H(n) is refutable, and is refutable iff H(n) is provable.
(2)With each predicate H is associated a predicate H# called the diagonalizer of H such that for each number n the sentence H#(n) is equivalent to the sentence H(n * n).

Set Representation
We shall say that a predicate H represents the set of all numbers n such that H(n) is provable. Thus to say that H represents a number set A is to say that for all numbers n, the sentence H(n) is provable iff n ∊ A. We call the set A representable if some predicate represents it.
Provability and Refutability Predicates
We shall call H a provability predicate if it represents the set P0. Thus the statement that H is a provability predicate is equivalent to the condition that for every n the sentence H(n) is provable iff Sn is provable.
We shall call K a refutability predicate if it represents the set R0. Thus the statement that K is a refutability predicate is equivalent to the condition that for every n the sentence K(n) is provable iff Sn is refutable.
Proposition 2.1. If H is a provability predicate, then H(n) is undecidable for some n.
Proposition 2.2. If one of the predicates is a provability predicate, then there is some n such that Hn(n) is undecidable.
Proposition 2.3. If K is a refutability predicate, then K(n) is undecidable for some n.
Recall that n′ is the index of the negation of Hn, so that (Hn′) = (Hn)′.
Proposition 2.4. If some predicate is a provability predicate, then there is some n such that Hn(n′) is undecidable. If some predicate is a refutability predicate, then there is some n such that Hn(n) is undecidable.
Creative Predicates
We shall call a predicate K creative if for every predicate H there is at least one number n such that H(n) is provable iff K(n) is provable.
Proposition 2.5. If K is creative, then K(n) is undecidable for some n.
Proposition 2.6. Every provability predicate is creative, and every refutability predicate is creative.
Fixed Points
We shall call a number n a fixed point of a predicate H if Sn is equivalent to H(n). [This is the same as saying that a number xy is a fixed point of a predicate H if Hx(y) is equivalent to H(xy).]
Proposition 2.7. Every predicate has a fixed point.
Proposition 2.8. Suppose that H is a provability predicate and that K is a refutability predicate. Then:

(a)If n is a fixed point of H′, then H(n) is undecidable.
(b)If n is a fixed point of K, then K(n) is undecidable.

Proposition 2.9. Suppose H is a predicate such that H(n) is provable for any n in the set P0, and H(n) is not provable for any n in R0. Then H(n) is undecidable for some n. [This is related to Rosser’s incompleteness proof.]
Proposition 2.10. There cannot be a predicate H such that for all numbers n the sentence H(n) is decidable iff Sn is undecidable.
Domination
We will say that a predicate Hb dominates a predicate Ha if Hb(n) is provable for every n for which Ha(n) is provable.
Proposition 2.11. If H dominates some provability predicate, and H′ dominates some refutability predicate, then H(n) is undecidable for some n.
Proposition 2.12. If one of the predicates is a provability predicate, then there exists a representable number set whose complement is not representable.
III.R-Systems
The systems of I and those of II are really the same, only in different dress. The common underlying theme will be fully explained in the next section of this chapter. Meanwhile, we will consider yet another embodiment of the theme, which has direct applications to recursion theory.
We consider a denumerable collection of number sets, which we will call “R-sets”, and an enumeration A1, A2, …, An, … of them in some order, and another enumeration B1, B2, …, Bn, … of them in some other order, such that for any disjoint pair (S1, S2) of R-sets, there is some n such that S1 = An and S2 = Bn. We shall call such a pair of enumerations an R-system if the following three conditions hold:
R0: For every n, the set An is disjoint from Bn i.e., 
R1: Associated with each number n is a number n′ such that An′ = Bn and Bn′ = An [thus the ordered pair (An′, Bn′) is the pair (Bn, An)].
R2: Associated with each n is a number n# such that for every number x,
(1)x ∊ An# iff xx ∊ An.
(2)x ∊ Bn# iff xx ∊ Bn.
Note: The collection of all recursively enumerable sets can be arranged in a pair of sequences A1, A2, …, An, … and B1, B2, …, Bn, … such that conditions R0, R1 and R2 all hold (for a recursive function f(x, y) = x * y). Thus all the results that follow are generalizations of results in recursion theory.
Universal and Contra-Universal Sets
We call an R-set Aw universal if for all numbers x and y the number xy ∊ Aw iff y ∊ Ax. We call an R-set Av contra-universal if for all numbers x and y the number xy ∊ Av iff y ∊ Bx.
Proposition 3.1. If some R-set Aw is universal, then Aw is not the complement of Bw (i.e. some number x lies outside both Aw and Bw).
Proposition 3.2. If some R-set Aw is universal, then there is some n such that n itself lies outside both An and Bn.
Proposition 3.3. If the R-set Av is contra-universal, then Av is not the complement of Bv (i.e. some number x lies outside both Av and Bv).
Proposition 3.4. If some R-set Aw is universal, then there is a number n such that n′ lies outside both An and Bn. If some R-set Av is contra-universal, then there is a number n such that n lies outside both An and Bn.
Creative R-Sets
We shall call an R-set Ac creative if for every R-set Ax there is a least one number n such that n ∊ Ac iff n ∊ Ax.
Proposition 3.5. If Ac is creative, then Ac is not the complement of Bc.
Proposition 3.6. Any universal or contra-universal R-set is creative.
Fixed Points
We now let A be the set of all numbers xy such that y ∊ Ax, and let B be the set of all numbers xy such that y ∊ Bx.
We shall call a number x a fixed point of the pair (An, Bn) if x ∊ An iff x ∊ A and x ∊ Bn iff x ∊ B. [An equivalent definition is that a number xy is a fixed point of a pair (An, Bn) if xy ∊ An iff y ∊ Ax and xy ∊ Bn iff y ∊ Bx.]
Proposition 3.7. Every pair (An, Bn) has a fixed point.
Proposition 3.8. (a) If Aw is a universal R-set, then any fixed point of (Aw′, Bw′) = (Bw, Aw) lies outside both Aw and Bw. (b) If Av is a contra-universal R-set, then any fixed point of (Av, Bv) lies outside both Av and Bv.
Proposition 3.9. If A ⊆ Ah and Ah is disjoint from B, then some number lies outside both Ah and Bh.
Proposition 3.10. There is no number n such that for all numbers x and y, xy ∈ An ∪ Bn iff y ∉ Ax ∪ Bx.
R-Separable R-Sets
We shall say that an R-set S1 is R-separable from an R-set S2 if there is some R-set S such that S1 is a subset of S and S2 is a subset of the complement  of S.
Domination
We say that n dominates m when Am ⊆ An, i.e. when Am is a subset of An.
Proposition 3.11. If Aw is universal and Av is contra-universal, then Aw is not R-separable from Av. [This generalizes an important result in recursion theory, namely that there exist two disjoint recursively enumerable sets that are not recursively separable.]
Note. Proposition 3.11 can be rewritten in terms of domination, but the rewritten version is very long. It is simpler just to say that the solution to Problem 11 in the synthesis section (whose statement is expressed in terms of the concept of domination) can be used to prove Proposition 3.11 (by contradiction), as the reader will be able to see in the solutions to the problems of this chapter.
Proposition 3.12. If some R-set is universal, then there is an R-set whose complement is not an R-set. [This generalizes the important fact that there is a recursively enumerable set whose complement is not recursively enumerable.]
IV.A Synthesis
The Synthesis System
As already mentioned, the discussions and problems of sections I, II and III are really all the same, only dressed differently. In each of the three, we have numerical relations A(x, y) and B(x, y) such that the following three conditions hold:
D0:A(x, y) and B(x, y) cannot both hold simultaneously for any ordered pair of numbers (x, y).
D1: Associated with each number x is a number x′ such that for all numbers y:
(1)A(x′, y) iff B(x, y);
(2)B(x′, y) iff A(x, y).
D2:Corresponding to each number x is a number x# such that for all y:
(1)A(x#, y) iff A(x, y * y);
(2)B(x#, y) iff B(x, y * y).
•For the decision machine M of I, A(x, y) is the relation “Rx affirms y”, and B(x, y) is the relation “Rx denies y”.
•For the mathematical system  of II, A(x, y) is the relation “Hx(y) is provable”, and B(x, y) is the relation “Hx(y) is refutable”.
•For the mathematical R-systems of III, A(x, y) is the relation “y ∈ Ax”, and B(x, y) is the relation “y ∈ Bx”.
We shall call the pair A(x, y) and B(x, y) a duo if conditions D0, D1 and D2 hold. Any result proved about duos in general is simultaneously applicable to the items of sections I, II, and III. Thus, in the problems that follow, for each n ≤ 12, the solution to Problem n simultaneously proves Propositions 1 n, 2n, and 3n of sections I, II, and III.
In this section we will again abbreviate x * y by xy and we will omit the word “section” in front of I, II, and III. We let A be the set of all numbers xy such that A(x, y) is true, and we let B be the set of all numbers xy such that B(x, y) is true. It will be valuable to know that not only are A(x, y) and B(x, y) disjoint [for every pair (x, y)], but A and B are also disjoint (contain no common element n). This is because every n is xy for some x and y, and it is given that A(x, y) and B(x, y) don’t both hold, which means that xy cannot be in both A and B, and thus n cannot be in both A and B.
•For the decision machine M of I, A is the set of all numbers xy such that Rx affirms y, and B is the set of all numbers xy such that Rx denies y.
•For the mathematical system  of II, A is the set P0 (which is the set of all numbers xy such that Hx(y) is provable), and B is the set R0 (the set of all numbers xy such that Hx(y) is refutable).
•For the mathematical R-systems of III, A is the set of all numbers xy such that y ∈ Ax, and B is the set of all numbers xy such that y ∈ Bx.
Undecidable Numbers
We shall call a number n undecidable if n is in neither A nor B. Thus xy is undecidable if neither A(x, y) nor B(x, y) holds. If the number n is not undecidable, we call it decidable.
•For the decision machine M of I, n = xy being undecidable means that Rx is stumped by y.
•For the mathematical system  of II, n = xy being undecidable means that the sentence Hx(y) is undecidable (i.e. neither provable nor refutable).
•For the mathematical R-systems of III, n = xy being undecidable means that y lies outside both Ax and Bx.
The following is an important fact: Since A is disjoint from B, it follows that to prove that a number n is undecidable, it suffices to show that n ∈ A iff n ∈ B, because this would mean that n is either in both A and B, or in neither one. Since it cannot be in both, it must be in neither one, i.e. it is undecidable.
Universal Numbers
We shall call a number w universal if for all x it is the case that wx ∈ A iff x ∈ A, or, which is the same thing, for all x and y, the relation A(w, xy) holds iff A(x, y) holds.
•For the decision machine M of I, to say that Rw is a universal register is to say that for all x and y, Rw affirms xy iff Rx affirms y.
•For the mathematical system  of II, to say that Hw is a provability predicate (a “universal predicate for provability”) is to say that for all x and y, Hw(xy) is provable iff Sxy is provable (which is the same as saying Hx(y) is provable).
•For the mathematical R-systems of III, to say that w is universal is to say that Aw is a universal set, i.e. that xy ∈ Aw iff y ∈ Ax.
Contra-Universal Numbers
We call a number v contra-universal if vx ∈ A iff x ∈ B, or, what is the same thing, for all x and y, A(v, xy) iff B(x, y).
•For the decision machine M of I, to say that Rv is a contra-universal register is to say that for all x and y, Rv affirms xy iff Rx denies y.
•For the mathematical system  of II, to say that Hv is a refutability predicate (a “contra-universal predicate”) is to say that for all x and y, Hv(xy) is provable iff Sxy is refutable (which is the same as saying that Hx(y) is refutable or that ∼ Hx(y) is provable).
•For the mathematical R-systems of III, to say that v is contra-universal is to say that, for all x and y, xy ∈ Av iff y ∈ Bx.
Creative Numbers
We call a number c creative if for every number n there is at least one number x such that cx ∈ A iff nx ∈ A.
• For the decision machine M of I, to say that a register Rc is creative is to say that for every register Rn, there is at least one number x such that Rc affirms x iff Rn affirms x.
•For the mathematical system  of II, to say the predicate Hc is creative is to say that for every predicate Rn, there is at least one number x such that Hc(x) is provable iff Hn(x) is provable.
• For the mathematical R-systems of III, to say that c is creative is to say that, for every number n, there is at least one number x such that x ∈ Ac iff x ∈ An.
Fixed Points
We let “x is a fixed point of n” mean that nx ∈ A iff x ∈ A and nx ∈ B iff x ∈ B.
• For the decision machine M of I, to say that a number x * y is a fixed point of a register R is to say that R affirms x * y iff x * y ∈ A (i.e. Rx affirms y) and R denies x * y iff x * y ∈ B (i.e. Rx denies y).
•For the mathematical system  of II, to say that xy is a fixed point of the predicate Hn is to say that Hn(xy) is equivalent to Sxy, i.e. both these sentences are provable, or both are refutable, or both are neither. [This is the same as saying that Hn(xy) is equivalent to Hx(y).]
• For the mathematical R-systems of III, to say that a number x is a fixed point of a pair (An, Bn) if x ∈ An iff x ∈ A and x ∈ Bn iff x ∈ B.
Problem 1. Prove that if w is universal, then there is at least one number x such that wx is undecidable.
Problem 2. Prove that if some number is universal, then there is at least one number x such that xx is undecidable.
Problem 3. Prove that if v is contra-universal, then there is at least one number x such that vx is undecidable.
Problem 4. Prove that if some number is universal, then there is some number x such that xx′ is undecidable. And prove that if some number is contra-universal, then there is some number x such that xx is undecidable.
Problem 5. Prove that if c is creative, then there is at least one number x such that cx is undecidable.
Problem 6. Prove that every universal number is creative, and that every contra-universal number is creative.
Problem 7. Prove that every number has a fixed point.
Problem 8. Suppose w is universal and v is contra-universal. Prove:

(a)If x is any fixed point of w′, then x is undecidable, and so is wx.
(b)If x is any fixed point of v, then x is undecidable, and so is vx.

Problem 9. Suppose h is a number such that, for all numbers x:

(a)If x ∈ A, then hx ∈ A.
(b)If x ∈ B, then hx ∉ A.

Prove that hx is undecidable for some x.
Why is this a strengthening of the result of Problem 1?
Problem 10. Prove that there is no number n such that for all numbers x the number nx is decidable if and only if x is undecidable.
Domination
Let us say that a number n dominates a number m if, for every number x, if mx ∊ A, then nx ∊ A.
•For the decision machine M of I, to say that the register Rn dominates the register Rm is to say that Rn affirms all numbers affirmed by Rm.
•For the mathematical system S of II, to say that the predicate Hn dominates the predicate Hm is to say that that for all x, if Hm(x) is provable, so is Hn(x).
•For the mathematical R-systems of III, to say that An dominates Am is to say that Am ⊆ An.
Problem 11. Show that if n dominates some universal number w and n′ dominates some contra-universal number v, then nx is undecidable for some x.
Set Representation
We will say that the number n represents the set S if, for all x, nx ∊ A iff x ∊ S.
•For the decision machines of I, to say that a register R represents a set S is just to say that the set S is the affirmation set of the register R.
•For the mathematical systems S of II, to say that predicate H represents a set S is just to say that S is the set of all numbers n such that H(n) is provable.
•For the mathematical R-systems of III, to say that the R-set An represents a set S is just to say that y ∊ An iff y ∊ S, i.e., An = S.
Problem 12. Prove that if some number is universal, then there is a representable set whose complement is not representable.
Solutions to the Problems of Chapter 3

1.Suppose w is universal. Then for any number x, w(w#′x) ∊ A holds iff w#′x ∊ A, which is true iff w#x ∊ B, which is true iff w(xx) ∊ B. Thus:
(1) w(w#′x) ∊ A iff w(xx) ∊ B.
If we take w#′ for x in (1), we obtain:
(2) w(w#′w#′) ∊ A iff w(w#′w#′) ∊ B.
But A and B are disjoint, so w(w#′w#′) can be in neither A nor B, which shows that w(w#′w#′) is undecidable; w(w′#w′#) is also undecidable, as the reader can verify.
2.Suppose w is universal. Then, by D1, for any number x, w#′x ∊ B holds iff w#x ∊ A, which is true iff w(xx) ∊ A, which is true iff xx ∊ A. Thus w#′x ∊ B iff xx ∊ A. We take w#′ for x and obtain w#′w#′ ∊ B iff w#′w#′ ∊ A. Hence w#′w#′ is undecidable.
3.Suppose v is contra-universal. Then for any number x, v(v#x) ∊ A is true iff v#x ∊ B, which is true iff v(xx) ∊ B. Thus v(v#x) ∊ A iff v(xx) ∊ B. We take v# for x and obtain v(v#v#) ∊ A iff v(v#v#) ∊ B. Thus v(v#v#) is undecidable.
4.Suppose w is universal. We have seen in the solution to Problem 2 that w#′w#′ is undecidable. Therefore w#w#′ is undecidable. [For any numbers x and y, x′y is undecidable iff xy is undecidable. (Why?)] Thus nn′ is undecidable for n = w#.
Next, suppose that v is contra-universal. For any number x, we know that v#x ∊ A iff v(xx) ∊ A. We take v# for x and obtain v#v# ∊ A iff v(v#v#) ∊ A, which in turn is the case iff v#v# ∊ B (since v is contra-universal). Thus v#v# ∊ A iff v#v# ∊ B, so that v#v# must be undecidable.
5.Suppose c is creative. Then for any x there is some n such that cn ∊ A iff xn ∊ A. Taking c′ for x, we obtain that for some n, cn ∊ A iff c′n ∊ A. But c′n ∊ A iff cn ∊ B. Thus cn ∊ A iff cn ∊ B, so that cn is undecidable.
6. (a)Suppose w is universal. For any number x, w(x#x#) ∊ A is true iff x#x# ∊ A, which is true iff x(x#x#) ∊ A. Thus wn ∊ A iff xn ∊ A, for n = x#x#.
(b)Suppose v is contra-universal. For any x, v(x#′x#′) ∊ A is true iff x#′x#′ ∊ B, which is true iff x#x#′ ∊ A, which is true iff x(x#′x#′) ∊ A. Thus vn ∊ A iff xn ∊ A, for n = x#′x#′.

7.For any number n, n#n# ∊ A holds iff n(n#n#) ∊ A, and n#n# ∊ B holds iff n(n#n#) ∊ B. Hence n#n# is a fixed point of n.
8. (a)Suppose w is universal and n is a fixed point of w′. Then n ∊ A iff wn ∊ A is true (since w is universal), which is true iff w′n ∊ B (by D1), which is true iff n ∊ B (since n is a fixed point of w′). Thus n ∊ A iff n ∊ B, so that n is undecidable. Since n is a fixed point of w′, then w′n is also undecidable (why?), and hence wn is undecidable.
(b)Suppose v is contra-universal and n is a fixed point of v. Then n ∊ A iff vn ∊ A is true (since n is a fixed point of v). But vn ∊ A iff n ∊ b (since v is contra-universal). Thus n ∊ A iff n ∊ B, and is thus undecidable. Since n is a fixed point of v, it is also true that vn ∊ B iff n ∊ B. Since n can be in neither A nor B, it follows from what we just saw that vn can be in neither A nor B either, and is thus also undecidable.
Remarks. We know that w′#w′# is a fixed point of w′ (since by the solution to Problem 7, x#x# is a fixed point of x, for any x). But it is also true that w#′w#′ is a fixed point of w′ (in fact x#′x#′ is a fixed point of x′, for any x, as the reader can verify). Thus (a) alone provides another proof that w(w#′w#′) is undecidable.

9.We are given that for all x:

Let k be the number h#′. We first show that kk is undecidable.
(a)Suppose kk ∊ A. Then h(kk) ∊ A [by (a)]. Thus h#k ∊ A so that h#′k ∊ B. And thus kk ∊ B [since k = h#′]. Thus if kk ∊ A it is also true that kk ∊ B, but kk cannot be in both A and B, and so kk∉ A.
(b)Suppose kk ∉ B. Then h(kk) ∉ A [by (b)]. Hence h#′k ∉ B, which means that kk ∉ B [since k = h#′]. Thus if kk ∊ B, then kk ∉ B, which is a contradiction. Therefore kk ∉ B.

Thus kk ∉ A and kk ∉ B, and so kk is undecidable. Thus h#′h#′ is undecidable, hence so is h#h#′, and therefore so is h(h#′h#′).
Note: The above proof is from scratch. A swifter proof is possible using the fact that every number has a fixed point. We leave it to the reader to show that if n is any fixed point of h, then hn is undecidable.
Next, why is this result a strengthening of the result of Problem 1? The answer is that if h is universal, then it obviously satisfies conditions (a) and (b).

10.If a number n exists, it must have a fixed point, by Problem 7. But if x is a fixed point of n, then (by the definition of a fixed point) nx ∊ A iff x ∊ A and nx ∊ B iff x ∊ B. Clearly, for such an x, it cannot be that nx if decidable iff x is undecidable.
11.This follows easily from Problem 9: Suppose h dominates w and h′ dominates v, where w is universal and v is contra-universal.

If x ∊ A, then wx ∊ A, so that hx ∊ A [since h dominates w]. Thus,

Next, suppose x ∊ B. Then vx ∊ A, so that h′x ∊ A, and then hx ∊ B, and finally hx ∉ A. Thus,

Thus, by (1) and (2), h satisfies the hypotheses of Problem 9, and so hx is undecidable for some x.
Because the relationship between the solution to Problem 11 and Propositions 11 and 35 are a little more complex than some of the others, we will give the proofs of these propositions (using the solution to Problem 11) here. First let us review what “n dominates m” means for the first three sections of this chapter:

First let us consider Proposition 11: Suppose U is universal and V is contra-universal and that R affirms all numbers affirmed by U and denies all numbers affirmed by V. Then R can be stumped.
In the text we gave a restatement of this claim in terms of domination:

Suppose U is universal and V is contra-universal and there is a register R such that R dominates U and R′ dominates V. Then there exists an x such that R is stumped by x.

It is easy to see that the restatement is equivalent to the original statement, for to say that R denies all numbers affirmed by V is equivalent to saying that R′ affirms all numbers affirmed by V. To bring this restatement closer to the expression of Problem 11 in the synthesis, all we have to do is the following: We let n be the index of R, w be the index of U, and v be the index of V ; then the restatement of Proposition 11 is equivalent to: If Rn dominates the universal register Rw and R′n dominates the contra-universal register Rv, then the register Rn can be stumped by some x. And this is the statement of Problem 11, reinterpreted in the vocabulary of the concepts of I.
The application to Proposition 23 is obvious.
Finally, let us consider Proposition 35: If Aw is universal and Av is contra-universal, then Aw is not R-separable from Av.
To show that Aw is not R-separable from Av, suppose that S1 and S2 are disjoint R-sets such that Aw ⊆ S1 and Av ⊆ S2. We must show that some number lies outside of both S1 and S2. Now, for some number n, S1 = An and S2 = Bn. Then S1 = An and S2 = An′. So Aw ⊆ An and Av An′, which means that n dominates w and n′ dominates v, and so, by the solution to Problem 11, there is some number x such that nx is undecidable, i.e x ∉ An and x ∉ Bn. Thus x ∉ An and x ∉ An′, i.e. x ∉ S1 and x ∉ S2, which is what was to be shown.

12.We first show that no number x can represent the complement  of A. To show this it suffices to demonstrate that, given any number x there is at least one number y such that it is not the case that xy ∊ A iff y ∊,  or, what is the same thing, that there is at least one number y such that it is the case that xy ∊ A iff y ∊ . Well, any fixed point y of x is such a number. Thus the set  is not representable.

Now, if there is a universal number w, then w represents A (why?), so that A is then a representable set whose complement  is not representable.







Chapter 4
Elementary Formal Systems and Recursive Enumerability
I.More on Elementary Formal Systems
Let us review the definition of an elementary formal system and some of the basic properties of such systems established in The Beginner’s Guide [Smullyan, 2014].
By an alphabet K is meant a finite sequence of elements called symbols. Any finite sequence of these symbols is called a string or expression in K.
By an elementary formal system (E) over K is meant a collection of the following items:

(1)The alphabet K.
(2)Another alphabet V of symbols called variables. We will usually use the letters x, y, z, w with or without subscripts, as variables.
(3)Still another alphabet of symbols called predicates, each of which is assigned a positive integer called the degree of the predicate. We usually use capital letters for predicates.
(4)Two more symbols called the punctuation sign (usually a comma) and the implication sign (usually “→”).
(5)A finite sequence of expressions, all of which are formulas (formulas will be defined below).

By a term is meant any string composed of symbols of K and variables. A term without variables is called a constant term. By an atomic formula is meant an expression Pt, where P is a predicate of degree 1 and t is a term, or an expression Rt1, …, tn, where R is a predicate of degree n and t1, …, tn are terms.
By a formula is meant an atomic formula, or an expression of the form F1 → F2 … → Fn, where F1, F2, …, Fn are atomic formulas. This concludes the definition of a formula.
By a sentence is meant a formula with no variables.
By an instance of a formula is meant the result of substituting constant terms for all variables in the formula.
An elementary formal system (E) over K is thus formed by distinguishing a particular finite subset of the formulas, which are called the axiom schemes of the system. The set of all instances of all the axiom schemes is called the set of axioms of the system.
A sentence is called provable in the elementary formal system (E) if its being so is a consequence of the following two conditions:

(1)Every axiom of the system is provable.
(2)For every atomic sentence X and every sentence Y, if X and X → Y are provable, so is Y.

More, explicitly, by a proof in the system is meant a finite sequence of sentences such that each member Y of the sequence is either an axiom of the system, or there is an atomic sentence X such that X and X → Y are earlier members of the sequence. A sentence is then called provable in the system if it is an element in the sequence of some proof in the system.
Elementary formal systems are designed to formalize the nature of a mechanical procedure, and the provable sentences of a given elementary formal system that have the form of some predicate P of degree n followed by an n-tuple of elements of the alphabet K are just those that can be generated by the given elementary formal system (which must of course include the predicate P in at least one of its axiom schemes if such a sentence is to be provable). And when such a term Pk1 … kn can be generated by the system, we say that it is true that the n-tuple k1, …, kn stands in the relation P, or we may say that Pk1 … kn is true. Thus we identify provability and truth when it comes to elementary formal systems.
Representability
We say that a predicate P of degree n represents a relation R of degree n of n-tuples of strings of K if for all constant terms x1, …, xn, the sentence Px1, …, xn is provable in (E) iff the relation Rx1, …, xn holds. For n = 1, a predicate P of degree (1) represents the set of all constant terms x such that Px is provable in (E). A set or relation is said to be representable in (E) if some predicate represents it, and a relation or set is called formally representable if it is representable in some elementary formal system.
Some Basic Closure Properties
For any n ≥ 2 we regard a relation of degree n on a set S as being a set of n-tuples (x1, …, xn) of members of S. We consider subsets of S to be special cases of relations on S, namely relations of one argument. We now wish to establish some basic closure properties of the collection of all relations (and sets) representable over K.
First, let us review some more facts discussed in The Beginner’s Guide [Smullyan, 2014]. Consider two elementary formal systems (E1) and (E2) over a common alphabet K. We shall call the two systems independent if they contain no common predicates. By (E1) ∪ (E2) we shall mean the EFS (elementary formal system) whose axioms are those of (E1) together with those of (E2). Obviously every provable sentence of (E1) and of (E2) is provable in (E1) ∪ (E2). And if (E1) and (E2) are independent, then any sentence provable in (E1) ∪ (E2) is clearly provable in (E1) or (E2) alone, and hence the representable relations of (E1) ∪ (E2) are the representable relations in (E1) alone, together with those representable in (E2) alone.
Similarly, if (E1), (E2), …, (En) are mutually independent (no two having a common predicate), then the representable relations of (E1) ∪ … ∪ (En), are the result of joining together those of each of (E1), …, (En) alone.
Now suppose R1, …, Rn are each formally representable over K. Since we are assuming an unlimited stack of predicates at our disposal, we can represent R1, …, Rn in mutually independent systems (E1), (E2), …, (En), and so we have:
Proposition 1. If R1, …, Rn are each formally representable over K, then they can all be represented in a common EFS over K.
Now for our closure properties:

(a)Unions and Intersections. For any two relations R1(x1 …, xn), R2(x1, …, xn) of the same degree, by their union R1 ∪ R2 — also written R1(x1, …, xn) ∨ R2(x1, …, xn) — is meant the set of all n-tuples (x1, …, xn) that are in either R1 or R2 or in both. Thus (R1 ∪ R2)(x1, …, xn) iff R1(x1, …, xn) ∨ R2(x1, …, xn). By the intersection R1 ∪ R2 — also written R1(x1, …, xn) Λ R2(x1, …, xn) — is meant the relation that holds for (x1, …, xn) iff

(b)Existential Quantifiers. For any relation R(x1, …, xn, y) of degree two or higher, by its existential quantification ∃yR(x1, …, xn, y) — also abbreviated ∃R — is meant the set of all n-tuples (x1, …, xn) such that R(x1, …, xn, y) holds for at least one y:
(c)Explicit Transformation. Let n be any positive integer and consider n variables x1, …, xn. Let R be a relation of degree k and let α1, …, αk each be either one of the n variables x1, …, xn or a constant term (i.e. a string of symbols of K). By the relation λx1, …, xnR(α1, …, αk) is meant the set of all n-tuples (a1, …, an) of strings of K such that R(b1, …, bk) holds, where each bi is defined as follows:
(1)If αi is one of the variables xj, then bi = aj.
(2) If αi is a constant c, then bi = c.

[For example, for n = 3 and constants c and d, λx1, x2, x3R(x3, c, x2, x2, d) is the set of all triples (a1, a2, a3) such that the relation R(a3, c, a2, a2, d) holds.]
In this situation, we say that the n-ary relation

is explicitly definable from the k-ary relation R and that the operation which assigns λx1, …, xnR(α1, …, αk) to the relation R is an explicit transformation.
Problem 1. Show that the collection of relations formally representable over K is closed under unions, intersections, existential quantification and explicit transformations. In other words, prove the following:

(a)If R1 and R2 are of the same degree and formally representable over K, then R1 ∪ R2 and R1 ∩ R2 are formally representable over K and of the same degree as R1 and R2.
(b)If the relation R(x1, …, xn, y) is a relation of degree n + 1(n greater than or equal to one) that is formally representable over K, then the relation ∃yR(x1, …, xn, y) is a relation of degree n that is formally representable over K.
(c)If the relation R(x1, …, xk) is a relation of degree k that is formally representable over K, then λx1, …, xnR(α1, …, αk) is a relation of degree n that is formally representable over K (here each αi is one of the variables x1, …, xn or a constant).

We recall that a set or relation W is said to be solvable over K if W and its complement  are both formally representable over K.
Problem 2. Prove that the collection of all sets and relations which are solvable over K is closed under union, intersection, complementation and explicit transformation.
II.Recursive Enumerability
We shall now identify the positive integers with the dyadic numerals which represent them (as defined in The Beginner’s Guide [Smullyan, 2014]). We consider the two-sign alphabet {1, 2}, which we call “D”, and define a dyadic system to be an elementary formal system over the alphabet D. We define a numerical relation to be recursively enumerable if it is representable in some dyadic system. (In The Beginner’s Guide we called these relations dyadically representable.) It turns out that the recursively enumerable sets and relations are the same as the ∑1-relations — and also the same as those representable in Peano Arithmetic.
We call a set R recursive if it and its complement  are both recursively enumerable. Thus a set is recursive if it is solvable in some dyadic system.
In The Beginner’s Guide, we showed that the relations Sxy (y is the successor of x), x < y, x ≤ y, x = y, x ≠ y, x + y = z, x × y = z and xy = z are all recursively enumerable (in fact, recursive).
We also know (taking D as our alphabet K) that the collection of recursively enumerable relations is closed under unions, intersections, explicit transformations, and existential quantifications. We now need some further closure properties.
Finite Quantifications (Bounded Quantifiers)
For any numerical relation R(x1, …, xn, y), by the relation

[sometimes abbreviated by ∀F R and called the finite universal quantification of R] is meant the set of all (n + 1)-tuples (x1, …, xn, y)such that R(x1, …, xn, z) holds for every z that is less than or equal to y. Thus (∀z ≤ y)R(x1, …, xn, z) holds iff R(x1, …, xn,1), R(x1, …, xn, 2), …, R(x1, …, xn, y) all hold. (∀z ≤ y)R(x1, …, xn, z) is equivalent to the formula

By the relation (∃z ≤ y)R(x1, …, xn, z) [sometimes abbreviated by ∃F R and called the finite existential quantification of R] is meant the set of all (n + 1)-tuples (x1, …, xn, y) such that there is at least one z ≤ y such that R(x1, …, xn, z) holds. Thus (∃z ≤ y)R(x1, …, xn, z) is the disjunction of the relations R(x1, …, xn,1), R(x1 …, xn,2), …, R(x1 …, xn, y). The formula (∃z ≤ y)R(x1, …, xn, z) is equivalent to the formula

The quantifiers (∀x ≤ c) and (∃x ≤ c) are called bounded quantifiers.
Problem 3. Show the following:

(a)If R(x1 …, xn, y) is recursively enumerable, so are ∀FR and ∃F R.
(b)If R(x1 …, xn, y) is recursive, so are ∀FR and ∃F R.

Before continuing, let us recall the definitions of ∑0 and ∑1 formulas of elementary arithmetic. In The Beginner’s Guide [Smullyan, 2014] we defined a ∑0 formula and a ∑0 relation as follows: By an atomic ∑0 formula we mean any formula of one of the forms c1 = c2 or c1 ≤ c2, where c1 and c2 are both terms. We then define the class of ∑0 formulas of Elementary Arithmetic by the following inductive scheme:

(1)Every atomic ∑0 formula is a ∑0 formula.
(2)For any ∑0 formulas F and G, the formulas ∼ F, F ∧ G, F ∨ G, and F ⊃ G are ∑0 formulas.
(3)For any ∑0 formula F, and any variable x, and any c which is either a Peano numeral or a variable distinct from x, the expressions (∀x ≤ c)F and (∃x ≤ c)F are ∑0 formulas.

No formula is a ∑0 formula unless its being so is a consequence of (1), (2) and (3) above.
Thus a ∑0 formula is a formula of Elementary Arithmetic in which all quantifiers are bounded. A set or relation is called a ∑0 set or relation (or just “∑0”) if it is expressed by a ∑0 formula.
A relation is called ∑1 if it is expressed by a formula of the form ∃zR(x1, …, xn, z), where the relation R(x1, …, xn, z) is ∑0.
We noted in The Beginner’s Guide [Smullyan, 2014] that all ∑0 and ∑1 relations and sets are arithmetic, because the formulas that express them are just a subset of all the formulas of Elementary Arithmetic that can be used to express number sets and relations. Moreover, we discussed the fact that, given any ∑0 sentence (a ∑0 formula with no free variables), we can effectively decide whether it is true or false.
By Problem 2, taking D for the alphabet K, the collection of all recursive sets and relations is closed under unions, intersections, complements, and explicit transformations. It is also closed under finite universal and existential quantifications, as we have just seen. This collection also contains the relations x + y = z and x × y = z. From all these facts, we have the following proposition:
Proposition 2. Every ∑0-relation is recursive.
We also know by Problem ∑1, taking D for the alphabet K, that the existential quantification of a recursively enumerable relation is recursively enumerable. Now, every ∑1-relation is the existential quantification of a ∑0-relation, hence of a recursively enumerable (in fact recursive) relation, and is therefore recursively enumerable. This proves that:
Proposition 3. All ∑1-relations are recursively enumerable.
We now wish to prove that, conversely, all recursively enumerable relations are ∑1, and hence that being recursively enumerable is the same as being ∑1!
First, for some very useful facts about ∑1-relations:
Problem 4. Suppose R(x1, …, xn, y) is ∑1. Show that the relation ∃yR(x1, …, xn, y) [as a relation between x1, …, xn] is ∑1.
For any function f(x) (from numbers to numbers), we say that the function f(x) is ∑1 if the relation f(x) = y is ∑1.
Proposition 4. If A is a ∑1 set and f(x) is a ∑1-function, and A′ is the set of all x such that f(x) ∊ A, then A′ is ∑1. [A′ is what is called the domain of the function f(x) when the range of the function is restricted to the set A.]
Problem 5. Prove the above proposition.
To prove that every recursively enumerable relation is ∑1, we will illustrate the proof for recursively enumerable sets. We will show that every recursively enumerable set A is ∑1. The proof for relations is a modification we leave to the reader.
We use the dyadic Gödel numbering of The Beginner’s Guide [Smullyan, 2014]: For any ordered alphabet K, viz. {k1, k2, …, kn}, we assign the Gödel number 12 to k1, 122 to k2, and a 1 followed by n 2’s to kn. And then, for any compound expression, we take its Gödel number to be the result of replacing each symbol of K by its Gödel number. We use g to denote the Gödel number function, e.g. the Gödel number of g(k3k1k2) = 122212122.
In The Beginner’s Guide, we showed that if a set S of strings of symbols of K is formally representable over K, then the set S0 of Gödel numbers of the members of S is ∑1. Of course this also holds when K is the two-sign alphabet {1, 2}. Thus if A is recursively enumerable, then the set A0 of the Gödel numbers of the members of A (in dyadic notation) is ∑1. Our job now is to get from the fact that a set of Gödel numbers A0 is ∑1 to the fact that A itself is ∑1. Now, A is the set of all x such that g(x) ∊ A0, and so [by Proposition 4 above] it suffices to show that the relation g(x) = y is ∑1. To this end, we use the following lemma, which was proved in The Beginner’s Guide:
Lemma K. There exists a ∑0 relation K(x, y, z) such that the following two conditions hold:

(1)For any finite set (a1, b1), …, (an, bn) of ordered pairs of numbers (positive integers), there is a number z (called a code number of the set) such that for all x and y, K(x, y, z) holds if and only if (x, y) is one of the pairs (a1, b1), …, (an, bn).
(2)For all x, y, z if K(x, y, z) holds, then x < z and y < z

Problem 6. Using Lemma K, prove the relation g(x) = y is ∑1.
Extending this results to all relations gives:
Proposition 5. Every recursively enumerable relation is ∑1 and every ∑1 relation is recursively enumerable.
A Recursive Pairing Function δ(x, y)
A function f which assigns to each pair (x, y) of numbers a number f(x, y) is said to be 1-1 (which is read as “one-to-one”) if for every number z there is at most one pair (x, y) such that f(x, y) = z. In other words, if f(x, y) = f(z, w), then x = z and y = w. The function f is said to be onto the set N of numbers, if every number z is f(x, y) for some pair (x, y). Thus f is 1-1 onto N iff every number z is f(x, y) for one and only one pair (x, y).
We now want a recursive function δ(x, y) which is 1-1 onto N. [Note that such a function provides an enumeration of all ordered pairs of positive integers.] There are many ways to construct such a function. Here is one way: We start with (1, 1) followed by the three pairs whose highest number is 2, in the order (1, 2), (2, 2), (2, 1), followed by the five pairs whose highest number is 3, in the order (1, 3), (2, 3), (3, 3), (3, 1), (3, 2), followed by … (1, n), (2, n), …, (n, n), (n, 1), (n, 2), …, (n, n - 1), etc. We then take δ(x, y) to be the position that (x, y) takes in this ordering, which is the number of pairs which precede (x, y) plus 1. For example,

In fact, the following hold:

(a)If m ≤ n, then δ(m, n) = (n − 1)2 + m.
(b)If n < m, then δ(m, n) = (n − 1)2 + m + n.

To see that (a) is true, we first note that for any n, the pairs that preceded (1, n) in the ordering are those in which x and y are both less than n, and there are (n − 1)2 such pairs [since there are n − 1 numbers (positive integers) that are less than n]. Thus δ(1, n) = (n − 1)2 + 1. Hence δ(2, n) = (n − 1)2 + 2, δ(3, n) = (n − 1)2 + 3, …, δ(n, n) = (n − 1)2 + n. Thus, for any m ≤ n, δ(m, n) = (n − 1)2 + m. This proves (a).
As for (b), since δ(m,m) = (m − 1)2+m, then δ(m, 1) = (m − 1)2+m+1; δ(m,2) = (m − 1)2 + m + 2, …, δ(m,m − 1) = (m − 1)2 + m + (m − 1). Thus for any n ≤ m, δ(m,n) = (m − 1)2 + m + n.
Problem 7. Prove that the function δ(x,y) is recursive, in fact Σ0.
The Inverse Functions K and L
Each number z is δ(x, y) for just one pair (x, y). We let K (z) [to often be abbreviated Kz] be x, and let L(z) [or Lz] be y. Thus Kδ(x,y) = x and Lδ(x,y) = y.
Problem 8. (a) Prove that δ(Kz, Lz) = z. (b) Prove that the functions K(x) and L(x) are Σ0, and hence recursive.
The n-tupling Functions δn(x1, …, xn)
For each n ≥ 2 we define the function δn(x1, …, xn) by the following inductive scheme:

An obvious induction argument shows that for each n ≥ 2 the function δn(x1, …, xn) is a 1-1 recursive function onto the set N of positive integers.
Problem 9. Prove the following:

(a)For any recursively enumerable set A and n ≥ 2, the relation δn(x1, …, xn) ∊ A is recursively enumerable.
(b)For any recursively enumerable relation R(x1, …, xn), the set A of all numbers δn(x1, …, xn) such that R(x1, …, xn) is recursively enumerable.

The Inverse Functions 
For each n ≥ 2 and 1 ≤ i ≤ n, we define (x) as follows: x = δn(x1, …, xn) for just one n-tuple (x1, …, xn), and we take (x) to be xi. Thus (δn(x1, …, xn)) = xi.
Problem 10. Prove the following:

(a)δn((x), (x), …, (x)) = x.
(b)(x) = Kx and (x) = Lx.
(c)If x =δn+1(x1, …, xn+1), then Kx = δn(x1,…, xn) and Lx = xn+1.
(d)If i ≤ n then  (x) =(Kx). If i = n + 1, then  (x) = Lx.
(e)For any recursively enumerable relation R(x1, …, xn, y1, …, ym), there is a recursively enumerable relation M(x, y) such that for all x1, …, xn, y1, …, ym, we have R(x1, …, xn, y1, …, ym) iff M(δn(x1, …, xn), δm(y1, …, ym)).

III.A Universal System
We now wish to construct a “universal” system (U), in which we can express all propositions of the form that such and such a number is in such and such a recursively enumerable number set, or that such and such an n-tuple of numbers stands in such and such a recursively enumerable relation on n-tuples of numbers. (We recall that we are using the word “number” to mean “positive integer”.)
In preparation for the construction of the system (U), we need to “transcribe” all dyadic systems (elementary formal systems over the alphabet {1, 2}) into a single finite alphabet. We now define a transcribed dyadic system — to be abbreviated “TDS” — to be a system like a dyadic system, except that instead of taking our variables and predicates to be individual symbols, we take two signs, v and the “accent sign” ′ and we define a transcribed variable — to be abbreviated “TS variable” — to be any string v∝v, where ∝ is a string of accents. Similarly, we define a transcribed predicate — to be abbreviated “TS predicate” — to be a string of p’s followed by a string of accents; the number of p’s is to indicate the degree of the predicate. A transcribed dyadic system is then like a dyadic system, except that we use transcribed variables instead of single symbols for variables, and transcribed predicates instead of individual symbols for predicates. We thus have a single alphabet K7, namely the symbols from which all TS variables and predicates, and eventually formulas, are constructed:

Note that the symbols 1 and 2, from which we will construct our dyadic numerals for the positive integers, correspond to the alphabet K in the definition of an elementary formal system given in The Beginner’s Guide [Smullyan, 2014] and earlier here.
It is obvious that representability in an arbitrary dyadic system is equivalent to representability in a transcribed dyadic system.
We define “TS term”, “atomic TS formula”, “TS formula”, “TS sentence” as we did “term”, “atomic formula”, “formula”, “sentence”, only using TS variables and TS predicates instead of variables and predicates, respectively.
We now construct the universal system (U) in the following manner: We first extend the alphabet K7 to the alphabet K8 by adding the symbol “∧”. Then, by a base we mean a string of the form X1, …, Xk (or a single formula X, if k = 1). Here X and X1, …, Xk are all TS formulas. These TS formulas are called the components of the base X1 ∧ … ∧ Xk, or of the base X. Then we extend the alphabet K8 to the alphabet K9 by adding the symbol “ ⊢ ”, and we define a sentence of (U) to be an expression of the form B⊢ X, where B is a base and X is a TS sentence (a TS formula with no variables). We call the sentence B⊢ X true if X is provable in that TDS whose axiom schemes are the components of B. Thus X1 ∧ … ∧ Xk ⊢ X is true iff X is provable in that TDS whose axiom schemes are X1, …, Xk.
By a predicate H of (U) [not to be confused with a TS predicate], we mean an expression of the form B ⊢ P, where B is a base and P is a transcribed predicate. We define the degree of B ⊢ P to be the degree of P (i.e. the number of occurrences of p in P). A predicate H of the form B ⊢ P of (U) of degree n is said to represent the set of all n-tuples (a1, …, an) of numbers such that the sentence B ⊢ Pa1, …, an is true. Thus a predicate X1 ∧ … ∧ Xk ⊢ P of degree n represents (in U) the set of all n-tuples (a1, …, an) such that Pa1, …, an is provable in that TDS whose axiom schemes are X1, …, Xk; thus it represents in (U) the same relation as P represents in the TDS whose axiom schemes are X1, …, Xk. Thus a relation or set is representable in (U) if and only if it is recursively enumerable. In this sense, (U) is called a universal system for all recursively enumerable relations.
We shall let T be the set of true sentences of (U), and let T0 be the set of Gödel numbers of the members of T (using the dyadic Gödel numbering).
We now wish to show that the set T is formally representable in an elementary formal system over the alphabet K9, and that the set T0 is recursively enumerable. Our entire development of recursion theory is based on this crucial fact!
We are to construct an elementary formal system W over K9 in which T is represented. To do this, we must first note that the implication sign of W must be different from the implication sign of transcribed systems. We will use “→” for the implication sign of W, and use “imp” for the implication sign of transcribed systems. Similarly, we will use the ordinary comma for the punctuation sign of W and use “com” for the punctuation sign of transcribed systems. For variables of W, (not to be confused with transcribed variables), we will use the letters x, y, z, w, with or without subscripts. Predicates (not to be confused with predicates of (U) or transcribed predicates) are to be introduced as needed.
Problem 11. Construct w by successively adding predicates and axiom schemes to represent the following properties and relations:
N(x) : x is a number (dyadic numeral)
Acc(x) : x is a string of accents
Var(x) : x is a transcribed variable
dv (xy) : x and y are distinct TS variables
P (x) : x is a transcribed predicate
t(x) : x is a transcribed term
F0(x) : x is a transcribed atomic formula
F (x) : x is a transcribed formula
S0(x) : x is a transcribed atomic sentence
Sub(x, y, z, w) : x is a string compounded from TS variables, numerals, TS predicates, com and imp, and y is a TS variable, and w is the result of substituting z for every occurrence of y in x
pt(x, y) : x is a TS formula and y is the result of substituting numerals for some (but not necessarily all) TS variable of x (such a formula y is called a partial instance of x)
in(x, y) : x is a TS formula and y is the result of substituting numerals for all variables of x (such a y is an instance of x)
B(x) : x is a base
C(x, y) : x is a component of the base y
T (x) : x is a true sentence of (U)
Since T0 is formally representable, then T0 is ∑1, hence recursively enumerable.
The Recursive Unsolvability of (U)
We now wish to show that the complement  of T0 is not recursively enumerable, and thus that the set T0 is recursively enumerable, but not recursive.
For any set A of numbers, define a sentence X of (U) to be a Gödel sentence for A if either X is true and its Gödel number X0 is in A, or X is false and its Gödel number is not in A. In other words, a sentence X of (U) is a Gödel sentence for A if X is true iff X0 ∈ A. There obviously cannot be a Gödel sentence for , for such a sentence would be true iff it were not true (i.e. iff its Gödel number was not in T0), which is impossible. Thus, to show that is not recursively enumerable, it suffices to show that for any recursively enumerable set, there is a Gödel sentence.
To this end, for any string X of symbols of K7, we define its norm to be XX0, where X0 is the Gödel number of X. Any dyadic numeral n itself has a Gödel number n0, hence a norm nn0. We recall that our dyadic Gödel numbering has the advantage of being an isomorphism with respect to concatenation, i.e. if n is the Gödel number of X and m is the Gödel number of Y, then nm is the Gödel number of XY. Therefore, if n is the Gödel number of X, then nn0 is the Gödel number of Xn (the norm of X). Thus the Gödel number of the norm of X is the norm of the Gödel number of X.
For any number x, we let normx be the norm of x.
Problem 12. Show that the relation normx = y is recursively enumerable.
For any set A of numbers we let be the set of all numbers whose norm is in A.
Lemma. If A is recursively enumerable, so is .
Problem 13. Prove the above lemma.
Problem 14. Using the above lemma, now prove that if A is recursively enumerable, then there is a Gödel sentence for A.
As we have seen, there cannot be a Gödel sentence for, and thus by Problem 14, the set  cannot be recursively enumerable. And so the set T0 is recursively enumerable, but not recursive.
Solutions to the Problems of Chapter 4
1. (a)We have already proved this for when R1 and R2 are sets (i.e. relations of degree 1), but the proof for relations of degree n is essentially the same: Suppose R1 and R2 are relations of degree n and that both are formally representable over K. There they are representable in a common elementary formal system over K. In this system let P1 represent R1 and let P2 represent R2. Then take a new predicate Q and add the following two axioms:

Then Q represents R1∪R2.
To represent R1 ∩ R2, instead of the above two axioms, add the following single axiom:

Then Q will represent R1∩R2.

(b)Let P represent the relation R(x1, …, xn, y) in (E). Take a new predicate Q of degree n and add the following axiom scheme:

Then Q represents the relation

(c)Let P represent the relation R(x1, …, xk) in (E). Take a new predicate Q and add the following axiom scheme:

Then Q represents the relation



2.Complementation is trivial. As for unions and intersections, suppose that W1 and W2 are of the same degree and are both solvable over K. Then W1, W2,  and  are all formally representable over K. We already know from Problem 1 that W1 ∪ W2 and W1 ∩ W2 are formally representable over K. It remains to show that the complements of W1 ∪ W2 and W1 ∩ W2 are formally representable over K. But these complements are  and , which we can use our knowledge of Boolean sets to see to be equal to  and , respectively. Since these latter sets are the intersections and unions of sets we know to be formally representable over K, we now see that  and  are formally representable over K, and thus that the union or intersection of two sets that are solvable over K is solvable over K.
Now assume that the R(x1, …, xk) is solvable over K. Then its complement, which we will denote by  is also solvable over K, since the two are complements of each other. We have seen that the explicit transformations λ x1, …, xnR(α1,…, αk) and λ x1, …, xn , where each αi is one of the variables x1, …, xn or a constant, are also formally representable over K. But each of these is the complement of the other, so we see that the collection of sets and relations solvable over K is closed under explicit transformation as well.
3. (a)Suppose that the relation R(x1, …, xn, y) is recursively enumerable. Let D be a dyadic system in which R is represented by the predicate P, x < y is represented by E, and the relation “the successor of x is y” is represented by S.
Let W (x1, …, xn, y) be the relation z ≤ y)R(x1, …, xn, z). To represent the relation W, take a new predicate Q and add the following two axiom schemes:

Then Q represents W.
As for the relation (∃z ≤ y)R(x1, …, xn, z) (as a relation between x1, …, xn, y), it is equivalent to the relation ∃z(z ≤ y ∧ R(x1, …, xn, z). Let S(x1, …, xn, y, z) be the relation z ≤ y∧R(x1, …, xn, z). It must be recursively enumerable, because it is the intersection S1 ∩ S2 of the relations S1 and S2 where

and

Since S1 and S2 are respectively definable from the recursively enumerable relations z ≤ y and R(x1, …, xn, z) by explicit transformation, they are recursively enumerable, and so is their intersection S. Thus so is the existential quantification ∃zS(x1, …, xn, z), which is the relation (∃z ≤ y)R(x1, …, xn, z).
(b)Suppose R is recursive. Thus R and its complement  are both recursively enumerable. Since R is recursively enumerable, so are  and ∃F(R) [by (a)]. Since  is recursively enumerable, so are  and  [again by (a)]. But as the reader can verify,  and  are respectively the complements of ∃F(R) and , and so the complements of ∃F (R) and  are both recursively enumerable, which means that ∃F (R) and  are both recursive.
4.Let us first note that for any relation R(x1, …, xn, y, z) the following conditions are equivalent:
(1)∃y∃zR(x1, …, xn, y, z)
(2)∃ω(∃y ≤ ω)(∃z ≤ ω)R(x1, …, xn, y, z)
It is obvious that (2) implies (1). Now, suppose that (1) holds. Then there are numbers y and z such that R(x1, …, xn, y, z). Let ω be the maximum of y and z. Then y ≤ ω and z ≤ ω, and so (∃y ≤ ω)(∃z ≤ ω)R(x1, …, xn, y, z) holds for such a number ω. Hence (2) holds.
Now suppose R(x1, …, xn, y) is Σ1. Then there is a Σ0 relation S(x1, …, xn, y, z) such that R(x1, …, xn, y) is equivalent to ∃zS(x1, …, xn, y, z). Thus ∃yR(x1, …, xn, y) is equivalent to ∃y∃zS(x1, …, xn, y, z), which, as shown above, is Σ1 [since the relation (∃y ≤ ω)(∃z ≤ ω)S(x1, …, xn, y, z) is Σ0].
5.x ∊ A′ iff f(x) ∊ A, which is true iff ∃y(f(x) = y ∧ y ∊ A). We first show that the relation f(x) = y ∧ y ∊ A is Σ1.
Well, since the relation f(x) = y is Σ1, then there is a Σ0 relation S(x, y, z) such that f(x) = y iff ∃zS(x, y, z).
Since the set A is Σ1, there is a Σ0 relation R(x, y) such that x ∊ A iff ∃ωR(x, ω). Thus f(x) = y ∧ y ∊ A, iff

which is true iff ∃z∃ω(S(x, y, z) ∧ R(x, ω)). Since the condition S(x, y, z) ∧ R(x, ω) is Σ0, then ∃ω(S(x, y, z)R(x, ω), is Σ1, hence so is the condition ∃z∃ω(S(x, y, z)R(x, ω)) [by Problem 4].
Thus the condition f(x) = y ∧ y ∊ A is Σ1, and therefore so is ∃y(f(x) = y ∧ y ∊ A) [again by Problem 4]. Thus A′ is Σ1.
6.Our goal is to prove that the relation g(x) = y is Σ1. First let R1(x, y) be the Σ0 relation (x = 1 ∧ y = 12)∨ (x = 2 ∧ y = 122) Thus R1(x, y) holds iff x is the single digit 1 or the single digit 2 and g(x) = y.
Let R2(x, y, x1, y1) be the Σ0 relation

Call a set S of ordered pairs of numbers special, if for every pair (x, y) in S, either R1(x, y) or (∃x1 ≤ x)(∃y1 ≤ y)((x1, y1) ∊ S ∧ R2(x, y, x1, y1)). It is easily seen by mathematical induction on the length of x (i.e. the number of occurrences of 1 or 2) that if S is special, then for every pair (x, y) in S, it must be that g(x) = y.
Next we must show that conversely, if g(x) = y, then (x, y) belongs to some special set.
Well, the x in g(x) = y must be of the form x = d1d2 … dn, where each di is the digit 1 or the digit 2. For each i ≤ n, we let gi be the dyadic Gödel number of di (thus, if di = 1, then gi = 12 and if di = 2, then gi = 122). Now let Sx be the set


The set Sx is obviously special, and contains the pair (x, g(x)) [which is the pair (d1d2 … dn, g1g2 … gn)]. Thus if g(x) = y, then (x, y) ∈ Sx and so (x, y) belongs to a special set. Thus we have:
(1) g(x) = y iff (x, y) belongs to a special set.
We now use Lemma K. For any number z, we let Sz be the set of all pairs (x, y) such that K(x, y, z) holds. Thus (x, y) ∈ Sz iff K(x, y, z). Since (x, y) ∈ Sz iff K(x, y, z), then to say that Sz is special is to say that for all x and y, K(x, y, z) implies that the following ∑0 condition — call it A(x, y, z) — holds:

Thus Sz is special iff the following condition holds:

However, C(z) is equivalent to the following 0 condition:

To see the equivalence, it is obvious that C(z) implies P (z). To see the converse, suppose P (z) holds. We must show that C(z) holds. Let x and y be arbitrary numbers. We need to show that if K(x, y, z) holds, then so does A(x, y, z). If K(x, y, z) holds, then it must also be true that x ≤ z and y ≤ z [by the second condition in Lemma K], and therefore A(x, y, z) holds [by the assumption P (z)]. Thus, for all x and y, K(x, y, z) ⊃ A(x, y, z), and so C(z) holds.
This proves that C(z) is equivalent to P (z), and hence Sz is special iff P(z) holds.
Finally, g(x) = y iff (x, y) belongs to some special set, which is true iff

which is true iff

Thus the relation g(x) = y is ∑1 (since K(x, y, z) ∧ P (z) is ∑0), which completes the proof that every recursively enumerable set A is ∑1.
7.δ(x, y) = z iff

The condition z = (y − 1)2 + x can be written

The condition z = (x − 1)2 + x + y can be written

As the formula expressing δ(x, y) = z can be rewritten as a ∑0 formula, the function δ is ∑0, and consequently recursive [by Proposition 2].
8. (a)z = δ(x, y) for some x and y. Then Kz = x and Lz = y, so that δ(x, y) = δ(Kz, Lz). Thus z [which is δ(x, y)] equals δ(Kz, Lz).
(b)K(z) = x iff (∃y ≤ z)(δ(x, y) = z) and L(z) = y iff (∃x ≤ z)(δ(x, y) = z). (It is not difficult to verify that if δ(x, y) = z, both x ≤ z and y ≤ z.)
9.Conceptually, we know the following:
(a)δn(x1, …, xn) ∈ A iff ∃y(δn(x1, …, xn) = y ∧ y ∊ A).
(b)x ∊ A iff ∃x1 … ∃xn(x = n(x1, …, xn) ∧ R(x1, …, xn)).
To see that this understanding solves the problem, we can amplify as follows:
(a)When we say the set A is recursively enumerable, what we mean is that there’s a relation of degree 1, R(x), in the form of a 1 formula F1 with one free variable x (and one existential quantifier) such that the set of numbers in A is the set of all numbers x for which F1(x) holds. Also, since δn(x1, …, xn) = y has been shown to be recursive, there is a ∑0 formula F2 containing the free variables x1, …, xn, y which holds for an n + 1-tuple of numbers iff the equation δn(x1, …, xn) = y is true (when the n + 1 numbers are “plugged in” appropriately). So δn(x1, …, xn) ∊ A can be expressed by the formula

This can be seen to be a ∑1 formula, for the single unbound quantifier in F1 can be brought to the front in a way the reader should be familiar with by now.
(b)Since the relation R(x1, …, xn) is recursively enumerable, there is a Σ1 formula F1 with the free variables x1, …, xn such that F1(x1, …, xn) holds iff R(x1, …, xn) is true. Assuming F2 as in part (a) of this problem, x ∈ A can be expressed by the formula ∃x1 … ∃xn(F2(x1, …, xn, x)∧ F1(x1, …, xn)).
10.(a) For a given x, since δn is 1-1 and onto, x is equal to δn(x1, …, xn) for just one n-tuple x1, …, xn. Moreover for each  is defined to be xi. So

(b)Since x = δ2(Kx, Lx) [by the definition of δ2 and Problem 8], we have that  and 
(c)Suppose x = δn+1(x1, …, xn+1). Then [by the definition of δn+1], x = δ(δn(x1, …, xn), xn+1). Thus Kx = δn(x1, …, xn) and Lx = xn+1.
(d)Let x = δn+1(x1, …, xn, xn+1) for some x1, …, xn+1. Then by definition

Now suppose i≤ n. Then Kx = δn(x1, …, xn) [by (c)], so that

But also 
Now, 
(e)Take  Then R(x1, …, xn, y1, …, ym) iff M(δn(x1, …, xn), δm(y1, …, ym)).
11.N(x): x is a number

Acc(a): xs a string of accents

Var (x): x is a TS variable

dv(x, y): x and y are distinct TS variable

P(x): x is a TS predicate

t(x): x is a TS term

F0(x): x is a TS atomic formula

F (x): x is a TS formula

S0(x): x is a TS atomic sentence

S(x): x is a TS sentence

Sub(x, y, z): x is a string, y a TS variable, z a numeral, and …

pt(x, y): x is a TS atomic formula and y is a partial instance of x

in(x, y): x is a TS atomic formula and y is an instance of x

B(x): x is a TS base

C(x, y): x is a component of the base and y

T (x): x is a true TS sentence

12.Let D be a dyadic system with predicate G and the following axiom schemes:

Then G represents the relation “the Gödel number of x is y”. Now add a predicate M and the axiom scheme:

Then M represents the relation “norm(x) = y”.
13.Suppose A is recursively enumerable. Let D be a dyadic system in which “A” represents the set A and “M” represents the relation “the norm of x is y”. Add a predicate “B” and the axiom scheme

Then B represents the set A#.
Remark. Without using elementary formal systems or dyadic system, one can directly show that if A is Σ1, so is A#, and hence that if A is recursively enumerable, so is A#. As a good exercise, show that if A is Σ1, so is A#.
14.The following beautiful proof is a modification of Gödel’s famous diagonal argument in his incompleteness theorem proof, and does for recursion theory what Gödel did for mathematical systems.
Suppose A is recursively enumerable. Then so is A# (by the Lemma). Let H be a predicate of (U) that represents A#. Then for every number n,
Hn is true iff n ∈ A#, which if true iff nn0 ∈ A.
Thus Hn is true iff nn0 ∈ A.
We take for n the Gödel number h of H, and obtain that H h is true iff hh0 ∈ A.
However, hh0 is the Gödel number of Hh! Thus Hh is true iff its Gödel number is in A, and if thus a Gödel sentence for A.







Chapter 5
Some Recursion Theory
I.Enumeration and Iteration Theorems
We now turn to two basic theorems of recursion theory — the enumeration theorem, due to Emil Post [1943; 1944] and to Stephen Kleene [1952], and the iteration theorem, due to Kleene [1952] and independently discovered by Martin Davis [1958, 1982]. Once we have established these two results, the universal system (U) of the last chapter will have served its purpose, and will no longer be needed.
The Enumeration Theorem
We wish to arrange all recursively enumerable sets in an infinite sequence ω1, ω2, …, ωn, … (allowing repetitions) in such a way that the relation “ωx contains y” is a recursively enumerable relation between x and y.
We recall that for any number (dyadic numeral) y, we are letting y0 denote the dyadic Gödel number of y. For any x and y we now define r(x, y) to be xy0 [x followed by y0].
We use the letter T to denote the set of true sentences of the universal system (U) and use T0 to denote the set of Gödel numbers of the sentences in T. We now define ωn to be the set of all x such that r(n, x) ∈ T0. Thus x ∈ ωn iff r(n, x) ∈ T0.
Problem 1. (a) Prove that ωn is recursively enumerable. (b) Prove that for every recursively enumerable set A, there is some n such that A = ωn.
We shall call n an index of a set A if A = ωn. We have just shown that a set A is recursively enumerable if and only if it has an index, and so we have succeeded in arranging all recursively enumerable sets in an infinite sequence ω1, ω2, …, ωn, … in such a manner that the relation ωx contains y (which is the relation r(x, y) ∊ T0) is recursively enumerable.
We next wish to show that for each n ≥ 2, we can enumerate all recursively enumerable relations of degree n in an infinite sequence  such that the relation  is a recursively enumerable relation among the variables x, y1, …, yn. For this, it will be useful to use the indexing of recursively enumerable sets that we already have, along with the pairing function δ(x, y) and the n-tupling functions δn(x1, …, xn). We simply define  to be δn(x1, …, xn) ∊ ωi. Since ωi is recursively enumerable and the function δn(x1, …, xn) is also recursively enumerable, so is  [by (a) of Problem 9 of Chapter 4]. Also, for any recursively enumerable relation R(x1, …, xn), the set A of numbers δn(x1, …, xn) such that R(x1, …, xn) holds is recursively enumerable [by (b) of Problem 9 of Chapter 4]. Thus the set A has some index i so that R(x1, …, xn) holds iff δn(x1, …, xn) ∊ ωi, which is true iff . Thus  Thus every recursively enumerable relation has an index. [We also write  for x ∊ ωn, since we are regarding number sets as relations of degree 1.]
We have thus proved:
Theorem E. [The Enumeration Theorem] For each n, the set of all recursively enumerable relations of degree n can be enumerated in an infinite sequence  such that the relation , as a relation among x, y1, …, yn, is recursively enumerable.
For each n we let Un+1(x, y1, … yn) be the relation  [as a relation among x, y1, …, yn]. These relations are referred to as universal relations: U2 is the universal relation for all recursively enumerable sets, U3 is the universal relation for all recursively enumerable relations of degree 2, and so forth.
We sometimes write Rx(y1, …, yn) instead of , since the number of arguments makes the superscript n clear. And we write U(x, y1, …, yn) instead of Un+1(x, y1, …, yn).
Iteration Theorems
The second basic tool of recursion theory is the iteration theorem, which we will shortly state and prove in both a simple and more general form. To fully appreciate the significance and power of these theorems, the reader should first try solving the following exercises:
Exercise 1. We know that for any two recursively enumerable sets A and B, the union A ∪ B is recursively enumerable. Moreover, the process of finding this union is effective, in the sense that there is a recursive function φ(i, j) such that if i is an index of A and j is an index of B, then φ(i, j) is an index of A ∪ B — in other words there is a recursive function φ(i, j) such that for all numbers i and j, ωφ(i,j) = ωi ∪ ωj. Prove this.
For any relation R(x, y), by its inverse is meant the relation  satisfying the condition  (x, y) iff R(x, y).
Exercise 2. Show that there is a recursive function t(i) such that for every number i the relation Rt(i)(x, y) is the inverse of the relation Ri(x, y), i.e. Rt(i)(x, y) iff Ri(y, x). [This can be paraphrased: Given an index of a recursively enumerable relation of two arguments, one can effectively find an index of its inverse.]
For any function f(x) and number set A, by f-1(A) is meant the set of all numbers n such that f(n) ∊ A. Thus n ∊ f-1(A) iff f(n) ∊ A.
Exercise 3. Prove that for any recursive function f(x), there is a recursive function φ(i) such that for every number i, it is the case that

The reader who has solved the above exercises has probably had to go back to the universal system (U) each time. The iteration theorem, to which we now turn, will free us from this once and for all!
Theorem 1. [Simple Iteration Theorem] For any recursively enumerable relation R(x, y) there is a recursive function t(i) such that for all numbers x and i, x ∊ ωt(i) iff R(x, i).
Problem 2. Prove Theorem 1. [Hint: The recursively enumerable relation R(x, y) is represented in the universal system (U) by some predicate H. Consider the Gödel number h of H, as well as the Gödel number c of the punctuation symbol com.]
Theorem 2. [Iteration Theorem] For any recursively enumerable relation R(x1, …, xm, y1, …, yn) there is a recursive function φ(i1, …, in) such that for all x1, …, xm, i1, …, in,

One way to prove Theorem 2 is to use Theorem 1 and the fact that for any recursively enumerable relation R(x1, …, xm, y1, …, yn) there is a recursively enumerable relation M(x, y) such that R(x1, …, xm, y1, …, yn) iff M(δm(x1, …, xm), δn(y1, …, yn)) [by Problem 10 (e) of Chapter 4].
Problem 3. Prove Theorem 2.
Let us now see how the iteration theorem provides simple and elegant solutions to the three recently given exercises.
In Exercise 1, let R(x, y1, y2) be the recursively enumerable relation

Then, by the iteration theorem, there is a recursive function φ(i1, i2) such that for all numbers x, i1, i2,

Therefore, 
For Exercise 2, let R(x1, x2, y) be the recursively enumerable relation Ry(x2, x1). It is recursively enumerable because it is explicitly defined from the recursively enumerable relation U(x1, x2, y). Then by the simple iteration theorem there is a recursive function φ(i) such that for all x and i, Rφ(i)(x1, x2) iff R(x1, x2, i), which is true iff Ri(x2, x1).
For Exercise 3, given a recursive function f(x), consider the recursively enumerable relation R(x, y) – viz. f(x) ∊ ωy [it is recursively enumerable since it is equivalent to ∃z(f(x) = z ∧ z ∊ wy)]. By the simple iteration theorem there is a recursive function t(i) such that for all x and i, x ∊ ωt(i) iff R(x, i) which is true iff f(x) ∊ ωi. Thus ωt(i) = f-1(ωi).
More serious applications of the iteration theorem will soon follow.
Maximal Indexing
A denumerable sequence A1,A2, … ,An, … of all recursively enumerable sets is called a recursively enumerable indexing if the relation x ∊ Ay is recursively enumerable. It is called a maximal indexing if for every recursively enumerable indexing B1, B2, …, Bn, … there is a recursive function f(i) such that for all i the condition Af(i) = Bi holds. [Informally, this means that given a B-index of a recursively enumerable set, we can effectively find an A-index of the set.]
Another application of the iteration theorem yields:
Theorem 3. The enumeration ω1, ω2, …, ωn, … given by the enumeration theorem in maximal.
Problem 4. Prove Theorem 3.
The following theorem is easily obtained from the iteration theorem, combined with the enumeration theorem.
Theorem 4. [Uniform Iteration Theorem] For any numbers m and n, there is a recursive function φ(i, i1, …, in) [sometimes written ] such that for all x1, …, xm, i1, …, in,

Problem 5. Prove Theorem 4.
II.Recursion Theorems
Recursion theorems, which can be stated in many forms, and are sometimes called fixed point theorems, have many applications to recursion theory and metamathematics. To illustrate their rather startling nature, consider the following mathematical “believe it or nots.”
Which of the following propositions, if true, would surprise you?

1.There is a number n such that ωn = ωn+1.
2.For any recursive function f(x), there is a number n such that ωf(n) = ωn.
3.There is a number n such that ωn contains n as its only member, i.e. ωn = {n}.
4.For any recursive function f(x) there is a number n such that ωn = {f(n)}.
5.There is a number n such that for every number x, x∊ωn iff n ∊ ωx.

Well, believe it or not, all the above statements are true! They are all special cases of the theorem that follows:
Consider a recursively enumerable relation R(x, y). Now consider a number n and the set x: R(x, n), i.e. the set of all numbers x such that R(x, n) holds. This set is recursively enumerable, hence is ωn* for some n*. Given the relation R, what n* is depends on what n is. Wouldn’t it be curious if n could be chosen such that n* is n itself? Well, this is indeed so:
Theorem 5. [Weak Recursion Theorem] For any recursively enumerable relation R(x, y), there is a number n such that ωn = {x : R(x, n)} [in other words, for all x, we have n ∊ ωn iff R(x, n)].
I will give the beginning of the proof, and leave it as a problem for the reader to finish it.
The relation Ry(x, y), as a relation between x and y is recursively enumerable. Hence, by the simple iteration theorem there is a recursive function t(y) such that for all x and y, x ∊ ωt(y) iff Ry(x, y).
Now, take any recursively enumerable relation R(x, y). Then the relation R(x, t(y)) is also a recursively enumerable relation between x and y, hence has some index m. Thus Rm(x, y) iff R(x, t(y)). Now comes the great trick: ….
Problem 6. Finish the proof.
As a consequence of Theorem 5, we have:
Theorem 6. For any recursive function f(x) there is a number n such that ωf(n) = ωn.
Problem 7. (a) Prove Theorem 6. (b) Now prove the other four “believe it or nots”.
Discussion. We have derived Theorem 6 as a corollary of Theorem 5. One can alternatively first prove Theorem 6 directly, and then obtain Theorem 5 as a corollary of Theorem 6 as follows: Assume Theorem 6. Now consider a recursively enumerable relation R(x, y). By the iteration theorem, there is a recursive function f(x) such that for all x and y,

But by Theorem 6, there is a number n such that ωf(n) = ωn. Thus for all x, x ∊ ωn iff x ∊ ωf(n), which is true iff R(x, n). Thus x ∊ ωn iff R(x, n).
Unsolvable Problems and Rice’s Theorem
Some of the aforementioned believe it or nots may seem to be somewhat frivolous applications of the weak recursion theorem. We now turn to a far more significant application.
Let us call a property P(n) solvable if the set of all n having the property is a recursive set. For example, suppose P(n) is the property that ωn is a finite set. To ask whether this property is solvable is to ask whether the set of all indices of all finite sets is recursive.
Here are some typical properties P(n) that have come up in the literature:

(1)ωn is an infinite set.
(2)ωn is empty.
(3)ωn is recursive.
(4)ωn = ω1.
(5)ωn contains 1.
(6)The relation Rn(x, y) is single-valued, i.e. for each x there is at most one y such that Rn(x, y).
(7)The relation Rn(x, y) is functional, i.e. for each x there is exactly one y such that Rn(x, y).
(8)All numbers are in the domain of Rn(x, y), i.e. for each (positive integer) x there is at least one y such that Rn(x, y).
(9)All numbers are in the range of Rn(x, y), i.e. for each (positive integer) y there is a least one x such that Rn(x, y).

For each of these nine properties, we can ask if it is solvable. These are typical questions of recursion theory that have been answered by individual methods. Rice’s theorem, to which we now turn, answers a host of such questions in one fell swoop.
A number set A is called extensional if it contains with any index of a recursively enumerable set all other indices of it as well — in other words, for every i and j, if i ∊ A and ωi = ωj, then j ∊ A.
Theorem R. [Rice’s Theorem] The only recursive extensional sets are ℕ and .
We will soon see how Rice’s theorem [Rice, 1953] is relevant to the nine properties mentioned above, but first let us go to its proof. We first need the following:
Lemma. For any recursive set A other than ℕ and , there is a recursive function f(x) such that for all n, n ∊ A iff f(n) ∉ A
Problem 8. (a) Prove the above lemma. [Hint: If A is neither ℕ nor , then there is at least one number a ∊ A and at least one number b ∉ A.Use these two numbers a and b to define the function f(x). (b) Now prove Rice’s Theorem. [Hint: Use the function f(x) of the Lemma and then apply Theorem 6 to the function f(x).]
Applications
Let us consider the first of the aforementioned nine properties P (n), namely that ωn is infinite. Obviously, if ωi is infinite and ωi = ωj, then ωj is also infinite. Thus the set of the indices of all infinite sets is extensional. Also, there is at least one n such that ωn is infinite and at least one n such that ωn is not infinite. Hence by Rice’s Theorem, the set of the indices of all infinite sets is not recursive.
The same argument applies to all the other eight cases, so that none of them are solvable!
The Strong Recursion Theorem
By the weak recursion theorem, for any number i, there is a number n such that ωn = {x : Ri(x, n)}. Given the number i, can we effectively find such an n as a recursive function of i? That is, is there a recursive function φ(i) such that for every number i, ωφ(i) = {x : Ri(x, φ(i))}? The answer is yes, and this is Myhill’s famous fixed point theorem, which is a special case of the following:
Theorem 7. [Strong Recursion Theorem] For any recursively enumerable relation M(x, y, z), there is a recursive function φ(i) such that for all i, ωφ(i) = {x : M(x, i, φ(i))}.
We will give two proofs of this important theorem. The first is along traditional lines, and requires two applications of the iteration theorem. The second proof is along different lines, and requires only one application of the iteration theorem.
Here is the beginning of the first proof (to be finished by the reader in Problem 9(a)).
As in the proof of the weak recursion theorem, by the (simple) iteration theorem, for every y, there is a recursive function d(y) such that ωd(y) = {x : Ry(x, y)}. Now, given a recursively enumerable relation M(x, y, z), consider the recursively enumerable relation M(x, z, d(y)) as a relation among x, y and z. By another application of the iteration theorem, there is a recursive function t(i) such that for all i, x and y, Rt(i)(x, y) iff M(x, i, d(y)). Then ….
For the second proof (to be finished by the reader – Problem 9 (b) below), let S(x, y, z) be the recursively enumerable relation Rz(x, y, z). By the iteration theorem, there is a recursive function t(y, z) such that for all y and z, ωt(y,z) = {x : S(x, y, z)}, and thus ωt(y,z) = {x : Rz(x, y, z)}.
Now, given a recursively enumerable relation M(x, y, z), the relation M(x, y, t(y, z)) is recursively enumerable, hence has an index h. Thus Rh(x, y, z) iff M(x, y, t(y, z)). Then ….
Problem 9. (a) Finish the first proof. (b) Finish the second proof.
As a special case of Theorem 7, we have:
Theorem 8. [Myhill’s Fixed Point Theorem] There is a recursive function φ(y) such that for all y, ωφ(y) = {x : Ry(x, φ(y))}.
Problem 10. Why is this a special case?
Creative, Productive, Generated and Universal Sets
To say that a set A is not recursively enumerable is to say that given any recursively enumerable subset ωi of A there is a number n in A but not in ωi — a witness, so to speak that ωi is not the whole of A. A set A is called productive if there is a recursive function φ(x) — called a productive function for A — such that for every number i, if ωi is a subset of A, then φ(i) ∊ A – ωi [i.e. φ(i) is in A, but not in ωi].
A set A is called co-productive if its complement  is productive. This is equivalent to the condition that there is a recursive function φ(x) — called a co-productive function for A — such that for every number i, if ωi is disjoint from A, then φ(i) lies outside A and outside ωi.
A set A is called creative if it is co-productive and recursively enumerable. Thus a creative set A is one which is recursively enumerable, and is such that there is a recursive function φ(i) — called a creative function for A — such that for any number i, if ωi is disjoint from A, then φ(i) lies outside both A and ωi (it is a witness to the fact that  is not recursively enumerable).
Post’s Sets C and K
A simple example of a creative set is Post’s set C, the set of all numbers x such that x ∊ ωx. Thus for any number i, i ∊ C iff i ∊ ωi. If ωi is disjoint from C, then i is outside both ωi and C, and therefore the identity function I(x) is a creative function for C.
A less immediate example of a creative set is the set K of all numbers ∂(x, y) such that x ∊ ωy. The set K was also introduced by Post [1943] and is sometimes called the complete set. The proof that K is creative is less immediate than the proof for C, and involves use of the iteration theorem, as we will see.
Complete Productivity and Creativity
It is also correct to say that a set A is not recursively enumerable iff for every recursively enumerable set ωi, whether a subset of A or not, there is a number j such that j ∊ A iff j ≠ ωi — thus attesting to the fact that A ≠ ωi. By a completely productive function for A is meant a recursive function φ(x) such that for every number i, φ(i) ∊ A iff φ(i) φ ∉ ωi. A set A is called completely productive if there is a completely productive function for A.
A set A is called completely creative if A is recursively enumerable and there is a completely productive function φ(x) for the complement  of A; and such a function is called a completely creative function for A. Thus a completely creative function for A is a recursive function φ(x) such that for every number i, φ(i) ∊ A iff φ(i) ∊ ωi.
Post’s set C of all x such that x ∊ ωx is obviously not only creative, but completely creative, since the identity function is a completely creative function for C. Therefore a completely creative set exists. Post apparently did not capitalize on the fact that C is not only creative, but completely creative. I hazard a guess that if he had capitalized on this fact, then many results in recursion theory would have come to light sooner.
We will see later that in fact any productive set is completely productive, and hence that any creative set is completely creative. This important result is due to John Myhill [1955], and will be seen to be a consequence of the strong recursion theorem.
Generative Sets
Many important results about completely creative sets do not use the fact that the set is recursively enumerable, and so we define a set A to be generative if there is a recursive function φ(x) — called a generative function for A — such that for all numbers i, φ(i) ∊ A iff φ(i) ∊ ωi. Thus a generative set is like a completely creative set except that it is not necessarily recursively enumerable. A set A is generative iff its complement  is completely productive. Our emphasis will be on generative sets.
Many-One Reducibility
By a (many-one) reduction of a set A to a set B is meant a recursive function f(x) such that A = f−1(B); i.e. for all numbers x, x ∊ A iff f(x) ∊ B. A set A is said to be (many-one) reducible to B if there is a (many-one) reduction of A to B. There are other important types of reducibility that occur in the literature of recursion theory, but throughout this chapter, reducible will mean many-one reducible.
Theorem 9. If A is reducible to B and A is generative, then B is generative.
It will be useful to state and prove Theorem 9 in a more specific form: For any recursively enumerable relation R(x, y), by the iteration theorem there is a recursive function t(y) such that ωt(i) = {x : R(x, i)}. Such a function t(y) will be called an iterative function for the relation R(x, y). We will now state and prove Theorem 9 in the more specific form:
Theorem 9*. If f(x) reduces A to B and φ(x) is a generative function for A, and t(y) is an iterative function for the relation f(x) ∊ ωy, then f(φ(t(x))) is a generative function for B.
Problem 11. Prove Theorem 9*.
Universal Sets
A set A is called universal if every recursively enumerable set is reducible to A.
Post’s complete set K is obviously universal.
Problem 12. Why?
Theorem 10. Every universal set is generative.
Problem 13. Prove Theorem 10. [Hint: Use Theorem 9.]
Uniformly Universal Sets
For any function f(x, y) of two arguments, and for any number i, by fi is meant the function which assigns to each x the number f(x, i). Thus fi(x) = f(x, i).
A set A is called uniformly universal under a recursive function f(x, y) if for all i and x, x ∊ ωi iff f(x, i) ∊ A; in other words, iff fi reduces ωi to A. And we say that A is uniformly universal if it is universal under some recursive function f(x, y).
Theorem 11. Every universal set is uniformly universal.
Problem 14. Prove Theorem 11. [Hint: Use the fact that Post’s set K is not only universal, but is also uniformly universal. Why?]
Exercise. Suppose A is uniformly universal under f(x, y) and that t(y) is an iterative function for the relation f(x, x) ∊ ωy. Show that the function f(t(x), t(x)) is a generative function for A.
Remarks. The above, together with the fact that every universal set is uniformly universal, yields another proof that every universal set is generative, and unlike our previous proof of this fact, make no appeal to the existence of a previously constructed recursively enumerable generative set (for instance, Post’s set C). Actually, the two proofs can be seen to yield two different generative functions for Post’s set K.
Having shown that every universal set is generative, we now wish to show that, conversely, every generative set is universal, and thus that a set is generative if and only if it is universal.
To begin with, it is pretty obvious that if B is a generative set under φ(x) (i.e. if φ(x) is a generative function for B), then, for every i:
(1)If ωi = ℕ, then φ(i)∈ B [ℕ is the set of positive integers];
(2)If ωi = , then φ(i) ∉ B [ is the empty set].
Problem 15. Why is this so?
Problem 16. Prove that for every recursively enumerable set A, there is a recursive function t(y) such that for every number i :
(1)If i ∈ A, then ωt(i) = ℕ ;
(2)If i ∉ A, then ωt(i) = . [Hint: Define M(x, y) iff y ∈ A. The relation M(x, y) is recursively enumerable, hence by the iteration theorem there is a recursive function t(y) such that for all i, ωt(i) = {x : M(x, i)}.]
Theorem 12. Every generative set is universal.
Problem 17. Prove Theorem 12. [Hint: This follows fairly easily from Problems 15 and 16.]
We next wish to show that every co-productive set is generative. Then, by Theorem 12, we get Myhill’s result that every co-productive set (and hence every creative set) is universal. [We recall that a set is creative iff it is both co-productive and recursively enumerable.]
Weak Co-productivity
We recall that a set A is said to be co-productive under a recursive function φ(x) if for every number i such that ωi is disjoint from A, the number φ(i) lies outside both A and ωi. This obviously implies the following weaker condition:
C1: For every i such that ωi is disjoint from A and such that ωi contains at most one member, the number φ(i) lies outside both A and ωi.
Amazingly enough, this weaker condition on a recursive function is enough to ensure that A is generative (and hence universal).
This weaker condition C1 implies the following even weaker condition:
C2: For every number i,

(1)If ωi =  Then φ(i) ∉ A;
(2)If ωi = {φ(i)}, then φ(i) ∊ A.

Problem 18. Why does C1 imply C2?
We will say that A is weakly co-productive under φ(x) if φ(x) is recursive and condition C2 holds.
Theorem 13. [After Myhill] If A is co-productive — or even weakly co-productive — then A is generative.
To prove Theorem 13, we need the following lemma:
Lemma. For any recursive function φ(x) there is a recursive function t(y) such that for all y,

Problem 19. Prove the above lemma. [Hint: Let M(x, y, z) be the recursively enumerable relation x ∊ ωy ∧x = φ(z). Then use the strong recursion theorem, Theorem 7.]
Problem 20. Now prove Theorem 13. [Hint: Suppose φ(x) is a weakly co-productive function for A. Let t(y) be a recursive function related to φ(x) as in the above lemma. Then show that the function φ(t(y)) is a generative function for A.]
From Theorems 13 and 12 we have:
Theorem 14.[Myhill] If A is productive, or even weakly productive, then A is universal.
Discussion
One can go directly from co-productivity (or even weak co-productivity) to universality as did Myhill. Briefly, the argument is the following: Suppose B is weakly co-productive under φ(x) and that A is a set that we wish to reduce to B. Applying the strong recursion theorem to the recursively enumerable relation y ∊ A ∧ x = φ(z) [as a relation among x, y and z], we have a recursive function t(y) such that for all y, if y ∊ A, then ωt(y) = {φ;(y)}, and if y ∉ A, then  Then the function φ(t(x)) can be seen to reduce A to B. The reader should find it a profitable exercise to fill in the details.
Recursive Isomorphism
A set A is said to be recursively isomorphic to a set B if there is a recursive 1-1 function from A onto B. Such a function φ(x) is called a recursive isomorphism from A to B, and A is said to be recursively isomorphic to B under φ(x), if φ(x) is a recursive isomorphism from A to B.
If A is recursively isomorphic to B under φ(x), then obviously B is recursively isomorphic to A under the inverse function φ-1(x). [We recall that φ-1(x) = y iff φ(y) = x.]
Exercise. Suppose A is recursively isomorphic to B. Prove that if A is recursively enumerable, (recursive, productive, generative, creative, universal) then B is respectively recursively enumerable (recursive, productive, generative, creative, universal).
Myhill [1955] proved the celebrated result that any two creative sets are recursively isomorphic.
Bonus Question. Is it possible for a universal set to be recursive?
Solutions to the Problems of Chapter 5
1.(a) We showed in the last chapter that the relation g(x) = y (i.e. x0 = y) is recursively enumerable. Hence the relation r(x, y) = z is recursively enumerable, since it is equivalent to

Therefore, for any number n, the relation r(n, x) = y [as a relation between x and y] is recursively enumerable. And so for any recursively enumerable set A, the condition r(n, x) ∈ A [as a property of x] is recursively enumerable, since it is equivalent to ∃ y(r(n, x) = y ∧ y ∈ A). In particular, since T0 is recursively enumerable, the condition ∃ y(r(n, x) = y ∧ y ∈ T0) is recursively enumerable, and this is the condition x ∈ ωn. Thus ωn is recursively enumerable.
(b)Every recursively enumerable set A is represented in (U) by some predicate H. Thus for every number x, x ∈ A iff Hx ∈ T. We let h be the Gödel number of H, and so for all x, Hx ∈ T iff r(h, x) ∈ T0, which is true iff x ∈ ωh. Thus A = ωh.
2.The relation R(x, y) is represented in (U) by some predicate H, and so for all numbers i and x, R(i, x) iff Hicomx ∈ T, the latter of which is true iff hi0cx0 ∈ T0, where h, i0, c,x0 are the respective Gödel numbers of H, i, com, x (in dyadic notation). Thus R(i, x) iff r(hi0c,x) ∈ T0, which is true iff x ∈ ωhi0c. We thus take t(y) to be hy0c, and so t(i) = hi0c, so that now R(i, x) iff x ∈ ωt(i).
3.Given a recursively enumerable relation R(x1, …, xm, y1, …, yn), we use the recursively enumerable relation M(x, y), which yields, for all numbers x1, …, xm, i1, …, in,
(1)R(x1, …, xm, i1, …, in) iff M(δm(x1, …, xm), δn(i1, …, in)). We now apply Theorem 1 to the relation M(x, y), and so there is a recursive function t(i) such that for every x and i we have x ∈ ωt(i) iff M(x, i). We now take δm(x1, …, xm) for x and δn(i1, …, in) for i, and obtain
(2)δm(x1, …, xm) ∈ ωt(δn(i1,…,in)) iff M (δm(x1, …, xm), δn(i1, …, in)). By (1) and (2) we have:
(3)R(x1, …, xm, i1, …, in) iff δm(x1, …, xm) ∈ ωt(δn(i1,…,in)) which is true iff Rt(δn(i1,…,in))(x1, …, xm).
We thus take φ(i1, …, in) to be t(δn(i1, …, in)) and obtain

4.We will show that our recursively enumerable enumeration ω1, ω2, …, ωn, … is maximal. Given a recursively enumerable enumeration B1, B2, …, Bn, …, let R(x, y) be the recursively enumerable relation x ∈ By. By the simple iteration theorem, there is a recursive function f(x) such that for all x and i, we have x ∈ ωf(i) iff R(x, i), which is true iff x ∈ Bi. Thus x ∈ ωf(i) iff x ∈ Bi, and so ωf(i) = Bi.
5.Define S(x1, …, xm, y, y1, …, yn) iff Ry(x1, …, xm, y1, …, yn). The relation S is recursively enumerable (why?), and so by the iteration theorem, there is a recursive function φ(i,i1, …, in) such that for all x1, …, xm, i, i1, …, in: Rφ(i,i1,…,in)(x1, …, xm) iff S(x1, …, xm, i, i1, …, in) iff Ri(x1, …, xm, i1, …, in).
6.The great trick is to take m for y, and we thus obtain Rm(x, m) iff R(x, t(m)). But we also have Rm(x, m) iff x ∈ ωt(m)! Thus x ∈ ωt(m) iff R(x, t(m)), and so x ∈ ωn iff R(x, n), where n is the number t(m). [Clever, huh?]
7.(a) Given a recursive function f(x) let R(x, y) be the relation x ∈ ωf(y). It is recursively enumerable [since it is equivalent to ∃ z(f(y) = z ∧ x ∈ ωz)], so that by Theorem 5, there is some n such that for all x, x ∈ ωn iff R(x, n). But R(x, n) ≡ x ∈ ωf(n), and so, for all x, x ∈ ωn iff x ∈ ωf(n). Thus ωn = ωf(n).
(b)Believe it or not:
#1 is but a special case of (a), namely when f(x) = x + 1.
#2 is just Theorem 6!
#3 is but the special case of #4 in which f(x) = x. And so we must show #4: For f(x) a recursive function, the relation x = f(y) is recursively enumerable. Hence by Theorem 5, there is some n such that for all x, x ∈ ωn iff x = f(n). This means that f(n) is the one and only member of ωn, and so ωn = {f(n)}.
As for #5, let R(x, y) be the relation y ∈ ωx. This relation is recursively enumerable, and so there is a number n such that for all x, x ∈ ωn iff R(x, n), but R(x, n) iff n ∈ ωx, and so x ∈ ωn iff x ∈ ωx.
8.(a) We are given that A is recursive and that A ≠ ℕ and A ≠ . Since A ≠ , then there is at least one number a such that a ∈ A, and since A ≠ ℕ there is at least one number b such that b ∉ A. Define f(x) to be b if x ∈ A and to be a if x ∉ A. Since A is recursive, then A and its complement Ā are both recursively enumerable. Hence the function f(x) is recursive, since the relation f(x) = y is equivalent to the recursively enumerable relation (x ∈ A ∧ y = b) ∨ (x ∈ Ā ∧ y = a). If x ∈ A, then f(x) ∈ A [since f(x) = b], and if x ∉ A, then f(x) ∈ A [since f(x) = a]. Thus x ∈ A iff f(x) ∉ A.
(b)Let A be a recursive set that is neither ℕ nor . Take f(x) as in the lemma. By Theorem 6, there is some n such that ωn = ωf(n). Thus n and f(n) are indices of the same recursively enumerable set, yet one of them is in A and the other isn’t. Therefore A is not extensional.
9.(a) If we take t(i) for y in the statement Rt(i)(x, y) iff M(x, i, d(y)) we obtain Rt(i)(x, t(i)) iff M(x, i, d(t(i))). But it is also true that ωd(t(i)) = {x : Rt(i)(x, t(i))} [this is true because, for any n, ωd(n) = {x : Rn(x, n)}]. Thus ωd(t(i)) = {x : M(x, i, d(t(i))). We now take φ(i) to be d(t(i)), yielding ωφ(i) = {x : M(x, i, φ(i))}.
(b)As noted, Rh(x, y, z) iff M(x, y, t(y, z)). We take h for z and so Rh(x, y, h) iff M(x, y, t(y, h)). But since ωt(y,h) = {x : Rh(x, y, h)}, it follows that ωt(y,h) = {x : M(x, y, (t(y, h))) }. We thus take φ(y) to be t(y, h), yielding ωφ(y) = {x : M(x, y, φ(y))}.
10.This is the case that M(x, y, z) is the relation Ry(x, z).
11.Since f reduces A to B, then for all x, f(x) ∈ B iff x ∈ A. For any number i we take φ(t(i)) for x and we obtain:
(1)f(φ(t(i))) ∈ B iff φ(t(i)) ∈ A.
Also, φ(t(i)) ∈ A iff φ(t(i)) ∈ ωt(i) [since φ(x) is a generative function for A], so we have that:
(2)f(φ(t(i))) ∈ B iff φ(t(i)) ∈ ωt(i).
Next, since t(y) is an iterative function for the relation f(x) ∈ ωy, then for all x and i we have x ∈ ωt(i) iff f(x) ∈ ωi. We now take φ(t(i)) for x and obtain:
(3)φ(t(i)) ∈ ωt(i) iff f(φ(t(i))) ∈ ωi.
By (2) and (3), we see that f(φ(t(i))) 2 B iff f(φ(t(i))) ∈ ωi, and thus f(φ(t(x))) is a generative function for B.
12.For any number i, x ∈ ωi iff for all x,δ (x, i) ∈ K. Let φi be the function that assigns to each x the number δ(x, i). Thus φi(x) = δ (x, i). Consequently, x ∈ ωi iff φi(x) ∈ K, and so φi reduces ωi to K.
13.Suppose A is universal. Postφs set C is recursively enumerable and generative. Since C is recursively enumerable and A is universal, then C is reducible to A. Then, since C is generative, so is A [by Theorem 9].
14.Post’s set K is obviously uniformly universal under the recursive function δ (x, y). Now, suppose A is universal. Then K is reducible to A under some recursive function f(x). Then A must be uniformly universal under the function f(δ (x, y)), because for any numbers i and x, x ∈ ωi iff δ (x, i) 2 K, which is true iff f(δ (x, i)) ∈ A. Thus x ∈ ωi iff f(δ (x, i)) ∈ A.
15.We are given that φ(x) is a generative function for B. Thus for every number i,

(1)Suppose ωi = ℕ. Then by (a), φ(i) ∈ ℕ iff φ(i) ∈ B, but φ(i) is in ℕ, so φ(i) ∈ B.
(2)Suppose ωi = . Then by (a), φ(i) ∈ ; iff φ(i) ∈ B, but φ(i) ∉ , so φ(i) ∉ 2 B.
16.For all x, we have M(x, y) iff y 2 A. We are letting t(y) be an iterative function for the relation M(x, y), i.e. t is a recursive function such that for every x and i: x ∈ ωt(i) iff M(x, i), which is true iff i ∈ A. Thus x ∈ ωt(i) iff i ∈ A.
(1)If i ∈ A, then for every x, x ∈ ωt(i), which means that ωt(i) = ℕ.
(2)If i ∉ A, then for every x, x ∉ ∈ ωt(i), which means that ωt(i) is the empty set .
17.Suppose B is a generative set. We wish to show that B is universal. Let φ(x) be a generative function for B. Consider any recursively enumerable set A which we wish to reduce to B. Let t(y) be a recursive function related to A as in Problem 16. We assert that the function φ(t(x)) reduces A to B.
(1)Suppose i ∈ A. Then ωt(i) = ℕ [by Problem 16(1)]. Hence ωφ(t(i)) ∈ B [by Problem 15(a), taking t(i) for i].
(2)Suppose i ∉ A. Then ωt(i) =  [by Problem 16(2)]. Hence ωφ(t(i)) ∉ B [by Problem 15(b), taking t(i) for i].
Thus if i ∊ A, then φ(t(i)) ∈ B, and if i ∉ A, then φ(t(i)) ∉ B, or, what is the same thing, if φ(t(i)) ∈ B then i ∈ A. Thus i ∈ A iff φ(t(i)) ∈ B, and so the function φ(t(x)) reduces A to B.
18.Assume condition C1.
(1)Suppose ωi = . Obviously  is disjoint from A, and so by C1, the number φ(i) ∉ A [also, φ(i) ∉ ∈ ωi].
(2)Suppose ωi = {φ(i)}. If ωi were disjoint from A, then by C1, we would have φ(i) ∉ ∈ ωi, and thus φ(i) ∉φ(i)}, which is absurd. Thus φ(i) ∈ A.
19.Define M(x, y, z) to be x ∈ ωy ∧ x = φ(z). By the strong recursion theorem (Theorem 7), there is a recursive function t(y) such that

20.We want to show that for any number y, φ(t(y)) ∈ ωy iff φ(t(y)) ∈ A.
(1)Suppose φ(t(y)) ∈ ωy. Then ωy ∩ {φ(t(y))} = {φ(t(y))}. But also ωy ∩ {φ(t(y))} = ωt(y) [by the Lemma]. Then φ(t(y)) ∈ A [by condition C2, taking t(y) for i, since A is weakly co-productive under φ(x)]. Thus if φ(t(y)) ∈ ωy, then φ(t(y)) ∈ A.
(2)Suppose φ(t(y)) ∉ ωy. Then ωy ∩ {φ(t(y))} = . Again by the Lemma, ωy ∩ {φ(t(y))} = ωt(y). Thus ωt(y) = . Again by condition C2, taking t(y) for i, φ(t(y)) ∉ A. Thus if φ(t(y)) ∉ ωy, then φ(t(y)) ∉ A, or, equivalently if φ(t(y)) ∈ A, then φ(t(y)) ∈ ωy, and thus φ(t(y)) is a generative function for A.
Answer to the Bonus Question. Of course not! A universal set is generative and a generative set A cannot be recursive, for if it were, its complement Ā would be recursively enumerable, hence there would be a number n such that n ∈ A iff n ∈ Ā, which is impossible.







Chapter 6
Doubling Up
For my intended applications to the mathematical systems of the next chapter, we will need double analogues of earlier results.
Double Recursion Theorems
In preparation for the double recursion theorems we will consider here, we note that by the iteration theorem, there is a recursive function d such that for all y and z, ωd(y,z) = {x : Ry(x, y, z)}. This function will play a key role, and I shall call it the double diagonal function.
Theorem 1. [Weak Double Recursion Theorem] For any two recursively enumerable relations M1(x, y, z) and M2(x, y, z) , there are numbers a and b such that:

(1) ωa = {x : M1(x, a, b)};
(2) ωb = {x : M2(x, a, b)}.

Problem 1. Prove Theorem 1. [Hint: Use the double diagonal function d and consider the relations M1(x, d(y, z) , d(z, y)) and M2(x, d(z, y) , d(y, z)).]
The Strong Double Recursion Theorem
We now wish to show that in Theorem 1, the numbers a and b can be found as recursive function of the indices i and j of the relations M1(x, y, z) and M2(x, y, z). This is Theorem 3 below, which will soon be seen to be a consequence of:
Theorem 2. [Strong Double Recursion Theorem] For any recursively enumerable relations M1(x, y1, y2, z1, z2) and M2(x, y1, y2, z1, z2), there are recursive functions φ1(i, j) and φ2(i, j) such that for all numbers i and j:

(1)
(2)

Problem 2. Prove Theorem 2. [Hint: We again use the double diagonal function d. By the iteration theorem there are recursive functions t1(i, j) and t2(i, j) such that for all numbers i, j, x, y, z:

(1)
(2)

As an obvious corollary of Theorem 2 we have:
Theorem 3. For any two recursively enumerable relations M1(x, y, z1, z2) and M2(x, y, z1, z2), there are recursive functions φ1(i, j) and φ2(i, j) such that for all numbers i and j:

(1)
(2)

A special case of Theorem 3 is:
Theorem 4. [Strong Double Myhill Theorem] There are recursive functions φ1(i, j) and φ2(i, j) such that for all numbers i and j:

(1)
(2)

Problem 3. (a) Why is Theorem 3 a corollary of Theorem 2. (b) Why is Theorem 4 a special case of Theorem 3?
Discussion
The proof we have given of the strong double recursion theorem involved three applications of the iteration theorem. Actually, the theorem can be proved using only one application of the iteration theorem, and that can be done in more than one way.
Several workers in the field of recursion theory have found a way of deriving the weak double recursion theorem as a consequence of the strong (single) recursion theorem, and I have subsequently found a way of deriving the strong double recursion theorem as a consequence of the strong (single) recursion theorem.
All this can be found in a more in-depth study of recursion theorems in my book Recursion Theory for Metamathematics [Smullyan, 1993], in which several other variants of the recursion theorem are also treated.
More Doubling Up
We shall say that an ordered pair (A1, A2) is reducible to the ordered pair (B1, B2) under a recursive function f(x) if f(x) simultaneously reduces A1 to B1 and A2 to B2. This means that A1 = f–1(B1) and A2 = f–1(B2); in other words, it means that for every number x:
(1) If x ∈ A1, then f(x) ∈ B1;
(2) If x ∈ A2, then f(x) ∈ B2;
(3) If x ∉ A1 ∪ A2, then f(x) ∉ B1 ∪ B2.
If (1) and (2) above both hold, without (3) necessarily holding, we say that (A1, A2) is semi-reducible to the ordered pair (B1, B2) under the recursive function f(x).
We shall call a pair (U1, U2) doubly universal if every disjoint pair (A, B) of recursively enumerable sets is reducible to (U1, U2). We shall call a pair (U1, U2) semi-doubly universal if every disjoint pair (A, B) of recursively enumerable sets is semi-reducible to (U1, U2).
We shall call a disjoint pair (A, B) doubly generative under a recursive function φ(x, y) if, for every disjoint pair (ωi, ωj) of recursively enumerable sets, the following holds:
(1) φ(i, j) ∈ A iff φ(i, j) ∈ ωi;
(2) φ(i, j) ∈ B iff φ(i, j) ∈ ωj.
In this chapter we will prove that a pair (A, B) is doubly generative iff it is doubly universal.
We shall call a disjoint pair (A, B) doubly co-productive under a recursive function g(x, y) if for every disjoint pair of recursively enumerable sets (ωi, ωj), if ωi is disjoint from A and ωj is disjoint from B, then the number g(i, j) is outside all four sets ωi, ωj, A and B.
It is obvious that if (A, B) is doubly generative, then (A, B) is doubly co-productive. The converse is also true, but this is not trivial. The proof requires something like the strong double recursion theorem.
All this is closely connected to the topic of effective inseparability, to which we now turn.
Effective Inseparability
First we must discuss a very useful principle: Consider two recursively enumerable sets A and B. They are both Σ1 and so there are Σ0 relations R1(x, y) and R2(x, y) such that for all x,
(1) x ∈ A iff ∃yR1(x, y);
(2) x ∈ B iff ∃yR2(x, y).
Now define the sets A′ and B′ by the following conditions:

The sets A′ and B′ are both recursively enumerable and disjoint.
Problem 4. Why are the sets A′ and B′ disjoint?
We thus have:
Separation Principle: For any two recursively enumerable sets A and B, there are disjoint recursively enumerable sets A′ and B′ such that A – B ⊆ A′ and B – A ⊆ B′.
A disjoint pair of sets (A, B) is said to be recursively inseparable if there is no recursive superset of A disjoint from B, or, what is the same thing, if there is no recursive superset of B disjoint from A (these are the same because if C is a recursive superset of B disjoint from A, its complement  is a recursive superset of A disjoint from B). A pair of sets (A, B) is said to be recursively separable if the pair is not recursively inseparable (i.e. there does exist a recursive superset of A disjoint from B or a recursive superset of B disjoint from A). Clearly a pair of sets (A, B) that is recursively separable must be disjoint.
Now, to say that (A, B) is recursively inseparable is equivalent to saying that for any disjoint recursively enumerable supersets ωi and ωj of A and B respectively, there is a number n outside both ωi and ωj [i.e. ωi is not the complement of ωj].
Problem 5. Why are the characterizations of recursive inseparability in the two preceding paragraphs equivalent?
A disjoint pair (A, B) is called effectively inseparable if there is a recursive function ζ(x, y) [the Greek letter ζ is read as “zeta”] — called an effective inseparability function for (A, B) — such that for any recursively enumerable disjoint supersets ωi and ωj of A and B respectively, the number ζ(i, j) lies outside both ωi and ωj [ζ(i, j) is a witness, so to speak, that ωi is not the complement of ωj].
We shall call a disjoint pair (A, B) completely effectively inseparable if there is a recursive function ζ(x, y)—called a completely effective inseparability function for (A, B) —such that for any recursively enumerable supersets ωi and ωj of A and B respectively, whether they are disjoint or not, the number ζ(i, j) is either in both ωi and ωj or in neither, in other words ζ(x, y) ∈ ωi iff ζ(x, y) ∈ ωj. [If ωi and ωj happen to be disjoint, then ζ(x, y) is obviously outside both ωi and ωj, and so any completely effective inseparability function for (A, B) is also an effective inseparability function for (A, B).]
We will later show that the non-trivial result that if (A, B) is effectively inseparable, and A and B are both recursively enumerable, then (A, B) is completely effectively inseparable. The only proof of this that I know of uses the strong double recursion theorem.
Kleene’s Construction
There are several ways to construct a completely effectively inseparable pair of recursively enumerable sets. The following way is due to Kleene [1952].
We shall call a recursive function k(x, y) a Kleene function for a disjoint pair (A1, A2) if, for all numbers i and j,
(1) If k(i, j) ∈ ωi – ωj, then k(i, j) ∈ A2;
(2) If k(i, j) ∈ ωj – ωi, then k(i, j) ∈ A1.
Proposition 1. If k(x, y) is a Kleene function for a disjoint pair (A1, A2), then (A1, A2) is completely effectively inseparable under k(x, y).
Problem 6. Prove Proposition 1.
We shall call a disjoint pair (A1, A2) a Kleene pair if there is a Kleene function for (A1, A2).
By Proposition 1, any Kleene pair is completely effectively inseparable.
Theorem 5. There exists a Kleene pair (K1, K2) of recursively enumerable sets.
Problem 7. Prove Theorem 5. [Hint: Use the pairing function δ(x, y). Let A1 be the set of all numbers δ(i, j) such that δ(i, j) ∈ ωj and let A2 be the set of all numbers δ(i, j) such that δ(i, j) ∈ ωi. Show that δ(x, y) is a Kleene function for the pair (A1 − A2, A2 − A1). Then use the separation principle to get the recursively enumerable sets K1 and K2.]
From Theorem 5 and Proposition 1, we have:
Theorem 6. There is a completely effectively inseparable pair of recursively enumerable sets, to wit, the Kleene pair (K1, K2).
Proposition 2. If (A1, A2) is a Kleene pair and if (A1, A2) is semi-reducible to (B1, B2) and B1 is disjoint from B2, then (B1, B2) is a Kleene pair.
Problem 8. Prove Proposition 2. [Hint: Let f(x) be a function that semi-reduces (A1, A2) to (B1, B2), and let k(x, y) be a Kleene function for (A1, A2). By the iteration theorem, there is a recursive function t(y) such that ωt(y) = {x : f(x) ∈ ωy}. Show that ωt(y) = f−1(ωy). From the functions k(x, y), f(x), t(y) construct a Kleene function for (B1, B2).]
Theorem 7. Every disjoint semi-doubly-universal pair is completely effectively inseparable.
Problem 9. Why is Theorem 7 true?
Doubly Generative Pairs
We recall that a disjoint pair (A, B) is called doubly generative if there is a recursive function φ(x, y) — called a doubly generative function for (A, B) — such that, for every disjoint pair (ωi, ωj), the following holds:
(1) φ(i, j) ∈ A iff φ(i, j) ∈ ωi;
(2) φ(i, j) ∈ B iff φ(i, j) ∈ ωj.
Problem 10. Prove that if (A, B) is called doubly generative, then A and B are each generative sets.
The next theorem is vital:
Theorem 8. If (A, B) is completely effectively inseparable and A and B are both recursively enumerable, then (A, B) is doubly generative.
Problem 11. Prove Theorem 8. [Hint: (A, B) is completely effectively inseparable under some recursive function ζ(x, y). By the iteration theorem, there are recursive functions t1(n), t2(n) such that for all x and n:
(1) 
(2) 
Let φ(i, j) = ζ(t1(j), t2(i)). Show that (A, B) is doubly generative under the function φ(i, j).]
Since the Kleene pair (K1, K2) is completely effectively inseparable and K1 and K2 are both recursively enumerable, it follows from Theorem 8 that the pair (K1, K2) is doubly generative. Thus there exists a doubly generative pair of recursively enumerable sets.
Doubly Generative, Doubly Universal
Suppose (A1, A2) is a disjoint pair which is doubly generative under the function g(x, y). Then, for all numbers i and j, the following three conditions hold:
C1: If ωi = ℕ and ωj = , then g(i, j) ∈ A1;
C2: If ωi =  and ωj = ℕ, then g(i, j) ∈ A2;
C3: If ωi =  and ωj = , then g(i, j) ∉ A1 ∪ A2.
Problem 12. Prove this.
Problem 13. Suppose (A1, A2) is a disjoint pair and that g(x, y) is a recursive function such that conditions C1 and C2 above hold. Prove that for any disjoint pair (B1, B2) of recursively enumerable sets there are recursive functions t1(x), t2(x) such that:

(1)(B1, B2) is semi-reducible to (A1, A2) under g(t1(x), t2(x));
(2)If condition C3 also holds, then (B1, B2) is reducible to (A1, A2) under g(t1(x), t2(x)). [Hint: Use the result of Problem 16 of the previous chapter.]

From Problems 12 and 13 immediately follows:
Theorem 9. Every doubly generative pair is doubly universal.
We now see that for a disjoint pair (A, B), being completely effectively inseparable, being doubly generative, and being doubly universal, are all one and the same thing. Now to complete the picture.
Double Co-productivity
We recall that a disjoint pair (A, B) is defined to be doubly co-productive under a recursive function g(x, y) if for all i and j, if ωi is disjoint from ωj, and ωi is disjoint from A and ωj is disjoint from B, then g(i, j) lies outside all the four sets ωi, ωj, A and B.
We claim that if (A, B) is doubly co-productive under a recursive function g(x, y), then, for all numbers i and j, the following conditions hold:
D0: If ωi = ωj = , then g(i, j) ∉ A ∪ B;
D1: If ωi = {g(i, j)} and ωj = , then g(i, j) ∈ A;
D2: If ωi =  and ωj = {g(i, j)}, then g(i, j) ∈ B.
Problem 14. Prove that if (A, B) is doubly co-productive under the recursive function g(x, y), then conditions D0, D1 and D2 hold.
We will say that (A, B) is called weakly doubly co-productive under the recursive function g(x, y) if conditions D0, D1 and D2 hold.
We now wish to prove:
Theorem 10. If (A, B) is doubly co-productive, or even weakly doubly co-productive, under a recursive function g(x, y), then (A, B) is doubly generative (and hence also doubly universal).
For the proof of Theorem 1. we need:
Lemma. For any recursive function g(x, y), there are recursive functions t1(i, j), t2(i, j) such that for all numbers all i and j,

(1)
(2)

To prove this, we will need the strong double recursion theorem, or rather its corollary, Theorem 3.
Problem 15. Prove the above Lemma. [Hint: Let M1(x, y, z1, z2) be the recursively enumerable relation x ∈ ωy ∧ x = g(z1, z2), and let M2(x, y, z1, z2) be the same relation. Then use Theorem 3.]
Problem 16. Now prove Theorem 10. [Hint: Suppose (A, B) is weakly doubly co-productive under g(x, y). By the lemma, there are recursive functions t1(x, y), t2(x, y) such that for all i and j,

(1)
(2)

Let φ(x, y) = g(t1(x, y)t2(x, y)), and show that (A, B) is doubly generative under φ(x, y).]
Next we wish to prove:
Theorem 11. If (A, B) is effectively inseparable and A and B are both recursively enumerable, then (A, B) is doubly generative (and hence doubly universal).
Problem 17. Prove Theorem 11. [Hint: (a) First prove that for any recursively enumerable set A there is a recursive function t(i) such that for all i, ωt(i) = ωi ∪ A. (b) Now suppose that (A, B) is effectively inseparable under g(x, y) and that A and B are both recursively enumerable sets. Let t1(i), t2(i) be recursive functions such that for all i,  and . Let g(x, y) = ζ(t1(y), t2(x)) and show that (A, B) is doubly generative under the function g(x, y).]
Solutions to the Problems of Chapter 6

1.Let m be an index of the relation M1(x, d(y, z), d(z, y)), and let n be an index of M2(x, d(z, y), d(y, z)). Then:
(1)Rm(x, y, z) iff M1(x, d(y, z), d(z, y)),
(2)Rn(x, y, z) iff M2(x, d(z, y), d(y, z)).
In (1), take m for y and n for z, and in (2), take n for y and m for z. As a result, we have:
(1′) Rm(x, m, n) iff M1(x, d(m, n), d(n, m)),
(2′) Rn(x, n, m) iff M2(x, d(m, n), d(n, m)).
Now, ωd(m,n) = {x : Rm(x, m, n)} = {x : M1(x, d(m, n), d(n, m))} [by (1′)]; and
ωd(n,m) = {x : Rn(x, n, m)} = {x : M2(x, d(m, n), d(n, m))} [by (2′)].
We thus take a = d(m, n) and b = d(n, m), and have:

2.We define:

where t1(i, j) and t2(i, j) are the functions mentioned in the hint and d(x, y) is the double diagonal function. We will see that these two functions work.
By the definitions of t1 and t2, we know that for all numbers i, j, x, y, z,
(1) iff M1(x, i, j, d(y, z), d(z, y));
(2) iff M2(x, i, j, d(z, y), d(y, z)).
In (1), we take t1(i, j) for y and t2(i, j) for z, and in (2), we take t2(i, j) for y and t1(i, j) for z, obtaining:
(1′) iff M1(x, i, j, φ1(i1, i2), φ2(i1, i2));
(2′) iff M2(x, i, j, φ1(i1, i2), φ2(i1, i2)).
Also:
(3)
(4)
From (3) and (1′), we have ωφ1(i,j) = {x : M1(i, j, φ1(i, j), φ2(i, j))}.
From (4) and (2′), we have ωφ2(i,j) = {x : M2(i, j, φ1(i, j), φ2(i, j))}.
3.(a) Given the recursively enumerable relations M1(x, y, z1, z2) and M2(x, y, z1, z2) define two new relations  iff M1(x, y1, z1, z2) and  iff M2(x, y2, z1, z2). Then apply Theorem 2 to the relations  and .
(b)This follows from Theorem 3 by taking M1(x, y, z1, z2) and M2(x, y, z1, z2) to both be the relation Ry(x, z1, z2).
4.If x ∈ A′ and x ∈ B′, we get the following contradiction: There must be numbers n and m such that:
(1)R1(x, n).
(2)R2(x, m).
(3)If R2(x, m), then m > n [since (∀ z ≤ n) ∼R2(x, z)].
(4)If R1(x, n), then n > m [since (∀ z ≤ m) ∼R1(x, z)].
From (1) and (4) it follows that n > m. From (2) and (3) it follows that it is also true that m > n, which is impossible.
5.We are to show that the following two characterizations of the recursive inseparability of the disjoint pair of sets (A, B) are equivalent:
C1: There is no recursive superset of A disjoint from B.
C2: For any disjoint recursively enumerable supersets ωi of A and ωj of B, ωi cannot be the complement of ωj.
To show that C1 implies C2, assume C1. Now suppose ωi, ωj are disjoint supersets of A and B respectively. If ωi were the complement of ωj then ωi would be a recursive superset of A disjoint from B, contrary to C1. Therefore ωi is not the complement of ωj, which proves C2.
Conversely, suppose C2 holds. Let A′ be any superset of A disjoint from B. We are to show that A′ cannot be recursive and hence C1 holds. Well, if A′ were a recursive set ωi, its complement  would be a recursively enumerable superset ωj of B, disjoint from A, which would be contrary to C2. Thus A′ is not recursive.
6.We are given that k(x, y) is a recursive function such that for all i and j,
(1)k(i, j) ∈ ωi − ωj implies k(i, j) ∈ A2;
(2)k(i, j) ∈ ωj − ωi implies k(i, j) ∈ A1.
Now consider any recursively enumerable superset ωi of A1 and any recursively enumerable superset ωj of A2. We are to show that k(i, j) ∈ ωi iff k(i, j) ∈ ωj [and thus that (A1, A2) is completely effectively inseparable under k(x, y)].
If k(i, j) ∈ ωi − ωj, then k(i, j) ∈ A2 [by (1)], and hence k(i, j) ∈ ωj [since A2 ⊆ ωj], but no number in ωi − ωj could possibly be in ωj, and therefore k(i, j) ∉ ωi − ωj. By a symmetric argument, k(i, j) ∉ ωj − ωi. Consequently, k(i, j) ∈ ωi iff k(i, j) ∈ ωj.
This completes the proof.
7.Since δ(i, j) ∈ ωj iff δ(i, j) ∈ A1 and δ(i, j) ∈ ωi iff δ(i, j) ∈ A2, then δ(i, j) ∈ ωj − ωi iff δ(i, j) ∈ A1 − A2 and δ(i, j) ∈ ωi − ωj iff δ(i, j) ∈ A2 − A1. Thus δ(x, y) is a Kleene function for (A1−A2, A2−A1). By the separation principle, there are recursively enumerable sets K1 and K2 such that A1 − A2 ⊆ K1 and A2 − A1 ⊆ K2. Since δ(x, y) is a Kleene function for (A1 − A2, A2 − A1), and A1 − A2 ⊆ K1 and A2 − A1 ⊆ K2, it follows that δ(x, y) is a Kleene function for the recursively enumerable pair (K1, K2). [In general, it is easily verified that if ζ(x, y) is a Kleene function for a pair (B1, B2), and if  are disjoint supersets of B1B2 respectively, then ζ(x, y) must be a Kleene function for .]
8.As mentioned in the hint: Let f(x) be the function that semi-reduces (A1, A2) to (B1, B2), and let t(y) be a recursive function given by the iteration theorem such that for all x and y, x ∈ ωt(y) iff f(x) ∈ ωy [thus ωt(y) = {x : f(x) ∈ ωy}]. Also f(x) ∈ ωy iff x ∈ f−1(ωy) and so x ∈ ωt(y) iff x ∈ f−1(ωy). Let k be a Kleene function for the (disjoint) Kleene pair (A1A2). We now let k′(x, y) be the function f(k(t(x), t(y))), and we will show that k′(x, y) is a Kleene function for the pair (B1, B2). [At this point, the reader might like to try showing this without reading further.]
First we show that if k′(i, j) ∈ ωj − ωi, then k′(i, j) ∈ B1.
Well, suppose k′(i, j) ∈ ωj − ωi. Thus f(k(t(i), t(j))) ∈ ωj − ωi, and so

But

[since f−1(ωj) = ωt(j) and f−1(ωi) = ωt(i), as we have shown]. Thus

so that k(t(i), t(j)) ∈ A1 [since k(x, y) is a Kleene function for (A1, A2)], and therefore f(k(t(i), t(j))) ∈ B1 [since (A1, A2) is semi-reducible to (B1, B2) under f(x)]. Thus k′(i, j) ∈ B1.
This proves that if k′(i, j) ∈ ωj − ωi, then k′(i, j) ∈ B1.
By a similar argument, if k′(i, j) ∈ ωi − ωj, then k′(i, j) ∈ B2. Thus k′(x, y) is a Kleene function for (B1, B2).
9.If (A, B) is semi-doubly universal, then by definition the Kleene pair (K1, K2) is semi-reducible to (A, B). Hence [by Proposition 2] (A, B) is in turn a Kleene pair, and therefore completely effectively inseparable [by Proposition 1].
10.Suppose φ(x, y) is a doubly generative function for (A, B). Let c be any index of the empty set, and thus for every number i, the set ωi is disjoint from ωc. Thus φ(x, c) [as a function of x] is a generative function for A, because for every number i, ωi is disjoint from ωc, so that φ(i, c) ∈ A iff φ(i, c) ∈ ωi [by (1)]. Similarly, φ(c, x) [as a function of x] is a doubly generative function for the set B.
11.The set {x : x ∈ ωn ∨ x ∈ A} is simply ωn ∪ A, and the set {x : x ∈ ωn ∨ x ∈ B} is simply ωn ∪ B. Using the t1 and t2 recommended in the hint, we have:

(1)
(2)

Thus A and ωn are both subsets of  for all n, and B and ωn are both subsets of  for all n. Also as in the hint, we take



Given two numbers i and j, let k = φ(i, j). Thus k = ζ(t1(j), t2(i)). Since (A, B) is completely effectively inseparable under ζ(x, y) and A ⊆ ωt1(j) and B ⊆ ωt2(i) [remember, A ⊆ ωt1(n) and B ⊆ ωt2(n) for all n], and ζ(x, y) is a completely effective inseparability function for (A, B), then
(a) k ∈ ωt1(j) iff k ∈ .
Now, consider any disjoint pair (ωi, ωj). We are to show that k ∈ A iff k ∈ ωi and k ∈ B iff k ∈ ωj, and thus that (A, B) is doubly generative under φ(x, y).
Suppose that k ∈ A. Then k ∈ ωt1 (j) [since A ⊆ ωt1(j) for every n]. Hence k ∈ ωt2(i) [by (a)]. Thus k ∈ ωi ∪ B [since ωt2(i) = ωi ∪ B]. But k ∉ B, since B is disjoint from A. Thus k ∈ ωi. This proves that if k ∈ A then k ∈ ωi.
Conversely, suppose that k ∈ ωi. Then k ∈ ωt2(i) [since ωi ⊆ ]. Thus k ∈  [by (a)], and since ωt1(j) = ωj ∪ A, then k ∈ ωj ∪ A. But since ωj is disjoint from ωi we have the k ∉ ωj, and so k ∈ A. Thus if k ∈ ωi, then k ∈ A.
This proves that k ∈ A iff k ∈ ωi. The proof that k ∈ B iff k ∈ ωj is similar.


12.We are given that (A1, A2) is doubly generative under g(x, y).
(a)Suppose that ωi = ℕ and ωj = . Of course ωi is disjoint from ωj, so that g(i, j) ∈ ℕ iff g(i, j) ∈ A1 [and also g(i, j) ∈  iff g(i, j) ∈ A2], but since g(i, j) ∈ ℕ, it follows that g(i, j) ∈ A1.
(b)Similarly, if ωi =  and ωj = ℕ, then g(i, j) ∈ A2.
(c)If  and , since  is obviously disjoint from , then g(i, j) ∈ A1 iff , and  iff g(i, j) ∈ A2. But since , then g(i, j) ∈ A1 and g(i, j) ∈ A2.
13.By Problem 16 of the last chapter, for any recursively enumerable set B there is a recursive function t(y) such that for all numbers i, if i ∈ B then , and if i ∉ B, then .
Now, let (B1, B2) be any disjoint pair of recursively enumerable sets. There are recursive functions t1(i), t2(i) such that for any number i,
(1)If i ∈ B1, then ;
(2)If i ∉ B1, then ;
(3)If i ∈ B2, then ;
(4)If i ∉ B2, then .
Now assume that conditions C1 and C2 hold between the function g(x, y) and the pair (A1, A2).
(a)Suppose i ∈ B1. Then  [by (1)]. Since B2 is disjoint from B1, then i ∉ B2, and hence  [by (4)], and thus the pair  is (ℕ, ), and so g(t1(i), t2(i)) ∈ A1. This proves that i ∈ B1 implies g(t1(i), t2(i)) ∈ A1.
(b)The proof that i ∈ B2 implies g(t1(i), t2(i)) ∈ A2 is similar. Thus (B1, B2) is semi-reducible to (A1, A2) under the function g(t1(x), t2(x)).
(c)Suppose that condition C3 also holds. Now suppose that i ∉ B1 and i ∉ B2. Then by (2) and (4),  and , and therefore g(t1(i), t2(i)) ∉ A1 and g(t1(i), t2(i)) ∉ A2 [by C3]. Thus g(t1(x), t2(x)) reduces (B1, B2) to (A1, A2).
14.We are given that (A, B) is doubly co-productive under the recursive function g(x, y).
D0: This is obvious!
D1: Suppose ωi = {g(i, j)} and . We are to show that g(i, j) ∈ A. Consider the following conditions:
(1)ωi is disjoint from ωj;
(2)ωj is disjoint from B;
(3)ωi is disjoint from A.
If these three conditions all hold, then g(i, j) would be outside all four sets A, B, ωi, ωj [by the definition of double co-productivity]. Hence g(i, j) would be outside ωi, hence outside {g(i, j)}, which is not possible. Therefore it cannot be that all three of these conditions hold, but (1) and (2) do hold, therefore (3) doesn’t hold, which means that ωi is not disjoint from A. Thus g(i, j) is not disjoint from A, so that g(i, j) ∈ A This proves D1.
D2: The proof of D2 is symmetric to that of D1.
15.By Theorem 3 there are recursive functions t1(i, j) and t2(i, j) such that for all numbers i and j,
(1)

Thus,
(1′)
(2′)
16.Consider any disjoint pair (ωi, ωj). Let φ(i, j) = g(t1(i, j), t2(i, j)), where t1(y1, y2), t2(y1, y2) are as given by the lemma. Then by the lemma,
(a)
(b)
We first wish to show:
(1)If φ(i, j) ∈ ωi, then φ(i, j) ∈ A;
(2)If φ(i, j) ωj, then φ(i, j) ∈ B.
To show (1), suppose that φ(i, j) ∈ ωi. Then ωi ∩ {φ(i, j)} = {φ(i, j)}  is the set {φ(i, j)}.
Since φ(i, j) ∈ ωi and ωi is disjoint from ωj, then φ(i, j) ∉ ωj, and therefore . Hence, by (b), . Thus  is the unit set {φ(i, j)} and  is the empty set , and so by condition D1 of the definition of “weakly doubly co-productive,” g(t1(i, j), t2(i, j)) ∈ A. Thus φ(i, j) ∈ A, which proves (1).

The proof of (2) is similar (using D2 instead of D1).
It remains to prove the converses of (1) and (2). To this end, we first establish:
(3) If φ(i, j) ∈ A ∪ B, then φ(i, j) ∈ ωi ∪ ωj.
We will prove the equivalent propositions that if φ(i, j) ∉ ωi ∪ ωj, then φ(i, j) ∉ A ∪ B.
Well, suppose φ(i, j) ∉ ωi ∪ ωj. Then ω(i, j) ∈ ωi and φ(i, j) ∉ ωj. Since φ(i, j) ∉ ωi, then . Then by (a), . Similarly, since φ(i, j) ∉ ωj, then . Hence by condition D0 of the definition of “weakly doubly productive,”

Thus φ(i, j) ∉ A ∪ B. This proves that if φ(i, j) ∉ ωi ∪ ωj, then φ(i, j) ∉ A ∪ B, and therefore if φ(i, j) ∈ A ∪ B then φ(i, j) ∉ ωi ∪ ωj which is (3).
Now, to prove the converse of (1), suppose φ(i, j) ∈ A. Then φ(i, j) ∈ A ∪ B, so that φ(i, j) ∈ ωi ∪ ωj. Since φ(i, j) ∈ A, then φ(i, j) ∉ B [because B is disjoint from A]. Therefore φ(i, j) ∉ ωj [because if φ(i, j) ∈ ωj, then by (2) φ(i, j) would be in B, which it isn’t]. Since φ(i, j) ∈ ωi ∪ ωj but φ(i, j) ∉ ωj, then φ(i, j) ∈ ωi. Thus if φ(i, j) ∈ A, then φ(i, j) ∈ ωi, which is the converse of (1). The proof of the converse of (2) is similar.

17.(a) Suppose that A is recursively enumerable. Let M(x, y) be the recursively enumerable relation x ∈ ωy ν x ∈ A. By the iteration theorem, there is a recursive function t(i) such that for all i, ωt(i) = {x : M(x, i)}. Thus

(b)Suppose (A, B) is effectively inseparable under ζ(x, y) and A and B are both recursively enumerable. Now let (ωi, ωj) be a disjoint pair such that ωi is disjoint from A and ωj is disjoint from B. Since ωi is disjoint from both ωj and A, then ωi is disjoint from ωj ∪ A. Since B is disjoint from both ωj and A, then B is disjoint from ωj ∪ A. Thus ωi and B are both disjoint from ωj ∪ A, so that ωi ∪ B is disjoint from ωj ∪ A.
Thus, ωj ∪ A and ωi ∪ B are disjoint supersets of A and B respectively. By (a), there are recursive functions t1 and t2 such that  and . Therefore ,  are disjoint supersets of A and B respectively. Because the pair (A, B) is assumed to be effectively inseparable under ζ(x, y), ζ(t1(j), t2(i)) [which we define g(i, j) to be] lies outside both  and . But  and , which means g(i, j) must of course lie outside both ωi and ωj (and A and B as well). Thus we see that (A, B) is doubly generative under the function g(x, y).








Chapter 7
Metamathematical Applications
In The Beginner’s Guide [Smullyan, 2014] we studied the system of Peano Arithmetic. Some of the results regarding that system, as well as those regarding other formal systems, depend on the use of the logical connectives and quantifiers, but not all do. Those that do not can be neatly generalized to more abstract systems called representation systems, which I introduced in the Theory of Formal Systems [Smullyan, 1961]. In this chapter, I generalize further using systems I call simple systems, in terms of which it is shown that many results of Peano Arithmetic and other systems are essentially results of recursion theory in a new dress.
In what follows, number shall mean natural number, and function shall mean function whose arguments and values are natural numbers. Any argument of a function ranges over the whole set of natural numbers.
I.Simple Systems
By a simple system  I shall mean a pair (P, R) of disjoint sets, together with a function neg(x) and a function r(x, y) satisfying condition C1 below.
For any numbers h and n, we abbreviate neg(h) by h′, and r(h, n) by h(n), or sometimes by rh(n).
C1: For any numbers h and n, h′(n) ∊ P iff h′(n) ∊ R, and h′(n) ∊ R iff h(n) ∊ P.
Intended Applications
With any first-order system  of arithmetic we associate a simple system S as follows: We arrange all the sentences of  in an infinite sequence S1, S2, …, Sn, … according to the magnitude of their Gödel numbers. We refer to n as the index of the sentence Sn. We take one particular variable x and arrange all formulas with x as their only free variable in an infinite sequence φ1(x), φ2(x), …, φn(x), … according to the magnitude of their Gödel numbers. We refer to n as the index of φn(x). We arrange all formulas with the two variables x and y as their only free variables in an infinite sequence ψ1(x, y), ψ2(x, y), …, ψn(x, y), …, according to the magnitudes of their Gödel numbers, and for any numbers n, x and y, we define s(n, x, y) to be the index of the sentence 
For the associated simple system S of , we take P to be the set of indices of the provable sentences, and take R to be the set of indices of the refutable sentences. For any numbers h and n, we take r(h, n) to be the index of the sentence  [i.e. the sentence resulting from substituting the number  (the name of n) for all free occurrences of x in φh(x)]. We take h′ [which is the abbreviation of neg(h)] to be the index of the formula ∼φh(x) Of course φh′ (x) is provable (refutable) iff φh(x) is refutable (provable). And so the items (P, R), neg(h), and r(x, y) constitute a simple system. Consequently, whatever we prove for simple systems in general will hold for Peano Arithmetic and other systems.
Returning to simple systems in general, in accordance with intended applications, we might call members of P provable numbers, and members of R refutable numbers. We will call a number decidable if it is in either P or R, and call it undecidable otherwise. We call S complete if every number is decidable, otherwise incomplete.
We call two numbers equivalent if whenever either one is in P, so is the other, and if either one is in R, so is the other.
We will refer to the function r(x, y) as the representation function for sets of the system S, and we say that a number h represents the set A of all numbers n such that h(n) ∈ P. Thus, to say that h represents A is to say that for all n, h(n) ∈ P iff n ∈ A. We call A representable in  if some number h represents it.
We say that h completely represents A iff h represents A and h′ represents the complement  of A.
We say that h defines A if, for all numbers n:
(1) n ∈ A implies h(n) ∈ P ;
(2) n ∈  implies h(n) ∈ R.
We say that A is definable (completely representable) if some number h defines (completely represents) A.
We say that h contra-represents A if for all n the number h(n) ∈ R iff n ∈ A.
Problem 1. (a) Show that h defines A iff h completely represents A.
(b) Show that h contra-represents A iff h′ represents A, so that A is contra-representable iff it is representable.
For any function f(x) and set A, by f–1(A) is meant the set of all n such that f(n) ∈ A. Thus n ∈ f–1(A) iff f(n) ∈ A.
Recall that rh(n) is r(h, n) and that rh(n) = h(n), where r(x, y) is the representation function for sets.
Problem 2. Show that h represents A iff . Show also that h contra-represents A iff 
Problem 3. Prove that if some set A is represented in S and its complement  is not, then S is incomplete.
For any set A, by A* we shall mean the set of all numbers h such that h(h) ∈ A.
Problem 4. Prove that neither the complement  of P*, nor the complement  of R*, is representable in S.
By the diagonalization of a number h we shall mean the number h(h). By the skew diagonalization of h we shall mean the number h(h′).
It follow from Problems 3 and 4 that if either P* or R* is representable in S, then S is incomplete. The following problem yields a sharper result.
Problem 5. Prove the following:
(a)If h represents R*, then its diagonalization h(h) is undecidable.
(b)If h represents P*, then its skew-diagonalization h(h′) is undecidable.
The following problem yields still stronger results.
Problem 6. Prove the following:
(a)If h represents some superset of R* disjoint from P*, then h(h) is undecidable.
(b)If h represents some superset of P * disjoint from R*, then h(h′) is undecidable.
Why are these results stronger than those of Problem 5?
Discussion
Gödel’s original incompleteness proof essentially involved representing the set P*. [In the Theory of Formal Systems [Smullyan, 1961], I pointed out that incompleteness also follows if R* is representable.] To represent P* Gödel had to assume that the system was ω-consistent. It was John Barkley Rosser [1936] who was able to eliminate the hypothesis of ω-consistency by representing, not R*, but some superset of R* disjoint from P*.
The Diagonal Function d(x)
We define d(x) to be r(x, x), which is x(x). We note that for any set A the set A* is d–1(A).
Problem 7. Prove that for any sets A and B:
(a)If A ⊆ B, then f–1(A) ⊆ f–1(B).
(b)If A is disjoint from B, then f–1(A) is disjoint from f–1(B).
(c)f–1() is the complement of f–1(A).
In particular, taking d(x) for f(x),
(a1) If A ⊆ B, then A* ⊆ B *.
(a2) If A is disjoint from B, then A* is disjoint from B *.
(a3) 
Admissible Functions
We shall call a function f(x) admissible if for every number h there is a number k such that for every n the number k(n) is equivalent to h(f(n)).
Separation
Given a disjoint pair (A, B), we say that a number h separates A from B (in ) if h represents a superset of A and contra-represents some superset of B.
We say that h exactly separates A from B if h both represents A itself and contra-represents B.
The following problem is key.
Problem 8. Suppose f(x) is admissible. Prove the following:
(a)If A is representable, so is f–1(A). If A is contra-representable, so is f–1(A).
(b)If a disjoint pair (A, B) is exactly separable in S, then so is the pair (f–1A), f–1(B)).
Normal Systems
We shall call S a normal system if the diagonal function d(x) is admissible. This means that for every number h there is a number — which we shall denote h# — such that for all n the numbers h#(n) is equivalent to h(d(n)).
Problem 9. Suppose  is normal. If A is representable in , does it necessarily follow that A* is representable in ?
Problem 10. Show that for any normal system, neither  nor  is representable.
Fixed Points
By a fixed point of h is meant a number n such that h(n) is equivalent to n.
Problem 11. Prove that if  is normal, then every number h has a fixed point.
Problem 12. Prove the following:
(a)If h represents R, then any fixed point of h is undecidable.
(b)If h represents P, then any fixed point of h′ is undecidable.
Exercise. Show the following strange facts:
(1)If h represents some superset of R disjoint from P, then h(h) is undecidable.
(2)If h represents some superset of P disjoint from R, then h(h′) is undecidable.
We will later have important use for the following:
Problem 13. Show that no superset A of R*, with A disjoint from P*, can be definable in . [Hint: Show that if R* ⊆ A, and h defines A in , then h is in both A and P*(and hence that A is not disjoint from P*).]
Exercise. Show that if P* ⊆ A, and h defines A in , then h′ is in both A and R*(and hence that A is not disjoint from R*).
II.Standard Simple Systems
Now recursion theory enters the picture. We shall call a simple system  a standard simple system if the sets P and R are recursively enumerable, and the functions neg(x), r(x, y) and s(x, y, z) are recursive. In what follows,  will be assumed to be a standard simple system. [Recall that the function s(n, x, y) is the index of the sentence 
Problem 14. Show that for any set A,
(1) If A is recursively enumerable, so is A*.
(2) If A is recursive, so is A*.
More generally, show that for any recursive function f (x),
(1′) If A is recursively enumerable, so is f–1(A).
(2′) If A is recursive, so is f–1(A).
Problem 15. Show that every set representable in  is recursively enumerable, and that every set definable in  is recursive.
Problem 16. Show the following:
(a) If some recursively enumerable but non-recursive set is representable in , then  is incomplete.
(b) If all recursively enumerable sets are representable in , then  is incomplete.
Question: Suppose we are given, not that all recursively enumerable sets are representable in , but only that all recursive sets are representable in . Is that enough to guarantee that  is incomplete? The reader might like to try answering this on his or her own. The answer will appear in this text later on.
Undecidability and Incompleteness
The word “undecidable” has two very different meanings in the literature, which might sometimes cause confusions. On the one hand, a sentence of a system is called undecidable in the system if it is neither provable nor refutable in the system. On the other hand, the system as a whole is called undecidable if the set of provable sentence is not recursively solvable (or: under an effective Gödel numbering, the set of Gödel numbers of the provable sentences is not recursive).
We shall thus call a simple system  undecidable if the set P is not recursive.
The two senses of undecidable, though different, have the following important link:
Theorem 1. If  is undecidable (but standard), then  is incomplete (some number is undecidable in , i.e. outside both P and R).
Problem 17. Prove Theorem 1.
It was known prior to 1961 that if all recursively enumerable sets are representable in a system, then the system is undecidable. In the Theory of Formal Systems [Smullyan, 1961] I proved the stronger fact that if all recursive sets are representable in a system, the system is undecidable. My proof goes over to simple systems.
Theorem 2. If all recursive sets are representable in , then  is undecidable.
Problem 18. Prove Theorem 2. [Hint: Use the fact already proved that if P is recursive, so is P* (Problem 14).]
Remark. For a standard system , if all recursive sets are representable in , then  is undecidable by Theorem 2, hence incomplete by Theorem 1. Thus if all recursive sets are representable in a standard system , then  is incomplete, which answers the question raised following Problem 16.
Recursive Inseparability
Problem 19. If the pair (P, R) is recursively inseparable, does it necessarily follow that  is undecidable?
Problem 20. Show that for any recursive function f(x) and any disjoint pair (A, B), if the pair (f–1(A), f–1(B)) is recursively inseparable, then the pair (A, B) is also recursively inseparable. Conclude that if (P*, R*) is recursively inseparable, so is (P, R).
We next wish to prove:
Theorem 3. If every recursive set is definable in  then the pair (P*, R*) is recursively inseparable, and so is the pair (P, R).
Problem 21. Prove Theorem 3. [Hint: Use Problem 13.]
Recall that, given a disjoint pair (A, B), we say that a number h separates A from B (in ) if h represents a superset of A and contra-represents some superset of B. And we say that h exactly separates A from B if h both represents A and contra-represents B.
We next wish to prove the following theorem, which has the interesting corollaries that follow it:
Theorem 4. If some disjoint pair (A1, A2) is recursively inseparable in , but separable in , then the pair (P, R) is recursively inseparable.
Corollary 1. If some recursively inseparable pair is separable in , then  is undecidable.
Corollary 2. If some recursively inseparable pair is separable in , then  is incomplete (assuming S is standard).
Note: Corollary 2 is essentially Kleene’s symmetric form of Gödel’s theorem [Kleene, 1952]. Kleene proved this for a particular pair of recursively inseparable sets, but the proof works for any recursively inseparable pair of recursively enumerable sets.
Problem 22. Prove Theorem 4.
Rosser Systems
We shall call  a Rosser system if for any two recursively enumerable sets A and B, there is a number h that separates A − B from B − A. [If A and B are disjoint, then of course such an h also separates A from B.]
We shall call a Rosser system  an exact Rosser system, if for any two disjoint recursively enumerable sets A and B, there is a number h that exactly separates A from B.
Gödel’s proof of the incompleteness of systems like Peano Arithmetic involved representing a certain recursively enumerable set, but the only way of representing recursively enumerable sets that was known at that time involved the hypothesis of ω-consistency. However, Andrzej Ehrenfeucht and Solomon Feferman [1960] found a way of representing all recursively enumerable sets in Peano Arithmetic, without having to assume ω-consistency! In terms of simple systems, what they showed was that if  is a Rosser system in which all recursive functions of one argument are admissible, then all recursively enumerable sets are representable in . Later came a stronger result, which, for simple systems, is:
Theorem 5. [After Putnam, Smullyan, 1961] If  is a Rosser system in which all recursive functions of one argument are admissible, then  is an exact Rosser system.
Theorem 5 is an obvious consequence of the following two propositions:
Proposition 1. If  is a Rosser system, then some doubly universal pair is exactly separable in .
Proposition 2. If some doubly universal pair is exactly separable in  and if all recursive functions of one argument are admissible, then every disjoint pair of recursively enumerable sets is exactly separable in .
Problem 23. Prove Propositions 1 and 2. [Hint: For Proposition 1, show that if the Kleene pair (K1, K2) (or any other effectively inseparable pair of recursively enumerable sets) is separable in , then some doubly universal pair is exactly separable in .]
Effective Rosser Systems
We shall call  an effective Rosser system if there is a recursive function Π(x, y) such that for any numbers i and j, Π(i, j) separates (ωi − ωj) from (ωj − ωi). Such a function Π(x, y) will be called a Rosser function.
Effective Rosser systems have some very interesting properties, as we will see. For one thing, we will soon see that every effective Rosser system is an exact Rosser system (a result that is incomparable in strength with Theorem 5).
The following proposition is key for this, as well as for some further results.
Proposition 3. If  is an effective Rosser system, then for any recursively enumerable relations R1(x, y), R2(x, y), there is a number h such that for any n:
(1)R1(n, h) ∧ ∽ R2(n, h) implies h (n)∈ P ;
(2)R2(n, h) ∧ ∽ R1(n, h) implies h (n)∈R ;
Problem 24. Prove Proposition 3. [Hint: Apply the weak double recursion theorem to the relations R1 (x, Π(y, z)), R2(x, Π(y, z)), where Π(x, y) is a Rosser function for .]
Now we can prove:
Theorem 6. If  is an effective Rosser system, then  is an exact Rosser system.
Problem 25. Prove Theorem 6. [Hint: Let R1(x, y) be the relation x ∈ A ∨y(x) ∈ R, and let R2(x, y) be the relation x ∈ B ∨ y(x) ∈ P. Recall that y(x) = r(y, x). Apply Proposition 3 to R1(x, y) and R2(x, y).]
Rosser Fixed-Point Property
We say that S has the Rosser fixed point property if for any two recursive functions f1(x) and f2(x), there is a number h such that for all n for which ωf1(n) is disjoint from ωf2(n):
(1)h(n) ∈ ωf1(n) implies h(n) ∈ P ;
(2)h(n) ∈ ωf2(n) implies h(n) ∈ R.
Theorem 7. Every effective Rosser system has the Rosser fixed point property.
Problem 26. Prove Theorem 7. [Hint: Given recursive functions f1(x) and f2(x), apply Proposition 3 to the relations y(x) ∈ ωf1(x) and y(x) ∈ωf2(x).]
Uniform Incompleteness
By the principal part of a simple system S we mean the pair (P, R).
We call a simple system S ′ an extension of a simple system S if P ⊆ P′ and R ⊆ R′, where (P′, R′) is the principal part of S′, and the representation function r(x, y) and the function neg (x) are the same in both S and S′.
Now consider an infinite sequence  = 1, 2, …, n, …, where each Sn+1 is an extension of Sn. Such a sequence is called an effective sequence, or a recursively enumerable sequence, if the relations x ∈ Py and x ∈ Ry are both recursively enumerable. The system  is called uniformly incompletable if, for any recursively enumerable sequence 1, 2, …, n,…, of extensions of , there is a number h such that, for every n, the number h(n) is an undecidable number of n (i.e. h(n) is outside Pn ∪ Rn).
The following is one of the main results.
Theorem 8. Every effective Rosser system is uniformly incompletable.
Problem 27. Prove Theorem 8. [Hint: Given a recursively enumerable sequence of extensions of , show that there are recursive functions f1(x) and f2(x) such that for all n the set ωf1(n) is Rn and ωf2(n) is Pn. Then use the Rosser fixed point property.]

Before leaving this chapter, I should say a little more about the relation of simple systems to the first-order arithmetical systems in standard formalization of Tarski [1953].
Consider such a first-order system  and its associated simple system S, as defined much earlier in this chapter.
 is called a Rosser system, more specifically a Rosser system for sets, iff its associated simple system S is a Rosser system, as defined here, which is equivalent to the condition that for any pair of recursively enumerable sets A and B there is a formula φh(x) such that for all n, if n ω A − B, then φh() is provable (in ), and if n ∈ B − A, then φh() is refutable in . [In terms of the associated simple system S, if n ∈ A − B, then h(n) ∈ P and if n ∈ B − A, then h(n) ∈ R.]
The system  is called an exact Rosser system (for sets) if for any pair of disjoint recursively enumerable sets A and B, there is a predicate H such that for all n, n ∈ A iff H(n) is provable, and n ∈ B iff H(n) is refutable.
In the literature, a function f(x) is said to be definable if there is a formula Ψ(x, y) such that for all n and m:
(1)If f(n) = m, then Ψ(, ) is provable;
(2)If f(n) ≠ m, then Ψ(, ) is refutable;
(3)The sentence ∀x∀y((Ψ(, x) Λ Ψ(, y)) ⊃ x = y) is provable.
As far as I know, the term admissible function does not appear in the literature, except in my own writings. However, if a formula F (x, y) defines a function f(x), then for any formula H(x), if we take G(x) to be the formula ∃y(F (x, y) Λ H(x)), then it can be verified that for every n, not only is it the case that G() is equivalent to H(), but the very sentence G() ≡ H() is provable in the system. For details, see Theorem 2, Chapter 8 of Gödel’s Incompleteness Theorems [Smullyan, 1992], in which I use the term “strongly definable” for what I am calling “definable” here.
Thus every definable function f(x) is indeed admissible. The Ehrenfeucht–Feferman Theorem, as stated, is that any consistent Rosser system in which all recursive functions f(x) are definable is an exact Rosser system.
In the context of a first-order system Ƒ, a formula Ψ(x, y) is said to separate a relation R1(x, y) from a relation R2(x, y) if for all n and m,
(1)R1(n, m) implies that Ψ(, ) is provable.
(2)R2(n, m) implies that Ψ(, ) is refutable.
Ƒ is called a Rosser system for binary relations if for any two recursively enumerable relations R1(x, y), R2(x, y), there is a formula Ψ(x, y) that separates the relation R1(x, y)Λ ∼ R2(x, y) from R2(x, y) Λ ∼ R1(x, y). Sheperdson’s Theorem is that every consistent Rosser system for binary relations is an exact Rosser system for sets. This appears to be incomparable in strength with the Putnam–Smullyan Theorem, which is that every consistent Rosser system in which all recursive functions of one argument are definable is an exact Rosser system for sets.
The proof of Shepherdson’s Theorem (which can be found in [Shepherdson, 1961]) is not very different from my proof that every effective Rosser system is an exact Rosser system. Indeed, this theorem and its proof borrowed heavily from Shepherdson’s Theorem and proof.
Solutions to the Problems of Chapter 7
1.(a) It is obvious that if h completely represents A, then h defines A. To show the converse, suppose h defines A. Then for all n,
(1)n ∈ A implies h(n) ∈ P ;
(2)n ∈  implies h(n) ∈ R.
We are to show the converses of (1) and (2).
Suppose h(n) ∈ P. Then h(n) ∉ R (since R is disjoint from P). Thus by (2) n ∈  doesn’t hold, so that n ∈ A. The proof that h(n) ∈ R implies that n ∈ A is similar.
(b)h contra-represents A iff for every n, n ∈ A holds iff h ∈ R, which is true iff h′(n) ∈ P, which is true iff h′ represents A. Conversely, suppose h′ represents A. Then for all n, n ∈ A iff h′(n) ∈ P, which is true iff h(n) ∈ R. And so h contra-represents A.
2.We are to show that h represents A iff . We first recall that rh(n) = h(n), since rh(n) = r(h, n) = h(n).
(1)Suppose h represents A. Then n ∈ A iff h(n) ∈ P, which is true iff rh(n) ∈ P, which is true iff . Thus n ∈ A iff . Since this is true for all n, .
(2)Conversely suppose . Then n ∈ A iff . But  is true iff rh(n) ∈ P, which is true iff h(n) ∈ P. Thus n ∈ A iff h(n) ∈ P. Since this is true for all n, h represents A.
The proof that h contra-represents A iff  is similar.
(3)We prove the equivalent proposition that if  is complete, then the complement of any representable set is representable.
Well, suppose  is complete, and that h represents A. We will show that h′ represents . Consider any number n.
(1)Suppose n ∈ . Then n ∉ A. Hence h(n) ∉ P [since h represents A]. Hence h(n) ∈ R [since  is complete]. Thus h′(n) ∈ P. Thus n ∈  implies h′(n) ∈ P.
(2)Conversely, suppose h′(n) ∈ P. Then [by condition C1] h(n) ∈ R, so that h(n) ∉ P. Thus n ∉ A [since h represents A], so that n ∈ . Thus h′(n) ∈ P implies that n ∈ .
By (1) and (2), h′ represents A.
(4)If some number h represents , we can reach contradiction as follows: h(h) ∈ P iff h ∈[since h represents , for all n we have n ∈  iff h(n) ∈ P ]. But h ∈  iff h ∉ P*, which is true iff h(h) ∉ P. Thus we get the contradiction that h(n) ∈ P iff h(n) ∉ P, so that no number h represents .
By a similar argument (using R instead of P), the set  cannot be contra-representable. Thus  cannot be representable [by Problem 1(b)].
(5)(a) Suppose h represents R*. Then, for all n, h(n) ∈ P iff n ∈ R*, which is true iff n(n) ∈ R. Taking h for n, we have h(h) ∈ P iff h(h) ∈ R. Since h(h) cannot be in both P and R, h(h) must be outside both of them, and is thus undecidable.
(b)Suppose h represents P*. Then, for all n, h(n) ∈ P iff n ∈ P*. Thus h(h′) ∈ P iff h′ ∈ P*, and h′ ∈ P* iff h′(h′) ∈ P, which is true iff h(h′) ∈ R. Thus h(h′) ∈ P iff h(h′) ∈ R, and since P is disjoint from R, h(h′) is outside both P and R, and is thus undecidable.
(6)(a) Suppose h represents some superset A of R* that is disjoint from P*. We must show that h(h) is undecidable.
Well, if h ∈ P*, then h(h) ∈ P, and h ∈ A [since h represents A], contrary to the fact that P* is disjoint from A. Thus h ∉ P*, since assuming so led to a contradiction, and so h(h) ∉ P.
If h ∈ A, then h(h) ∈ P [since h represents A]. Thus h ∈ P*, again contrary to the fact that A is disjoint from P*. Thus h ∉ A, since assuming so led to a contradiction. Then h ∉ R* [since R* ⊆ A], and so h(h) ∉ R.
Thus h(h) ∉ P and h(h) ∉ R so that h(h) is undecidable.
(b)Suppose h represents some superset A of P* disjoint from R*. We must show that h(h′) is undecidable.
Well, if h′ ∈ A, then h(h′) ∈ P [since h represents A]. Hence h′(h′) ∈ R [by condition C1], and so h′ ∈ R*, contrary to the fact that R* is disjoint from A. Therefore h′ ∉ A, since assuming so led to a contradiction. Then h′ ∉ P* [since P* ⊆ A], and so h′(h′) ∉ P, and therefore h(h′) ∉ R.
If h′ ∈ R*, then h′(h′) ∈ R, so that h(h′) ∈ P, so that h′ ∈ A [which is represented by h], which is again contrary to the fact that R* is disjoint from A. Hence h′ ∉ R*, since assuming so led to a contradiction. Then h′(h′) ∉ R, and therefore h(h′) ∉ P.
Thus h(h′) ∉ R and h(h′) ∉ P, and therefore h(h′) is undecidable.
The reason why these results are stronger than those of Problem 5 is this: To begin with, since P is disjoint from R, then P* is disjoint from R* (why?). Therefore, if h represents R*, it automatically represents some superset of R* disjoint from P*, namely R* itself. [Any set is both a subset and a superset of itself.] Similarly, if h represents P*, then it represents a superset of P* disjoint from R*, namely P* itself. Thus the results of Problem 5 are but special cases of those of Problem 6.
7.(a) Suppose A ⊆ B. For any n, if n ∈ f−1(A), then f(n) ∈ A. Thus f(n) ∈ B [since A ⊆ B], and hence n ∈ f−1(B). Thus n ∈ f−1(A) implies n ∈ f−1(B), and so f−1(A) ⊆ f−1(B).
(b)Suppose A is disjoint from B. If n ∈ f−1(A), then f(n) ∈ A. Thus f(n) /2 B. Thus n ∈ f−1(A) implies that n ∉ f−1(B), and thus f−1(A) is disjoint from f−1(B).
(c)n ∈ f−1(A) iff f(n) ∈ A, which is true iff f(n) /2 A, which is true iff n ∉ f−1(A), which is true iff n ∈ f−1(A). Thus n ∈ f−1(A) iff n ∈ f−1(A), and since this holds for every n, we have

The statements (a1), (a2), and (a3) clearly follow from statements (a), (b), and (c) when f = d(x), because d-1(A) is the set A* for any set A.
8.Suppose f(x) is admissible. Given h, let k be such that k(n) is equivalent to h(f(n)) for all n.
(a)We assert that if h represents a set A, then k represents f-1(A), and that if h contra-represents A, then k contra-represents f-1(A).
To prove the former, suppose h represents A. Then n ∈ f−1(A) iff f(n) ∈ A, which is true iff h(f(n)) ∈ P [since h represents A], which is true iff k(n) ∈ P. Thus, for all n, n ∈ f-1(A) iff k(n) ∈ P, and thus k represents f-1(A).
The proof that if h contra-represents A, then k contra-represents f-1(A) is similar (just replace “P “ by “R” and “represents” by “contra-represents”).
(b)Suppose h exactly separates the set A from the set B. Then h represents A and contra-represents B. Then, by the solution to part (a), k represents f-1(A) and contra-represents f−1(B), and so k exactly separates f-1(A) from f−1(B).
9.Of course it follows! If  is normal, the diagonal function d(x) is admissible. Hence if A is representable, so is d-1(A) [by Problem 8], and d-1(A) is the set A*.
10.For a normal system, if  were representable, then  would be representable [by Problem 9]. But  [by Problem 7], so that  would be representable, which is contrary to Problem 4. A similar argument show that R is not representable.
11.Suppose  is normal. For any numbers h and n, h#(n) is equivalent to h(d(n)), so that h#(h#) is equivalent to h(d(h#)). But d(h#) = h#(h#) and so h#(h#) is equivalent to h(h#(h#)), which means that h#(h#) is a fixed point of h.
12.(a) Suppose h represents R and that n is some fixed point of h [i.e. n is equivalent to h(n)]. Since h represents R, then n ∈ R iff h(n) ∈ P. But h(n) ∈ P iff n ∈ P [since n is a fixed point of h]. Thus n ∈ R iff n ∈ P. But R and P are assumed to be disjoint, so the fixed point n of h can belong to neither P nor R, which means that n must be undecidable.
(b)Suppose h represents P and that n is some fixed point of h′. Since h represents P, then n ∈ P iff h(n) ∈ P. But h(n) ∈ P iff h′(n) ∈ R [by condition C1]. And h′(n) ∈ R iff n ∈ R [since n is a fixed point of h′]. Thus n ∈ P iff n ∈ R, and so again the arbitrary fixed point n of h′ is undecidable.
13.Suppose R ⊆ A and h defines A in . We will show that A cannot be disjoint from P*. Suppose h ∉ A. Then h(h) ∈ R [since h defines A]. Hence h ∈ R, which would mean h ∈ A [since R ⊆ A], which is impossible. Thus it cannot be that h ∉ A, which implies h ∈ A. It then further follows that h(h) ∈ P [since h defines, and hence represents, A]. Therefore h ∈ P*. Thus h ∈ A and h ∈ P*, so that A is not disjoint from P*.
14.We will first consider the general case.
(1′)Let f(x) be a recursive function and suppose the set A is recursively enumerable. For any number x, x ∈ f-1(A) iff f(x) ∈ A which is true iff ∃y(y = f(x) Λ y ∈ A). Thus f-1(A) is recursively enumerable.
(2′)If A is also recursive, then A and its complement  are both recursively enumerable, so that f−1(A) and f−1() are both recursively enumerable, as just shown. However, f−1() is the complement of f−1(A), so that f−1() is recursive.
To apply the general case to (1) and (2): Since the function r(x, y) is recursive, so is the diagonal function d(x) [which is r(x, x)], and since A* = d-1(A), it follows from (1′) and (2′) that if A is recursively enumerable (recursive), so is A*.
15.We must show that (a) every set representable in  is recursively enumerable, and that (b) every set definable in  is recursive.
(a)Suppose h represents A. Then for all n, n ∈ A iff r(h, n) ∈ P. Let f(x) = r(h, x) Since the function r(x, y) is recursive, so is the function f(x). Thus for all, n ∈ A iff f(n) ∈ P, and so A = f−1(P). Since P is recursively enumerable, and f(x) is a recursive function, then f−1(P) is recursively enumerable [by Problem 14], and thus A is recursively enumerable.
(b)If A is completely representable [which is equivalent to A being definable, by Problem 1(a)], then A and  are both representable, hence both recursively enumerable, and so A is then recursive.
16.(a) Suppose that A is a recursively enumerable set that is not recursive, and that A is representable in . Since A is not recursive, its complement  is not recursively enumerable, hence not representable [since only recursively enumerable sets are representable in , by Problem 15]. Thus A is a representable set whose complement is not representable. Hence  is incomplete [by Problem 3].
(b)This follows from (a) and the fact that there exists a recursively enumerable set that is not recursive, such as Post’s complete set K [if all recursively enumerable sets were representable in , K would also have to be recursively enumerable].
17.Suppose  is undecidable and effective. Then the set P is not recursive, and so its complement  is not recursively enumerable. Yet the set R is recursively enumerable [since  is effective], and so R is not the complement of P. But R is disjoint from P, which means that some n must be outside both P and R, and is thus undecidable. Thus  is incomplete.
18.Suppose all recursive sets are representable in . If P were recursive, then P* would be recursive [by Problem 14]. Thus  would be recursive, hence representable in , which would be contrary to Problem 4. Therefore P is not recursive, and  is thus undecidable [by the definition of an undecidable system].
19.Of course it does! If a disjoint pair of sets (A, B) is recursively inseparable, then by definition no recursive superset of A can be disjoint from B. Thus A cannot be recursive [for it were, then A itself would be a recursive superset of A disjoint from B]. Similarly, B cannot be recursive.
Thus, if (P, R) were recursively inseparable, then P would not be recursive, so that  would be undecidable.
20.We are to show that if f(x) is a recursive function and (A, B) is any disjoint pair, if (f−1(A), f−1(B)) is disjoint and recursively inseparable, so is (A, B). We will prove the equivalent proposition that if a disjoint pair (A, B) is recursively separable, so is (f−1(A), f−1(B)).
Well, suppose that A is disjoint from B and that A is recursively separable from B. Then there is a recursive superset A′ of A disjoint from B. Hence f−1(A) is disjoint from f−1(B) [by Problem 7(b)]. And f−1(A) is recursive [by Problem 14]. Thus f−1(A′) is a recursive superset of f−1(A) that is disjoint from f−1(B), and so f−1(A) is recursively separable from f−1(B).
Since (P*, R*) = (d−1(P)d−1(R)) and d(x) is a recursive function, it follows from what was just shown that if (P* R*) is recursively inseparable, so is (P, R).
21.Suppose all recursive sets are definable in . We will show that (P*, R*) is recursively inseparable, from which we will be able to conclude that so is the pair (P, R) [by Problem 20].
Let A be any superset of R* disjoint from P*. By Problem 13, A cannot be definable in . If A were recursive, then it would be definable in  [since the problem assumes this]. Thus A cannot be recursive. Consequently there is no recursive superset of R* disjoint from P*, which means that (P*, R*) is recursively inseparable.
22.Suppose (A1, A2) is disjoint and recursively inseparable, and that h separates A1 from A2 in . Then h represents some superset B1 of A1 and contra-represents some superset B2 of A2. In this situation B1 and B2 must be disjoint [because n ∈ B1 iff h(n) ∈ P and n ∈ B2 iff h(n) ∈ R, but P and R are disjoint]. Since (A1, A2) is recursively inseparable, and A1 ⊆ B1 and A2 ⊆ B2, then the pair (B1, B2) is recursively inseparable (verify this!). Now, by Problem 2, B1 = (P) and B2 = (R), where r(x, y) is the representation function for sets. Thus the pair ((P), (R)) is recursively inseparable, and since the function rh(x) is recursive [because r(x, y) is], then the pair (P, R) is recursively inseparable [by Problem 20].
Now, Corollary 1 follows because if (P, R) is recursively inseparable, then of course P is not recursive, so that  is undecidable.
Corollary 2 then follows from Corollary 1 and Theorem 1 (if  is effective but undecidable, then  is incomplete).
23.(a) To prove Proposition 1, suppose K1 is separable from K2 in  and let h effect the separation. Then h represents some superset A1 of K1 and contra-represents some superset A2 of K2. Since P is disjoint from R, then A1 is disjoint from A2. Of course, h exactly separates A1 from A2.
Since (K1, K2) is effectively inseparable, so is the pair (A1, A2) [verify this!]. Since  is an effective system, then A1 and A2 are recursively enumerable sets. Thus (A1, A2) is an effectively inseparable pair of recursively enumerable sets, hence is doubly universal [by Theorem 1. of Chapter 6]. Thus h exactly separates the doubly universal pair (A1, A2).
(b)To prove Proposition 2, suppose that some doubly universal pair (U1, U2) is exactly separable in  and that all recursive functions of one argument are admissible in . Let (A, B) be a disjoint pair of recursively enumerable sets that we wish to exactly separate in .
Since (U1, U2) is doubly universal, then there is a recursive function f(x) such that A = f−1(U1) and B = f−1(U2). By hypothesis, f(x) is admissible (all recursive functions of one argument are). Thus, by Problem 8, f−1(U1) is exactly separable from f−1(U2), and thus A is exactly separable from B.
24.Suppose Π(x, y) is a Rosser function for . Given recursively enumerable relations R1(x, y), R2(x, y), by the weak double recursion theorem (Theorem 1 of Chapter 6), taking M1(x, y, z) to be R1(x, Π(y, z)) and M2(x, y, z) to be R2(x, Π(y, z)), there are numbers a and b such that:
ωa = {x : R1(x, Π(a, b))};
ωb = {x : M2(x, Π(a, b))}.
We let h be the number Π(a, b). Then ωa = {x : R1(x, h)} and ωb = {x : R2(x, h)}. Thus ωa − ωb = {x : R1(x, h) ∧ ∽R2(x, h)} and ωb − ωa = {x : R2(x, h) ∧ ∼R1(x, h)}. Since Π(x, y) is a Rosser function, then h, which is Π(a, b), separates ωa − ωb from ωb − ωa, and thus separates {x : R1(x, h)∧ ∼ R2(x, h)} from {x : R2(x, h) ∧ ∼ R1(x, h)}.
25.Let A and B be disjoint recursively enumerable sets. Let R1(x, y) be the relation x ∈ A ∨ y(x) ∈ R and let R2(x, y) be the relation x ∈ B ∨ y(x) ∈ P. Applying Proposition 3 to the pair (R1, R2), there is a number h such that for all n,
R1(n, h) ∧ ∼R2(n, h) implies h(n) ∈ P and
R2(n, h) ∧ ∼R1(n, h) implies h(n) ∈ R. Thus
(1) [(n ∈ A ∨ h(n) ∈ R) ∧ ∼(n ∈ B ∨ h(n) ∈ P)] ⊃ h(n) ∈ P ;
(2) [(n ∈ B ∨ h(n) ∈ P) ∧ ∼(n ∈ A ∨ h(n) ∈ R)] ⊃ h(n) ∈ R.
Also
(3) ∼(n ∈ A ∧ n ∈ B) [since A is disjoint from B];
(4) ∼(h(n) ∈ P ∧ h(n) ∈ R) [since P is disjoint from R].
From (1), (2), (3) and (4) it follows by propositional logic that n ∈ A iff h(n) ∈ P, and n ∈ B iff h(n) ∈ R.
To see this, let us use the following abbreviations:

Thus we have:
(1) [(p1 ∨ q2) ∧ ∼(p2 ∧ q1)] ⊃q1;
(2) [(p2 ∨ q1) ∧ ∼(p1 ∧ q2)] ⊃q2;
(3)∼ (p1 ∧ p2);
(4)∼ (q1 ∧ q2).
We are to infer that p1 ≡ q1 and p2 ≡ q2.
(a)First we show that p1 ⊃ q1. Well, suppose p1. Then p1 ∨ q2 is true. Thus (1) is equivalent to

Since p1 is true (by assumption), then p2 is false [by (3)], so that p2 ∨ q1 is equivalent to q1, so that (1′) is equivalent to

From this q1 follows, which proves that p1 ⊃ q1.
(b)For the converse, suppose q1 is true. Then p2 ∨ q1 is true, and so (2) is equivalent

Since q1 is true (by assumption), then q2 is false [by (4)], so that p1 ∨ q2 is equivalent to p1, so that (2′) is equivalent to

Since q2 is false, it follows from (2′′) that p1 is true. This proves that q1 ⊃ p1. Thus p1 ≡ q1.
The proof that p2 ≡ q2 is similar (just interchange p1 with p2 and q1 with q2).
Note: This proof is a modification of a proof of a result of John Shepherdson.
26.Suppose  is an effective Rosser system. Given two recursive functions f1(x) and f2(x) let R1(x, y) be the relation y(x) ∈ ωf1(x) and let R2(x, y) be the relation y(x) ∈ ωf2(x).
Applying Proposition 3 to these two relations, there is a number h such that for every n,
(1) h(n) ∈ ωf1(n) ∧ ∼(h(n) ∈ ωf2(n)) implies h(n) ∈ P ;
(2) h(n) ∈ ωf2(n) ∧ ∼(h(n) ∈ ωf1(n)) implies h(n) ∈ R.
If  is disjoint from , then (1) and (2) are respectively equivalent to:
(1′) h(n) ∈  implies h(n) ∈P ;
(2′) h(n) ∈  implies h(n) ∈ R.
Thus  has the Rosser fixed point property.
27.Consider a recursively enumerable sequence  of extensions of . The relation x ∈ Ry is recursively enumerable. Hence by the iteration theorem, there is a recursive function f1(x) such that for all n, we have  = {x : x ∈ Rn}. Thus  = Rn. Similarly, there is a recursive function f2(x) such that, for all n, we have  = Pn.
Now suppose that  is effectively a Rosser system. Then  has the Rosser fixed point property, since for every n the sets  and  are disjoint. So by the Rosser fixed point property, there is a number h such that for all n:
(1)h(n) ∊ wf1(n) implies h(n) ∊ P ;
(2)h(n) ∊ wf2(n) implies h(n) ∊ R.
Thus,
(1′)h(n) ∊ Rn implies h(n) ∊ P ;
(2′)h(n) ∊ Pn implies h(n) ∊ R.
Since P ⊆ Pn and R ⊆ Rn, then by (1′) and (2′), we have:
(1′)h(n) ∊ Rn implies h(n) ∊ Pn;
(2′)h(n) ∊ Pn implies h(n) ∊ Rn.
Since Pn is disjoint from Rn, it must be the case that h(n) ∉ Pn and h(n) ∉ Rn. Thus  is uniformly incompletable.







Part III
Elements of Combinatory Logic







Chapter 8
Beginning Combinatory Logic
The subject of combinatory logic, initiated by Moses Schönfinkel [1924], is a most elegant one, as the reader will see. It plays a significant role in computer science and provides yet another approach to recursion theory. In this book we provide only the beginnings of the subject, hoping to whet the reader’s appetite for more.
By an applicative system is meant a set  together with an operation that assigns to each element x and each element y (in that order) an element denoted (xy). In combinatory logic we often omit parentheses, with the understanding that parentheses are to be restored to the left: xyz abbreviates ((xy)z), not (x(yz)). Also xyzw abbreviates (((xy)z)w), etc. Note that this is not the same as the common set of rules for eliminating parentheses in school mathematics and even in many college mathematics courses, so it takes some getting used to. But have courage, for it will become natural fairly quickly. (But always remember to put outside parentheses around an expression when you replace a variable by a compound expression; for instance, replacing the variable x with SI in the expression S(Kx)K would result in S(K(SI))K.) Also note that, in general, in applicative systems xy is not the same as yx, nor is x(yz) the same as (xy)z (i.e. the commutative and associative rules of elementary arithmetic do not generally apply here).
The elements of  are called combinators. It is to be understood that  contains more than one element.
Fixed Points
An element y is called a fixed point of an element x if xy = y. Fixed points play a key role in combinatory theory.
The Composition Condition
For any elements A, B and C, we say that C combines A with B if Cx = A(Bx) for every element x. We say that the composition condition holds if for every A and B, there is some C that combines A with B.
Introducing the Combinator M, the Mocker (the Mockingbird)
A combinator M will be called a mocker, or a duplicator, if

for every element x. Here is our first interesting result:
Theorem 1. The following two conditions are sufficient to ensure that every element A has a fixed point:
C1: The composition condition holds.
C2: There is a mocker M.
Problem 1. Prove Theorem 1.
Note: Theorem 1 states a basic fact of combinatorial logic. The solution, though quite brief, is very ingenious, and derives ultimately from the work of Gödel. It is related to many of the fixed point results in several areas in mathematical logic.
The solution given in the proof of Theorem 1 reveals more information than is given in the statement of the theorem. This extra information is important, and we record it as follows:
Theorem 1*. If C combines x with M, then CC is a fixed point of x.
Egocentric Elements
We shall call an element x egocentric if it is a fixed point of itself, i.e. if xx = x.
Problem 2. Prove that if conditions C1 and C2 of the hypothesis of Theorem 1 hold, then at least one element is egocentric.
Agreeable Elements
We shall say that two elements A and B agree on an element x if Ax = Bx. We shall call an element A agreeable if for every element B, there is at least one element x on which A and B agree. Thus A is agreeable if, for every B, Ax = Bx for at least one x.
Problem 3. (a) [A Variant of Theorem 1] Suppose we are given condition C1 of the hypothesis of Theorem 1 [i.e. for every A and B, there is some C that combines A with B], but instead of being given that there is a mocker M, we are given that at least one element A is agreeable. Prove that every element has a fixed point. (b) Theorem 1 is really but a special case of (a). Why?
Problem 4. As an exercise, show that if the composition condition C1 holds, and if C combines A with B, then if C is agreeable, so is A.
Problem 5. Again, assuming the composition condition C1, show that for any elements A, B and C, there is an element E such that Ex = A(B(Cx)) for every x.
Compatible Elements
We will say that an element A is compatible with B if there are elements x and y such that Ax = y and By = x. When such a pair of elements (x, y) exists, it is called a cross point of the pair (A, B).
Problem 6. Prove that if conditions C1 and C2 of Theorem 1 hold, then any two elements A and B are compatible, i.e. any two elements have a cross point.
Smug Elements
We shall call an element smug if it is compatible with itself.
Problem 7. If an element has a fixed point, is it necessarily smug?
Problem 8. Prove that if the composition condition C1 holds and if there is also at least one smug element, then there is at least one element that has a fixed point.
Fixation
We shall say that the element A is fixated on B if Ax = B for every element x.
Problem 9. Is it possible for an element A to be fixated on more than one element?
Problem 10. Which, if either, of the following two statements are true?

(a)If y is a fixed point of x, then x is fixated on y.
(b)If x is fixated on y, then y is a fixed point of x.

Narcissistic Elements
We shall call an element A narcissistic if it is fixated on itself, i.e. if Ax = A for every x.
Problem 11. Which, if either, of the following statements is true?

(a)Every narcissistic element is egocentric.
(b)Ever egocentric element is narcissistic.

Problem 12. Determine whether or not the following statement is true: If A is narcissistic, then for all elements x and y, Ax = Ay.
Problem 13. If A is narcissistic, does it necessarily follow that for all x and y, Axy = A?
Problem 14. Narcissism is contagious! If A is narcissistic, then so is Ax for every x. Prove this.
Introducing the Combinator K, the Kestrel
We shall call a combinator K a cancellator, or a kestrel, if

for every x and y. This K plays a key role in combinatory logic, as we shall see.
Note: Why the alternative word “kestrel”? Well, in my book To Mock a Mockingbird [Smullyan, 1985], I used talking birds as combinators. If one calls out the name of a bird y to a bird x the bird x calls back the name of some bird, and this bird I call xy. Thus xy is the bird named by x upon hearing the name of y. I gave bird names to the standard combinators of the literature, usually names whose first letter is the first letter of the combinator in the literature. For example, I call the bird representing the standard combinator K by the name kestrel. The standard combinators B, C, W and T, which we will soon come across, I respectively called the bluebird, cardinal, warbler and thrush. I called the mocker M a mockingbird, because in my bird model, Mx = xx means that M’s response to x is the same as x’s response to x.
To my surprise, I recently found out that many of my bird names have been adopted in the more recent combinatory literature! For example, there is a book by Reginald Braithwaite entitled Kestrels, Quirky Birds and Hopeless Egocentricity [2013]. In light of this, I feel justified in sometimes using bird names for various combinators.
In what follows, K is a kestrel.
Problem 15. Prove that any fixed point of K is egocentric.
Problem 16. Prove that if Kx is egocentric, then x is a fixed point of K.
Problem 17. Prove that if Kx is a fixed point of K, then x is a fixed point of K.
Problem 18. In general, it is not the case that if Ax = Ay, then x must necessarily be y, but it is the case if A is a kestrel. Prove that if Kx = Ky, x must necessarily be y.
The principle expressed in Problem 18 (that if Kx = Ky, x must necessarily be y) is important and will be called the cancellation law.
Problem 19. Prove that for all x, Kx ≠ K.
Problem 20. (a) Prove that no kestrel can be egocentric. (b) Prove that any fixed point of a kestrel is narcissistic.
Next I want to show that if  contains a kestrel, then  contains infinitely many elements (recall that  always contains more than one element).
Problem 21. Consider the set {K, KK, KKK, …}. Is this set infinite?
Problem 22. What about the set {K, KK, K(KK), K(K(KK)), …}, i.e. the set {K1, K2, …, Kn, …} where K1 = K and Kn+1 = K(Kn) for all n. Is this set infinite?
Introducing the Combinator L, the Lark
We shall call a combinator L a lark if for all x and y, the following condition holds:

The lark was not a standard combinator in the literature prior to the publication of my To Mock a Mockingbird, but it is so now. Several papers have been written about it. The lark plays a key role in my treatment of combinatory logic.
One nice thing about the lark is that without assuming conditions C1 and C2, the mere existence of a lark ensures that every element has a fixed point.
Problem 23. Prove this.
Problem 24. Prove that no element can be both a lark and a kestrel.
Problem 25. Prove that if a lark L is narcissistic, then L must be a fixed point of every element x.
Problem 26. Prove that no kestrel can be a fixed point of a lark.
However, it is possible for a lark to be a fixed point of a kestrel.
Problem 27. Show that if a lark L is a fixed point of a kestrel, then L must be a fixed point of every element.
An interesting thing about larks is that the existence of a lark L, without any other information, is enough to guarantee the existence of an egocentric element. In fact, we can write down an expression using only the letter L and parentheses that names an egocentric element. The shortest expression I have been able to find uses twelve occurrences of the letter L. Is there a shorter one? I raised this as an open problem in To Mock a Mockingbird, and I don’t know if this problem has been solved yet.
Problem 28. Given a lark L, prove that there is an egocentric element, and then write down the name of such an element, using only the letter L and parentheses.
Discussion
Given two expressions involving only L and parentheses, is there a purely mechanical procedure to decide whether or not they name the same element? I posed this as an open problem in To Mock a Mockingbird, and it has been solved affirmatively by at least three people — Richard Statman in his paper “The Word Problem for Smullyan’s combinatorial lark is decidable [1989], and by M. Sprenger and M. Wymann-Böni in “How to decide the lark” [1993].
Introducing the Combinator I, the Identity Combinator
By an identity combinator is meant an element I satisfying the condition that Ix = x for every x.
Despite its apparently lowly status, the identity operator I will later be seen to play an important role. Meanwhile, here are some trivial facts about I:

(1)I is agreeable if and only if every element has a fixed point.
(2)If every pair of elements is compatible, then I is agreeable.
(3)I, though egocentric, cannot be narcissistic.

Problem 29. Prove these three facts. [It is not necessary to assume the composition principle, but it is necessary to use the fact that  contains more than one element.]
Problem 30. Prove that if  contains a lark L and an identity element I, then  contains a mocker M.
Sages
An element θ will be called a sage if its existence not only guarantees that every element has a fixed point, but θ is also such that when applied to any element x, it yields a fixed point of x. Thus, θ is a sage, if, for every x, θx is a fixed point of x, i.e., x (θx) = θx for every element x.
In the classic literature of combinatory logic, sages are called fixed point combinators. I introduced the word sage in my book To Mock a Mockingbird, and the term sage has subsequently become standard in the literature, so I will use the term sage here. Much study has been devoted to sages, and in a later chapter, I will show many ways of deriving a sage from various other combinators. For now, let me just state that if  contains a lark L and a mocker M, and if the composition law holds, them  contains a sage.
Problem 30. Prove this.
Note: It follows from the last problem that if  contains a lark L and an identity combinator I, and obeys the composition condition, the  contains a sage, because in that case  must also contain a mocker M by Problem 30.
Solutions to the Problems of Chapter 8
1.Let C be an element that combines A with M. Thus, for every element x, we have Cx = A(Mx). But Mx = xx, and so Cx = A(xx) for every x. Now take C for x, and we see that CC = A(CC) which shows us that CC is a fixed point of A. [Clever, eh?]
2.From conditions C1 and C2, there is a mocker M. Moreover, every element has a fixed point [by Theorem 1]. Thus M itself has a fixed point E. Thus ME = E. Also ME = EE [since M is a mocker], and so E = EE, which means that E is egocentric. Thus any fixed point of M is egocentric.
Remark. Since E is egocentric and ME = E, it follows that ME is egocentric. Doesn’t the world “ME” tell its own tale?
3.(a) Suppose A is agreeable. Given any element x, by condition C1 there is some element C that combines x with A. Thus Cy = x(Ay) for every element y. Since A is agreeable, then there is some y* such that Cy* = Ay*, and hence it follows from Cy* = x(Ay*) that Ay* = x(Ay*), and so Ay* is a fixed point of x.
(b)Theorem 1 is a special case of (a), since a duplicator M is obviously agreeable: M agrees with every element x on some element, namely on x [since Mx = xx].
4.We are given that the composition condition C1 holds and that C combines A with B and that C is agreeable. We are to show that A is agreeable. Well, consider any element D. We are to show that A agrees with D on some element. Well, let E be an element that combines D with B. Now, C agrees with E on some element F, since C is agreeable. We will show that A agrees with D on BF.
Since C agrees with E on F, then CF = EF. Also EF = D(BF), since E combines D with B. Thus CF = EF = D(BF), and so CF = D(BF). But also CF = A(BF), since C combines A with B, and so A(BF) = D(BF). Thus A agrees with D on BF.
5.We assume that the composition condition C1 holds. Given A, B and C, let D combine B with C. Thus for any x, Dx = B(Cx). Now let E combine A with D. Then Ex = A(Dx) = A(B(Cx)).
6.For any elements A and B, we are given that there is some C that combines A with B. Thus for all y, we have Cy = A(By). We are also given that there is a mocker M, and so C has some fixed point y. Thus y = Cy and Cy = A(By), yielding y = A(By). Now take x to be By, so that y = Ax and x = By. Thus Ax = y and By = x.
7.Of course it is: Suppose x is a fixed point of A, so that Ax = x. Now take y to be x itself. Then Ax = y and Ay = x, which means that A is smug.
8.We are given that the composition condition C1 holds and that there is a smug element A. Thus there are elements x and y such that Ax = y and Ay = x. Since Ax = y, we can substitute Ax for y in the equation Ay = x, yielding A(Ax) = x. Now let B be an element that combines A with itself. Thus Bx = A(Ax), and since A(Ax) = x, we have Bx = x. Thus B has the fixed point x.
9.Of course not! If A is fixed on x and A is fixed on y, then for any element z we see that Az = x and Az = y so that x = y.
10.It is obviously (b) that is true. Suppose x is fixated on y. Then xz = y for every z. Hence, taking y for z, we see that xy = y, so that y is a fixed point of x.
11.It is obviously (a) that is true. Suppose A is narcissistic. Then Ax = A for every x, so of course AA = A.
12.As given, the statement is obviously true: If A is narcissistic, then Ax = A and Ay = A. Thus Ax and Ay are both equal to A, so that Ax = Ay.
13.Yes, it does: If A is narcissistic, then Ax = A, so that Axy = Ay. But also Ay = A, yielding Axy = A.
14.Suppose A is narcissistic. Then by the last problem Axy = A. Thus (Ax)y = A for every y, which means that Ax is narcissistic.
15.Suppose Kx = x (i.e that x is a fixed point of K). Then Kxx = xx. But also Kxx = x (since Kxy = x for every y). Thus Kxx is equal to both xx and to x, so that xx = x, which means that x is egocentric.
16.Suppose Kx is egocentric. Thus Kx(Kx) = Kx. Also Kx(Kx) = x (since Kxy = x for every y). Thus Kx(Kx) is equal to both Kx and x, so that Kx = x.
17.Suppose Kx is a fixed point of K. Then Kx is egocentric by Problem 15. Hence x is a fixed point of K by Problem 16.
18.Suppose Kx = Ky. Now, Kxz = x for every z. Since Kx = Ky, it follows that Kyz = x for every z. But also Kyz = y. Hence x = y.
19.We will show that if Kx = K, then for any elements y and z, it must be the case that y = z, which is contrary to the assumed condition that C contains more than one element.
Well, suppose Kx = K. Then for any y and z, Kxy = Ky and Kxz = Kz. But Kxy = x and Kxz = x, so that Ky = Kz. Then by the cancellation law (see Problem 18), y = z.
20.(a) This is immediate from the last problem: Since Kx ≠ K for all x, then KK ≠ K, so that K is not egocentric.
(b)Suppose KA = A (i.e. A is a fixed point of K). Then Ax = KAx for all x. But also KAx = A for all x. Thus Ax = A for all x, which means that A is narcissistic.
21.Let U1 = K; U2 = KK; U3 = KKK, etc. We show that the set {U1, U2, U3, …, Un, …} is far from infinite. It contains only K and KK. We see this as follows:
If Un = K, then Un+2 = KKK, which is K (because K is a kestrel). Thus if Un = K, then Un+2 = K, and, since U1 = K, then Un = K for every odd number n.
Also, if Un = KK then Un+1 = KKK which is K (again because K is a kestrel). Thus, if Un = KK then Un+2 = KK and since U2 = KK it follows that Un = KK for every even n. Therefore we see that, for all n, Un = K or Un = KK.
22.This is a very different story! Let K1 = K, K2 = KK; K3 = K(KK) [which is K(K2)], etc. Thus for each n, we have Kn+1 = K(Kn). The set {K1, K2, …, Kn, …} is indeed infinite, as we will see.
We will show that for any numbers n and m, if n ≠ m, then Kn ≠ Km. To show this, it suffices to show that if n > m, then Kn ≠ Km (because if n ≠ m, then either n < m or m < n). Thus it suffices to show that for any positive integers n and b, Kn ≠ Kn+b.
By Problem 19, K ≠ K(Kb); in fact K ≠Kx for any x. Since K = K1 and K(Kb) = Kb+1, we have:

Next, by the cancellation law (Problem 18), if Kx = Ky, then x = y. Equivalently, if x ≠ y, then Kx ≠ Ky. Thus if Kn ≠ Km, then KKn ≠ KKm, and so we have:

Now, by (1), we have K1 ≠ K1+b. Hence by repeated applications of (2), we see that K2 ≠ K2+b, K3 ≠ K3+b, …, Kn ≠ Kn+b. This concludes the proof.
23.We consider a lark L. For any x and y, (Lx)y = x(yy). Now, taking Lx for y, we see that (Lx)(Lx) = x((Lx)(Lx)). Thus (Lx)(Lx) is a fixed point of x! Note that (Lx)(Lx) is the same element as Lx(Lx) by how we define the use of parentheses (or rather the lack thereof).
24.Consider a kestrel K. For any x and y, Kxy = x. Taking K for both x and y, we have:

If K were a lark, then Kxy = x(yy) for all x and y, so that for x = K and y = K, we would have:

For (1) and (2) it would follow that K = K(KK), violating the fact that for no x can K = Kx (by Problem 19). Thus K cannot be a lark.
25.Suppose that a lark L is narcissistic. Then Lxy = L for every x and y by the solution to Problem 13. Taking Lx for y, we obtain Lx(Lx) = L. But Lx(Lx) is a fixed point of x [by the solution to Problem 23]. Thus L is a fixed point of x.
26.Suppose that LK = K (i.e. that K is a fixed point of L). Then LKK = KK. But LKK = K(KK) (since L is a lark), so that K(KK) = KK. Then by the cancellation law, taking KK for x and K for y, we see that KK = K, which means that K is egocentric, which is contrary to Problem 20. Therefore K cannot be a fixed point of L.
27.If the lark L is a fixed point of the kestrel K, then KL = L. So, for any x, KLx = Lx. But KLx = L, since K is a kestrel. So we have Lx = L for all x, and L is narcissistic. Thus L is a fixed point of every element x by Problem 25.
28.We first show that if x is any fixed point of LL, then xx must be egocentric.
Well, suppose that LLx = x. Now LLx = L(xx) [since L is a lark]. Thus x = L(xx). Therefore xx = L(xx)x, which is xx(xx). Thus xx is egocentric.
We can actually compute a fixed point of LL, in terms of just L: As we saw in the solution of Problem 23, for any x, a fixed point of x is Lx(Lx). Taking LL for x, we see that a fixed point of LL is L(LL)(L(LL)), and therefore an egocentric element is L(LL)(L(LL)) repeated, which is:

29.(1)Suppose I is agreeable. Then for any x there is some y such that Iy = xy. But Iy = y, so that y = xy, and thus y is a fixed point of x. Conversely, suppose that every element x has a fixed point y. Then xy = y. But y = Iy, so that xy = Iy, and thus I agrees with x on y.
(2)Suppose that every pair of elements is compatible. Now consider any element A. Then A is compatible with I. Thus there are elements x and y such that Ax = y and Iy = x. Since Iy = x, then y = x (since y = Iy). Thus Ax = y and y = x and it follows that Ax = x, which means that x is a fixed point of A. Thus every element has a fixed point, and consequently I is agreeable by (1).
(3)Since Ix = x for every x, it follows that II = I, which means that I is egocentric. If I were narcissistic, that would mean that for every x, Ix = I. But also Ix = x. Hence if I were narcissistic, then we would have x = I for every element x, contrary to the fact that  has more than one element.
30.Take M to be LI. Well, LIx = I(xx) = xx. Thus LI is a mocker.
31.We will see that any element θ that combines M with L must be a sage.
We already know that Lx(Lx) is a fixed point of x (by the solution to Problem 23). Now, Lx(Lx) = M(Lx), and consequently M(Lx) is a fixed point of x. Now, suppose that combines M with L. Then θx = M(Lx), and so θx is a fixed point of x. Thus θ is a sage.







Chapter 9
Combinatorics Galore
We now discuss some of the most prominent combinators of the literature, as well as some which I introduced in To Mock a Mockingbird.
I.The B-Combinators
Introducing the B Combinator, the Bluebird
The B-combinator is a combinator satisfying the following condition (for all x, y and z):

This combinator is also known as the bluebird, a name I introduced in To Mock a Mockingbird. I shall sometimes use that name. This combinator is one of the basic combinators.
Problem 1. Suppose  contains a bluebird B and a mocker M. Then the composition condition must hold (why?). Hence, by Theorem 1 of the last chapter, every element x has a fixed point. Moreover, one can write down an expression for a fixed point of x, using just the symbols B, M and x (and parentheses, of course). Write down such an expression.
Problem 2. Show that if B and M are present (as members of ), then some element of  is egocentric; in fact, write down the name of an egocentric element in terms of B and M.
Problem 3. Now write down a narcissistic element in terms of B, M and K (kestrel).
Some Derivatives of B
From B alone, one can derive combinators D, B1, E, B2, D1B+, D2, , which are defined by the following conditions:

(a)Dxyzw = xy(zw), the dove
(b)B1xyzw = x(yzw)
(c)Exyzwv = xy(zwv), the eagle
(d)B2xyzwv = x(yzwv)
(e)D1xyzwv = xyz(wv)
(f)B+xyzw = x(y(zw))
(g)D2xyzwv = x(yz)(wv)
(h) xy1y2y3z1z2z3 = x(y1y2y3)(z1z2z3).

All these combinators, including B, are called compositors, since they serve to introduce parentheses. The standard ones are the bluebird B and D, which I call the dove. We shall also have good use for E, which I call the eagle.
Problem 4. (1) Derive these combinators from B. [Hints: Do the following in order:

(a)Express D in terms of B.
(b)Express B1 in terms of B and D (which can then be reduced to an expression in terms of B alone).
(c)Express E in terms of B and B1.
(d)Express B2 in terms of B and E.
(e)Express D1 in terms of B and B1, or, alternatively, in terms of B and D.
(f)Express B+ in terms of B and D1.
(g)Interestingly, D2 can be expressed in terms of D alone!
(h)Express  in terms of E alone!]

(2) Using mathematical induction, show that for each positive integer n, there is a combinator Bn, derivable from B, satisfying the condition

Note that by this definition, the bluebird B also has the name B0, and B1 and B2 are the same as defined above (where different variables were used in the definition).
II.The Permuting Combinators
We now come to an interesting family of combinators known as permuters.
Introducing the Combinator T, the Thrush
The simplest permuting combinator is T, which is defined by the following:

This permuter T is standard. In To Mock a Mockingbird, I called it the thrush.
Two elements x and y are said to commute if xy = yx.
Problem 5. Prove that from the thrush T and the lark L one can derive a combinator A that commutes with every element x.
Introducing the Combinator R, the Robin
In To Mock a Mockingbird, I introduced a combinator R, which I termed a robin. It was defined by the condition:

Problem 6. Show that a robin R can be derived from B and T.
Introducing the Combinator C, the Cardinal
The combinator C is defined by the condition:

This C is standard in the literature, and of basic importance. I called it the cardinal in To Mock a Mockingbird. It is derivable form B and T, as was discovered by Alonzo Church [1941]. Church’s construction was quite tricky! It used eight letters, and I doubt that it can be done with fewer.
Having derived R from B and T, it is quite simple to get C.
Problem 7. How can one derive C from R?
Problem 8. The solution of Problem 7, when reduced to B and T has nine letters. It can easily be shortened to eight, thus yielding Church’s expression for C. Do this reduction.
Problem 9. Can a thrush T be derived from C and the identity combinator I?
Problem 10. We have seen that C is derivable from R alone. Can one derive R from C alone?
Note: Taking BBT for R, and RRR for C, a useful fact to observe is that for any x:

because Cx = RRRx = RxR = BBT xR = B(T x)R.
Introducing the Combinator F, the Finch
A finch F is defined by the following condition:

A finch F can be derived from B and T in several ways. It can be derived from B and R, or from B and C, or from T and the eagle E.
Problem 11. (a) It is easiest to derive F from all three of B, R and C. How? [Then, of course, F is derivable from B and R, or from B and C, since R and C and inter-derivable.] (b) Show how to derive F from B and E.
Introducing the Combinator V, the Vireo
A vireo is a combinator V satisfying the following condition:

The combinator V has a sort of opposite effect to that of the finch F. V is derivable from B and T. It is easiest to derive it from C and F.
Problem 12. Derive V from C and F.
Problem 13. We have seen that V is derivable from C and F. It is also true that F is derivable from C and V. How?
Problem 14. As a curiosity, show that the identity combinator I is derivable from R and K (kestrel).
Some Relations
Given a permuting combinator A satisfying the condition Axyz = abc, where abc is some permutation of xyz, by A* is meant the combinator satisfying the condition:

Thus C*, R*, F*, V* are combinators defined by the following conditions:

Each of these combinators is derivable from B and T. It is easiest to derive them from combinators already derived from B and T.
Problem 15. Derive:

(a)C* from B and C;
(b)R* from B and C;
(c)F* from B, C* and R*;
(d)V* from C* and F*.

Consider again a permutation combinator A satisfying the condition Axyz = abc, where abc is a permutation of xyz. We let A** be the combinator defined by the condition

Thus C**, R**, F**, V** are combinators defined by the following conditions:

Problem 16. Show that C**, R**, F**, V** are all derivable from B and T.
The problem is far simpler than it might appear. One can handle all four cases in one fell swoop!
Vireos Revisited
We have derived V from C and F, obtaining an expression, which, when reduced to B and T, has 16 letters. It can alternatively be derived from C* and T, yielding an expression, which, when reduced to B and T, has only ten letters. How?
Problem 17. Derive V from C* and T.
Problem 18. We have not yet considered a combinator A satisfying the condition Axyz = yxz. Is there one derivable from B and T ?
III.The Q-Family and the Goldfinch, G
We now come to a family of combinators, all derivable from B and T, that both permute and introduce parentheses. The first member is Q defined by the condition:

I termed Q the queer bird in To Mock a Mockingbird.
Problem 19. Derive Q from B and T. [Hint: There is a two-letter expression involving B and one other combinator already derived from B and T that works!]
Q1 and Q2
The two combinators Q1 and Q2 are defined by the following conditions:

In To Mock a Mockingbird, the combinators Q1 and Q2 were respectively called the quixotic and the quizzied birds.
Problem 20. Derive Q1 and Q2 from B and T, or from any combinators already derived from them.
There is an old Chinese proverb that says that if a cardinal C is present, then you cannot have a quixotic bird without a quizzied bird, nor a quizzied bird without a quixotic bird, or if there isn’t such a Chinese proverb, then there should be!
Problem 21. What is the sense behind such a proverb?
The Birds Q3 (Quirky), Q4 (Quacky), Q5 (Quintessential), Q6 (Quivering)
The quirky bird, Q3, the quacky bird, Q4, the quintessential bird, Q5, and the quivering bird, Q6, are defined by the following conditions:

Problem 22. Derive these from combinators derivable from B and T.
There is, or should be, another Chinese proverb which says that if a cardinal is present, then you cannot have a quirky bird without a quacky bird, nor a quacky bird without a quirky bird.
Problem 23. What is the sense behind that one?
Problem 24. Show that Q4 can be derived from T and Q1.
Problem 25. We have seen that Q is derivable from B and T. But also, B is derivable from Q and T. Prove this. [It is not obvious.]
Problem 26. One can derive C from Q and T more easily than from B and T, in fact, with an expression of only four letters. How can this be done?
The Combinator G, the Goldfinch
In To Mock a Mockingbird I introduced a combinator G, called the goldfinch, which I found quite useful. It is defined by the condition:

I do not know if this combinator was previously known or not.
Problem 27. Derive G from B and T, or from combinators already derived from B and T.
IV.Combinators Derivable from B, T, M and I (λ-I Combinators)
Combinators derivable from B, T, M and I form an extremely important class, whose significance will be discussed in a later chapter. These combinators are known as λ – I combinators.
A useful combinator M2 is defined by the condition

It is easily derivable from M and B.
Problem 28. Do so.
The Combinator W, the Warbler and the Combinator W′, the Converse Warbler
A standard combinator is W, which is defined by the condition

W should not be confused with the lark L: Lxy = x(yy).
Alonzo Church showed how to derive W from B, T, M and I. His derivation was both bizarre and ingenious. His expression for W involved 24 letters and 14 pairs of parentheses! Fairly soon afterwards, J. B. Rosser found an expression for W in terms of just B, T and M that included only ten letters.
I called W the Warbler in To Mock a Mockingbird. Before deriving it from B, T and M, it will prove convenient to first consider another combinator W′, which might be called a converse Warbler. W′ is defined by the condition

Problem 29. There are two interesting ways of deriving W′ from B, T and M. One way is to first derive W′ from M2 and the robin R, and then reduce that expression to B and T, which should yield an expression of five letters. Another way is to first derive W′ from B, T and M2, which when reduced to B, T and M yields a different expression, one that also contains five letters. The reader should try to find both these expressions!
Problem 30. One can easily derive W from W′ and the cardinal C, getting an expression which, when reduced to B, T and M, will have 13 letters. However, it can be further reduced to an expression having only ten letters in two different ways, depending on the choice for W. Do these things.
Problem 31. M is obviously derivable from W and T, and also from W and I. Also, I is derivable from W and K. Find these derivations.
Problem 32. Some useful relatives of W are W1 , W2, W3, defined by the following conditions:

Show that these are all derivable from B, T and M, in fact from W and B.
The Combinator H, the Hummingbird
I have found good use for a combinator H (dubbed the hummingbird in To Mock a Mockingbird). The combinator H is defined by the condition

Problem 33. Show that H is derivable from B, C and W, and thus from B, M and T.
Problem 34. One can also derive W from H and R, or, more neatly, from C, H and R. How? [Hint: First derive W′.]
Larks Revisited
We recall the lark L defined by the condition Lxy = x(yy). The lark can be derived from B, T and M in many ways.
Problem 35.
(a)Show that L is derivable from B, R and M or from B, C and M, and then reduce the latter expression to one in terms of B, M and T.
(b)Show that L is derivable from B and W. This fact is rather important.
(c)My favorite derivation of L is from M and the queer combinator Q. It is also the simplest. When reduced to B, M and T, we get the same expression as in (a). Try this!
The Combinator S, the Starling
One of the most important combinators in the literature of Combinatory Logic is the combinator S, which is defined by the following definition:

I called S the starling in To Mock a Mockingbird. One reason why S is so important is that from S and K one can derive all possible combinators, in a sense I will make precise in a later chapter.
The starling is derivable from B, M and T, and more easily from B, C and W. The standard expression for S in terms of B, C and W has seven letters, but in To Mock a Mockingbird, I found another expression having only six letters. For this I used the goldfinch G, which we recall is defined by the condition Gxyzw = xw(yz).
Problem 36. Derive S from B, W and G, and then reduce the resulting expression to B, C and W. [Hint: Use W2, which we learned from the solution to Problem 32 can be expressed as B(BW).]
We recall that the hummingbird H was defined by the condition Hxyz = xyzy.
Problem 37. Derive H from S and R.
Problem 38. Show that both W and M are derivable from S and T.
Problem 39. Express a warbler W in terms of S and C.
Note: We now see that the class of combinators derivable from B, C and W is the same as the class of combinators derivable from B, C and S, since S is derivable from B, C and W [by Problem 36], and W is derivable from S and C [by Problem 39].
The Order of a Combinator
By a combinator of order one is meant a combinator A such that Ax can be expressed in terms of x alone (a single variable and no combinators). For example, M is of order 1, since Mx = xx, and the expression xx does not involve the letter M. Another example is the identity combinator I, since Ix = x. Another is W L, since W Lx = Lxx = x(xx).
By a combinator of order two is meant one whose defining condition involves just two variables (and no combinators), as is the case for W and L. In general, for any positive integer n, by a combinator of order n is meant one whose defining condition involves n variables. The permuting combinators C, R, F and V are of order three, as are B and Q.
It is not true that every combinator has an order, for T I cannot be of any order n, since T Ix1 , …, xn = x1 Ix2 , …, xn, which cannot be reduced any further (and so we cannot get rid of the combinator I on the right side of this equation). On the other hand IT has an order, since it is equal to T, which is of order two.
A useful fact to note is that for any combinators A1 and A2, the combinator RA1A2 is equal to CA2A1, since RA1A2 x = A2xA1, and also CA2A1 x = A2xA1.
The P Group of Combinators
Problem 40. We shall have good use for some of the combinators P, P1 , P2 and P3 satisfying the following conditions:

These can be derived from B, M and T, in fact from B, Q and W (and hence from B, C and W). How?
Problem 41. Show that M is derivable from P and I.
The Φ Group of Combinators
We will later need combinators Φ, Φ2, Φ3, Φ4, defined by the following conditions:


Problem 42. (a) Derive Φ from B and S. (b) Show by induction that for each n, there is a combinator Φn derivable from B and S satisfying the equation:

Solutions to the Problems of Chapter 9

1.To begin with, let us note that if B is present, then the composition condition must hold, since for any elements x and y, an element that combines x with y is Bxy [since (Bxy)z is Bxyz, which is x(yz)].
We recall from the solution of Problem 1 of the last chapter that if C is any element that combines x with M, then CC is a fixed point of x (we stated this result as Theorem 1*). Well, BxM combines x with M, and so BxM(BxM) is a fixed point of x. But BxM(BxM) is also equal to M(BxM), by the definition of M [as applied to the first M in the expansion of M(BxM)]. So M(BxM) is also a fixed point of x (for every x!). We will use this result several times in this chapter and the next.
2.Since B is present, the composition condition holds. Then by the solution of Problem 2 of the last chapter, any fixed point of M is egocentric. Now, M(BMM) is a fixed point of M [by the previous problem of this chapter, taking M for x]; hence M(BMM) is egocentric. Let us double-check: To reduce clutter, let A be BMM. We are to show that MA is egocentric. Well, MA = M(BMM) = BMM(BMM). But BMM(BMM) = M(M(BMM)), since B is a bluebird. Then we have M(M(BMM)) = M(MA) = MA(MA). Thus MA = MA(MA), and MA is egocentric.
3.By Problem 20 (b) of the last chapter, any fixed point of a kestrel K is narcissistic. Since M(BKM) is a fixed point of K [by the solution to Problem 1], it must be narcissistic [as the reader can check directly, by showing that M(BKM)x = M(BKM)].
4. (1) (a)We take D to be BB. Let us check that this works: BBxy = B(xy). Hence BBxyz = B(xy)z. Then

Thus Dxyzw = xy(zw).
(b)We take B1 to be DB, which can then be expressed in terms of B alone, since D can be so expressed. We can see this definition works as follows:

Thus,

Since D = BB, then DB = BBB. Thus in terms of B alone B1 = BBB.
(c)We take E to be BB1, which in terms of B alone is B(BBB). We can see this definition works as follows:
Exy = BB1xy = B1(xy). Thus

(d) We take B2 to be EB (which, in terms of B, is B(BBB)B). Now,

Hence

(e) We can take D1 to be B1B, or we can take D1 to be BD. Actually B1B is the same as BD because

It will be quicker to take D1 to be B1B. Then

Hence,

(f) We take B+ to be D1B. Thus,

(g) We take D2 to be DD. Then

(h) >We take Ê to be EE. Then

(2)We will show by mathematical induction that we can define all the combinators

in terms of the combinator B. Well, we have already noted that B0 is B itself. Now assume that Bn can be defined in terms of the combinator B. Then we claim that Bn+1 = BBBn:

5.Actually, any fixed point A of T will commute with every element x. For suppose T A = A. Then for every element x, we see that Ax = T Ax = xA. Thus A commutes with x.
6.Take R to be BBT. Then

7.Actually C is derivable from R alone! Take C to be RRR. Then:

8.In terms of B and T, C = BBT (BBT)(BBT), which has nine letters. It is equal to B(T (BBT))(BBT), which is Church’s expression.
9.Easily! Take T to be CI. Then Txy = CIxy = Iyx = yx.
10.Yes. Take R to be CC. Then Rxyz = CCxyz = Cyxz = yzx.
11.(a) BCR is a finch, because

In terms of B and R, F = B(RRR)R. In terms of B and C, F = BC(CC).

(b)ETT ET is a finch, because

We might note that in terms of B and T, the expression BCR for the finch has 12 letters (since C has 8 and R has 3), while using the expression B(RRR)R for the finch gives us 13 letters in terms of B and T; using the expression BC(CC) for the finch, we get 25 letters in terms of B and T.
As for F = ETT ET, as it stands, when E is replaced by B(BBB), we get an expression of 11 letters, but the expression can be reduced to only 8 letters in terms of B and T, as we will now see:

Hence,

Thus, expressing F in terms of E and T, and then reducing that expression to one in terms of B and T, yields the shortest expression for the finch F.
12.Take V to be CF. Then CF xyz = F yxz = zxy.
Note: When reduced to B and T, the expression CF has 16 letters (since C and F each have 8). This 16-letter expression cannot be reduced further. However, one can get an expression for V in terms of B and T which has only ten letters, as we will later see.
13.CVxyz = Vyxz = zyx. Thus CV is a finch, F.
14.For any element A, the element RAK must be an identity combinator, for RAKx = KxA = x. In particular, RKK and RRK are both identity combinators.
15.Here are the derivations requested for C*, R*, F* and V*:
(a)Take C* to be BC: BCwxyz = C(wx)yz = wxzy.
(b)We have just derived C* from B and C, and R* can in fact be derived from C* alone. Thus take R* to be C*C*. Well, C*C* wxy = C* wyx. Hence

So, in terms of B and C, R* = BC(BC).
(c)Take F* to be BC*R*. Then,

(d)Take V* to be C*F*. Well, C*F* wxyz = F* wyxz = wzxy.
16.Here is the secret! Let A be any of the four combinators C, R, F, V. We can take A** to be BA*. Here is why:

17.Take V to be C* T. Well,

Thus C*T is a vireo.
18.Of course! The combinator T itself is such a combinator, since T xy = yx, so that T xyz = yxz.
19.The expression CB works:

We thus take Q to be CB.
20.Take Q1 to be C*B. Well, C* Bxyz = Bxzy = x(zy). Take Q2 to be R*B. Now, R* Bxyz = Byzx = y(zx).
21.The fact is that Q2 is derivable from C and Q1, and Q1 is derivable from C and Q2:

22.Q3: Take Q3 to be V* B. Well, V* Bxyz = Bzxy = z(xy). Alternatively, we can take Q3 to be BC(CB) or QQC. In terms of B and T, we could simply take Q3 to be BT.
Q4: We could take Q4 to be F* B: We see that

Alternately, we could take Q4 to be

Q5: Take Q5 to be BQ: Q3: BQxyzw = Q(xy)zw = z(xyw).
Q6: Take Q6 to be B(BC)Q5. Then

23.Q4 is derivable from C and Q3, and Q3 is derivable from C and Q4:

24.We can express Q4 as Q1T:

25.The bluebird B can be seen to be the same as QT (QQ) as follows:

26.The cardinal C can be expressed as QQ(QT):

27.Take G to be BBC. Then

28.Obviously, take M2 to be BM :

29.For the first method, we take W′ to be M2R. Then

Reducing to B, T and M, we take BM for M2 and BBT for R, obtaining the expression BM(BBT).
As for the second way to derive W′, we take W′ to be B(M2B)T. In this case,

Taking BM for M2, this version of W′ reduces to B(BMB)T. Thus we have two ways of expressing W′ from B, T and M: BM(BBT) and B(BMB)T.
30.CW′ is a warbler, since CW′ xy = W′yx = xyy = W xy.
We now use the fact previously noted that for any x, Cx = B(T x)R (this was shown right after the statement of Problem 10 in this chapter), and so CW′ = B(TW′)R. Thus B(TW′)R is a warbler. Hence B(T W′)BBT is a warbler (since R = BBT). As seen in the last problem, we can take W′ to be either BM(BBT) or B(BMB)T, and so we can take W to be either

The latter is Rosser’s expression for W.
31. 

32.We will take W1 to be BW, W2 to be BW1, W3 to be BW2 Then

Note: In general, if for all n we take Wn+1 to be BW n, then, for every n,

as can be shown by mathematical induction.
33.We take H to be W* C*, which is BW (BC). Then

Thus,

34.First we take W′ to be HR. Thus HRxy = Rxyx = yxx = W′xy. Then take W to be CW′. This is a warbler: CW′xy = W′yx = xyy = W xy. Thus C(HR) is a warbler, and since C is RRR, the warbler is also derivable from H and R alone.

35.(a) RMB is a lark, since RMBxy = BxMy = x(My) = x(yy) = Lxy. Also, so is CBM (as the reader can verify). To express RMB in terms of B, M and T, we take BBT for R in RMB, so that

Thus B(T M)B is a lark.
(b)BWB is also a lark, since

(c)QM is a lark, since QMxy = x(My) = x(yy) = Lxy. Reducing this to B, M and T, we see that QM = CBM [since Q = CB by Problem 19]. Also, for any x and y, it is a fact that Cxy = Ryx, because [by Problem 7] Cxy = RRRxy = RxRy = Ryx. Thus Cxy = Ryx. In particular, CBM = RMB, so that QM = RMB, which is the expression we obtained for L in (a), and which reduced to B(T M)B.

36.Recall that W2 was defined by the condition W2xyzw = xyzww in the statement of Problem 32 and that in the solution to Problem 32 we saw W2 to be B(BW), i.e. defined in terms of B and W. We can take the starling S to be W2G, since  Since we saw in the solution to Problem 27 that G = BBC, we also see that S is B(BW)(BBC) in terms of to B, C and W.
37.SR is a hummingbird, since 

38.(a) ST is a warbler, since 
(b)ST T is a mocker, since  Another way to see this is based on our having seen that W T in a mocker [in the solution to Problem 31], and since we can take ST for W [because ST xy = T y(xy) = xyy = W xy], again we see that STT is a mocker.

39.By Problem 34, C(HR) is a warbler W. By the solution to Problem 37, SR is a hummingbird (H). Thus C(SRR) is a warbler. But by the solution to Problem 10, CC is a robin R, and so, for W, we can take C(S(CC)(CC)).
40.(a) Take P to be W2Q5. Then 
(b)Take P1 to be LQ. Then 
(c)Take P2 to be CP1. Then 
(d)Take P3 to be BCP. Then

41.Take M to be P II. Then 

42.(a) Take Φ to be B(BS)B. Then

(b)We take Φ = Φ. Now, suppose Φn is defined and behaves as shown in the equation Φnxyzw1 … wn = x(yw1 … wn)(zw1 … wn). We take Φn+1 to be B ΦnΦ. Then









Chapter 10
Sages, Oracles and Doublets
Sages Again
We recall that by a sage is meant a combinator θ such that θx is a fixed point of x for every x; in other words, x(θx) = θx.
Several papers in the literature are devoted to the construction of sages.
In the chapter before last, we did not construct any sages. We merely proved that a sage exists, providing that the mocker M exists and that the composition condition holds. We now turn to several ways of constructing sages.
Problem 1. Construct a sage from B, M and the robin R, recalling that Rxyz = yzx. [Hint: Recall from the solution to Problem 1 in Chapter 9 that M(BxM) is a fixed point of x.]
Problem 2. Now derive a sage from B, M and the cardinal C.
Problem 3. A simpler construction of a sage uses M, B and the lark L. One such construction provides simple alternative solutions to the last two problems. Can you find it?
Problem 4. Derive a sage from B, M and W.
It is a bit tougher to derive a sage from B, W and C. We will consider some ways of doing so.
Problem 5. Let us first derive a sage from B, L and the queer bird Q [Qxyz = y(xz)].
Problem 6. Using the result of the last problem, now write down an expression for a sage in terms of B, W and C.
Problem 7. A particularly neat and simple construction of a sage is from Q, M and L. Can you find it?
Problem 8. Actually, a sage can be constructed from Q and M alone. How?
We recall the starling S satisfying the condition Sxyz = xz(yz).
Problem 9. Construct a sage from L and S.
Problem 10. Now show that a sage can be constructed from B, W and S. This can be done in several ways, one of which uses only five letters. (The sage presented in the set of solutions with 5 letters is due to Curry.)
Problem 11. Construct a sage from M and P. [Pxyz = z(xyz)].
Problem 12. Construct a sage from P and W.
Problem 13. Construct a sage from P and I.
The Turing Combinator
In 1937, the great Alan Turing discovered a remarkable combinator U, which will soon surprise you! It is defined by the condition:

It is ultimately derivable from B, T and M in several ways (it is also derivable from S, L and I).
Problem 14. Derive U from W1 and P1.
Problem 15. Derive U from W and P.
Problem 16. Derive U from P and M.
Problem 17. Derive U from S, I and L.
Now for the surprise!
Problem 18. There is a sage that is derivable from U alone! How?
The Oracle O
A truly remarkable combinator O is defined by the condition:

You will soon see why O is so remarkable! In To Mock a Mockingbird, I dubbed it the owl, but I now prefer the grander name oracle, which is befitting it.
Problem 19. Derive O from Q and W.
Problem 20. Derive O from B, C and W.
Problem 21. Derive O from S and I.
Problem 22. Derive O from P and I.
Problem 23. Derive a sage from O, B and M.
Problem 24. Derive a sage from O and L.
Problem 25. Derive a Turing combinator U from O, B and M.
Problem 26. Derive a Turing combinator from O and L.
Now, why is the oracle O so remarkable? Well, in the solutions to Problems 23 and 24, we have seen two sages constructed from O and other combinators, and in both cases the sages happened to be fixed points of O. Was that a mere coincidence? No! The first remarkable fact about the oracle O is that all of its fixed points are sages.
Problem 27. Prove that all fixed points of O are sages.
Now for the second remarkable fact about O. The converse of the statement of Problem 27 holds, i.e., all sages are fixed points of O. Thus the class of all fixed points of O is the same as the class of all sages!
Problem 28. Prove that every sage is a fixed point of O.
Double Sages
A pair (y1, y2) is called a double fixed point of a pair (x1, x2) if x1y1y2 = y1 and x2y1y2 = y2. We say the system has the weak double fixed point property if every pair (x1, x2) has a double fixed point. We say that the system had the strong double fixed point property if there is a pair (θ1, θ2), called double sage pair — or, more briefly, a double sage — such that for every x and y, the pair (θ1xy, θ2xy) is a double fixed point of (x, y), i.e.

There are many ways to construct double sages from the combinators B, M and T, several of which can be found in Chapter 17 of my book Diagonalization and Self-Reference [Smullyan, 1994], and one of which we shall now consider. Others, which I will leave as exercises, have solutions which can be found in the book just mentioned.
The Nice Combinator N
My favorite construction of a double sage uses a combinator N satisfying the following condition:

The existence of such an N obviously implies that the system has the weak double fixed point property.
Problem 29. Why?
Before considering the construction of a double sage, let us see how a nice combinator N can be ultimately derived from B, T and M. To this end, we will use a combinator n defined by the condition:

Problem 30.

(a)Derive n from B, C and W or from combinators derivable from them. [It is easiest to derive n directly from C, W, W*, V* and Φ3.]
(b)Now show that any fixed point of n (such as M(Ln)) is a nice combinator.

Problem 31. Show that from N, C, W and W1 one can construct combinators θ1 and θ2 such that (θ1, θ2) is a double sage.
Double Oracles
We will call a pair (A1, A2) a double oracle if every double fixed point of (A1, A2) is a double sage. It turns out that every double sage is a double fixed point of any double oracle (A1, A2).
To my delight, double oracles exist, if B, T and M are present.
Let O1 and O2 be defined by the following conditions:

Problem 32. Show that (O1, O2) is a double oracle.
Solutions to the Problems of Chapter 10
1.Since BxM(BxM) = M(BxM) is a fixed point of x (by the solution to Problem 1 of Chapter 9), we want to find a combinator θ such that θx = M(BxM). Well, by the definition of R, BxM = RM Bx, and so M(BxM) = M(RM Bx). Also, by the definition of B,

We thus take θ to be BM(RM B).
2.Let us now use a fact stated at the end of the last chapter, namely that for any combinators A1 and A2, the combinator RA1A2 is equal to CA2A1. In particular, RM B is equal to CBM, so that RM Bx = CBMx. Then, since M(RM Bx) is a fixed point of x, so is M(CBMx). Also, M(CBMx) = BM(CBM)x. We thus now take θ to be BM(CBM).
3.We now use the fact that Lx(Lx) is a fixed point of x. Thus so is M(Lx), which is also BM Lx. Thus BML is a sage.
We proved in the last chapter (in the solution to Problem 35) that RMB is a lark, so that CBM is also a lark, since it is equal to RMB. We can replace the L in BML with either RMB or CBM, showing that both BM(RMB) and BM(CBM) are sages.
This shows us alternative ways of solving Problems 1 and 2.
4.As we saw in the solution of Problem 35 of the last chapter, BW B is also a lark. Thus we can also replace the L In BML by BW B, showing us that BM(BW B) is also a sage.
5.We again use the fact that Lx(Lx) is a fixed point of x. Now,

Thus W (QL(QL)) is a sage.
6.We have just proved that W (QL(QL)) is a sage. We can take CB for Q (by the solution to Problem 19 in Chapter 9), thus getting the expression W (CBL(CBL)), which can be shortened to W (B(CBL)L). Then, by the solution of Problem 35 of Chapter 9, we can take BW B for L, thus getting the sage

We will later find another solution to this problem.
7.Again we use the fact that Lx(Lx), and hence M(Lx), is a fixed point of x. But M(Lx) = QLMx. Thus QLM is a sage.
8.In the expression QLM of the previous problem, we can take QM for L (by Problem 35 of Chapter 9), thus obtaining Q(QM)M as another sage.
9.Lx(Lx) is a fixed point of x. But SLLx = Lx(Lx). Hence SLL is a sage!
10.Here is one way to get a sage from S, W and B: Since SLL is a sage (as shown in the solution to the last problem), we can take BWB for L in SLL, obtaining S(BWB(BWB)), which can be shortened to S(W (B(BWB))), which has six letters. However, there is a more clever and more economical way: By the definition of W, SLL = WSL. Now replace L by BWB in WSL, and we have the sage WS(BWB), one letter shorter! This is Curry’s expression for a sage.
Note that since S is derivable from B, C and W (by Problem 36 of Chapter 9), so is W S(BWB), thus getting a third way of deriving a sage from B, C and W.
11.M(PM) is a sage: M(PM)x = PM(PM)x = x(M(PM)x).
12.WP (WP) is a sage: WP (WP)x = P (WP)(WP)x = x((WP)(WP)x).
13.PII(P(PII)) is a sage, since M(PM) is a sage by the solution to Problem 11 above, and M can be expressed by PII, as seen in the solution to Problem 41 of Chapter 9.
14.W1P1 is a Turing combinator: W1P1xy = P1xyy = y(xxy) = Uxy.
15.Another Turing combinator is WP, since

16.Another Turing combinator is PM, since

17.Another Turing combinator is L(SI), since

18.In the equation Uxy = y(xxy), just substitute U for x, and we have UUy = y(UUy). Thus UU is a sage.
19.QQW is an oracle, since QQW xy = W (Qx)y = Qxyy = y(xy) = Oxy.
20.We take CB for Q in QQW from the previous problem, which shows us that CB(CB)W is an oracle.
21.SI is an oracle, since SIxy = Iy(xy) = y(xy) = Oxy.
22.PI is an oracle, since PIxy = y(Ixy) = y(xy) = Oxy.
23.See the solution to Problem 25 below.
24.See the solution to Problem 26 below.
25.BOM is a Turing combinator, since

Now, since BOM is a Turing combinator, and if U is any Turning combinator, then UU is a sage (by the solution to Problem 18), it follows that BOM(BOM) is a sage (which can also be verified directly), which solves Problem 23.
Please note that BOM(BOM) is also a fixed point of O. Indeed (as noted in the solution to Problem 1 of Chapter 9 and recalled in the solution to Problem 1 of this chapter), BxM(BxM) is a fixed point of x, for any x.
26.LO is a Turing combinator, since LOxy = O(xx)y = y(xxy) = Uxy. Now, since LO is a Turing combinator, then LO(LO) is a sage, which solves Problem 24. [Now we can say, “Lo! Lo! A sage!!”] Again LO(LO) is not only a sage, but also a fixed point of O!
27.Suppose A is a fixed point of O. Then OA = A. Hence

Since Ax = x(Ax), A is a sage.
28.Consider any sage θ. For any x, we have θx = x(θx). Also Oθx = x(θx). Thus θx = O(θx) (they are both equal to x(θx)). Since x = Oθx for every term x, it follows that θ = Oθ, which means θ that θ is a fixed point of O!
29.In the equation Nzxy = z(Nxxy)(Nyxy) substitute x for z to obtain:
(1)Nxxy = x(Nxxy)(Nyxy).
Next, in the equation Nzxy = z(Nxxy)(Nyxy) substitute y for z to obtain:
(2)Nyxy = y(Nxxy)(Nyxy).
By (1) and (2), the pair (Nxxy, Nyxy) is a double fixed point of (x, y).
30. (a) We take n to be C(V*Φ3W (W2C)). We leave it to the reader to verify that

(b)Let N be any fixed point of n. Then N = nN. Thus

31.Take θ1 to be WN and θ2 to be W1(CN) Then
(a)θ1xy = WNxy = Nxxy, so that θ1xy = Nxxy.
(b)θ2xy = W1(CN)xy = CNxyy = Nyxy, so that θ2xy = Nyxy.
Now,
(1)Nxxy = x(Nxxy)N(yxy) and
(2)Nyxy = y(Nxxy)N(yxy).
Since Nxxy = θ1xy and Nyxy = θ2xy, then, by (1) and (2), we have:
(1′)θ1xy = x(θ1xy)(θ2xy) and
(2′)θ2xy = y(θ1xy)(θ2xy).
Thus (θ1, θ2) is a double sage.
32.Suppose (A1, A2) is a double fixed point of (O1, O2). Then

and O2A1A2 = A2. Hence:

Thus (A1, A2) is a double sage, and (O1, O2) is a double oracle, as we were asked to show.
Before proving that every double sage is a double fixed point of (O1, O2), let us note that if A1xy = A2xy for all x and y, then A1 = A2, because from A1xy = A2xy for all x and y it follows that A1x = A2x for all x, and hence that A1 = A2.
Now, suppose that (θ1, θ2) is a double sage. Thus:
(1)θ1xy = x(θ1xy)(θ2xy),
(2)θ2xy = y(θ1xy)(θ2xy).
Also,
(1′)O1θ1θ2xy = x(θ1xy)(θ2xy),
(2′)O2θ1θ2xy = y(θ1xy)(θ2xy).
Therefore, θ1xy = O1 θ1 θ2xy and θ2xy = O2 θ1 θ2xy.
Then θ1 = O1 θ1 θ2 and θ2 = O2 θ1 θ2, which means that (θ1, θ2) is a double fixed point of (O1, O2).







Chapter 11
Complete and Partial Systems
I.The Complete System
From the two combinators S and K, all possible combinators are derivable! Let me explain just what I mean by this.
In the formal language of combinatory logic, we have a denumerable list x1, x2, …, xn, … of symbols called variables, and some symbols called constants, each of which is the name of an element of the applicative system.
The notion of term is recursively defined by the following conditions:

(1)Every variable and constant is a term.
(2)If t1 and t2 are terms, so is (t1t2).

It is understood that no expression is a term unless its being so is a consequence of conditions (1) and (2) above.
Problem 1. Give a more explicit definition of what it means for an expression to be a term.
We continue to abbreviate terms by removing parentheses if no ambiguity can result, using the afore-mentioned convention that parentheses are to be restored to the left, e.g. xyz is to be read ((xy)z) etc.
For a moment, let us recall in general all the combinators you have been introduced to in Chapters 8 through 10. You learned each combinator’s meaning from what we have called its “defining equation” (its defining condition). Most of those defining equations for combinators are of the form Ax1x2 … xn = t(x1x2, …, xn), where x1x2, …, xn are the variables that occur in the term t and the term t is built only from these variables (and parentheses; we will now [mostly] stop mentioning those necessary parentheses, which are always there whenever a term is not a single variable or a single constant, even if we suppress writing some of them for easier reading). For instance, you were introduced to the robin by its defining equation Rxyz = yzx = ((yz)x) and to the cardinal by its defining equation: Cxyz = xzy = ((xz)y). However, the defining equation of the kestrel, Kxy = x is different in that not all the variables that we wish to have as arguments of the combinator K occur in the term t (which is x in this case), although all the variables in t occur in the list of arguments for K, the combinator being defined.
Combinators A of the first type, in which the variables of the term and the combinator being defined are identical, are called non-cancellative combinators (or λ-I combinators). We will discuss this subgroup of combinators further in Section II. Combinators A of the second type, in which not all the variables in the arguments of the combinator being defined occur in the term t are called cancellative combinators, because A effectively cancels out those of the variables that do not occur in t. For example, the bluebird has the defining equation Bxyz = x(yz) in which the variables are identical on both sides of the equation, so it is non-cancellative. But in Kxy = x and (KI)xy = y, one of the two arguments on the left is missing in the term on the right, so the two combinators K and KI are cancellative (these are the only cancellative operators you have seen so far).
Thus the general form of a defining equation for a combinator is Ax1x2 … xn = t, where t is constructed only from variables and the variables of t are among x1, x2, …, xn. I.e. although t must contain at least one variable in order to be a term, and every variable in t is one of the variables x1, x2, …, xn, not all the variables that occur in x1, x2, …, xn need to occur in t (e.g. as in the case of the kestrel).
When t is a term constructed only from variables that are among x1, x2, …, xn, and our applicative system  contains an element A (no element of  contains variables) such that Ax1x2 … xn = t holds, then we say that A is a combinator of the applicative system  and also say that the combinator A with arguments x1, x2, …, xn realizes the term t. The equation Ax1x2 … xn = t is called the defining equation of the combinator A.
It turns out that all constants in any applicative system are combinators, because in applicative systems the meaning of constants is always specified by a defining equation. And the systems are also always defined so that every term constructed from constants alone is assumed to be a combinator of the system. Thus the combinators and elements of the system are one and the same. But the constants may be a subset of the set of elements.
So now you know what I mean by “every possible combinator” when I said that every possible combinator is derivable from S and K. I was talking about any combinator that can be specified by a defining equation of the above form!
Thus the claim that every combinator in our applicative system C can be derived from (constants) S and K just says that for every combinator A specified by a defining equation Ax1x2 … xn = t (where t is built only from variables, and those variables are among x1, x2, …, xn), we can find a term AS,K constructed only from S and K (and no variables at all!) such that we can replace the A in its defining equation by AS,K and get a true equation (true because of the meanings of S and K): AS,Kx1x2 … xn = t.
Consequently, if we assume our applicative system  includes the elements (combinators) S and K, and we can prove our claim, then every defining equation you have seen that has defined one of the combinators you were introduced to, specifies a combinator that can be derived only from S and K, so that the only constants we need in our system to obtain all the combinators you have seen (and infinitely many more) are S and K.
But why should a specification of the ordered arguments of a combinator and a term built only from variables included in those arguments be enough to specify any combinator? Well, first of all, as you have just seen, such a specification includes an infinite number of combinators, including all you have studied, as well as all those that can be defined from any term built from variables, and an ordered list of argument variables that includes the variables of the term. The claim that these are “all the combinators” essentially says that the essence of combinators is hidden in those terms built from variables and the argument list for the combinator. Here’s one way to think about that: Since applicative systems are to be imagined as being concerned with the order of “applications” of some sort of things to other things of the same sort (say, applications involving functions, algorithms, processes, etc.; you will see some quite varied interpretations of applications in the next chapter), the structure of parentheses in complex terms involving variables can be seen as defining an order on the applications of whatever is filled in for the variables at some point in time. For instance, consider the order of applications defined by the parentheses of (((rs)((tu)v))((wx)y)). [It certainly has nothing to do with the alphabetical order of the names of the variables.] In any case, you can see that, if we can prove what we have claimed, then in any applicative system including at least S and K as constants, there is a combinator A for which

as well as a (cancellative) combinator A′ for which

Combinatorial Completeness
Now we wish to prove our principal claim, namely that given any ordered list of variables to be taken as arguments for a combinator, and any term t built only from variables included in the list of arguments, there is a combinator with the given arguments (in the order specified in the list) that is derivable from just S and K (no variables) which realizes the term t.
To begin with, the identity combinator I is derivable from S and K: just take I to be SKK, for SKKx = Kx(Kx) = x = Ix. Thus it suffices to show that every combinator is derivable from S, K and I. This is what we will now do.
Let us first mention the following induction principle, which we will call the term induction principle: Consider a set S whose members are variables or constants or both, and let S+ be the set of all terms built from elements of S (and parentheses). To show that all terms in S+ have a certain property P, it suffices to show that all elements of S have the property, and that for any terms t1 and t2 of S+, if t1 and t2 have property P, then so does (t1t2). This principle is as self-evident as mathematical induction, and can in fact be derived from mathematical induction on the number n of occurrences of elements of S in the term.
Let us call  a simple term if it is built only from variables and S, K, and I. In the algorithm we are about to present, we will assume we will be starting with a simple term W built only from variables, but we will successively turn it into a simple term built from S, K, and I and variables. In the end, we will have built a term  from  that is built only of S, K, and I (by having eliminated all the variables in a specified ordered list of arguments for a combinator , a list that includes all the variables in ). And if x1, …, xn is the list of variables specified as arguments for the combinator, it will be the case that x1 … xn =, where the  in question is the original one containing only variables (all of which are included among the x1, …, xn).
Now, consider any simple term  and consider a variable x, which may or may not be in . We shall call a term x an x-eliminate of  if the following three conditions hold:

(a)The variable x does not occur in x.
(b)All variables in x are in .
(c)The equation  = holds.

We wish to first show that every simple term  has an x-eliminate x that is derivable from S, K and I and variables in  excluding the variable x. We define x by the following inductive rules:

(1)If  is x itself, we take x to be I.
(2)If  is any term in which x does not occur, we take x to be K.
(3)If  is a compound term  in which x occurs, we take ()x to be .

Note that for any possible simple term, one and only one of the three rules will apply. When rule (3) is applied, it will require finding the x-eliminates of the two parts of the compound term.
When we called the scheme of rules for x-elimination we just introduced inductive, we meant that if we have a simple term from which we wish to eliminate the variable x we operate repeatedly on the components of our term working from outside inwards looking for x-eliminates of the components of the term, until we have eliminated x from all the subterms and finally the whole term.
Let us consider an example. Let’s see if we can derive from S, K and I a combinator A of one argument x with the defining equation Ax = (xx). (Thus we are checking whether the mocker/mockingbird is derivable from S, K and I.) We want to see if eliminating x, the only variable in the term (and the only variable in the list of required arguments for the combinator) will give us a term in S, K and I. Well, (xx) is a compound term in which x occurs, so rule (3) tell us that the x-eliminate of (xx) consists of the starling S placed in front of the x-eliminates of the two parts of the compound term, which are both x. By rule (1), the x-eliminate of x is I. So the x-eliminate of (xx) is SII. Let us check that the three conditions which an x-eliminate must satisfy actually hold here: (a) The variable x does not occur in SII; (b) All variables in SII are in (xx) [this holds vacuously, since there are no variables in SII]; (c) SIIx = Ix(Ix) = xx. (Of course xx is just (xx) with its outer parentheses suppressed for easier reading.)
Now let us prove that for any simple term, the method of x-eliminates we have described does result in a term that satisfies the conditions on an x-eliminate.
Proposition 1.  is an x-eliminate of . In other words:
(1) The variable x does not occur in .
(2) All variables of  occur in .
(3) The equation x =  holds.
This proposition is proved by term induction.
Problem 2. (i) Show that if  is x, then  is an x-eliminate of .
(ii) Show that if  is any term in which x does not occur, then  is an x-eliminate of .
(iii) Show that if  and  are x-eliminates of  and  respectively, so is 
It then follows by term induction that the term Wx is an x-eliminate of W for every simple term .
Problem 3. Since I is an x-eliminate of x, it follows that an x-eliminate of  is SI. However, if x does not occur in , there is a much simpler x-eliminate of , namely  itself! Prove this.
Now the reader might already realize that if the term t from which we wish to “eliminate” variables contains a number of variables, we must successively “eliminate” each of the variables that occur in the term, if we wish to arrive at a variable-free term (i.e. the desired combinator). Indeed, we must also “eliminate” all the variables that do not occur in our original term, but which occur in the ordered list of argument variables for the combinator we are seeking. Moreover, we must do our variable eliminations in a particular order. To that process we now turn.
For two variables x and y, by  is meant .
We shall call a term  an x, y eliminate of  if the following three conditions hold:
(1) Neither of the variables x, y occurs in .
(2) All variables of  occur in .
(3) The equation  holds.
Problem 4. Which, if either, of the following statements is true?
(1)  is an x, y eliminate of .
(2)  is an y, x eliminate of .
We now take  as an abbreviation of .
We define a term  to be an x1, x2, …, xn eliminate of  if the following three conditions hold:
(1) None of the variables x1, x2, …, xn occur in .
(2) All the variables in  occur in .
(3) The equation  holds.
From the result of Problem 4, and mathematical induction, we have:
Proposition 2.  is an x1, x2, …, xn eliminate of .
Now let  be a term built only from variables among x1, x2, …, xn. Suppose we want a combinator A with arguments x1x2 … xn derivable from only S, K and I (using no variables) and satisfying the condition Ax1 … xn = . Well, we take A to be , which is an x1, x2, …, xn eliminate of .
All variables of A occur in , hence are among x1, x2, …, xn, yet none of the variables x1, x2, …, xn are in A, which means that A contains no variables at all. Since A has been formed by a variable-elimination scheme which introduces only the combinators S, K, and I, A must be built only from S, K, and I. And the condition Ax1x2 … xn =  holds.
Now we define a class of combinators to be combinatorially complete when, for every simple term t and every list of possible arguments x1, x2, …, xn for a combinator, if every variable in the term t occurs in x1, x2, …, xn, there is a combinator A in the class such that Ax1x2 … xn = t (i.e. when, for any such term t, there is a combinator A with arguments x1, x2, …, xn that realizes t).
Thus we have proved:
Theorem. The class of combinators derivable from S and K is combinatorially complete.
We would like to point out that, given a term t in S, K and I (that is, including terms in which variables are liberally strewn throughout) and an ordered list of variables x1, x2, …, xn including all the the variables in t, the method of eliminates described here can also be used, if one wishes, to transform any such term t into an expression Wx1x2 … xn in which W is a combinator built only with S, K and I. (To see this one only has to look once more through the method and the proof that the method works to see that it applies as well in this situation.)
Now let us consider an example of how to find a cancellative combinator with more than one variable using the method of eliminates: Suppose we want a combinator A satisfying the condition Axy = y. Thus A has the two arguments x, y and the term we want to use to define A does not include x. Well, we want A to be a y, x eliminate of y, i.e. an x-eliminate of a y-eliminate of y. Thus we must first find a y-eliminate of y, and that is simply I. We then want an x-eliminate of I, which is KI. Thus our solution should be KI. Let us check: KIx = I, so that KIxy = Iy = y.
Now for a permuting combinator: How can we derive the thrush T from S, K and I? [Txy = yx]. We have to find an x-eliminate of a y-eliminate of yx. Well, a y-eliminate of y is I, and a y-eliminate of x is Kx, and so a y-eliminate of yx is SI(Kx). We now need an x-eliminate of SI(Kx) = (SI)(Kx). Well, an x-eliminate of SI is K(SI), and an x-eliminate of Kx is simply K (by Problem 3), and so an x-eliminate of SI(Kx) is S(K(SI))K. Thus S(K(SI))K should be our solution. Let us check:

Then S(K(SI))Kxy = SI(Kx)y = Iy(Kxy) = y(Kxy) = yx.
It would be a good exercise for the reader to try to derive various combinators for M, W, L, B, C from S, K and I, using the procedure of eliminates. The procedure can be easily programed for a computer. However, it should be pointed out that the procedure, surefire as it is, can be very tedious and often leads to much longer expressions than can be found by using some cleverness and ingenuity.
There is an interesting table on Internet called “Combinator Birds” that gives an expression in S and K alone (no I) for all the combinators you have encountered in this book and more. This table can currently be found at angelfire.com/tx4/cus/combinator/birds.html. For instance, there you will see how exceedingly long are the derivations from S and K alone of the finch and the hummingbird. In the first column of the table it is immediately after the Greek letter λ (and before the period) that the creators of the table list the arguments of the combinator in question, since those arguments cannot be determined simply from the constant-free term used to define the combinator. For the goldfinch, the first column entry in the goldfinch row, λabcd.ad(bc), tells us that the combinator referred to in this row of the table has the arguments a, b, c and d, and the defining equation is Gabcd = ad(bc). The next two column entries for the row tells us that the common symbol for this combinator is G and that it is commonly called the goldfinch. The next column entry on the row tells us that this combinator can be derived from B, C and W as BBC. And then the final column entry for the goldfinch row tells us the goldfinch can be derived from S and K alone by the term

A Fixed Point Problem
There are two results of combinatory logic known as the First and Second Fixed Point Theorems. The second one will be dealt with in the next chapter. The first will be considered now. To motivate this, the reader should try the following exercise:
Exercise. (a) Find a combinator Γ that satisfies the condition Γxy = x(Γy). (b) Find a combinator Γ that satisfies the condition Γxy = Γyx.
The solutions of (a) and (b) are special cases of the First Fixed Point Theorem.
In what follows t(x, x1, …, xn) is a term in which x, x1, …, xn are precisely the variables that occur in the term. Recall that all realizable terms are built up only from variables (and parentheses).
Theorem F1. [First Fixed Point Theorem] For any realizable term t(x, x1, …, xn) there is a combinator Γ satisfying the condition Γx1 … xn = t(Γ, x1, …, xn).
Remarks. This theorem may be a bit startling, since the satisfying condition for Γ involves Γ itself! This is related to self-reference or recursion, as will be seen in the next chapter.
Problem 5.

(1)Prove Theorem F1. [There are two different proofs of this. One involves a fixed point of a combinator that realizes the term t(x, x1, …, xn). The other uses a combinator that realizes the term t((xx), x1, …, xn).]
(2)Now find solutions to (a) and (b) of the exercise stated before Theorem F1.

Doubling Up
Theorem F1 has the following double analogue:
Theorem F1F1. [First Double Fixed Point Theorem] For any two realizable terms t1(x, x1, …, xn) and t2(x, x1, …, xn) there are combinators Γ1, Γ2 such that:
(1) Γ1x1 … xn = t1(Γ2, x1, …, xn).
(2) Γ2x1 … xn = t2(Γ1, x1, …, xn).
Problem 6. Prove Theorem F1F1.
II.Partial Systems of Combinatory Logic
λ-I Combinators
We will now consider combinators A defined by some condition Ax1 … xn = t, where t is a term all of whose variables are among the variables x1 … xn. We recall that if all of the variables x1 … xn occur in t, then A is called a λ-I combinator, or a non-cancellative combinator, while all other combinators (i.e. those for which one or more of the variables x1 … xn do not occur in t) are called cancellative combinators (e.g. K and KI), since they cancel out those of the variables x1 … xn that do not occur in t.
For any class  of combinators, a set  of elements of  is called a basis for  if all elements of  are derivable from elements of S. We have already shown that the class of all combinators has a finite basis, namely the two elements S and K. We now wish to show that the four combinators S, B, C and I form a basis for the class of all λ-I combinators (and hence that B, M, T and I also form a basis, since S, B, C and I are themselves derivable from B, M, T and I).
To this end, let us define a nice term to be a term built from the symbols S, B, C and I and variables. If a nice term has no variables, then it is a combinator, and will accordingly be called a nice combinator. Thus a nice combinator is a combinator built only from S, B, C and I.
We define an x-eliminate of a term as before and we must now show that if  is any nice term, then  has a nice x-eliminate, provided that x actually occurs in ! Well, for this purpose we must redefine our set of recursive rules to produce the appropriate x-eliminate .

1.If  = x, we take  to be I. [Thus xx = I.]
2.For a compound term  in which x occurs, it occurs either in  or in  or in both.
(a)If x occurs both in  and , we take  to be .
(b)If x occurs in  but not in , we take ( to be .
(c)If x occurs in  but not in , then:
(c1)if  consists of x alone, we take  to be . [Thus , if x does not occur in .]
(c2)if V ≠ x (but x occurs in  and not in ), we take  to be .

Note that this set of rules, unlike the previous set of rules for x-elimination, contains no rule for eliminating x when the original term you wish to eliminate x from is a term in which x doesn’t occur. But this causes no problem in our current non-cancellative combinator situation. If our original term contains no occurrence of x, we would have no reason to even start off on an x-elimination, because we only eliminate variables from the list of arguments for the desired combinator, and if the original term contains no x, there would be no x in the argument list, so we wouldn’t try to do x-elimination at all. So we are only concerned with doing an x-elimination either on an x standing alone, which is covered by Rule (1), or on a compound term  in which x occurs in either  or  or both. If we do find after eliminating x from such a compound term (and we are told by the rules as to what to do when x doesn’t occur in one of the terms) that there are no longer any x’s in the compound term that result, we just know that we have the final x-eliminate for the term (or subterm) we are working on.
Proposition 3. For any nice term  in which x occurs, the term  is a nice x-eliminate of .
Problem 7. Prove Proposition 3.
We leave it to the reader to show that for a nice term , if variables x and y occur in , then  is a nice y, x eliminate of , and more generally that if x1, …, xn occur in , then  is a nice x1, …, xn eliminate of , and so if x1, …, xn are all the variables of , then  is a nice combinator A satisfying the condition . Since A is nice, then it is also a λ-I combinator (since S, B, C and I are all λ-I combinators, and for any two λ-I combinators A1A2 is again a λ-I combinator).
Notice that this time we are proving Proposition 3 for every term built up from S, B, C, I and variables (we said any nice term). I pointed out earlier we also could have done this for terms built from S and K and variables with the earlier method of eliminates and the same proofs. But since terms built up only from variables are also terms built up from S, B, C, I and possibly variables, we have the following immediate consequence of Proposition 3:
Corollary. For every constant-free term t and every list of possible arguments x1, x2, …, xn for a combinator, if the variables occurring in the term t are x1, x2, …, xn too, there is a combinator A in the class of variable-free terms built from S, B, C and I such that Ax1x2 … xn = t (i.e. there is a non-cancellative combinator A that realizes the term t).
This proves that S, B, C and I form a basis for the class of all λ-I combinators.
It is known that no proper subset of {S, B, C, I} is a basis for the class of λ-I combinators, but J. B. Rosser found a curious two-element basis for the class of λ-I combinators, namely I and a combinator J defined by the condition:

How Rosser found that is a mystery to me! If the reader is interested, a derivation of B, T and M from J and I can be found in To Mock a Mockingbird, and probably in other books on combinatory logic.
B, T, I Combinators
The class of all combinators derivable from B, T and I has been studied by Rosser [1936] in connection with certain logical systems in which duplicative combinators like M, W, S, J have no place. In To Mock a Mockingbird I showed that these three combinators can be replaced by only two, namely I and the combinator G (the goldfinch), which we recall is defined by the condition:

Whether this discovery was new or not, I honestly don’t know. Now let us derive B and T from G and I.
Problem 8.

(a)First derive from G and I the combinator Q3 (which, we recall, is defined as satisfying the condition Q3xyz = z(xy)).
(b)Then derive the cardinal C from G and I. [Cxyz = xzy.]
(c)From C and I, we can obtain T. [Alternatively, we can get T from Q3 and I].
(d)From C we can get the robin R [Rxyz = yzx], and then from C, R and Q3, we can get the queer combinator Q [Qxyz = y(xz)], and then from Q and C we can obtain B.

Solutions to the Problems of Chapter 11

1.t is a term if and only if there is a finite sequence t1, …, tn = t such that for each i ≤ n, either ti is a variable or constant, or there are numbers j1 and j2 both less than i such that ti = (tj1, tj2).
2.We will show that no matter which rule applies to our simple term , then  will be an x-eliminate of .
(i) Let us first consider the case that  is x itself. Then, by rule (1), . We must thus show that I is an x-eliminate of . Well, since there are no variables in the term I, then of course the variable x does not occur in I, and it is vacuously true that all variables in I (of course there are none) occur in . Also, Ix = x, and so I is an x-eliminate of .
(ii) Now we will show that if  is any term in which x does not occur, then, by rule (2)  is an x-eliminate of . Clearly, x does not occur in . Secondly, the variables of  are variables of , so that all variables of  are variables of . Thirdly, . Thus  is indeed an x-eliminate of .
(iii) Finally, we consider the composite term  in which x does occur, so that rule (3) applies. Suppose that  is an x-eliminate of , and that  is an x-eliminate of . We must show that  is an x-eliminate of . Let us check the three conditions to see if  is an x-eliminate of :


(a)Since x does not occur in  (since  is an x-eliminate of ) and x does not occur in ), and x does not occur in S, it follows that x does not occur in .
(b)Every variable y in  must occur in , because y either occurs in  or  or both, which means it occurs in  or  or both. So it occurs in the compound term .
(c)Lastly, we must show that . Well,


Since  and , then . Thus .
By (a), (b), and (c),  is an x-eliminate of  (assuming  is an x-eliminate of  and assuming  is an x-eliminate of ).

3. (a) The variable x does not occur in  (by hypothesis).
(b)All variables of  obviously occur in .
(c)Obviously .
4. It is clear that the statement (2) is true. Here is why:
(a)The variable y does not occur in . The variable x does not occur in , hence not in (, since all the variables in  occur in . Thus neither x nor y occurs in .
(b)All variables of  occur in , and all variables in  occur in . Hence all variables of  occur in .
(c)
By (a), (b), and (c),  is an y, x eliminate of .
5. For the first proof, let A be a combinator that realizes the term t. Thus


We let Γ be a fixed point of A, so that

Thus Γx1 … xn = t(Γ, x1, …, xn).
For the second proof, suppose A realizes the term t(xx, x1, …, xn), i.e. for all x, x1, …, xn

If we take A for x, we obtain

We thus take AA to be Γ!
Note to the reader: I believe that virtually all fixed points and recursion theorems in recursion theory and in combinatory logic, as well as the construction of Gödel self-referential sentences, are simply elaborate variations on this trick of the above second proof. In all cases, one has an equation containing Ax of the left side of an equal sign, and xx on the right side. Taking A for x, we end up with AA on both sides.
Now here is the solution of the Exercise:

(a)Since Qzxy = x(zy), we take Γ to be a fixed point of Q, so that

Thus Γxy = x(Γy).
(b)Since Czxy = zyx, we take Γ to be a fixed point of C, so that

Thus Γxy = Γyx.
6.Again, there are two different ways of proving this, and both are of interest.
For the first proof, we recall that by a cross point of a pair of compatible points (A, B) is meant a pair of elements (x, y) such that Ax = y and By = x. Since we are working with a complete system, the hypothesis of Problem 6, Chapter 8 (that conditions C1 and C2 both hold) is met, so that every pair of terms has a cross point (in the language of that problem, any two combinators are compatible).
Now let A1 realize t1 and let A2 realize t2. Thus
(3)A1xx1 … xn = t1(x, x1, …, xn).
(4)A2xx1 … xn = t2(x, x1, …, xn).
We now let (Γ1, Γ2) be a cross point of (A2, A1) [not of (A1, A2) but of (A2, A1)]. Thus, A2Γ1 = Γ2 and A1Γ2 = Γ1. Then
(1′)Γ1x1 … xn = A1Γ2x1 … xn = t1(Γ2, x1, …, xn).
(2′)Γ2x1 … xn = A2Γ1x1 … xn = t2(Γ1, x1, …, xn).
For the second proof, given terms t1(x, x1, …, xn) and t2(x, x1, …, xn), let A1 and A2 be combinators satisfying the following conditions:
(1)A1xyx1 … xn = t1(xyx, x1, …, xn).
(2)A2xyx1 … xn = t2(xyx, x1, …, xn).
Then,
(1′)A1A2A1x1 … xn = t1(A2A1A2, x1, …, xn).
(2′)A2A1A2x1 … xn = t2(A1A2A1, x1, …, xn).
We thus take Γ1 = A1A2A1 and Γ2 = A2A1A2.
7.We are given that  is a nice term and that x occurs in . At each stage of the construction of , no new constant was introduced other than S, B, C or I, so that  is certainly nice.
We leave it to the reader, using term induction, to prove that x does not occur in  and that all variables in  occur in . It remains to show that . We do this by induction.
1If  = x, then  is I and . So . It remains to show the statement for when we have a compound term .
2(a)Suppose that x is in both  and , and that  and . We are to show that . Well,

so that

2(b)For the case when x occurs in  but not in , we assume by the induction hypothesis that . Now, in this case,

so that

2(c)Now for the case that x is in  but not in .
(c1)Suppose  = x. Then , so that

(c2)In the case that  (and when x does not occur in ), we assume that  and we must show that . Well, in this case , so that

This concludes the proof.
8.(a) We take Q3 to be GI. Then GIxyz = Iz(xy) = z(xy) = Q3xyz.
(b)We take C to be GGII. Then GGIIx = Gx(II) = GxI, so that

(c)We can take T to be CI. Then CIxy = Iyx = yx = Txy. Alternatively, we can take T to be Q3I. Then

(d)We recall that CC is the robin R, since

We now take Q to be GRQ3. Then

Finally, we take B to be CQ. Then









Chapter 12
Combinators, Recursion and the Undecidable
In this chapter, we work with the complete system: all combinators derivable from S and K. We shall now see how combinatory logic is related to propositional logic, recursion theory, and the undecidable.
Logical Combinators
We now want two combinators t and f to stand for truth and falsity respectively. Several workable choices are possible, and the one we take is due to Henk Barendregt [1985]. We take t to be the kestrel K, and for f we take the combinator KI, so that in what follows t is an abbreviation for K, while f is an abbreviation for KI. We shall call t and f propositional combinators. We note that for any combinators x and y, whether propositional combinators or not, txy = x and fxy = y, which turns out to be a technical advantage.
There are only two propositional combinators t and f (i.e. K and KI). We now use the letters p, q, r, s as standing for arbitrary propositional combinators, rather than propositions, and we call p true if p = t, and false if p = f. We define the negation ∼ p of p by the usual conditions: ∼t = f and ∼f = t; we define the conjunction p ∧ q by the truth-table conditions: t ∧ t = t, t ∧ f = f, f ∧ t = f, f ∧ f = f. We define the disjunction p ∨ q, the conditional p ⊃ q, and the bi-conditional p ≡ q similarly. We now want combinators that can realize these operations.
Problem 1. Find combinators N (negation combinator), c (conjunction combinator), d (disjunction combinator), i (implication combinator), and e (equivalence or bi-conditional combinator), satisfying the following conditions:

(a)Np = ∼ p,
(b)cpq = p ∧ q,
(c)dpq = p ∨ q,
(d)ipq = p ⊃ q,
(e)epq = p ≡ q.

We now see that by combining the above combinators in various ways, we have combinators that can do propositional logic, in the sense that they can compute compound truth tables.
Arithmetic Combinators
We shall now see how the arithmetic of the natural numbers can be embedded in combinatory logic.
Each natural number is represented by a combinator denoted  (not to be confused with the Peano numeral designating n). There are several schemes of doing this, one of which is due to Church [1941]. The scheme we will use is again due to Henk Barendregt [1985]. Here the vireo V plays a major role [Vxyz = zxy]. We let σ (read as “sigma”) be the successor combinator Vf (which is V(KI)). We take to  be the identity combinator I, take to be ; take to be ,… take …. The first thing that needs to be shown is that the combinators, , are all distinct.
Problem 2. Show that if m ≠ n, then . [Hint: First show that for every , where n+ is an abbreviation for n + 1. Then show that if , then . Finally, show, for all m and n and all positive k, that 
The combinators  are called numerical combinators. A combinator A is called an arithmetical combinator of type 1 if, for every n, there is some m such that . We call A an arithmetical combinator of type 2 if, for every n and m, there is some number p such that . In general, A is called an arithmetical combinator of type n if, for all numbers x1, …, xn, there is a number y such that .
We now wish to show that there is an addition combinator, which we will denote ⊗, such that , and a multiplication combinator, which we will denote ⊗, such that  and an exponential combinator  such that .
We need some items in preparation for all this:
For any positive number n, by its predecessor is meant the number n − 1. Thus, for any number n, the predecessor of n+ is n. We now need a combinator P that calculates predecessors, in the sense that  for all n.
Problem 3. Find such a combinator P.
The Zero Tester
We have vital need of a combinator Z, called the zero tester, such that  and  for any positive n.
Problem 4. Find such a combinator Z.
Problem 5. Does there exist a combinator A such that , but , for  positive?
We now consider some recursion properties.
Problem 6. Prove that for any combinator A and any number k, there is a combinator Γ satisfying the following two conditions:

Hints:

1.C2 is equivalent to the following condition: For any positive n,

2.Use the zero tester Z as in Problem 5.
3.Use the fixed point theorem.

Problem 7. Now show that there are combinators ⊗, ⊗ and  satisfying the following conditions:

(1)
(2)
(3)

The Recursion Property for Combinatory Logic, in its most general form, is that for any terms f(y1, …, yn) and g(x, z, y1, …, yn), there is a combinator Γ such that for all numerals, the following two conditions hold:

(1)
(2)

A combinator Γ that does this exists by the fixed point theorem, and is defined by the condition:

This brilliant idea was due to Alan Turing!
I.Preparation for the Finale
Property Combinators
By a property combinator is meant a combinator such that for every number n,  or .
A set A of (natural) numbers is said to be (combinatorially) computable if there is a property combinator Γ such that  for every n in A, and  for every n that is not in A. Such a combinator is said to compute A.
We might remark that the important thing about combinatorial computability is that a set A is combinatorially computable if and only if it is recursive.
Problem 8. Show that the set E of even numbers is computable. [Hint: The property of being even is the one and only property satisfying the following conditions: (1) 0 is even; (2) for positive n, n is even if and only if its predecessor is not even. Now use the fixed point theorem.]
Problem 9. Suppose Γ computes A. Does it follow that NΓ computes the complement  of A? [N is the negation combinator Vft.]
Relational Combinators
By a relational combinator of degree n is meant a combinator A such that for all numbers k1, …, kn, either  or , and such a combinator A is said to compute the set of all n-tuples (k1, …, kn) such that . Thus for any relation R(x1, …, xn), we say that A computes R to mean that for all n-tuples (k1, …, kn), if R(k1, …, kn) holds, then , and if R(k1, …, kn) doesn’t hold, then .
Problem 10. Show that there is a relational combinator g that computes the relations x > y (x is greater than y). [Hint: This relation is uniquely determined by the following conditions:

1.If x = 0, then x > y is false.
2.If x ≠ 0 and y = 0, then x > y is true.
If x ≠ 0 and y ≠ 0, then x > y is true if and only if x − 1 > y − 1 is true.]

Functional Combinators
Consider a function f(x) of one argument, i.e. an operation that assigns to each number n a number devoted by f(n). A combinator A will be said to realize the function f(x) if, for every number n, the condition  holds.
Problem 11. Show that if functions f(x) and g(x) are both realizable, so is the function f(g(x)), i.e. the operation that assigns to each number n the number f(g(n)).
Problem 12. Suppose R(x, y) is computable and that f(x) and g(x) are both realizable. Show that the following are computable:

(a)The relation R(f(x), y).
(b)The relation R(x, g(y)).
(c)The relation R(f(x), g(y)).

The Minimization Principle
Consider a relational combinator A of degree 2 such that for every number n, there is at least one number m such that . Such a combinator is sometimes called regular. If A is regular, then, for every n, there must be a smallest number k such that . The minimization principle is that for every regular combinator A, there is a combinator A′, called a minimizer of A, such that for every number , where k is the smallest number for which 
We seek to prove the minimization principle. The following preliminary problem will be most helpful:
Problem 13. Show that for any regular relational combinator A of degree 2, there is a combinator A1 such that for all numbers n and m,

(1)If , then .
(2)If , then .

Problem 14. Now prove the Minimization Principle: for every regular relational combinator A of degree 2, there is a combinator A′, called a minimizer of A, such that for every number n, , where k is the smallest number for which . [Hint: Use the A1 of Problem 13 and the cardinal C [Cxyz = xzy].]
The Length Measurer
By the length of a number n we shall mean the number of digits in n when n is in ordinary base 10 notation. The numbers from 0 to 9 have length 1; those from 10 to 99 have length 2, those from 100 to 999 have length 3, etc. Actually the length of n is the smallest number k such that 10k > n.
We now want a combinator L′ that measures the length of any number, that is, we want such that , where k is the length of n.
Problem 15. Prove that there is a length measurer L′.
Concatenation to the Base 10
For any numbers n and m, by n * m we shall now mean the number which, when written in ordinary base 10 notation, consists of n (in base 10 notation) followed by m (in base 10 notation). For example

We now want a combinator  that realizes this operation of concatenation *.
Problem 16. Show that there is a combinator  such that for every n and m, . [Hint: x * y = (x × 10k) + y, where k is the length of y.
For example,

and 3 is the length of 587.]
II.The Grand Problem
By an S, K term we shall mean a term without variables, built from just the constants S and K. We use letters X, Y, Z to stand for unspecified S, K terms. In what follows “term” will mean S, K term.
Of course, distinct terms might designate the same combinator. For example, KKK designates the same combinator as KKI. For both designate the kestrel K, or as we say the equation KKK = KKI holds, or is true. An equation is true if and only if its being so is a consequence of the given conditions (which can be called the axioms for combinatory logic) — SXYZ = XZ(Y Z) and KXY = X — and by using the inference rules of identity, namely X = X and if X = Y, then XZ = Y Z and ZX = ZY ; also, the additional rules of equality: if X = Y then Y = X; if X = Y and Y = Z then X = Z.
The grand question now is this: Given two terms X and Y, is there a systematic way of deciding whether or not the equation X = Y holds? This can be reduced to a question of deciding whether a given number belongs to a certain set of numbers. This translation uses the device of Gödel numbering. How this translation is effected is the first thing we shall now consider.
All sentences (frequently called equations here) are built from the following five symbols:

In this section all such sentences will be equations of the form

Under each symbol I have written its Gödel number. The Gödel number of a compound expression (a complex term, or a sentence) is the number obtained by replacing S by 1, K by 2, …, = by 5, and reading the result as a number in base 10. For example, the Gödel number of KS(= is 2135).
We let  be the set of true sentences, and let  be the set of the Gödel numbers of the true sentences. The question now is whether the set  is combinatorially computable. Is there a combinator Γ such that for every  if n ∊ , and  if n ∉ ?
The question is of staggering importance, since any formal mathematical question can be reduced to whether a certain number belongs to . The question is thus equivalent to the question of whether there can be a universal computer that can settle all formal mathematical questions.
As the reader has doubtless guessed, the answer is no, which we now set out to prove.
Gödel Numerals
Let me first point out that there are two different schemes for constructing terms, which involve two different ways of introducing parentheses. One scheme consists of building terms according to the following two rules:

(1)S and K are terms.
(2)If X and Y are terms, so is (XY).

The second scheme replaces (2) by:

(2′)If X and Y are terms, so is (X)(Y).

We shall continue to use the first scheme.
Now, for any number n, the numeral , like any other term, has a Gödel number. We let n# be the Gödel number of the numeral . For example,  is the combinator I, which in terms of S and K is ((SK)K), whose Gödel number is 3312424. Thus 0# = 3312424.
As for 1#, it is the Gödel number of the numeral , and , where σ is the combinator Vf, and Vf, when reduced to S and K, is a horribly long expression, and consequently has an unpleasantly long Gödel number, which I will abbreviate by the letter “s”. Thus, in what follows, s is the Gödel number of (the term whose abbreviation is) σ, and so 1# is the Gödel number of , which is 3 * s * 0# * 4.
Next, 2# uses the Gödel number of , and the Gödel number of that term is 3 *s * 1# * 4. Similarly 3# = 3 * s * 2# * 4. And so on ….
We now need to show that the function f(n) = n# is combinatorially realizable, i.e. that there is a combinator δ such that  holds (for every n).
Problem 17. Show that there is such a combinator δ.
Normalization
For any expression X, by ⌈X⌉ is meant the numeral that designates the Gödel number of X; we might call ⌈X⌉ the Gödel numeral of X. By the norm of X is meant X⌈X⌉, i.e. X followed by the Gödel numeral of X. If n is the Gödel number of X, then n# is the Gödel number of ⌈X⌉, so n * n# is the Gödel number of X⌈X⌉, i.e. of the norm of X.
We now want a combinator Δ, called a normalizer, such that  holds, for all n.
Problem 18. Exhibit such a combinator Δ.
Problem 19. Which, if either, of the following statements is true?

(a)Δ⌈X⌉ = X⌈X⌉
(b)Δ⌈X⌉ = ⌈X⌈X⌉⌉

The normalizer can do some amazing things, as you will see!
The Second Fixed Point Principle
We say that a term X designates a number n if the equation  holds. Obviously one term that designates n is the numeral , but there are many others. For example, for n = 8, the terms  all designate 8. Indeed there are infinitely many terms that designate , e.g. , etc.
We call a term an arithmetic term if it designates some number. Every numeral is an arithmetic term, but not every arithmetic term is a numeral.
It is impossible for any numeral to designate its own Gödel number, because the Gödel number of  is larger than n (n# > n). However, this does not mean that no arithmetic term can designate its own Gödel number. In fact, there is an arithmetic term that designates its own Gödel number! Also there is one that designates twice its Gödel number; one that designates eight times its Gödel number plus 15 — indeed, for any realizable function f(x), there is an arithmetic term X which designates f(n), where n is the Gödel number of X! This is immediate from the second fixed point principle, which is that for any combinator A, there is a term X such that A⌈X⌉ = X holds. We now wish to prove this principle.
Problem 20. Prove the Second Fixed Point Theorem: show that for any combinator A, there is a term X such that A⌈X⌉ = X holds.
Problem 21. Prove that for any realizable function f(x) there is an (arithmetic term) X that designates f(n), where n is the Gödel number of X. [Hint: Use the second fixed point theorem.]
Representability
We shall say that a combinator Γ represents a number set A if for every number n, the equation  holds iff n ∊ A. Thus Γ represents the set of all n such that  holds. (Note that the combinatorial computability of a number set A by a combinator Γ that we worked with earlier requires more than representability, for in the case of combinatorial computability if n ∉ A then it must be the case that , while for representability, it is only necessary that .)
Problem 22. Suppose Γ computes A. Which, if either, of the following statements are true?

(1)A is representable.
(2)The complement  of A is representable.

Recall that for any function f(x) and set A, by f−1(A) is meant the set of all numbers n such that f(n) ∊ A. Thus n ∊ f−1(A) iff f(n) ∊ A.
Problem 23. Prove that if the function f(x) is realizable, and if A is representable, then f−1(A) is representable.
Gödel Sentences
We shall call a sentence (equation) X a Gödel sentence for a number set A if X is true if and only if the Gödel number of X is in A. We now aim to prove that if A is representable, then there is a Gödel sentence for A.
Problem 24. Suppose A is representable. Prove that the set of all n such that (n * 52) ∈ A is representable. [The significance of n * 52 is that if n is the Gödel number of X, then n * 52 is the Gödel number of X = t.]
Problem 25. Now prove that for any representable set A, there is a Gödel sentence for A.
Now we have all the key pieces to prove that the set  of Gödel numbers of the true sentences is not computable.
Problem 26. Prove that  is not computable.
Discussion
We have just seen that the complement of the set  is not representable. What about the set ? Is it representable? Yes, it is. As suggested earlier, combinatory logic can be formalized. The following is an axiom system for the full combinatory logic:
Symbols: S, K, (,), =
Terms: As previously defined recursively
Sentences (or Equations): Expressions of the form X = Y, where X and Y are terms.
Axiom Schemes: For any terms X, Y, Z:
(1)SXY Z = XZ(Y Z)
(2)KXY = X
(3)X = X.
Inference Rules
R 1 : From X = Y to infer Y = X.
R 2 : From X = Y to infer XZ = YZ.
R 3 : From X = Y to infer ZX = ZY.
R 4 : From X = Y and Y = Z to infer X = Z.
It is relatively easy to construct an elementary formal system in which one can successively represent the set of terms, sentences, and provable sentences. Thus the set  is formally representable, from which it easily follows that the set  is recursively enumerable. And it is well known that recursive enumerability is the same thing as representability in combinatory logic. Thus the set  is indeed representable.
The set  is another example of a recursively enumerable set that is not recursive.
As previously mentioned, any formal mathematical question can be reduced to a question of whether a certain number is in , that is, one can associate with each formal mathematical question a number n such that n ∊  iff the question has an affirmative answer. Since there is no purely mechanical method of determining which numbers are in , there is then no purely mechanical method of determining which formal mathematical statements are true and which are false. Mathematics requires intelligence and ingenuity. Any attempt to fully mechanize mathematics is doomed to failure. In the prophetic words of Emil Post [1944], “Mathematics is and must remain essentially creative.” Or, in the witty words of Paul Rosenbloom: “Man can never eliminate the necessity of using his own intelligence, regardless of how cleverly he tries.”
Solutions to the Problems of Chapter 12

1.(a) We need a negation combinator N satisfying the condition Nx = xft. Well, we can take N to be V ft, where V is the vireo [V xyz = zxy]. Thus Nx = V ftx = xft as desired. Then Nt = tft = f (since t = K, or because txy = x, for all x and y), and Nf = fft = t (since f = KI, or because fxy = y, for all x and y). Thus Np =∼ p, where p is either t or f.
(b)This time we want the conjunction combinator c to be such that cxy = xyf. We take c to be Rf, where R is the Robin [Rxyz = yzx]. Thus cxy = Rfxy = xyf, as desired. Then (using the fact that for all x and y, txy = x and fxy = f):
(1)ctt = ttf = t,
(2)ctf = tff = f,
(3)cft = ftf = f,
(4)cff = fff = f.
(c)Now we want the disjunction combinator d to be such that

For this purpose, we take d to be Tt, where T is the thrush [Txy = yx]. Then dxy = xty (verify!). And dpq = p ∨ q (verify all four cases!).
(d)Take the implication combinator i to be Rt, where R is the robin [Rxyz = yzx]. Then ixy = xyt, because ixy = Rtxy = xyt. Then ipq = p ⊃ q (verify!).
(e)We want the equivalence combinator e to be such that

For this we may take e to be CSN:

Then epq has the same values as p ≡ q. Verify, recalling that we have shown above that Nt = f, that Nf = t, and that, for all x and y, txy = x and fxy = f.
2.Suppose  for some n. Thus  Consequently,  Thus IK = KI, so that K = KI contrary to Problem 19 of Chapter 8.
Next, we must show that if  then  To do so, we instead show the equivalent fact that if  then .
Well, suppose that . Then  or  so that  Applying V here, we see that  which implies that  [again since it is always true that fxy = y]. This proves that if  then 
Now, for any positive k, we saw at the beginning of this solution that  After what we have just proved, we now see as well that it follows from  that  This proves that if m ≠ n, then  (because if m ≠ n then for some positive k, m = n + k or n = m + k).
3.Since  we want a combinator P such that  We take P to be Tf, where T is the thrush [Txy = yx]. Then

Voila!
4.We take Z to be Tt, where T is the thrush [Txy = yx]. Recall that  and σ = Vf.
(1) Thus 
(2) Thus 
5.Yes, the zero tester Z is such an A.

6.Conditions C1 and C2 are respectively equivalent to:

 if  for n positive
Now, by Problem 5,  is  if  and is A(Γ(Px)) if  for n positive. We thus want a combinator Γ which satisfies the condition:

Well, such a combinator Γ exists by the fixed point theorem. Specifically, we can take Γ to be a fixed point of a combinator θ satisfying the condition:  Then 
7.(1) The addition combinator ⊗ is uniquely determined by the following two conditions:
a. n + 0 = n.
b. n + m+ = (n + m)+.
We therefore want a combinator ⊗ such that:


We thus want ⊗ to be such that, for any n and m, whether 0 or positive, the following holds:
Such a combinator exists by the fixed point theorem.
(2)The multiplication operation × is uniquely determined by the following two conditions:
a. n × 0 = 0.
b. n × m + = (n × m) + n.
Thus the combinator ⊗ that we want, and which exists by the fixed point theorem, is defined by the condition:

(3)The exponential operation is uniquely determined by the following two conditions:
a. n0 = 1.

Thus the combinator  we want, and which exists by the fixed point theorem, is defined by the condition 
8.By virtue of conditions (1) and (2) of the hint, we want a combinator Γ to satisfy the condition:

Such a combinator can be found by the fixed point theorem.
9.No, it does not follow that NΓ computes . Suppose n ∈ A. Then  However, it does not follow that  what does follow is that  Thus it is not NΓ that computes  but rather BNΓ [Bxyz = x(yz)]. For  when  (i.e. when n ∈ A) and  when  (i.e. when n ∈ ).
10.We want a combinator g such that for all numbers x and y the following conditions hold:
(1) If  then 
(2) If  then
(a) If  then 
(b) If  then 
We thus want g to satisfy the following:

Such a g exists by the fixed point theorem.
11.Suppose A1 realizes f(x) and A2 realizes g(x). Then the functions f (g (x)) is realized by BA1A2, since

12.Let Γ compute the relation R(x, y), and let A1 realize f(x) and A2 realize g(x). Then:
(a)BΓA1 computes the relation R(f(x),y), because:

which is t if R(f(n),m) holds, and is f if R(f(n),m) doesn’t hold.
Thus BΓA1 computes R(f(x),y).
(b)This is trickier! A combinator that computes the relation R(x, g(y)) is BCDΓA2, where C is the cardinal [Cxyz = xzy] and D is the combinator defined by Dxyzw = xy (zw). We learned that D = BB.
Well,


which is t if R(n, g(m)) holds, and is f if R(n, g(m)) doesn’t hold. Thus BCDΓ A2 computes R(x, g(y)).
(c)Let S(x, y) be the relation R(x, g(y)), which is computed by BCDΓ A2. Then by (a) S(fx), y), which is R(f(x), g(y)) is computed by B(BCDΓ A2)A1.
Alternatively, let δ(x, y) be the relation R(f(x), y). It is computed by BΓ A1, according to (a). Then R(f(x), g(y)) is δ(x, g(y)), hence is computed by BCD(BΓA1)A2, according to (b).

13.Given a regular relational combinator A of degree 2, by the fixed point theorem, there is a combinator A1 satisfying the condition:



14.We take A′ to be CA1 , where C is the cardinal and A1 is the combinator of Problem 13. Then, for any number n, A′ Now let k be the smallest number such that  = t. We illustrate the proof for k = 3 (the reader should have no trouble in generalizing the proof for arbitrary k). Thus it must be the case that  = f; 

Now, . Since , then . Since , then . Since , then . Since , then . Thus . So .

15.Let R(x, y) be the relation 10y > x. We must first find a combinator A that computes the relation 10y > x.

We let R1(x, y) be the relation R(y, x), i.e. the relation 10x > y. If we find a combinator A1 that computes the relation R1 (x, y), then CA1 will compute the relation R(x, y) (why?).
To compute the relation 10x > y, we know that g computes the relation x > y and that  realizes the function 10x. Hence by Problem 11, the relation 10x > y is computed by Bg(). Thus C(Bg()) computes the relation 10y > x. We thus take the length measurer L′ to be a minimizer of C(Bg()).
16.Take 
17.Let A be a combinator satisfying the equation  [Specifically, we can take A to be  where B is the bluebird and C is the cardinal, as the reader can verify.] Then 
Then, by the Recursion Property, or more directly by Problem 6, there is a combinator δ such that:

By mathematical induction it follows that  by the following reasoning:

(b)Now suppose n is such that 
We must show that  Well, by (2),  And, of course,  by the induction hypothesis. Therefore,

And so

This completes the induction.
18.Take Δ to be S  where S is the starling [Sxyz = xz(yz)]. Then, recalling that  we have:

Note: In To Mock a Mockingbird, I took Δ to be W (DC), which also works, which should not be surprising, since for any x, y and z, Sxyz = W (DCxy)z, as the reader can verify.

19.It is (b) that is true. Let n be the Gödel number of X. Then  so that  and since  we have: 

Since n is the Gödel number of X, then n#, which we first defined to be the Gödel number of , must also be the Gödel number of  since  Thus n*n# is the Gödel number of X and so:
(2)
(c) By combining (1) and (2), we see (b), i.e. that 
20.We take X to be BA where Δ is the normalizer. By the definition of the bluebird B, we have BAΔ  = A(Δ Consequently,

Here is the reason for (2): By Problem 19,Δ  for any Y.
Hence

which is  which proves (2).
By (1) and (2), we have X =  and so X = 
21.Suppose the function f(x) is realizable, and let A realize f(x). By the second fixed point principle just proved, there is a term X such that X =  Let n be the Gödel number of X, so that  and therefore  since A realizes f(x), which means that X designates f(n), where n is the Gödel number of X.
22.Both (1) and (2) are true: Suppose Γ computes A. This means that for every n:


(1)We must show that A is representable. Well, it is Γ itself that represents A. To see this, it suffices to see that the converse of (a) holds, i.e. that if  then n ∈ A. So suppose Γn = t. If n were not in A, then by (b) we would have that  and hence that t = f, which cannot be. Thus n ∈ A.
(2)Now we must show that  is representable. Since Γ computes A, then BNΓ computes the complement  of A [by the solution of Problem 9]. Thus BNΓ represents [by (1)].

23.Suppose that Γ1 realizes f(x) and that Γ2 represents A. We let = BΓ2Γ1 [Bxyz = x(yz)], and we show that Γ represents f–1(A). Well,

Thus  which is true iff f(n) ∈ A [since Γ2 represents A], which is true iff n ∈ f–1(A). Thus  iff n ∈ f–1(A), which means that Γ represents f–1(A).
24.This is but a special case of Problem 23: Let f(x) = x*52. By Problem 16, f(x) is realizable by the combinator  If A is representable, so is f–1(A) [by Problem 23], and f–1(A) is the set of all n such that f(n) ∈ A, i.e. the set of all n such that (n*52) ∈ A.
25.Suppose A is representable. Let A′ be the set of all n such that the number n*52 is in A. Then A′ is representable [by Problem 24]. Let Γ represent the set A′. By the fixed point theorem, there is some X such that  We will show that the sentence X = t is a Gödel sentence for A.
Let n be the Gödel number of X. Then  Hence  and therefore  which is true iff n ∈ A′ [since represents A′]. And n ∈ A′ iff (n * 52) ∈ A. Thus  iff (n * 52) ∈ A. But n * 52 is the Gödel number of the sentence X = t. Thus the sentence X = t is a Gödel sentence for A.
26.There cannot be a Gödel sentence for the complement of  for such a sentence X would be true if and only if its Gödel number n was not in  which means that X would be true iff its Gödel number was not the Gödel number of a true sentence, and this is impossible. [Looked at it another way, every sentence is a Gödel sentence for the set  and no sentence can be a Gödel sentence for both a set A and its complement (why?). Hence no sentence can be a Gödel sentence for the complement of .]
Since there is no Gödel sentence for the complement of  then the complement of  is not representable [by Problem 25], and therefore the set  is not computable [by Problem 22].







AfterwordWhere to Go from Here
This book and its predecessor have only scratched the surface of the collection of topics subsumed under the name of “Mathematical Logic.” We have done only the beginnings of the fields of recursion theory and combinatory logic, and have done virtually nothing in model theory, proof theory, and other logics such as modal logic, intuitionistic logic, relevance logic, and others.
I strongly suggest that you next turn your attention to set theory and the continuum problem. Let me tell you a wee bit about this, hoping to whet your appetite.
One purpose of the subject known as axiomatic set theory is to develop all mathematics out of the notions of logic (the logical connectives and the quantifiers) together with the notion of an element being a member of a set of elements. We have used the symbol “∈” for “is a member of” and “x ∈ A” for “x is a member of the set A”.
A pioneer in the development of axiomatic set theory was Gottlob Frege [1893]. His system had, in addition to axioms of first-order logic, just one axiom of set theory, namely that for any property P, there is a unique set consisting of those and only those things that have the property P. Such a set is written {x : P(x)}, which is read “the set of all x’s having the property P.
This principle of Frege is sometimes referred to as the abstraction principle, or the unlimited abstraction principle. One can define the identity relation x = y in terms of set inclusion as “x and y belong to the same sets”  Frege’s abstraction principle has the marvelous advantage of allowing us to obtain just about all the sets necessary for mathematics, for example, the following:
P1: The empty set , which is{x :∼ (x = x)}.
P2: For any two elements a and b, the set {a, b} whose members are just a and b. This set is {x: x = a ν x = b}.
P3: For any set a, the power set  thus

P4: For any set a, the union ∪a, i.e. the set of all elements that are members of an element of a, thus ∪a = {x: ∃y(y ∈ a ∧ x ∈ y)}.
Despite the useful things Frege’s system can do, it has an extremely serious drawback: it is inconsistent! This was pointed out to Frege in a letter by Bertrand Russell [1902], who observed that according to Frege’s abstraction principle, there would exist the set A of all sets which are not members of themselves (A = {x: x ∉ x}). Thus, for any set x, one would have x ∈ A iff x ∉ x Taking A for x, we would have A ∈ A iff A ∉ A, which is a contradiction! Thus Frege’s system is inconsistent.
Frege was broken-hearted over Russell’s discovery, and felt that his whole life’s work had been in vain. Here he was wrong: His work was salvaged by Zermelo and others, and is really the basis for Zermelo–Fraenkel set theory (frequently abbreviated as ZF), which is one of the most significant systems of set theorem in existence, the other of similar rank being the system in Principia Mathematica of Whitehead and Russell [1910].
What Zermelo [1908] did was to replace Frege’s abstraction principle by the following, known as the limited abstraction principle, which is that for any property P of sets and any set a, there exists the set of all the elements of a having property P. Thus for any set a, there exists the set {x: P(x)∧ x ∈a}. This principle appears to be free of any possible contradiction. However, Zermelo had to take the existence of the sets  {a,b},  and ∪a as separate axioms. He also took an additional axiom known as the axiom of infinity, which we will discuss later.
On the basis of the Zermelo axioms on hand, one can now derive the positive integers. First, let us note that the set {a, b} is well-defined even if a and b are the same element, and then {a, a} is simply the set {a}. Well, Zermelo took 0 to be the empty set ; 1 to be the set whose only element is 0; 2 to be the set whose only element is 1, and so forth. Thus the Zermelo natural numbers are etc. Thus for each natural number n, n+1 is{n}.
Later, Von Neumann [1923] proceeded differently. In his scheme, each natural number is the set of all lesser natural numbers. Thus 0 is . and 1 is {} as with Zermelo, but 2 is now not {1}, but rather the set {0, 1}; 3 is the set {0, 1, 2}; …, n+1 is the set {0, 1, …, n}. This is the scheme now generally adopted, since it generalizes in the infinite case to the important sets called ordinals, which we will soon discuss. Note that in Zermelo’s scheme, each natural number other than 0 contains just one element, whereas in Von Neumann’s scheme, each number n contains exactly n elements.
Now that we have the natural numbers, how do we define what it means to be a natural number? That is, we want a formula N(x) using just the logical connectives and quantifiers and the single predicate symbol ∈ such that for any set a, the sentence N(a) is true if and only if a is a natural number. Well, for any set a, we define a+ as a ∪ {a}, i.e. the set whose elements are those of a, together with a itself. Thus

We now define a set a to be inductive if 0 ∈ a, and if, for all sets b, if b ∈ a, then b+ a. We then define a set x to be a natural number if it belongs to all inductive sets. The principle of mathematical induction follows immediately from the very definition of a natural number. Indeed, the five Peano Postulates follow from the definition of “natural number”.
However, we don’t yet have any guarantee that there is such a thing as the set of all natural numbers. Zermelo had to take this as an additional axiom, and this is known as the axiom of infinity. The set of all natural numbers is denoted “ω”.
Let us recall the Peano Postulates:
1.0 is a natural number.
2.If n is a natural number, so is n+.
3.There is no natural number such that n+ = 0.
4.If n+ = m+, then n = m.
5.[Principle of Mathematical Induction] Every inductive set contains all the natural numbers.
All these five postulates are easy consequences of the definition of natural number. The axiom of infinity is not necessary to prove the Peano Postulates! This fact appears to not be completely well known. I recall that when I once announced these facts at a lecture, a very eminent logician later told me that he was quite shocked!
Now for the ordinals: First we define a set x to be transitive if it contains with each of its elements y all elements of y as well; in other words, each element of x is a subset of x. We recall that for any set a, we are taking a+ to be the set a ∪ {a}, and a+ is called the successor of a. Roughly speaking, the ordinals are those sets, starting with , which can be obtained by taking the successor of any ordinal already obtained, and by taking any transitive set of ordinals already obtained. That is, we want to define ordinals such that 0 is an ordinal, every successor of an ordinal is an ordinal, and every transitive set of ordinals is an ordinal. One definition that works is this: Define x to be an ordinal if x is transitive and every transitive proper subset of x is a member of x. [We recall that by a proper subset of x is meant a subset of x that is not the whole of x.] It can be seen from this definition that is an ordinal, the successor of any ordinal is an ordinal, and any transitive set of ordinals is an ordinal.
It is obvious that every natural number is an ordinal (use mathematical induction). Also, the set ω of all natural numbers is transitive (verify!), and so ω is an ordinal. Once we have ω, we have the ordinal ω+, also denoted ω+1. Successively, we have ω+2 (ω++), ω+3, …, ω+n, …. However, we do not yet have a set which contains all these ordinals, and the existence of such a set cannot be derived from Zermelo’s axioms. But then Abraham Fraenkel [1958] added another axiom known as the axiom of substitution, also called the axiom of replacement, which does ensure the existence of such a set, and this set is denoted ω×2 or ω.2.
Roughly speaking, the axiom of replacement is that given any operation F which assigns to each set x a set F (x), and given any set a, there is a set [denoted F-1(a)] consisting of all, and only, those sets x such that F (x) ∈a.
Well, there is the set denoted ω . 2 of all elements ω+ n, where n is any natural number. Then we have the ordinals

After that finally come ω·4, ω·5, …, ω · ω, … ωω, …. We can keep going.
The ordinals so far considered are all denumerable. But there exist non-denumerable ordinals as well. Indeed, with the axiom of substitution/replacement (of Zermelo–Fraenkel set theory), for every set x, there is an ordinal α of the same size as x (i.e. α can be put into 1-1 correspondence with x).
An ordinal is called a successor ordinal if it is α+[α∪{α}] for some ordinal α. Ordinals other than 0 and successor ordinals are called limit ordinals. All natural numbers other than 0 are successor ordinals. The first limit ordinal is ω. The ordinals ω + 1, ω + 2, …, ω+n are all successor ordinals, and ω·2 (the set of all of them) is the next limit ordinal. The ordinals ω·3, ω·4, …, ω· n are all limit ordinals, and so is ω·ω. Actually, an infinite ordinal is a successor ordinal if and only if it is of the form +n, for some limit ordinal α and some positive integer n.
An ordinal α is said to be less than an ordinal β(in symbols, α < β) if α ∈ β. Thus each ordinal is the collection of all lesser ordinals.
Rank
Given a scheme that assigns to each ordinal α a set denoted S, for any ordinal α, by Uα<λ Sα is meant the union of all the sets S, where α < λ. By an important result known as the transfinite recursion theorem, one can assign to each ordinal α a set Rα such that for every ordinal α and every limit ordinal λ, the following three conditions hold:
(1)
(2)Rα+ =  [the set of all subsets of Rα].
(3)Rα = Uα<λ Rλ.
A set is said to have rank if it is a member of some Rα, and by the rank of such a set x is meant the least ordinal α such that x ∈ Rα.
Do all sets have rank? This is so iff the ’ relation is well founded, i.e. iff there exists no infinite sequence x1, x2, … xn, … such that for each n, the set xn+1 is a member of xn. This condition is taken as another axiom of set theory, and is known as the axiom of foundation. It rules out sets without rank.
The Axiom of Choice and the Generalized Continuum Hypothesis
The Axiom of Choice (frequently abbreviated AC) in one form is that for any non-empty set a of non-empty sets, there is a function C (a so-called choice function for a) that assigns to each member x of a an element of x. [The function, so to speak, chooses one element from each of the elements of a.]
An equivalent form of AC is that for any non-empty set a of non-empty sets, there is a set b whose members consist of just one member of each of the members of a.
What is the status of AC? Most working mathematicians of the world accept it as being true. There are a few, though, who don’t. What is its status with respect to ZF (Zermelo–Fraenkel Set Theory)? Well, Gödel [1940] showed that it is consistent with ZF (assuming that ZF is itself consistent, which we will continue to assume). Thus AC is not refutable in ZF. Sometime later, Paul Cohen [1963, 1964] proved that the negation of AC is consistent with ZF. Thus AC is undecidable in ZF.
We recall Cantor’s theorem, that for any set a, its power set  is larger than a. Cantor conjectured that for an infinite set there is no set b intermediate in size between a and  [that is, that there is no set b which is larger than a but smaller than ], and this result is known as the Generalized Continuum Hypothesis (frequently abbreviated as GCH). What is its status? This is far more puzzling! Unlike the case of the axiom of choice, which most logicians believe is true, most mathematicians, as well as most logicians specializing in set theory, don’t have the slightest idea as to whether GCH is true or false. Gödel conjectured that it is false, despite the fact that he proved it consistent with the axioms of ZF. His proof of the consistency of GCH with ZF is surely one of the most remarkable things in mathematical logic! Equally remarkable is the proof of Paul Cohen that the negation of GCH is consistent with ZF, even when AC is added to the axioms of ZF. Thus GCH is undecidable in ZF.
Gödel’s proof of the consistency of AC and GCH used his notion of constructible sets, to which we now turn.
Constructible Sets
Consider a sentence X whose constants are all in a set a. X is said to be true over a if it is true when the quantifiers are interpreted as ranging over all elements of a, i.e. it is true when  is read as “for all x in a” and “∃x” is read as “for some x in a”. A formula φ(x) whose constants are all in a is said to define over a the set of all elements k such that φ(k) is true over a, and a subset b of a is called definable over a (more completely, first-order definable over a) if it is defined over a by some formula whose constants are all in a.
We let  be the set of all sets definable over a.
Gödel introduced the notion of constructible sets as follows: Again by the transfinite recursion theorem, one can assign to each ordinal α a set Mα such that for every ordinal α and every limit ordinal λ, the following three conditions hold:
(1)
(2)Mα+ = F (Mα).
(3)Mλ = Uα<λ Mα.
The definition of the Mα’s differs from that of the Rα’s only in condition (2): Whereas Rα+1 consists of all subsets of Rα+1, the set Mα+1 consists of only those subsets of Mα that are definable over Mα.
A set is called constructible if it is a member of some Mα.
Are all sets constructible? This is a grand unsolved problem! Gödel conjectured that this is not the case, even though he proved that it was consistent with the axioms of ZF. The significance of the constructible sets is that the constructability of all sets implies the generalized continuum hypothesis! Moreover, this implication is provable in ZF. Thus if we add to the axioms of ZF the axiom that all sets are constructible (which is known as the axiom of constructability), the continuum hypothesis (as well as the axiom of choice) becomes provable. And since, as Gödel showed, the axiom of constructability is consistent with ZF, so is the generalized continuum hypothesis (as well as the axiom of choice). This is how Gödel showed the consistency of GCH with ZF.
Some years later, Paul Cohen [1963, 1964] showed, by a completely different method, that the negation of GCH is consistent with ZF, even with the addition of AC. Thus GCH is undecidable in ZF.
Despite the undecidability of GCH in ZF, the question remains as to whether or not it is really true. To many so-called “formalists", the question has no meaning. They say that is provable in some axiom systems and disprovable in others. To the so-called “Platonist” like Gödel, this position is most unsatisfactory. I would liken it to the following: Suppose a bridge is built and on the next day an army is to march over it. Will the bridge hold or not? It does no good to say, “Well, in some axiom systems, it can be proved that the bridge will hold, and in others, it can be proved that it won’t.” We want to know whether it will really hold or not. And likewise with the generalized continuum hypothesis. It is not enough to say it is undecidable in ZF. The fact remains that for every infinite set a, either there is a set b intermediate in size between a and  or there isn’t, and we want to know which. As already, indicated, Gödel conjectured that there is such a set b for some infinite set a, and that when more is known about sets, we will see that Cantor’s conjecture is wrong. [Realize that for any non-empty finite set a, there are always sets b with more elements than a and less elements than .]
The whole study of the independence of GCH in ZF is fascinating beyond belief, and the reader who studies this is in for a real treat!







References
Barendregt, Henk, The Lambda Calculus — Its Syntax and Semantics, Studies in Logic and the Foundations of Mathematics Vol. 103, North-Holland, 1985.
Beth, Evert, The Foundations of Mathematics. A Study in the Philosophy of Science, North-Holland, 1959.
Braithwaite, Reginald, Kestrels, Quirky Birds and Hopeless Egocentricity, ebook published by http://leanpub.com, 2013.
Church, Alonzo, The Calculi of Lambda Conversion, Princeton Univ. Press, 1941.
Cohen, Paul J., “The independence of the continuum hypothesis,” Proceedings of the National Academy of Sciences of the United States of America 50(6): 1143–1148, 1963.
Cohen, Paul J., “The independence of the continuum hypothesis, II,” Proceedings of the National Academy of Sciences of the United States of America 51(1): 105–110, 1964.
Craig, William, “Three uses of the Herbrand–Gentzen theorem in relating model theory and proof theory,” The Journal of Symbolic Logic 22(3): 269–285, 1957.
Davis, Martin, Computability and Unsolvability, McGraw-Hill, 1958; Dover, 1982.
Ehrenfeucht, Andrzej and Feferman, Solomon, “Representability of recursively enumerable sets in formal theories,” Arch. Fur Math. Logick und Grundlagenforschung 5: 37–41, 1960.
Fraenkel, Abraham Bar-Hillel, Yehoshua and Levy Azriel, Foundations of Set Theory, North-Holland, 1973 (originally published in 1958). (Fraenkel’s final word on ZF and ZFC, according to Wikipedia.)
Frege, Gottlob, Grundgesetze der Arithmetic, Verlag Hermann Pohle, Vol. I/II, 1893. Partial translation of volume I, The Basic Laws of Arithmetic, by M. Furth, Univ. of California Press, 1964.
Gentzen, Gerhard, “Untersuchungen über das logische Schliessen I,” Mathematische Zeitschrift 39(2): 176–210, 1934.
Gentzen, Gerhard, “Untersuchungen über das logische Schliessen II,” Mathematische Zeitschrift 39(3): 405–431, 1935.
Gödel, Kurt, The Consistency of the Axiom of Choice and of the Generalized Continuum Hypothesis with the Axioms of Set Theory, Princeton Univ. Press, 1940.
Henkin, Leon, “The completeness of the first-order functional calculus,” J. Symbolic Logic, 14: 159–166, 1949.
Hintikka, Jaakko, “Form and content in quantification theory,” Acta Philosophica Fennica, 8: 7–55. 1955.
Kleene, Stephen Cole, “Recursive predicates and quantifiers,” Trans. Amer. Math. Soc. 53: 41–73, 1943.
Kleene, Stephen Cole, Introduction to Metamathematics, North-Holland, 1952.
Myhill, John, “Creative sets,” Z. Math. Logik Grundlagen Math. 1: 97–108, 1955.
Post, Emil Leon, “Formal reductions of the general combinatorial decision problem,” American Journal of Mathematics 65: 197–215, 1943.
Post, Emil Leon, “Recursively enumerable sets of positive integers and their decision problems,” Bull. Amer. Math. Soc. 50(5): 284–316, 1944.
Putnam, Hilary and Smullyan, Raymond, “Exact separation of recursively enumerable sets within theories,” Journal of the American Mathematical Society, 11(4): 574–577, 1960.
Rice, H. Gordon, “Classes of recursively enumerable sets and their decision problems,” Trans. Amer. Math. Soc. 74(2): 358–366, 1953.
Rosser, John Barkley, “Extensions of some theorems of Gödel and Church,” Journal of Symbolic Logic, 1(3): 87–91, 1936.
Russell, Bertrand, “Letter to Frege,” 1902. This very interesting letter can be found (translated from the German) in Van Heigenoort, Jean, From Frege to Gödel, A Source Book in Mathematical Logic, 1879–1931, Harvard Univ. Press, 1967. It is also available online at a Harvard website: http://isites.harvard.edu/fs/docs/icb.topic1219929.files/FregeRussellCorr.pdf
Schönfinkel, Moses, “ Über die Bausteine der mathematischen Logik”, Mathematische Annalen 92, 1924; translated as “On the building blocks of mathematical logic” and included in Van Heigenoort, Jean, From Frege to Gödel, A Source Book in Mathematical Logic, 1879–1931, Harvard Univ. Press, 1967.
Shepherdson, John, “Representability of recursively enumerable sets in formal theories,” Archiv für Mathematische Logik und Grundlagenforschung, 119–127, 1961.
Smullyan, Raymond, Theory of Formal Systems, Princeton Univ. Press, 1961.
Smullyan, Raymond, “A unifying principle in quantification theory”, Proceedings of the National Academy of Sciences, 49(6): 828–832, 1963.
Smullyan, Raymond, To Mock a Mockingbird, Alfred A. Knopf, 1985.
Smullyan, Raymond, Gödel’s Incompleteness Theorems, Oxford Univ. Press, 1992. Smullyan, Raymond, Recursion Theory for Metamathematics, Oxford, 1993.
Smullyan, Raymond, Diagonalization and Self-Reference, Oxford Science Publications, 1994.
Smullyan, Raymond, First-Order Logic, Springer-Verlag, 1968; Dover, 1995. Smullyan, Raymond, Logical Labyrinths, CRC Press, 2008; A. K. Peters, Ltd. 2009.
Smullyan, Raymond, The Beginner’s Guide to Mathematical Logic, Dover, 2014.
Sprenger, M. and Wymann-Böni, M., “How to decide the lark,” Theoretical Computer Science, 110: 419–432, 1993.
Statman, Richard, “The word problem for Smullyan’s lark combinator is decidable,” Journal of Symbolic Computation, 7(2): 103–112, 1989.
Tarski, Alfred, Undecidable Theories, North-Holland, 1953.
Von Neumann, John, “On the introduction of transfinite numbers,” in Jean van Heijenoort, From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931 (3rd ed.), Harvard Univ. Press, 1923, pp. 346–354 (English translation of von Neumann 1923), 1973.
Whitehead, Alfred North and Russell, Bertrand, Principia Mathematica, Cambridge Univ. Press, 1910, Vol. 1. Reprinted by Rough Draft Printing, 2011.
Zermelo, Ernst, “Untersuchungen über die Grundlagen der Mengenlehre I,” Mathematische Annalen 65(2): 261–281, 1908. English translation: Heijenoort, Jean van (1967), “Investigations in the foundations of set theory,” From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931, Source Books in the History of the Sciences, Harvard Univ. Press, 1967, pp. 199–215.







Index
1-1 (one-to-one) function, 97
δ(x, y), recursive pairing function, 97
δ n(x1, …, xn), recursive n-tupling function, 98
Γ-consistent set of sentences, 49–56
λ-I combinators, 192–196
ωn, 113
Φ, Φ2, Φ3, Φ4 combinators, 195, 196
Φn combinator, 196
σ, successor combinator, 234
∑0 formula of elementary arithmetic, 94, 95
∑0 set or relation, 95
∑1 formula of elementary arithmetic, 95
∑1 set or relation, 93, 95
∾ AC is consistent with ZF (Cohen), 258
A*, C*, R*, F* and V* combinators, 189
A*number set in a simple system, 151
A**, C**, R **, F** and V** combinators, 189
abstraction principle, 253
AC (Axiom of Choice), 258–260
AC is consistent with ZF (Gödel), 258
AC is undecidable in ZF (Gödel and Cohen), 258
addition combinator, 234, 235
admissible function in a simple system, 152
affirmation of a number by a register, 70
affirmation set of a register, 73
agreeable element, in combinatory logic, 175
algebraic approach to verifying tautologies, 4–7
alphabet of an elementary formal system, 89, 90
altered tableau method, 40–43
analytic consistency property, 49–56
analytic tableau, 34–36, 40, 49–56
applications of magic sets, 27, 28
applications of Rice’s Theorem, 120
applications of simple systems, 150
arithmetic combinators, 234, 235
arithmetic set or relation, 95
arithmetic term of combinatory logic, 241
arithmetical combinator of type n, 234
associate regular set for a finite set of sentences, 53
atomic formula of an elementary formal system, 89, 90
Axiom of Choice (AC), 258–260
axiom of constructability, 259
axiom of foundation, 257
axiom of infinity, 254, 256
axiom of replacement, 256
axiom of substitution, 256
axiomatic set theory, 253–260
axioms (and axiom schemes) of an elementary formal system, 90
axioms and rules of inference for combinatory logic, 239, 243
axioms of set theory, 253–256, 258–260
B combinator (bluebird), 185
B, M, T, and I are a basis for all λ-I combinators, 224–226
B, M, T and I derivable combinators (λ-I combinators), 192–196
B, T and I combinators, 226, 227
B1, B2, Bn, and B+ combinators, 186
Barendregt, Henk, 233, 234
base in the universal system (U), 100
basis for a class of combinators, 224
Beth’s Definability Theorem, 46–48
Beth, Evert, 34, 46, 48, 56
block tableau, 34–39
bluebird combinator B, 185
Boolean consequence relation, 11–15
Boolean equations, verification by truth tables, 3, 4
Boolean ring, 4, 7
Boolean truth set for first-order logic, 25
Boolean truth set for propositional logic, 7
Boolean valuation for first-order logic, 23
Boolean valuation for propositional logic, 9
bounded quantifiers, 94
Braithwaite, Reginald, 177
C and K (Post sets), 122
C combinator (cardinal), 187
C1, condition that a simple system must satisfy, 149
cancellation law, in combinatory logic, 177
cancellative combinator, 216
cancellator (kestrel) combinator K, 176
Cantor’s theorem, 258
cardinal combinator C, 187
Church, Alonzo, 187, 188, 192, 234
closure properties on representable relations in elementary formal systems, 91, 92
closure under existential quantification of representable relations in elementary formal systems, 92
closure under explicit transformation of representable relations in elementary formal systems, 92
closure under unions and intersections of representable relations in elementary formal systems, 92
co-productive function for a set, 122
co-productive set, 122
Cohen, Paul, 258, 260
combinator birds table on Internet, with derivations from S and K, 222, 223
combinator B (bluebird), 185
combinator C (cardinal), 187
combinator D (dove), 186
combinator Ê, 186
combinator E (eagle), 186
combinator F (finch), 188
combinator for addition, 234
combinator for exponentiation, 235
combinator for multiplication, 234
combinator G (goldfinch), 191
combinator H (hummingbird), 193
combinator I (identity), 179
combinator J (Rosser), 226
combinator K (kestrel, cancellator), 176
combinator L (lark), 178
combinator M (mockingbird, mocker, duplicator), 174
combinator M2, 192
combinator n, 208
combinator N (nice combinator), 208
combinator O (oracle, owl), 207
combinator Φn, 196
combinator Q (queer bird), 190
combinator Q1 (quixotic bird), 190
combinator Q2 (quizzied bird), 190
combinator Q3 (quirky bird), 191
combinator Q4 (quacky bird), 191
combinator Q5 (quintessential bird), 191
combinator Q6 (quivering bird), 191
combinator R (robin), 187
combinator S (starling), 194
combinator σ (successor), 234
combinator T (thrush), 187
combinator U (Turing combinator), 206
combinator V (vireo), 188
combinator W (warbler), 192
combinator W′ (converse warbler), 192
combinatorial completeness of a system of combinatory logic, 215–223
combinatorially complete class of combinators, 215–223
combinatorially computable number set, 236
combinatorics, 171–251
combinators, 173
combinators A*, C*, R*, F* and V*, 189
combinators A**, C**, R**, F** and V**, 189
combinators B, T and I, 226, 227
combinators B1, B2, Bn, and B+, 186
combinators D1 and D2, 186
combinators Φ, Φ2, Φ3, Φ4, 195, 196
combinators P, P1, P2, P3, 195
combinators W 1, W2, W2, 193
combinators derivable from B, M, T and I (λ-I combinators), 192–196
combinators derived from the B combinator alone, 186
combinators for t and f and for the logical connectives, 233, 234
combinatory logic, 171–251
compact property of sets, 12
Compactness Theorem (denumerable), 12, 49, 52
compatible pair of elements, in combinatory logic, 175
complete infinite regular set of sentences, 26
complete mathematical system , 74
complete representation of a number set by a number in a simple system, 151
complete set K, 122
complete simple system, 150
complete system for combinatory logic, 215–223
completely creative function for a set, 123
completely creative set, 122
completely effective inseparability of two sets, 137, 139
completely effectively inseparable pair of sets, 137–139
completely productive function for a set, 122
completely productive set, 122
completeness of a first-order axiom system, 49, 52
completeness of the altered tableau method, 43
completeness of the analytic tableau method, 49, 51
completeness of the block tableau method for propositional logic, 36, 37
completeness of the Gentzen axiom system for first-order logic G1, 39
completeness of the Gentzen system G0, 34–38
completeness of the (symmetric)
Gentzen system for first-order logic GG, 40–43
completeness proof for propositional logic based on maximal consistency, 7–15
components of a base in the universal system (U), 100
composition condition for a combinatory logic, 174
compositor combinator, 186
computation of a number set by a property combinator, 236
concatenation in base 10 combinator, 238
consequence relation, 10–15
consistency of the Axiom of Choice (AC) with ZF (shown by Gödel), 258
consistency of the Generalized
Continuum Hypothesis (GCH) with ZF (shown by Gödel), 258, 259
consistency of the negation of the Axiom of Choice (AC) with ZF(shown by Cohen), 258
consistency of the negation of the Generalized Continuum Hypothesis (GCH) with ZF (shown by Cohen), 258
consistent/inconsistent set of formulas, 10
consistent mathematical system , 73
constant term of an elementary formal system, 89
constructible set, 258, 259
construction of the universal system (U), 99–101
continuum hypothesis, 258–260
contra-representation of a number set by a number in a simple system, 151
contra-universal number in a synthesis system, 81
contra-universal R-set, 77
contra-universal register, 71
converse warbler combinator W′, 192
correctness of the altered tableau method, 43
correctness of the Gentzen system G0 for propositional logic, 32, 33
Craig’s Interpolation Lemma (Craig’s Lemma), 43–46, 49, 53
Craig, William, 29, 31, 43–46, 49, 53
creative function for a set, 122
creative number in a synthesis system, 81
creative predicate in a mathematical system , 75
creative R-set, 77
creative register, 71
creative set, 122
critical parameter of a regular set, 52
cross point of a pair of compatible elements, in combinatory logic, 175
Curry’s sage, 206
D combinator (dove), 186
D1 and D2 combinators, 186
Davis, Martin, 113
decidable number in a synthesis system, 80
decidable number of a simple system, 150
decidable sentence in a mathematical system , 73
decidable simple system, 155
decision (calculating) machine, 69–73
deduction in an axiom system, 13
deduction property of an axiom system, 13
Deduction Theorem, 13, 14
definable function, 160
defining equation (defining condition) of an element of a combinatory system, 215
definition of a number set by a number in a simple system, 151
degree of a predicate of an elementary formal system, 89
denial of a number by a register, 70
derivatives of the combinator B alone, 186
designation of a number by an S, K term, 241
diagonal function d(x) in a simple system, 152
diagonalization of a number in a simple system, 151
diagonalizer  of a sentence H in a mathematical system , 74
diagonalizer  of a register R, 70
domination of one number by another in a synthesis system, 83
domination of one predicate by another in a mathematical system , 76
domination of one R-set by another, 78
domination of one register over another, 72
double diagonal function, 133, 134
double fixed point of a pair of elements, 207, 208
double oracle, 209
double recursion theorems, 133–135
double sage (double sage pair), 207, 208
doubly co-productive ordered pair of sets, 135, 140
doubly generative ordered pair of sets, 135, 138–141
doubly universal ordered pair of sets, 135, 139–141
dove combinator D, 186
duplicator (mockingbird, mocker) combinator M, 174
dyadic (elementary formal) system, 93
dyadic numeral, 93
dyadically representable set or relation, 93
Ê combinator, 186
E combinator (eagle), 186
E-complete set of sentences, 54–56
eagle combinator E, 186
effective (and recursively enumerable) sequence of simple systems, each of which is an extension of the preceding system, 159
effective inseparability of two sets, 136–139, 141
effective method of finding a set, 115
effective Rosser system (as a simple system), 158, 159, 161
egocentric element, in combinatory logic, 174
Ehrenfeucht, Andrzej, 157, 160
elementary formal systems and recursive enumerability, 89–103
enumeration and iteration theorems, 113–117
Enumeration Theorem, 113, 114
equivalence of Craig’s Interpolation Lemma for first-order logic and for the Gentzen system GG, 44
equivalence of provability and truth in elementary formal systems, 90
equivalence of representability in an arbitrary dyadic system and representability in a transcribed dyadic system, 100
equivalence of valuations and interpretations, 9, 10, 27
equivalent numbers of a simple system, 150
equivalent sentences in a mathematical system , 74
exact Rosser (first-order) system for sets, 160
exact Rosser system (as a simple system), 157–161
exact separation of a disjoint pair of number sets in a simple system, 153
explicit definition of a predicate with respect to a set of sentences, 46, 47
explicit definition of a relation from another relation in an elementary formal system, 92
explicit transformation of a relation into another relation in an elementary formal system, 92
exponential combinator, 235
extension of a simple system, 159
extensional number set, 119
F combinator (finch), 188
Feferman, Solomon, 157, 160
fidelity (faithfulness) to modus ponens, 13
finch combinator F, 188
finitary proof, 30
finite character property, 54–56
finite existential quantification of a relation R, 94
finite quantifications of relations, 94
finite universal quantification of a relation R, 94
First Double Fixed Point Theorem for combinators, 224
First Fixed Point Theorem for combinators, 223
first-order logic, 23–66
first-order satisfiable sentence, 24
first-order truth set, 25
first-order valuation, 23
fixation of one element on another, in combinatory logic, 175
fixed point combinator, 179, 205–208
fixed point of a number in a simple system, 153
fixed point of a number in a synthesis system, 81
fixed point of a pair of R-sets, 77
fixed point of a predicate in a mathematical system , 75
fixed point of a register, 72
fixed point of an element, in combinatory logic, 174
fixed point theorems (recursion theorems) in recursion theory, 117–121
formally representable set or relation, 91
formula of an elementary formal system, 89, 90
Fraenkel, Abraham, 254, 256–260
Frege, Gotttlob, 253, 254
functional combinator, 237
∼GCH is consistent with ZF(Cohen), 258, 260
G combinator (goldfinch), 191
G0, 31–38
G0 in uniform notation, 33
G1, 38, 39
GCH (Generalized Continuum Hypothesis), 258–260
GCH is consistent with ZF (Gödel), 258, 259
GCH is undecidable in Z (Gödel and Cohen), 258
Generalized Continuum Hypothesis (GCH), 258–260
generative function for a set, 123
generative set, 123
Gentzen axiom system (symmetric) for first-order logic GG, 39–46
Gentzen axiom system for first-order logic G1, 38, 39
Gentzen axiom system for propositional logic G0, 31–38
Gentzen sequent, 29–46
Gentzen type axiom systems, 31–46
Gentzen, Gerhard, 29–46, 56
GG, 39–46
Gödel, Kurt, 73–76, 96, 102, 103, 156, 240–242, 258, 259
Gödel numbering using dyadic numerals, 96
Gödel numeral of an S, K term of combinatory logic, 241
Gödel sentence (equation of combinatorial logic) of a number set, 242
Gödel sentence of the universal system U for a number set A, 102
goldfinch combinator G, 191
grand problem: Is there a systematic way to decide whether two S, K combinators are equal?, 239–243
H combinator (hummingbird), 193
H′, negation of a sentence H in a mathematical system , 74
, diagonalizer of a sentence H in a mathematical system , 74
halting of a decision machine, 72
Henkin, Leon, 54–56
Henkin-style completeness proof for first-order logic, 54–56
Hintikka set, 50, 51, 55, 56
Hintikka, Jaakko, 34, 50, 51, 55, 56
hummingbird combinator H, 193
I combinator (identity), 179
identity combinator I, 179
implication sign of an elementary formal system, 89
implicit definition of a predicate with respect to a set of sentences, 46, 47
incomplete simple system, 150, 155
inconsistency of Frege’s system of set theory, 254
independent elementary formal systems, 91
index of a predicate in a mathematical system , 73
index of a recursively enumerable set, 114
index of a register, 69
index of a sentence in a mathematical system , 73
index of a sentence in a simple system, 150
indexing method for verification of tautologies by way of Boolean equations, 3, 4
inductive set, 255
instance of a formula of an elementary formal system, 90
interpolant for a first-order sentence X ⊃ Y, 43
interpolant for a first-order sequent X →Y, 44
interpolant for a propositional sequent X → Y, 44
Interpolation Lemma (Craig’ Interpolation Lemma), 43–46, 49, 53
interpretation, 8–11, 24, 27, 28, 46, 47
interpretation of first-order sentences in the domain of the parameters, 24
inverse functions K and L, 98
inverse functions , 99
inverse of a binary relation, 115
iteration and enumeration theorems, 113–117
Iteration Theorem, 116
iterative function for a relation R(x, y), 124
J and I are a basis for all λ-I combinators, 226
J combinator (Rosser), 226
K combinator (kestrel, cancellator), 176
kestrel (cancellator) combinator K, 176
Kestrels, Quirky Birds and Hopeless Egocentricity, 177
Kleene function for a disjoint pair of sets, 137, 138
Kleene pair of disjoint sets, 137–139
Kleene’s construction of a completely effectively inseparable pair of recursively enumerable sets, 137, 138
Kleene’s symmetric form of Gödel’s Theorem, 156
Kleene, Stephen, 113, 137–139, 156, 157
L combinator (lark), 178
lark combinator L, 178
Lemma K, 96
length measurer combinator, for a base 10 number, 238
less than relation on ordinals, 257
limit ordinal, 257
limited abstraction principle, 254
Lindenbaum’s method of extending a consistent set S to a maximally consistent set M when consistency is of finite character, 54–56
Lindenbaum, Adolf, 15, 54–56
logical (propositional) combinators, 233, 234
logical connectives and Boolean equations on sets, correspondence between, 3, 4
M combinator (mockingbird, mocker, duplicator), 174
M2 combinator, 192
magic set, 23–28
many-one reducible set, 123
many-one reduction of one set to another, 123
mathematical system  (variations on a theme of Gödel), 73–76
maximal indexing (recursively enumerable) of all recursively enumerable sets, 117
maximally consistent set of formulas, 11, 12
mechanical procedure, 90
metamathematical applications, 149–161
method of eliminates required to obtain a combinator derived from S and K, 219
method of eliminates required to obtain a combinator derived from S, B, C and I, 225
minimization principle, 237, 238
minimizer of a regular relational combinator, 238
mocker (mockingbird, duplicator) combinator M, 174
mockingbird (mocker, duplicator) combinator M, 174
modus ponens, 7
multiplication combinator, 234, 235
Myhill’s Fixed Point Theorem, 120, 121
Myhill, John, 120, 121, 123, 125–127, 134
n combinator, 208
N combinator (nice combinator), 208
n-tupling function δn (x1, …, xn), 98
narcissistic element, in combinatory logic, 176
natural number (numerical) combinators, 234
neg(x), requisite function of a simple system, 149
negated form of an iff statement, 7, 8
negation H′ of a sentence H in a mathematical system , 74
negation of Generalized Continuum Hypothesis (GCH) is consistent with ZF (Cohen), 258, 260
nice combinator N, 208
non-cancellative (λ-I) combinator, 216
non-denumerable ordinal, 257
non-standard formula, 44
norm of a string of symbols of the alphabet of the universal system U, 102
norm of an S, K term, 241
normal simple system, 153
normalizer of a natural number combinator, 241
numerical (natural number) combinators, 234, 235
numerical relation A(x, y) in a synthesis system, 79
numerical relation B(x, y) in a synthesis system, 79
O combinator (oracle owl), 207
one-to-one (1-1) function, 97
onto function, 97
opposer R′ of a register, 70
oracle (owl) combinator O, 207
order of a combinator, 194
order of elimination of variables in the method of eliminates, 221
ordinal, 255–257
owl (oracle) combinator O, 207
P, P1, P2, P3 combinators, 195
(P, R), pair of disjoint number sets, making up a simple system, 149
parentheses conventions for logical formulas including set inclusion/exclusion, 7
partial system for λ-I non-cancellative)combinators, 224–226
partial systems for combinatory logic, 224–227
partition interpolant between two sets, 53
partition of a set, 53
Peano Postulates, 255
permuting combinator (permuter), 187–191
Platonists versus formalists on the undecidability of the Generalized Continuum Hypothesis, 260
positive integers, as defined in set theory, 254, 255
Post’s sets C and K, 122
Post, Emil, 113, 122–125, 244
predecessor combinator, 235
predicate of a mathematical system , 73
predicate of an elementary formal system, 89
predicate of the universal system U, 100
principal part (P, R) of a simple system, 159
Principia Mathematica, 254
productive function for a set, 121, 122
productive set, 121
proof of a sentence of an elementary formal system, 90
proof of a sequent in Gentzen type system, 34
proof of Beth’s Definability Theorem by using Craig’s Lemma, 47, 48
proof of Craig’s Interpolation Lemma by use of the Unification Theorem, 53
proof of Craig’s Interpolation Lemma for the Gentzen first-order system GG, 44–46
proof of the Compactness Theorem by use of the Unification Theorem, 52
proof of the Compactness Theorem for first-order logic using magic sets, 28
proof of the completeness of an axiom system for first-order logic by use of the Unification Theorem, 52
proof of the completeness of propositional logic based on maximal consistency, 7–15
proof of the completeness of standard axiom systems for first-order logic by Henkin’s method, 54–56
proof of the completeness of the method of analytic tableaux by use of the Unification Theorem, 51
proof of the efficacy of the method of eliminates required to obtain a combinator derived from S and K, 220, 221
proof of the efficacy of the method of eliminates required to obtain a combinator derived from S, B, C and I, 225, 226
proof of the Regularity Theorem by use of the Unification Theorem, 52, 53
proof of the Regularity Theorem using magic sets, 28
proof of the Skolem–Löwenheim Theorem by use of the Unification Theorem, 51
proof of the Skolem–Löwenheim Theorem using magic sets, 28
proof of the Unification Theorem by means other than tableaux or König’s Lemma, 50, 51
proof of the Unification Theorem using tableaux, 50
proof that the set of Gödel numbers of trecursively separable pair of setshe true sentences of U is not recursive, 102, 103
proof that the set of true sentences T of the universal system U is formally representable and that the set T0 is recursively enumerable, 101, 102
property combinator, 236
propositional logic, 3–22, 35–38
propositional logic combinators, 233, 234
provability predicate in a mathematical system , 74
provable number of a simple system, 150
provable sentence of a mathematical system , 73
provable sentence of an elementary formal system, 90
punctuation sign of an elementary formal system, 89
pure sentence of first-order logic, 25
Putnam, Hillary, 157, 160
Putnam–Smullyan Theorem, 160
Q combinator (queer bird), 190
Q1 combinator (quixotic bird), 190
Q2 combinator (quizzied bird), 190
Q3 combinator (quirky bird), 191
Q4 combinator (quacky bird), 191
Q5 combinator (quintessential bird), 191
Q6 combinator (quivering bird), 191
quacky bird combinator Q4, 191
queer bird combinator Q, 190
quintessential bird combinator Q5, 191
quirky bird combinator Q3, 191
quivering bird combinator Q6, 191
quixotic bird combinator Q1, 190
quizzied bird combinator Q2, 190
R combinator (robin), 187
r(h, n), in the simple system associated with a first-order system of arithmetic, 150
r(x, y), for recursion theory, 113
r(x, y), requisite (representation) function of a simple system, 149, 150
R-separable pair of R-sets, 78
R-set, 76
R-system, 76–78
R′, opposer of a register R, 70
R#, diagonalizer of a register R, 70
rank of a set, 257
realization of a function by a combinator, 237
realization of a term by a combinator, 216
Recursion Property for Combinatory Logic, 236
recursion theorems (fixed point theorems) in recursion theory, 117–121
recursion theory, 67–170
recursive inseparability of two number sets in a simple system, 156
recursive inseparability of two sets, 136, 156
recursive isomorphism between two sets, 127
recursive pairing function δ (x, y), 97
recursive set, 93
recursive unsolvability of the universal system U, 102, 103
recursively enumerable (effective) sequence of simple systems, each of which is an extension of the preceding system, 159
recursively enumerable indexing of all recursively enumerable sets, 117
recursively enumerable set or relation, 93–103
recursively separable pair of sets, 136, 156
reducibility of one ordered pair of sets to another ordered pair of sets, 135, 139
reducible set, 123
reduction of one set to another, 123
refutability predicate in a mathematical system  74
refutable number of a simple system, 150
refutable sentence of a mathematical system , 73
register of a decision (calculating) machine, 69
regular relational combinator, 237, 238
regular set of sentences of first-order logic, 26
Regularity Theorem, 23, 28, 49, 52, 53
relational combinator, 237
representability in a transcribed dyadic system, 100
representability in an arbitrary dyadic system, 93
representability in the universal system U, 100
representation function r(x, y) for number sets with respect to a simple system, 149, 150
representation of a natural number by a combinator, 234
representation of a number set by a combinator, 242
representation of a set by a number in a simple system, 150
representation of a set by a number in a synthesis system, 83
representation of a set in a mathematical system  74
representation of a set or relation by a predicate in an elementary formal system, 91
Rice’s Theorem, 119, 120
Rice, H. G., 119, 120
robin combinator R, 187
Rosenbloom, Paul, 244
Rosser system (as a simple system), 157–161
Rosser (first-order) system for binary relations, 160
Rosser (first-order) system for sets, 159, 160
Rosser fixed point property in a simple Rossor system, 158
Rosser function in an effective Rosser(simple) system, 158
Rosser system (as a simple system), 157–161
Rosser, J. Barkley, 75, 152, 157–161, 192, 226
Russell, Bertrand, 254
S combinator (starling), 194
S(n, x, y), in the simple system
associated with a first-order system of arithmetic, 150
s and K as constants from which all other combinators can be derived, 215–223
S, B, C, and I are a basis for all λ-I combinators, 224–226
S, K term, 239
sage constructions, 205–208
sage element, in a combinatory logic, 179
satisfaction of a set of sentences by a valuation, 25
satisfiability of first-order sentences in the domain of the parameters, 24
Schöfinkel, Moses, 173
Second Fixed Point Theorem, for combinators, 242
semi-doubly universal ordered pair of sets, 135, 138
semi-reducibility of one ordered pair of sets to another ordered pair of sets, 135, 138, 139
sentence of a mathematical system , 73
sentence of an elementary formal system, 90
sentence of the universal system U, 100
separability of a pair of R-sets, 78
separation of a disjoint pair of number sets in a simple system, 153
separation of two binary relations by a formula in a first-order system, 160
Separation Principle, 136, 138
sequent (Gentzen), 29–46
sequent corresponding to a set of signed formulas, 33
set A in synthesis system, 79
set B in synthesis system, 79
set operations, similarity with logical connectives, 3, 4
Shepherdson’s Theorem, 161
Shepherdson, John, 161
similar registers, 70
Simple Iteration Theorem, 115, 116
simple system, 149–161
simple system associated with a system of first-order arithmetic, 150
simple term, in an S, K combinatorial system, 218
skew diagonalization of a number in a simple system, 151
Skolem–Löwenheim Theorem, 23, 27, 28, 49, 51
smug element, in a combinatory logic, 175
solvable property P, 119
solvable set or relation, 93
Sprenger, M., 179
standard axiom system for propositional logic, 13
standard simple system, 154–161
starling combinator S, 194
Statman, Richard, 179
strong double fixed point property, 208
Strong Double Myhill Theorem, 134
Strong Double Recursion Theorem, 133–135, 137, 140
Strong Recursion Theorem, 120, 121
stump detector register, 72
stumping of a machine by a number, 70
subformula principle, 30
successor combinator σ, 234
successor ordinal, 256, 257
successor set, 256
symmetric Gentzen axiom system for first-order logic GG, 39–46
synthesis of results from three fields (special topics), 69–83
Synthesis System, 79–83
synthetic consistency property, 54–56
T combinator (thrush), 187
tableau proof of a sequent, 34
Tarski, Alfred, 159
tautological consequence of a set of formulas, 11, 12
tautological implication of a sentence by a set of sentences, 26
tautological sequent, 30
tautologically complete consequence relation, 11, 12
term induction principle, 218
term of a combinatory system, 215
term of an elementary formal system, 89
thrush combinator T, 187
To Mock a Mockingbird, 175–226
transcribed dyadic system (TDS), 99, 100
transcribed predicate (TS predicate), 99
transcribed variable (TS variable), 99
transitive rescursion theorem, 257
transitive set, 256
true sentence of an elementary formal system, 90
true sentence of the universal system U, 100
truth set in first-order logic, 25
truth table verification of Boolean equations, 3, 4
truth-functional basis for first-order logic, 26
truth-functional implication of a sentence by a set of sentences, 26
truth-functionally satisfiable, of first-order sentences, 24
TS predicate (transcribed predicate), 99
TS variable (transcribed variable), 99
Turing combinator U, 206
Turing, Alan, 206, 207, 236
U combinator (Turing combinator), 206
U system, universal system for recursively enumerable sets and relations, 99–103
undecidability of the Axiom of Choice (AC) with ZF (shown by Gödel and Cohen), 258
undecidability of the Generalized Continuum Hypothesis (GCH) with ZF (shown by Gödel and Cohen), 258, 260
undecidable number of a simple system, 150
undecidable number of a synthesis system, 80
undecidable sentence in a mathematical system , 73
undecidable simple system, 155
Unification Theorem, 48–56
uniform incompleteness (for a simple system), 159
Uniform Iteration Theorem, 117
uniformly incompletable simple system, 159
uniformly universal set, 124
universal number in a synthesis system, 80
universal R-set, 77
universal register, 71
universal relation, 114
universal set, 124
universal system U for recursively enumerable sets and relations, 99–103
unlimited abstraction principle, 253
unsolvable problems and Rice’s Theorem, 119, 120
V combinator (vireo), 188
valid sequent, 30
valuation, 9, 10, 23–27
valuation for first-order logic, 23
variable of an elementary formal system, 89
variations on a theme of Gödel (a mathematical system ), 73–76
vireo combinator V, 188
Von Neumann’s positive integers as sets, 255
Von Neumann, John, 255
W combinator (warbler), 192
W′ combinator (converse warbler), 192
W1, W2, W3 combinators, 193
warbler combinator W, 192
weak double fixed point property, 207
Weak Double Recursion Theorem, 133, 134
Weak Recursion Theorem, 118
weakly co-productive set, 125, 126
weakly doubly co-productive ordered pair of sets, 140
well-founded ∈ relation, 257
Whitehead, Alfred North, 254
word problem for Smullyan’s combinatorial lark is decidable, 179
Wymann-Böni, M., 179
x-eliminate of a combinatorial term, 219
x1, …, xn eliminate of a combinatorial term, 221
Zermelo’s positive integers as sets, 254, 255
Zermelo, Ernst, 254–260
zero tester combinator, 235
ZF, Zermelo–Fraenkel set theory, 254–260



