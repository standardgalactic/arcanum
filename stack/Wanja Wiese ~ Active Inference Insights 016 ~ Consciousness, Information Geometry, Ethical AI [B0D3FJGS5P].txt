Hello, everyone, and welcome back to Active Inference Insights.
I'm your host, Darius Parvizzi-Wayne, and today I'm delighted to be speaking to Vanya
Wieser.
Vanya is a postdoctoral fellow at Johannes Gutenberg University Mainz in the department
of...
Royal University Bochum, sorry.
The internet has lied to me.
The internet has lied to me.
I don't like this introduction bit already.
But I think this is true nonetheless.
It's his research centers on consciousness, medical representation, philosophy of cognitive
science, philosophy of mind, and, of course, Active Inference.
We also both share a love of self-modeling, attention, autopoetic intentionality, and much,
much more.
And I'm sure these are just some of the topics that we will discuss today.
Vanya, welcome to the show.
Thank you so much for joining me.
Thank you for having me.
It's a pleasure.
No, this is really exciting.
Yeah, as I said in the intro, we share a lot of interests that might appear quite niche
to people, but actually, you know, as far as I can tell, pertain to their lived experience
at every moment.
So it's worthwhile dissecting.
Funnily enough, the first time I think I'd heard of you was actually from Jakob Hovi in
the conversation I had with him.
And he said that you know Thomas Metzinger's work perhaps better than Thomas and certainly
better than anyone else.
So I wonder whether we could start by outlining some of these crucial concepts that sometimes
seem a bit counterintuitive to people.
So, for example, when I say we both share a love of self-modeling or phenomenal self-modeling,
what does that mean?
What does this sort of acronym PSM, the phenomenal self-model, mean?
Yeah, good.
So maybe as a disclaimer, of course, Thomas knows more about this than he's invented the
term self-models and has developed the self-model theory of subjectivity over many, many years.
And actually, I think I can say that there's probably no living philosopher who has read as
much relevant literature about this topic as Thomas, because he's devoted his entire career to this
and works a lot.
And I would never, even if I tried, be in a position to know as much about this as he does.
And I think it might be a good idea even to invite him to this podcast.
He's been invited.
He has said yes.
That was a while ago.
I tried to push it.
So, Thomas, if you're listening, which I think he might be, you know, you never know.
Please do come on.
Yeah.
So, actually, it's quite likely that some of the things that I'll be saying might not
be 100% true.
So, Thomas, if you're listening now, you may want to correct some of the misrepresentations
of your work.
But anyway, yeah.
What is a self-model?
So, for me, it's, I think, easiest to first think about bodily self-models.
So, just imagine some creature or a robot that has a body and has to control the body.
And as we all know, in many cases, control can be facilitated or improved if you have a
model of the thing that you're controlling.
And if you want to control your body and the interaction with the environment, it can be
highly useful to have a model of your body and a model of your environment.
And this is actually then already a form of self-model, which is part of the world model.
So, the world model would just be a representation of the world or the relevant part of the world.
The parts that are relevant to you and your environment and your body is part of this world.
So, the self-model is part of the world model.
But, of course, that's just a form of self-model.
And it's quite primitive, probably, compared to the self-models that we human beings have.
And one thing that distinguishes our self-models is that we don't have only unconscious self-models,
but actually, we also consciously experience the contents of our self-models.
So, this brings us into the notion of a phenomenal self-model, where phenomenal refers to phenomenal experience, phenomenal consciousness.
So, if you have not just an unconscious self-model, but a phenomenal self-model,
you actually experience your body as your own body.
And another form of self-hood that you then might experience is experiencing your own actions as your action.
So, this is often called a sense of agency that you have over your action and a sense of ownership that you have over your body.
And both can also be, well, both can get lost or be affected in various ways.
And so, these are some concepts that are relevant here.
There's one thing that I wanted to add, or maybe an example that Thomas also likes to use, I think,
is this robot starfish that was developed by Josh Bongard and colleagues.
And in order to make it flexible, enable it to adapt to changes to the environment and also to its own body.
So, for instance, if one of its limbs is damaged or is cut off, you want the robot to still be able to control its body and control movements.
And so, it can be useful to have a model of the body, a self-model.
But, of course, in that robot, it's not a phenomenal self-model.
It's not a conscious robot.
So, that would be an example of an unconscious bodily self-model.
Yeah.
Let's start with the body.
I think that's a good place to start because, as you say, it's kind of the low-hanging fruit,
the lowest level of what people think about in terms of the phenomenal self-model.
And I think a potential point of confusion for people is the fact that the body has what, I guess, Merleau-Ponty call the dual aspect,
which is both that it's a lived body, but it is also a body that can be observed and can be objectified.
And so, I guess the question here is, is there a difference between saying, I am my body, I live through my body, or versus I have a body?
And what are those different aspects of selfhood?
Because on one part, I feel like you're getting what we call perspectivalness.
I am my body, I live through my body, I live, yeah, I mean, literally through it as a vehicle through which I can be in the world, versus possession.
I am some, in this case, it seems like people would feel like a disembodied ego who has a body that they own.
Is there a way of coalescing these kind of two canonical features of selfhood, one being possession and one being perspectivalness or association with the lived body?
Yeah, that's a great question.
So, I think these different perspectives are, to some extent, compatible, whereas it depends on what you mean.
So, one could, for instance, wonder whether an animal, a non-human animal, which maybe does not have a form of consciousness that is comparable to human conscious experience,
and that doesn't reflect about its own body, nevertheless lives through its body.
And so, there's a lived body, but that doesn't mean that we don't have that.
I would say it's just that we can also have different forms of experience or richer forms of bodily experience and reflect about our own experience in different ways.
And so, having a body versus living through a body.
So, it seems to me that the lived body is more characteristic of experiences or situations in which we're not so much aware of having a body at all.
So, maybe when we're engaging in some physical activity, playing sports and just not thinking about where our limbs are, but just moving and interacting and, in the sense, living through the body.
I don't know if that captures what is usually meant by that expression.
Yes.
I wonder whether, I mean, it's a good question.
We wrote this paper, not you and I, me, Carl, Lars and others wrote this paper on flow states.
And flow states are exactly this, you know, playing sports.
And we tried to ground it in a self-modeling architecture.
And actually, the revisions have been really useful for that because I've been able to get a little bit more concrete on what I'm talking about.
And the kind of dichotomy that we're saying is present there, I think I can say this because hopefully it'll be published.
Well, it's the preprints out, is the distinction really between a minimal phenomenal self, which is like these really basal fundamental aspects of selfhood, mindness, presentness, perspectivalness versus epistemic agency or strong epistemic agency.
And by that, I mean, you know, when we, let's say, plan or when I'm talking to you and I have to think about my next sentence, there's a very strong sense of self-guiding that process where I can use, let's say, former resources, former knowledge to apply to deep temporal planning.
And we're saying that that's attenuated in flow.
And what we're also saying, therefore, is that metacognition, because I think it's really important here to distinguish between pre-reflective self-awareness and reflective self-awareness.
And I'm treating reflective self-awareness as a metacognitive act.
Your self is reflecting on itself because reflective self-consciousness is itself a kind of part of form of epistemic foraging.
That, too, is attenuated in flow states.
But I think the interesting question there is, if I was in flow, would I purely live through my body or could I be perceptually aware of having a body nonetheless?
Could I have both?
And these are very difficult questions to tease out because, you know, they're already done retrospectively.
And so you only get qualitative reports.
That's an interesting dimension I haven't thought about.
But I think, OK, so I emailed you, however, you know, a month ago, two months ago, and I said, so I've been trying to get my, I've been trying to think about how Metzinger's phenomenal self-model can be coalesced with active inference.
And this is the Active Inference Insights podcast.
So I messaged you, I said, active inference takes cognition to be fundamentally inferential.
Are the processes within or the processes that generate the phenomenal self-model inferential?
Are the processes that are themselves being modeled by the phenomenal self-inferential or both?
Because I think, Vangie, you would do a great service to everyone if you could explain that what Thomas is talking about is kind of the cognitive system recapitulating its own data structures into phenomenal space.
And I think that's an interesting idea.
He talks about this in sort of his 2003 book and 2008 book and some other works that the phenomenal self-model is a, in an implicitly reflective way, is, as you say, a model of itself,
whether that is a, you know, giving relevant features into consciousness or coarse-grained features into consciousness, whatever it is,
but it's a reflective process where the data structures in the body and brain are being transposed into phenomenal space.
So maybe we can start there.
I may have butchered it, and I'm sure Thomas is pulling his hair out.
But maybe we can start there and then feed an inference and see where it might exist within those cycles.
Yeah, so I think this is actually quite a complex topic because when we're talking about self-models,
it seems to presuppose that there's already an experienced I, right?
That I have a body, I am moving my hand and so on.
But I don't think that's necessarily already present in any, in the most basic forms of phenomenal self-models.
And what it's, at least the way I think about it is that having a, at least a bodily self-model is mainly about just making a distinction between one,
between inner and outer, as it were, so I'm just trying to avoid the word self and non-self.
So a distinction between two types of processes, one is the environment and the other is here where I am.
And I think that actually suffices for having a bodily self-model or even a phenomenal self-model.
But of course, when we think about our experience and the way we, the different forms of self-hood that we experience,
there's, there's, there's more to it, there's, there's something like an, an I and maybe at this point it's useful to bring another aspect of the self-model theory on the table,
which is the phenomenal model of the intentionality relation.
Exactly. And so the idea is that there's in a complex self-model, phenomenal self-model in which there's not just a distinction between this belongs to me,
this does not belong to me, this is my body, this is not my body and so on,
well, this is my, my action or this is an involuntary movement and so on and more complex self-models.
You also have this, this perspectivalness, which, and subjectivity in the sense that you not just make the distinction between what is self and non-self
or what is outside and what is inside, but also how you are related to my environment, how you are related to your, your body and how your, your thoughts are related to the world and so on.
So this brings a reflexive component to it and makes it more, more complex.
I find it a bit difficult to put this into words.
Yeah, it is difficult. I mean, so I can, I can hopefully assist.
I have some sort of quotes here.
Great.
So this is the way that I've understood it at least.
And so we'll start with the quote.
So Thomas Metzinger, 2008.
So the paper, the ego tunnel, I think is 2009.
So this is a really actually very useful paper.
He wrote in 2008 for something like the empirical approach to phenomenal self-modeling.
It's really, it's actually very lucid.
He says that the subjectively experienced content of the phenomenal self is the representational content of a currently active dynamic data structure in the system's central nervous system.
And that is, it's a representational entity whose content is determined by the system's very own properties.
So what I mean by that is it's, it's standing in for something that is lower level to it, namely these data structures, right?
And we obviously here are taking as axiomatic that there are representational structures and maybe that's problematic and we can get there.
And then the way that I've always read the PMIR is that the cognitive system can pay attention to the fact that it itself is directed at other things.
And therefore what happens is that it, well, I'm not going to say it makes the inference, but it projects into phenomenal space this notion of itself and something else.
I guess my question here is that's taking as a presupposition that there is some division that the self-model can model, i.e. it can reflect on the fact that its own attention can be directed at so-called external worldly objects or things in the world model.
My question here would be, how does the brain know what in the sense is external to it and what is internal to it in the sense that I can introspectively pay attention to parts of my own lived experience, but not, but have a sense of mindness and not feel that kind of duality or that distinction.
But if in principle I pay attention to the chair next to me, that feels like a separate object to me.
So if the brain is recapitulating some distinctions that are in its self-model, I guess my real question is, are we not, as you say, already invoking an a priori distinction between the thing that's paying attention and the attended object?
Yeah, so if I understand your question correctly, then the answer would be yes.
Yes, and so you mentioned knowledge, so how does the brain know?
And of course, knowledge requires truth, so you're assuming that there is a fact to the matter whether something is really inside or outside or something is really I or mine or self or not self.
And I don't think we have to make this assumption that there is, that these are truth claims and that the brain has knowledge about this, but it does have to make some prior assumptions about that.
And these can be, can be delusions.
So in that sense, I think the question, how does the brain know can be dissolved, but of course, when the question is, where do these priors come from and why are they relatively immune to revision?
And what happens if they do break down or the sense of self dissolves and so on?
So might it be feasible to say that at the very bottom level, all we have is attention in some way?
So I've recognized that I almost, well, I did commit this kind of dualistic fallacy when I said there is the attender and the attended object.
It seems to me that actually that might be an inference too far.
Rather, there just is the attention onto these kinds of objects in these data structures.
Again, Metzinger puts this far better than I do.
He says that when he talks about the PMIR, the phenomenal model of the intentionality relation,
he says that you mentally simulate yourself as currently being directed at a target object or goal state.
And that's a really interesting notion, right?
There's mental simulation of yourself.
Maybe he's committing a linguistic, you know, fallacy there that there's a self there in the first place.
So.
So we can pay attention, let's say, to the fact that we can be targeted at certain areas of our phenomena of our data state space.
But then we can also pay attention to the fact that we're paying attention.
And that seems to me, I don't want you to think about this proposal, that seems to me to be maybe the heart of reflective self-consciousness.
That I can pay attention to the fact that there seems to be something that is paying attention to that which I'm, that which is, that which is where attention is going to.
There's a kind of triple hierarchy.
One, does that sound like a feasible, you know, underlying explanation of metacognitive self-conceptualization or reflective self-consciousness?
And two, how deep do those recursive patterns go?
What I mean by that is, like, how many times can I reflect on my own attentional processes?
And I know Lars Sandberg-Smith and others have been doing this in terms of so-called meta-awareness.
But I'm thinking more in terms of actually the underlying phenomenal reality of being a self.
Is there a point in which the very notion just becomes, you know, irreconcilable with a certain degree of recursive operations?
Yeah.
Great question.
So regarding this triple distinction, I think that's already quite difficult to imagine.
So maybe we can just try to clarify this and just restate this again together.
But before that, there's something that came to my mind.
Because when we think about how does the brain know what's inside and what's outside,
then when it comes to, I mean, to some extent, there might be truth to claims about this.
And so when it comes to attention, we can just make this really broad distinction between attentional processes
that are driven by external influences.
So when some salient object or processor, loud noise and the environment suddenly captures my attention,
and we can contrast that with cases in which I'm controlling my attention.
And so the changes in attention are generated or caused by, more caused by internal processes than by external processes.
And the brain can be more or less accurate about whether some changes in attention have been more caused by external or internal processes.
And it can make sense to model attention to improve these inferences and improve the control of attention and so on.
So maybe we can use a sort of just re-describe this triple distinction that you mentioned,
because I'm not sure I fully got what you asked.
Okay.
Yeah, I'm aware that there are lots of layers to this, as is the problem when we talk about hierarchies.
So I can actually read.
I was thinking maybe it's best for me to just read what I've...
So this is...
So if people go and look at that flow paper right now on the preprint server, they won't see this.
Hopefully, I'm not spoiling anyone's party.
By the time this comes out, this is at the very end of a review process.
So hopefully it's fine.
Anyway, people can shout at me if they want.
But I wrote, so it's my words.
I said, in planning...
So we're talking about epistemic agency.
In planning, the organism not only knows that it is intentionally directed at something else,
yielding a weak form of epistemic agency and perspectivalness,
but it is also in the act of knowing about its own knowledge,
which encodes as multiple counterfactual representations of what might unfold
as a result of my actions in the section of my current eco-niche that I seem to control.
In this way, planning involves strong epistemic agency,
since the cognizer can epistemically model itself as epistemically agentive,
i.e. it can come to know its own capacity to control itself and its eco-niche
over a protracted period of time through stored knowledge.
And then I go on to say that that self-modelling process is transparent
in the sense that the agent often does not know that it knows its intentional objects.
Rather, there just is the sense of epistemic agency,
which permeates a pre-reflexive self-awareness.
This is where the triple thing comes in.
When the agent does enter a state of knowing the fact that it knows its epistemic capacities,
it evolves into a phenomenologically metacognitive, metamodelling,
or perhaps meta-epistemic organism, which reflects on its being.
Yeah.
So I think the issue that I had when writing this was,
you have to start from the original, if you're using the Metzinger Foundation,
you have to start with the original basis, which is that the self-model itself is a simulation.
It's a recapitulation of a data structure.
But it's a recapitulation of a data structure, which in itself be reflected on.
And that reflects that can also be reflected on.
So that's where I'm building in this nature of hierarchical recursion.
How does that sound to your ears?
And again, what does it mean for us to continue to go up these recursive ranks?
What happens, for example, if I...
Is there a point where I just really concretize my sense of self?
Or is it...
Does it actually almost become dispersive?
Because all I realize that's going on is that I'm paying attention.
And I'm just paying attention to different levels of the hierarchy.
So I'm kind of curious about what you think might be the phenomenological consequences
of going up this hierarchy of reflective recursion.
Yeah, so I don't think I can give a full answer to this question, because there might be multiple
things going on.
So when I'm not just planning my actions or planning how to interact with the environment or whatever,
but also reflecting upon my planning, becoming aware that I'm planning, I might be evaluating
this, I'm becoming aware of the fact that I'm uncertain about different options, or I might become
aware of the fact that I really want to do one thing.
And so this might then lead to further experience, experience of certainty, or maybe also uncertainty, right?
Or confidence that I'm making a good decision or doubtfulness about the decision.
And I think we can then repeat this process and reflect upon this already metacognitive process.
But the resulting experience that we have might not add too much.
So I don't know if it makes sense to say, yeah, I'm confident about the confidence that I
have in my confidence in my confidence.
Yeah, and so, yes, I mean, it sounds, you know, on one hand, one could read that as a genuinely
vertical recursion.
On one hand, you can also read it as a kind of, like the way people talk about the fact
that a triple bluff is the same as a single bluff.
You know, we might actually just turn our, you know, turn our heads onto our tails, so to speak,
and end up actually experiencing something.
It's a curious inquiry.
Someone needs to model that.
Maybe I need to model.
Yeah, just what I wanted to say is that I think it makes a lot of sense for three levels or three
recursions.
And then above and beyond that, I really don't have a clear intuition.
And it's probably an empirical question also to what extent it can improve control.
And as I said, just from the point of view of experience, we can mentally build further
models, further iterations, but they might not have a real functional significance for the
lower order processes.
Good.
Okay.
We will park the self-models.
I had to do it because it's been a bugbear of mine, but we'll get Thomas on and we'll,
we can all chat out together and see where we end up.
There's a broader question here about the so-called generative passage, which is this idea that
is really nicely articulated in Maxwell Ramsey's 22 paper and colleagues, of course, which is
this idea that, okay, we kind of have two things that we are trying to balance.
One is experiential reality, lived experience.
And the other are, in some sense, the tools of science.
That's one thing.
The other thing could also be, let's say, neurological systems.
And Ramsey and colleagues present three forms of what they call the generative passage, or
this kind of ways of tying those things together.
One is ontological.
And so that's not necessarily a subscription to reductionism.
It's not necessarily saying that phenomenal reality is neural structures, but that's one
option, whether it's super, that's that whole conversation.
One is epistemological, which is that the kind of tools that we use in our science can be
applied fruitfully or can be used to describe and explain phenomenal reality, lived experience.
And the last one is methodological naturalization, which I think I've been using a little bit
more in my own writing, which doesn't really have a commitment.
It's quite agnostic about ontological or epistemological claims and just says, well, we can do some good
explanatory work by thinking about Pondi-P schema or predictive processing or whatever it is.
But I'm not going to say anything about whether consciousness is these architectures or neurons.
I was curious about, I think Thomas, I want to speak on his behalf and you'll be able to say
Ben, I do.
I feel like he's quite materialist or reductive about this.
And at least this is something that came to my attention from Julian Kiverstein when I spoke
to Julian on this podcast, who was very keen to reject that reductionism and forefront
intentional experience and forefront lived experience and say that that's irreducible.
So I was just curious about where you stand on this generative passage.
And I guess the sourciest question is the ontological passage.
Yeah.
Deep question.
So first of all, I think different views on this all make sense and are useful.
And there's a lot of uncertainty about how different levels of reality relate to each other.
And so to some extent, we just have to follow some assumptions that we make.
And then there will be different views that are compatible with the models and data that we
have.
So I'm not claiming that we can say, well, we can, obviously we can reduce conscious experience
to brain activity or something like that.
And it's, to some extent, also an open question for me, what role computational models actually
play in explaining conscious experience.
And what role do they play?
And what role do they play?
Yeah.
So I think there's, so maybe the simple answer, or one simple answer is to say, well, consciousness
is so puzzling.
And it's so difficult to grasp its different features and talk about what we are experiencing
from moment to moment.
It's difficult to put that into words.
And computational models.
And computational models can help to make certain ideas a bit more precise.
And this might then just be very, on a very, in a very simple way, a use of computational models.
It's not that, it's not even an explanation.
It's just trying to provide a better description of that which we want to explain.
What are the properties, the different features of conscious experience that we need to explain?
And computational models, I think, can very legitimately be used to provide such more detailed descriptions.
And once you have that, you can, of course, then try to make predictions about the implementation
in the brain or make predictions about how conscious experience will change in certain situations.
Or you can compare to descriptions of conscious experiences that we have to phenomenological reports
from persons in altered states of conscious experience and so on.
And then at some point, you may wonder, well, maybe these computational models provide more
than just a metaphor that is useful to describe complex and puzzling features of experience.
Maybe they also point to the mechanisms and go some way towards explaining how conscious experience
arises or why it has the features that it has, even if we cannot maybe explain why there is conscious
experience in the first place.
And to the extent that the computational models that are used are also neurobiologically at least
not implausible and we might be able to find some empirical confirmation or to the extent that empirical neurobiological
and data from cognitive neuroscience are compatible with these models,
because we might be inclined to make stronger claims about the relationship between conscious experience and computational models and neurobiological processes.
And of course, ideally, we'd have very detailed computational models that can be used to describe, predict, and maybe even control many features of conscious experience.
And ideally, we know how they might be implemented in the brain and therefore really have something like a computational explanation of certain features of experience.
But I can also understand if some people might be more hesitant in saying that.
Now that I'm thinking about it, I'm curious, actually, I have to, I guess I have to watch the episode with Julian Kivosti because...
It's a long one.
Yeah.
Okay.
Yeah, I'd be really interested in what exactly is his view on these matters.
Yeah, we go pretty deep, actually.
Yes.
Yeah, he was very much striking down the line of the irreducibility of conscious experience,
which I don't think is necessarily that controversial.
But he was very, you know, he was being stubborn about that point.
I don't think he'd be upset if I said that.
Yes.
Yes.
Okay, good.
We can remain agnostic.
The hard problem is the hard problem.
And unfortunately, it's the most frequent question that I ask.
And no one has solved it for me yet.
And I don't think they will.
So I'm going to ask a kind of what Chalmers might say is a less hard problem or an easier problem,
which is about the features of consciousness, especially as it's lived by us.
Is all consciousness self-consciousness?
I would say no, it's not.
So I'm not an expert on this topic.
And I haven't read Thomas' new book yet.
No, me neither.
But I intend to.
And hopefully then I will know a bit more about this.
But I take reports from people who claim to have experienced states of non-subjective experience seriously.
Or I think we should not reject these reports.
And I'm definitely open to the possibility and also find the strategy of trying to look at the simplest form of conscious experience,
which may not be a form of self-consciousness.
I find that quite promising.
Yes.
So do I.
Does this map onto something I think you've written about, indeed you have, subject unity and phenomenal unity?
Is there a link there?
Well, maybe we can start with what's the difference between subject unity and phenomenal unity.
Yeah.
So that's quite, can become quite complicated.
So first of all, subject unity can mean different things.
Some people just mean, well, different experiences that are had by a subject are subject unified if they're had by the same subject of experience.
Now, what is a subject of experience?
And some people just identify a subject with an organism.
So in that sense, subject unity would not entail subjective experience or having some form of self-consciousness would just mean subject unity would then just mean that there's a conscious organism that has different experiences at the same time.
So, and these are all subject unified in the sense that they are had by the same subject.
But of course, if you have a form of self-consciousness, you might say that there's different form of subject unity, maybe one in which you're, you have different self-conscious experience or different conscious experiences that are all forms of self-consciousness.
Self-consciousness and somehow refer to the same self, something along these lines.
Yeah.
So what, going back to the kind of notion that all consciousness is self-consciousness, it goes back to actually what Mr. Kivestein said.
So he's got this paper, I think it's a 2018 paper.
I'll double check that.
That's the, no, 2020, the free energy in the self and ecological inactive interpretation.
He says very clearly, he says, in line with the arguments of the phenomenologists, I will claim that every feeling must be felt by someone.
It must have mind that's built into it, if it is to feel a particular way.
It's such a, it's such a strong claim.
And again, I haven't read Thomas's new book, but I do kind of understand this perspective because at least, for example, let's talk about if one was imagining depersonalization, it's a depersonalization experience for whom, right?
Or if you think about someone who's experiencing a psychedelic experience, it isn't so much that perspectivalness itself is eliminated.
It's more that one maybe has this more oceanic sense of self, which is dispersive and that, but that still arguably is a qualia for the self, right?
Or for the, the space of awareness that is endowed with perspectivalness.
So I'm just curious about whether you can think, and again, this is a tricky question.
It's a big question.
And there probably isn't in a, we can't probably conceive of this at all, but sort of, well, I guess actually the question I'll start with is when I learned about consciousness, for example, I learned that there were three fundamental tenets to a conscious experience, qualia, intentionality and subjectivity.
This is what the British schooling system teaches you.
When I hear sort of Julian speak about, well, you're always going to have that perspectivalness or intentionality.
It's always something for you.
Can we, you know, is there a way that we could say, okay, some features, at least out of those three that I've just picked up are more fundamental than others.
And that we can have this very, very minimal conscious experience or minimal phenomenal experience, the MPE, another acronym.
And what would you say are the defining characteristics of a minimal phenomenal experience, therefore?
Yeah, I can only say it's an empirical question.
So we can speculate about what the minimal form of experience might be, but it seems that there's a lot going on in ordinary waking experiences that can get lost in altered states of consciousness.
But what exactly will remain, I don't know.
So Thomas can say more about that.
Is there a problem with relying on phenomenological reports to inform such questions?
Yeah, so to some extent, there's, of course, the challenge that these reports are given in retrospect and, how do you say, retrospect?
Yeah, yeah, yeah.
And then some people say, well, it's a bit paradoxical if you say you have had an experience in which you were not present or something like that.
But, of course, the mere fact that you can remember such experiences doesn't mean that you must have had a form of self-consciousness or that there must have been subjectivity in the experience.
And also, when people say, well, I realized that I had not been there, that there was no sense of self or no form of self-consciousness.
I realized that when I transitioned into the ordinary form of conscious experience.
So while, as it were, the self was returning, I realized, hey, there's something additional coming to my conscious experience.
So it could not have been there before.
Yeah, that's very interesting.
That's really interesting.
Yeah, I guess it makes me think, you know, to even have those reports, whether that's in the internal speech or external reports, requires propositional language.
And in my conceptualization of the scale of phenomenal self-modeling, propositional language is super high up in the epistemic agent.
It's the way that we plan.
It's the way that we reflect on our own being and so on.
And it sometimes makes me think, well, is there a bias, therefore, when we don't have the capacity for propositional claims, either internally or externally, we're biased to think that that constitutes some form of self-annihilation.
And the reason why I think that is because, like, testimonies, for example, about flow states, they'll say, at least the testimonies won't be as concrete as this, but the researchers' interpretations, which are problematic, will say the self disappeared or there was a reduced self-awareness.
That's probably a weaker form of that claim.
Well, I think all that we're really coming to here is that you just need to get a lot more particular about what part of the self-model you're talking about and how it's been attenuated or accentuated.
And actually, I think, hopefully, for the listeners, what they're kind of gathering is the self is not this kind of single monolithic thing that we take it to be.
And maybe that's the influence of Descartes and the tradition that followed from him.
But rather, it's a complex hierarchical, well, appearance in many ways, but also potentially an underlying data structure that will oscillate and modulate and change given different contexts.
Good. Let's take that a little bit more now into just sort of...
Maybe just one thing to make it slightly more concrete.
So what is, I think, very plausible is to assume that a sense of temporal self-location can get lost.
So you can have conscious experiences in which there's no time, no temporal experience.
You don't experience any now, as it were, or maybe only now, but not as distinguished from future and past.
And then experiences in which you're not identifying yourself with some place in space.
So there's no spatial self-location.
And if I understand Thomas correctly, he would suggest that in such minimal phenomenal experience, there's some potential for certain control processes is still experienced.
So, well, I mean, maybe we can try to illustrate this with attention.
So you can either be in the process of controlling or changing your attention and have the experience of being in control, having an experience of attentional agency.
And you, but you can also have a sense of control or the potential for changing your focus of attention.
So, you know, now you're fixating, focusing your attention on something or maybe on my voice, but you are aware of the fact that you could shift your focus of attention.
And this is, so this potential aspect might be something that in one form or the other could still be present in a minimal phenomenal experience.
And Thomas can probably describe this more eloquently and in more detail.
It also makes me think, what are the prerequisites for attention?
It makes me think that something like temporality or just some dynamic needs to be there, right?
Because how are you going to go from one thing to the other without it being over time?
Which also makes me think there might have to be also over space.
Well, I guess we also have covert, you know, mental attention or covert attention, mental attention, whether that's going through some space is kind of depends on how you define space.
That's a really interesting question.
I guess, yeah, there's a, there's a nice link there.
You could say, okay, I've got a function that remains in the minimal phenomenal experience.
But let me think also about what the necessary features of the experience or the world would have to be for me to have that function.
But, yeah, it makes me think of these so-called pure consciousness events, which I came to know about through John Bavakey.
And he sort of describes how in these pure consciousness events, so-called adjectival qualia, redness, blueness, sweetness, disappear.
We call it adverbial qualia, remain.
And by that, he means nowness, hereness, presentness, whatever, you know, perspectivalness.
And maybe that's a nice road in.
But, yeah, I will probe that with Mr. Metzinger.
Your, well, I've read, I've read your papers before and I read them again because I think they're so wonderful and interesting.
You have these two with Carl, which I love.
And in one of them, you talk about sort of life-mind continuity, which I've also been really into because I've been reading my Evan Thompson.
And in addition to sort of life and mind distinction, you also have a distinction between basic minds and non-basic minds, which I think is important as a prior thing to understand before we get into a sort of life-mind continuity.
So at just a very basic level, what would you say is the distinction between a basic mind and a non-basic mind?
Yeah, so if I remember correctly, I think we mainly make this distinction in terms of representation.
So a basic mind would be one that does not have representations and a non-basic mind is one that has representations.
Good. And how does that feed? So how does that feed? So when Evan Thompson was talking about mind and life continuity, he was kind of seeking underlying functions, structures, processes that are involved both in the constitution of mind and life.
So for example, autopoiesis, self-organization, or what else would he have had? Operational closure.
These are all things that were coming out of sort of the dynamical systems theory and Varela's work.
And now we've got active inference. So I was wondering whether, how does active inference?
Because I think at one point you say in that paper, I won't misspeak, I think I've got the quote here.
All systems possessing a Markov blanket have properties that are relevant for understanding the mind and consciousness.
If such systems have mental properties, then they can have them partly by virtue of possessing a Markov blanket, and hence your Markovian monism.
This is, I think, this is a 2020 paper, so not the life-mind one.
But it calls to mind a sort of life-mind continuity, because active inference postulates anything that exists has a Markov blanket,
and it can be seen as if it's parametricizing Bayesian beliefs about the external state.
So everything has a Markov blanket, anything that we can call a thing has a Markov blanket.
And as you say, anything having a Markov blanket have properties that are relevant.
And I think you've been very careful with your words here for understanding mind.
Why would that be the case?
And also, how is that the case without us resorting to a sort of panpsychism?
Yeah, that's a good question, and really a fair question, because I think what we say in that paper can easily be misunderstood.
And maybe, yeah, we should have found a better way of conveying what, or I should have found a better way of conveying what I had in mind.
So the idea is not that as soon as you have a Markov blanket, you have a mind, and it just becomes more complex.
The idea is more that as soon as you have the Markov blanket, if you are a self-organizing system, which conforms to the free energy principle,
So there's a Markov blanket, and then you can interpret internal states as encoding probabilistic beliefs about external states given blanket states.
And this is the feature that is also relevant to understanding, I would claim, the mind.
And it does not mean that once you have that, you already have a mind in an interesting sense, but it's just that if you have a mind, then you can describe, you can understand,
I would say, mental processes in terms of minimizing variation free energy, and understand them in terms of computational processes that involve these probabilistic beliefs about external states given blanket states.
And so what I really wanted to say is that this is not something that comes at a later stage when we have really complex systems.
But according to the free energy principle, we already have that for very simple self-organizing systems.
And so to contrast this with inactive approaches that would say, well, we have these simple systems, and they are still there, they can be quite autonomous, and so on, and remarkable.
And so remarkable, in fact, that even higher, even non-basic minds, in a different sense, or maybe I shouldn't say non-basic here, but so even more complex systems, more complex organisms like us that have minds, many of their properties can be explained, many of their mental properties can be explained.
By reference to the same dynamics and features that are already present in simple autonomous systems, and maybe you don't even need to invoke representations to explain these more complex organisms.
And I'm suggesting, and I'm suggesting, well, no, because you already have some form of as-if representationality or intentionality, and even simple self-organizing systems.
And this as-if representationality can then, or you can, making some further assumption, you can argue that in more complex systems, it's not just a useful way of describing them as if they had representations, but they actually have representations.
And so there's something in simple self-organizing systems that's already there, that's also there in more complex organisms that have a mind and consciousness.
And in order to understand the mind, to the extent that the free energy principle and active inference are useful to understanding the mind, we have to refer to these internal states and how they encode beliefs about external states given blanket states.
And that's something we already have in very simple self-organizing systems.
So that's just the idea, which I think, if described in the way in which we did, can be really misleading.
And I hope this also suggests how it avoids panpsychism.
But if not, I'm happy to elaborate on that.
No, I think so.
And to be fair, you make it actually pretty clear.
You say that, you sort of propose some rhetorical questions.
Does it follow that all systems with a Markov-Planck have a mind?
Are such systems conscious?
This formalism itself does not answer these questions.
So I think you're fine.
I think maybe what people might be slightly confused over is this notion of an information geometry, if they've ever read this paper.
So the way that I understood it was you have sort of two informational geometries, let's say, two ways of describing a system that has a Markov-Planck at internal and external states, right?
That coupled system.
And you have this so-called intrinsic geometry, which actually just describes the behavior of the internal states if it, you know, I don't, would that, well, I'll ask whether that includes the particular states, i.e. the blanket states.
And then you have the so-called extrinsic geometry, which are the beliefs that the internal states actually encode about the external states.
So that's a very sort of rough draft of the distinction between the intrinsic geometry and the extrinsic geometry.
It would be great if you could flesh that out a little bit more.
But how does that pertain here to mind?
Because I think you also make it quite clear that that property of the Markov-blanketed system itself is relevant to the question of mind.
Yes.
So it is, in a sense, relevant, but I would say in a rather trivial way.
So, yeah, I think this can be really confusing.
I'm not sure how useful it is to even refer to information geometry to get at these more general conceptual points.
But so an information geometry is just you have a space in which every point corresponds to a probability distribution.
And then you can you can have different shapes and manifolds in this space.
So, for example, surfaces and compute distances between points, distances between probability distributions.
And these distances will not be Euclidean distances.
So it's it's just a different form of a different kind of space.
Yeah.
And so every so the internal states, if you interpret them as as probabilistic beliefs about external states given blanket states, you can identify them with points in the in an information geometry.
And then the way in which these beliefs are updated and what's updated in which they change can be described as a trajectory through that space.
And you can also and you also have a different space of probability distributions for the what one could call the physical dynamics of the system.
So if you're not referring to probability distributions encoded by internal states, but about the probabilities of internal states or maybe paths.
Then you can also identify them with points in such a space, space of probability distributions, but it's a different, these are different probability distributions.
And the thing just as that you can then describe trajectories through both of these information geometries and in a way they are two sides of two different sides of the same coin.
So the idea is that you can describe the physical dynamics of these self-organizing systems equivalently by describing them in terms of the dynamics, the belief dynamics of Bayesian mechanics in terms of changes of probability distributions encoded by internal states rather than in terms of probability distributions.
Yeah.
Over states of the system.
Yeah.
Yeah.
Yeah.
Okay.
So I think the reason why this might be a bit confusing for people is because then people have heard that all that we're talking about when we talk about internal states is this parametricized, if that's a word, beliefs about external states.
That's all the internal states are, right?
They just are probability distributions over external states.
Okay.
So that, that, that, that might be a commitment, an ontological commitment to say that, but let's take that as a kind of starting point.
That's what we mean by internal states.
And then we have an intrinsic geometry, which is mapping the evolution of those beliefs.
Right.
And that's over time.
Right.
And that's over time.
Mm-hmm.
But given that those beliefs just are over the external states, is there not an isomorphism between the intrinsic and the extrinsic insofar as when the intrinsic changes, i.e. when the half of the state of the internal states changes, by necessity, the extrinsic information geometry would have changed.
Because the intrinsic, the internal states just are the probabilistic distributions that the extrinsic geometry defines.
So for me, I can't right now see how there's a, I say they're isomorphic, I almost think, well, I almost can't see how they're not one and the same.
But I might be missing something.
I probably am missing something.
I'm always missing something.
No, I think you're on the right track.
So there, yeah, it's essentially just two different ways of describing the system.
But, okay, so, but do they, so what is the utility of that, I guess, is the question.
Because, to me, it seems like you, just a very, you know, philosophy of science point, the utility of two different descriptions of the same phenomenon has to have some utility, right?
Like, if we're talking about heat, I can talk about how it feels to be hot, or I can talk about the velocity of particles within a heat bath or within whatever.
But that's useful, because one, I can attend to phenomenological facts, and the other one, I can attend to facts about physics.
Whereas this, it just seems like, prima facie, there are two distinct descriptions.
But under the blanket, because the internal states really just are probability distributions about external states, there's nothing over and above their evolution beyond that which can be described by the intrinsic information geometry.
So is there something more to these two types of descriptions which make them distinct?
Or is it just semantics?
I don't know, I don't want to say that, because that always sounds very derogatory, and I don't mean to be derogatory.
Yeah, so on the one hand, one could say, yeah, and to some extent, it's just semantics.
But that doesn't mean that it's not useful to use one instead of the other description.
So think about describing a system in terms of representations.
So you can, of course, just describe the vehicles without referring to the content.
And so you're dispensing with the representational description and just focusing on the physical material realizers of the representations.
And that's fine, but you typically get some benefit of going to a different level of description.
So when you're describing a system in terms of representations, you have, you can refer to contents, provide a normative description.
So the system can have true or false representations, or they can be more or less accurate.
Yeah.
And you also have that for the Bayesian mechanics.
So in a way, it's just a different way of describing the system, and you don't need that.
You can describe the system, you can stick to your physical, purely physical description, and don't need to refer to probabilistic beliefs.
But this gives you some benefits.
So on the one hand, it reduces complexity, because when you're referring to internal states as encoding probabilistic beliefs, you're actually coarse-graining the system.
And not just talking about the billions of neurons, but about neural populations that encode different beliefs.
And so you can reduce the complexity of your description, but still, given that, if the free energy principle is correct,
you have an equivalent description, it can still be as accurate or almost as accurate, but without having to deal with all the unnecessary details.
So that's one benefit that you get, just a reduction of complexity.
Hmm.
Yes.
I sense that that's a reduction of complexity for the scientist.
I guess, really, you very much put it nicely in terms of the distinction between the vehicle and the content, which certainly pertains when we're talking about representational systems.
I guess my point really here is that I arguably think that there is no vehicle.
The internal state is not a vehicle of anything.
And I think this really comes down to actually how you describe it, because the way you describe it is that they encode probabilistic beliefs.
Whereas I think if you're abandoning the vehicle, you can just say they are probabilistic beliefs.
And I think if you have the vehicle...
Yeah, but sorry.
So then it seems like you're dispensing with the physical, purely physical description, and just happy to say, well, we only need the Bayesian mechanics, right?
I would just say that we are sometimes misled to think that active inference entails a representational approach.
And yeah, you can do the descriptions in terms of the Bayesian mechanics.
That itself will map onto some physical organization.
But I don't think it necessarily dispenses with it.
It just says that when we as scientists try to carve up these spaces, we are making...
Well, we are making these very strong distinctions.
And this is something that I've been tackling with literally ever since I discovered active inference and learned about the free energy principle, which is what is unique about the internal states, right?
Is there something over and above the parametrization of beliefs about external states that are in the internal states that make the thing the thing it is?
I.e., is all that we are preferred external states?
Or are we a thing that is trying to get to preferred external states, if you can see the difference there?
So I think that, again, comes around to whether we embody a generative model or have a generative model.
And I think I've been leaning just in terms of the physics towards the idea that we embody a generative model.
All that means is that we are just a bundle of external states.
And you can describe the direction, the information, travel of those external states in terms of Bayesian inference.
But the internal-external distinction is somewhat illusory because we just are the external states.
It might sound very sort of Buddhist, actually.
But, yeah, I think maybe that's where we don't necessarily...
We're not disagreeing, but maybe where I don't really see the strength of this distinction,
because I don't really see the strength of the distinction between the internal and the external.
And arguably, I just see it as an instrumental tool.
I don't know if that answers your question.
Yeah, no, no.
That's a good point.
Also, it speaks to what we talked about in the beginning, right?
To what extent there's an effect to the matter whether something is internal or external.
Yeah, I just think a big mistake that people...
Well, I think a big stumbling block that people have,
although it may also paradoxically be a very useful tool for them,
is to think of active inference.
And this is more like a meta-conversation about how we get educated on these things.
Think of active inference as a theory that needs a homunculus.
And I'd argue that it's not.
But then you have to be very concrete on the fact that the free energy principle
and active inference is a theory of physics.
And it's just describing physical manifolds.
Well, not even physical manifolds.
They say statistical manifolds, whether they're physical, mental, and so on.
And that, I think, gets you away from the kind of more homunculus,
predictive coding, predictive processing approach.
And so, again, I mean, ultimately, I think it comes down to one's aims.
And I think, as far as I'm aware, the math stands up on both fronts.
And maybe it's a question I need to speak to Karl about, which is...
So, the free energy principle rests upon the Markov blanket distinction.
Why do we not just have one unified state space where things are tending...
Well, different things are tending towards different manifolds?
Why do we need them to be tending towards those manifolds over the parametricization of external states?
And I think here the really important point is, and we've come back to it over and over again on this podcast,
the free energy principle tells you what things look like they're doing, not necessarily what they are doing.
And maybe that deflates, to use Karl's favorite word, it deflates some of our worries about really making a strong ontological claim, right?
It's just a descriptive thing.
Then that makes me think, well, what's the point?
But I don't think what's the point, because I think it's important.
And I think you do get some good stuff out of it.
Yeah, that's just one thing I would like to add.
So, I think there's one perspective from which one can argue that it's not completely arbitrary,
whether we say we interpret or we describe internal states as probabilistic beliefs,
and identify them with these beliefs and just focus on this,
or whether we also think about the other perspective or the way of describing the dynamics of the system.
Because the same Bayesian mechanics, the same dynamics of belief can be realized in different ways.
And so you can simulate this on a computer or you can implement it in a living organism.
And I think when we consider the possibility of artificial consciousness, this actually may make a difference.
Sure.
Because if we're just focusing on the, if we're just seeing internal states as probabilistic beliefs and nothing else,
then we cannot make this distinction between different types, different kinds of realizations of variation of free energy minimization.
And I think it's an open question and very important question.
To what extent there can be a conscious experience in a computer, in a digital computer with a classical architecture?
Or to what extent it needs a biological body or maybe a robot that can interact with the environment?
And for this reason, I think it's important to keep this to, yeah, not to ignore this distinction between two ways of looking at the system.
Yeah, fair enough.
I guess, ultimately, as well, something I do think is important to say is that it depends on how you define beliefs.
And that's something that we all, like not me and you necessarily, but all of us equivocate on the whole time because we're not clear where we're talking about folks' psychology beliefs or Bayesian beliefs.
And I guess when I'm talking about the internal states just being manifestations of external states, I'm talking about Bayesian beliefs in terms of the beliefs that those internal states have.
When I'm talking about the rejection of the homunculized view of the internal states, in many ways I'm rejecting, I'm not rejecting the Bayesian beliefs that they have or embody or entail or so on.
I'm rejecting the folk psychology, homunculized, representational, propositional beliefs that we talk about in philosophy.
So I think that's an important thing as well for people to bear in mind that there's often risk in active inference that we equivocate and talk past one another.
Again, not you and I generally, but, you know, everyone, because we don't get concrete on what we're talking about with respect to beliefs.
But that's a nice interesting segue because you wrote this paper in 2022 with Carl on AI ethics in computational psychiatry.
And it actually takes us nicely full circle to Mr. Metzinger, Professor Metzinger to me, because he actually was the first person I ever read in the ego tunnel talking quite explicitly about the ethics of generating artificial intelligence in a very philosophically robust way.
Obviously, obviously, people have been talking about, well, what are the ethics in terms of job opportunities, in terms of, you know, trolley problems when my car might run over a grandmother or a child and so on.
But I think what Thomas said that was really impactful for me was we might be actually creating conscious life.
So it's not just, you know, it's not just the ethical effects on us.
It's actually the generation of sentience, feeling, consciousness into the universe, an uptick in that, whereby those things start to actually have moral value, which are arguably equivalent or perhaps given their greater complexity and depending on how you define your morality, more important than us.
So you can throw out the window your kind of classic Sam Harris worry that AI is just going to stomp on us like we're ants, right?
That's one ethical issue about us.
But also, and again, this is I don't mean that this should happen or will happen, but arguably, you know, those AI creatures are going to have sentience and feelings and consciousness just like us.
And we have to take their, you know, rights or values if they have them, if we give them that into account.
So it's not just a single-sided relationship that's going to emerge.
I'm curious about what you think about that.
Is it a concern that we really have to take seriously that if, for example, we end up making AGI and very importantly that AGI is conscious, because I think there's a really important point there.
There's a difference here between intelligence and consciousness, what concerns do we have to have not only about ourselves, but also about what we are bringing into the world?
Yeah, good.
I think there are many different and to some extent related questions that are relevant here.
And one question is, of course, what about the risk of actually creating artificial consciousness?
What consequences would that have?
And should we do it or should we refrain from even attempting to create artificial consciousness?
And there's things become more complicated when we also consider the fact that there will likely be a lot of uncertainty about whether a given artificial system is conscious or not.
So Eric Schwitzgabel has done a lot of very relevant and interesting work on this.
And he describes the problem roughly as follows.
So I'll try to summarize the main ideas, but there's people who are interested in this can easily find his publications and podcast interviews with Eric Schwitzgabel.
So I highly recommend looking these things up.
And so the basic problem is this.
There might be situations in which there are artificial systems that may seem to be conscious and we might, in addition, also have no way of being certain that they are not conscious.
So there might be no clear evidence as to whether they are conscious or not, even from a scientific point of view, given that we don't have a consensus in consciousness science, we don't really know what consciousness is.
There's no generally agreed upon theory of consciousness and so on.
There's many different metaphysical views about consciousness that all have some currency.
So there's a lot of uncertainty about which systems could be conscious.
And as artificial systems become more and more capable and have more and more impressive cognitive capacities,
there will likely be points at which they seem conscious and also fulfill some indicators of consciousness that we might have.
But they might still be quite different from human beings, conscious human beings or other conscious animals in many respects.
So that there will be a lot of uncertainty and there are two types of errors that one could make.
One is believing that such systems are conscious and maybe also believing that they therefore deserve a moral status, maybe because they can feel pain and pleasure.
So assigning a moral status to such systems, believing that they're conscious, although in fact there's nothing going on, they're completely unconscious.
And then there might be situations in which we have to make trade-offs between the interests of human beings, conscious human beings and these seemingly conscious artificial systems, which are in fact unconscious.
So just think about, I mean, just as a slightly silly example, a trolley case in which we can save five artificial systems that we believe are conscious or one conscious human being.
And of course, if these systems actually are unconscious, but we just think, well, we should be better safe than sorry.
We should not, we should just err on the side of being a bit too liberal, then we might say it would be a good idea to save the artificial systems or in other situations in general, just put more weight on their interests.
And in fact, they're completely unconscious and yeah, so that would be, I think, yeah, that's a terrifying idea.
Yeah.
Yeah.
Of course, the other case is that the artificial systems that don't seem to be conscious and we firmly believe or we, maybe there's also some uncertainty, but for whatever reason,
we decide, no, these systems are probably not conscious, maybe because they don't seem to be conscious, but actually they are conscious.
And then in certain situations, we might, without thinking much about it, sacrifice them or destroy them or whatever.
And in fact, maybe induce a lot of suffering in these artificial systems, but without knowing that.
And so these are two scenarios that we, I think, would like to avoid.
And Schwitzgeber certainly argues for this, that we should avoid such situations, namely by avoiding to create systems for which there's no certainty about whether they're conscious or not.
And so this is not, it's a bit different from Metzinger's position.
And then the question, of course, is, is this really feasible or is it realistic that we will in the future only create systems that are either clearly non-conscious or that are clearly conscious?
And I think in order to get there, it would be really useful to have a handle on some necessary conditions for consciousness that might not be fulfilled by certain classes of artificial systems.
And if, for instance, having a body is required for being conscious, then we could rule out that there's consciousness in a computer simulation.
So I'm not claiming that I have a good argument for this, but I think it would be extremely desirable to have a justified account of necessary conditions for consciousness
consciousness that are not fulfilled by many artificial systems.
And so that's the line of research that I'm trying to pursue of finding good reasons to believe that there are certain necessary conditions for consciousness.
Of course, the other approach, which I must say, from a certain point of view, seems a lot more compelling is to say, well, we don't really know what is necessary for consciousness or not.
We cannot rule out that certain systems are conscious if they don't fulfill certain conditions.
But we can look at theories of consciousness.
We can look at work on animal consciousness and derive indicators for consciousness.
And that's this preprint from last year by Patrick Butlin, Robert Long, and a whole bunch of other authors.
Eric Schwitz gave us also one of the co-authors, and they do exactly that.
They say, well, under the assumption of computational functionalism, we can derive a certain number of indicators from theories of consciousness
and can think about how they could be implemented in artificial systems and AIs.
We can also assess to what extent they are already fulfilled by current AIs.
But based on this, it's highly unlikely that current AIs are conscious.
But if future AIs fulfill many of these indicators, it becomes more likely that they're conscious.
So each time an artificial system fulfills one of these indicators, the probability that it's conscious rises a little.
Sure.
And of course, that's a very elegant approach because you don't have to make any strong metaphysical commitments.
You can just say, well, we don't really know what consciousness is, but we can say what features make it a bit more likely that a system is conscious.
And as soon as we have systems that fulfill many of these indicators or many markers of consciousness,
we might reasonably say, well, it's highly likely that a system is conscious and we should assign a moral status to the system.
But of course, then the question is what happens in the meantime when we have many systems that fulfill some indicators or many,
but maybe not so many indicators that we can really be certain that these systems are conscious.
And how can we avoid these situations?
Yes, it's fascinating.
It's fascinating.
I mean, we don't have time for me to put on my philosopher's skeptical hat and point out all the problems with trying to find behavioral indicators of consciousness,
which are so obvious that I don't really need to say them.
But again, that's the philosopher's point.
And this is more of a pragmatic point, which is we are probably going to have to make that decision at some point.
And so it's actually kind of interesting.
We're setting up a sort of Bayesian process with which to come up with a posterior that a given object is conscious or not.
So it's kind of fun that we're inverting the Bayesian process to figure out something.
But as you know, your model is only as good as its priors.
And if those behavioral priors or behavioral indicators are wrong, then you are really barking down the wrong tree.
You might be Bayes optimal, but you're Bayes optimal for a dysfunctional purpose.
Cool.
That's really, yeah, that's fascinating.
I mean, I'd love to go further, but I'm aware that we are running a bit low on time.
So you mentioned this is kind of your main focus of research at the moment.
What else can people expect from you coming up next couple of months this year?
And also, how can people get in touch with you if they want to ask you questions or just chat?
Yeah, sure.
So I have a BlueSky account, and I think my BlueSky handle is vanya.visa.
Probably.
I should check.
And I also have a Twitter account.
I don't really understand what it is.
I like Twitter.
Yeah, so I also have a Twitter account, and then my handle is W-A-W-I-E-S-E, I think.
Excellent.
And what can people look forward to reading or hearing from you about?
And also maybe plug the workshop that you're at now, because that's a really cool thing that you guys are doing.
Yeah, so I'm currently at the IK in Gnne in Germany.
IK is the Interdisciplinary College.
It's a spring school for cognitive science.
And it's an entire week with a lot of fascinating courses from AI to neuroscience, neurotechnology.
We also have some more philosophical courses, some more theoretical, some more practical.
It's a really great mixture.
And there are many fascinating people over here that you can talk to.
So I highly recommend coming to the IK in Gnne.
And maybe you have show notes or something like that for the podcast.
Maybe you can provide a link to the website.
Everything goes in the description.
I'll put your blue sky in the description as well.
I'll make sure I do that.
Thank you.
Cool.
And also maybe one thing what people can expect.
Please.
So I signed a contract with MIT Press for a book on artificial consciousness.
Oh, congratulations.
So that's something I'm working on.
And also I think I should remind myself that I should be busy working on the manuscript.
Fantastic.
Well, congratulations.
Do people know this already?
Thank you.
Or is this a big reveal?
Have we done an exclusive break on Active Inference Insights?
Or do people know about this book already?
I think most people don't know about it.
I've just told a few persons about it.
But on the other hand, most people don't know me.
Yeah, true.
No one knows us.
Well, I forget that.
I was just trying to sort of embody a speculative paparazzi-esque person.
I didn't do it very well.
Vanya, this was super fun.
I feel like there's still a lot to get into.
So maybe when Thomas comes, it would be great to have you on, even if partially for a bit,
so we can get deeper into these ideas.
And thank you for placating me and listening to my ramblings about recursive properties and operations.
No, no, thank you.
Thank you very much for having me.
And this was really a pleasure.
So I think we had a nice flow in the conversation.
We did.
We did.
All right.
Thank you everyone for watching as well.
Take care.
Thank you.
Thank you all.
