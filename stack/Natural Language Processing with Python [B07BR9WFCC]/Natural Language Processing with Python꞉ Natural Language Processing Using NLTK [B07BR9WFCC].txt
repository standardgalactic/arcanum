This is Audible.
Natural Language Processing with Python.
Natural Language Processing using NLTK by Frank Milstein.
Narrated by John Wilkins.
Copyright Information.
Copyright 2018 by Frank Milstein.
All rights reserved.
This document is geared towards providing exact and reliable information
in regards to the topic and issue covered.
The publication is sold with the idea that the publisher is not required
to render accounting officially permitted or otherwise qualified services.
If advice is necessary, legal or professional,
a practiced individual in the profession should be ordered.
For declaration of principles, which was accepted and approved equally
by a committee of the American Bar Association
and Committee of the Publishers and Associations,
in no way is it legal to reproduce, duplicate, or transmit any part of this document
by either electronic means or in printed format.
Recording of this publication is strictly prohibited,
and any storage of this document is not allowed
unless with written permission from the publisher.
All rights reserved.
The information provided herein is stated to be truthful and consistent
in that any liability, in terms of inattention or otherwise,
by any usage or abuse of the policies, processes, or direction contained within
is the solidarity and utter responsibility of the recipient reader.
Under no circumstance will any legal responsibility or blame be held against the publisher
for any damages or monetary laws due to the information herein,
either directly or indirectly.
Respective authors own all copyright not held by the publisher.
The information herein is offered for informational purposes solely
and is universal as so.
The presentation of the information is without contract
or any type of guarantee assurance.
The trademarks that are used are without any consent
and publication of the trademark is without permission or backing
by the trademark owner.
All trademarks are brand within this book
without permission or backing by the trademark owner.
All trademarks and brands within this book
are for clarification purposes only
and are owned by the owners themselves,
not affiliated with this document.
Introduction
As we are going to extensively explore here in this book,
natural language processing is a broad field of artificial intelligence
which is focused on finding interactions between computer and human language.
In fact, natural language processing sits at the intersection of three different studies
including computational linguistics, artificial intelligence, and computer science.
NLP, as we also call natural language processing,
is a wide-ranging field which covers manipulation and computer understanding of human language.
You will usually hear about NLP in context of analyzing large pools of documents set
or legislation where people are attempting to discover patterns
which suggest corruption and other inconsistencies.
In fact, NLP is the amazing ability of computer programs
to understand spoken human language
and thus, the far-reaching field of artificial intelligence.
The development of NLP applications is very challenging
as computers and machines require humans to speak to them
in numerous programming languages
which is highly structured and precise through voice commands.
However, as you know,
human speech is not always so precise
and human speech linguistic structure
frequently depends on various complex variables
such as social context, regional dialect, or slang.
Therefore, natural language practice generally comes across many challenges
when it comes to interpreting spoken human language.
Today, NLP as a branch of artificial intelligence
is of amazing importance to our data science research,
computer science, and deep learning
as it helps machines or computers to understand, then interpret,
and finally manipulate human language with great success.
Therefore, NLP draws together many other disciplines
such as computational linguistics and computer science
to bridge those gaps between computer understanding
and human communication.
Natural language processing can broadly be defined
as the automatic interpretation and manipulation
of natural language like text or speech by software.
This study has been around for over 50 years
and has grown from the field of computational linguistics
following the rise of computers.
Natural language refers to the way humans communicate with each other,
i.e., namely with the text of their speech.
Therefore, we are surrounded by text
as this is the main means of communication.
We are surrounded by text in emails, signs, menus, webpages, and much more.
We are also surrounded by speech
as another way we communicate with each other.
Therefore, text and voice are the main means
we communicate with each other.
Given the great importance of the textual type of data
and the role it plays in our lives today,
we have amazing methods of technique for understanding NLP
just like computers have techniques
for analyzing other types of data like videos or images.
Challenges of Natural Language Processing
Natural language processing has been studied
for more than 50 years and during this time,
NLP field came across countless challenges
as every human speaks in a different way with different accents.
NLP is principally hard since it is messy.
Human language is very ambiguous
and it is always evolving and changing.
Humans are great at producing
as well as understanding languages
as they are capable of perceiving,
expressing, and interpreting language
in a very nuanced and elaborate meaning.
However, while humans are great
when it comes to the use of language,
they are also very poor
when it comes to understanding
and manipulating language.
However, thanks to technology advancements,
natural language processing has risen
by introducing new technologies and methods
which help us interpret and manipulate
natural language accurately by using software.
History of Natural Language Processing
Even though natural language processing
is not a new science,
the NLP technology and methods
are rapidly advancing
due to the greatly increased interest
in human speech and computer interaction
and communications.
The addition of big data
and its availability to greatly enhance algorithms
and power computing machines and software
has amplified the interest
in connecting the gaps in the field.
As a human,
you may write and speak in Spanish,
English, or Chinese,
but a computer has its native language
known as machine language
or machine code
which is largely incomprehensible
to most people.
At your machine's lowest level,
communications happen only with
that textual data or words
thanks to millions of ones and zeros.
This code produces
the logical actions we can understand.
Therefore,
data scientists and programmers
used punch cards
to communicate with those
first computers 70 years ago.
This arduous and manual process
was understood by only a small number of people.
Things have changed since the beginning
of natural language methods
as you can now tell Google Assistant
that you like a song
and your device playing the music
will respond back in a human-like voice
saying that your writing has been saved.
Your device will then adapt its algorithm
and play that song again if you say it,
or it will play that song every time
whenever you come across that music station.
When taking a closer look at that interaction,
you know that your device is activated
when it heard you say something.
Therefore, your device understands
what you are saying,
so your words are not executed
into actions providing you feedback
in well-formed sentences in English
which takes no more than several seconds
for you to get the feedback.
This complete interaction
between you and your machine
was made possible
by natural language processing
along with some other artificial intelligence elements
such as deep learning and machine learning.
Natural language processing
went from linguistics
and computational linguistics
to this independent study
of spoken human language
which plays a vital role
in many other computer science fields.
Natural language processing
was derived from both linguistics
and computational linguistics.
As you probably already know,
linguistics is the study of language,
its phonetics, grammar, and semantics.
Classical linguistics
is involved in evaluating
and devising many rules of language.
When it comes to classical linguistics,
great progress was made
on those traditional methods
of semantics and syntax,
but the main natural language processing
understanding normally resists
that clean mathematical formalism.
A linguist is everyone
who studies language more colloquially.
When studying natural language processing,
you must involve a bit of mathematics
as mathematics is the main tool of science.
Therefore, natural language processing
involves both linguistics
and mathematical tools
as mathematicians who work on NLP problems
focus on the use of discrete mathematical theories
and formalism from their natural language processing studies.
As you already know,
natural language processing
was derived from the modern study
of linguistics and computational linguistics
that is the study which is
immunerable tools of computing science
to interpret and understand spoken human language.
Computational linguistics, in fact,
is the broad study of complex computer systems
and methods used for generating
and understanding natural language.
One of the main natural functions
of computational linguistics
is the testing of language syntax
and grammar as proposed
by theoretical linguistics.
With the introduction of fast computers
and large data sets,
new and different things
can be easily discovered
from large textual data sets
just by writing and running some software.
Back in the 1990s,
statistical machine learning
and statistical modeling
began to replace
those traditional, classic,
top-down rule-based approaches
and techniques
for understanding natural language,
primarily because of their better speed,
results, and robustness.
Therefore, this statistical modeling approach
to studying NLP
determines the field,
but in the future,
it may define the field.
To be more precise,
data-driven methods and techniques
used for interpreting
and manipulating natural language
have become so popular
that they will soon
definitely be considered
as the mainstream approach
to the overall
computational linguistics field.
One of the strongest
contributing factors
to this computational linguistics development
is the increased amount
of electronically stored data
to which these methods
can be easily applied.
Another contributing factor
is a specific disenchantment
with the traditional approach
and methods
which rely only on those
hand-crafted rules,
mainly due to their brittleness.
This statistical approach
to natural language processing
is not limited to
nor reliant on statistics per se.
However,
it includes some advanced
machine learning techniques
as well as understanding
natural language.
This requires a huge amount
of knowledge
about natural language syntax,
morphology,
pragmatics and semantics,
as well as general knowledge
about the world we live in.
Encoding and acquiring
this knowledge
about natural language
and about the world
is one of the main impediments
when it comes to developing
robust and effective language systems.
Just like different statistical methods,
natural language processing methods
and machine learning techniques
promise the automatic acquisition
of this knowledge
from unannotated
and annotated language corpora.
Growing Importance
of Natural Language Processing
Most of the research
and studies that were done
on natural language processing,
in fact,
revolves around search,
especially when it comes
to enterprise search.
This encompasses
allowing users
to easily and rapidly
query data sets
in the question form
that they may propose
to other people.
Then,
the machine or software
interprets those important
data elements
of the natural language science,
like those that may correspond
to a certain feature
in a data set
and they return an answer.
NLP is commonly used
when it comes
to interpreting free text
and making it more analyzable.
There is a massive amount
of information stored
in free text,
like medical records
and much more.
Before the introduction
of deep learning-based
natural language processing models,
this information stored
in textual data
was inaccessible
to those computer-assisted analyses,
so this information
could not be analyzed
in any type of systematic way.
However,
natural language processing
allows data scientists
to easily sift
through these massive troves
of free textual data
to obtain relevant information
from those files
otherwise inaccessible.
Another primary use
of natural language processing
is sentiment analysis.
Using sentiment analysis,
data analysts
can easily assess
hundreds of comments
posted on social media
to gain better insight
into their business'
brand performance.
For instance,
sentiment analysis
is very important
when it comes to
reviewing notes
obtained by customer service teams
to help them identify
those areas
where certain businesses
perform better.
Google,
as well as
every other search engine,
in fact,
base their machine learning
translation technology
purely on
natural language processing models.
Using this technology,
the method enables them
to develop accurate NLP algorithms
which read text
on a web page,
then interpret its meaning
and translate
to other languages.
Natural language processing
is a way for machines
and computers
to analyze,
understand,
and then derive meaning
from spoken human language
in a useful and smart way.
By utilizing natural language
processing techniques
and methods,
data scientists
can easily organize
as well as structure
gained knowledge
to easily perform
an assortment of tasks
such as translation,
automatic text summarization,
named entity recognition,
sentiment analysis,
speech recognition,
relationship extraction,
as well as topic segmentation.
Apart from those
common word processor operations
which treat textual data
as a sequence
of different symbols,
natural language processing
considers them
as the hierarchical structure
of the human language
in which several words
make phrases,
phrases make sentences,
and sentences ultimately
convey a variety of ideas.
By analyzing human language
for its fundamental meaning,
natural language processing
techniques and streams
have long filled
important roles
like correcting grammar,
automatic translation
between different languages,
and conveying speech
to textual data.
Natural language processing
methods, systems,
and techniques
are used to analyze
textual data,
allowing machines
and computers
to fully understand
the way of how humans speak.
Natural language processing
is also commonly used
in automatic question answering,
machine translation,
and text mining.
In fact,
NLP is characterized
as a challenge problem
in the broad field
of computer science
as human language
is plainly spoken
and it is rarely precise.
To fully understand
spoken human language,
you must understand
not only the words
but also the relationship
between words
and the concepts
to create meaning.
Therefore,
despite the fact
human language
is without a doubt
one of the easiest things
for humans
to understand and learn,
the great ambiguity
of human language
is what makes NLP
very difficult
and problematic
for machines
and computers
to master.
NLP methodologies
and techniques
help machines
and computers
communicate with humans
in their own scales
and languages.
For instance,
natural language processing
techniques
make it possible
for machines
to hear any speech,
read text,
interpret it,
measure text sentiment,
and then determine
which parts of textual data
are relevant
and important
while working
on large volumes
of textual data,
accurately delivering
amazing results.
Today's computers
and machines
can easily
and accurately
analyze more language-based
textual data
than the humans
leaving it.
They can do it consistently
without any fatigue problems
and in an unbiased way.
Considering the amazing amounts
of unstructured data present
which is generated
every day
from social media
to medical records,
this automation
is critical
when it comes
to fully understanding,
analyzing speech,
and text data
efficiently
and accurately.
Natural language processing
tools are also
very important
when it comes
to structuring
highly unstructured
data sources.
As you know,
the human language
is very diverse
and astonishingly complex.
We commonly
express ourselves
in many infinite ways
both in writing
and verbally.
Not only
are there hundreds
of different dialects
and languages,
but there are also
unique collections
of grammar
and syntax rules
within every language
and dialect.
When people write,
they sometimes
abbreviate
or misspell
some words.
They also sometimes
omit punctuation.
When we speak,
we speak differently
due to our regional accents.
We sometimes
shudder or mumble
when we speak,
and we commonly
borrow a few terms
from other languages.
While unsupervised
and supervised
deep learning techniques
are commonly used
for modeling
and interpreting
the spoken human language,
there is also
a great need
for systematic
and syntactic
understanding
as well as
domain expertise.
These are not
necessarily present
in those traditional
machine learning
methods and techniques.
Natural language processing
is of great importance,
especially today
when we have
massive amounts
of unstructured data.
It is important
as it helps us
to resolve
those ambiguities
in language.
Natural language processing
also adds
useful numeric structure
to the data
we obtain
for countless
downstream applications
like text analytics
or speech recognition.
How natural language
processing works
currently approaches
to natural language
processing
are based purely
on deep learning,
which is an
artificial intelligence
field that examines
and uses patterns
present in data
to improve
a computer understanding.
Deep learning models
require huge amounts
of labeled data
to train
as well as identify
those relevant
and important
data correlations.
Assembling this type
of big data set
is the main hurdle
of natural language
processing.
Those early approaches
to natural language
processing involved
those more
rules-based approaches
where simpler
machine learning models
were told what phrases
and words to look for
in text providing
certain responses
when those words
and phrases appeared.
However,
deep learning
is a more intuitive
and more flexible
approach in which
the algorithms learn
to identify speakers' intent
from many different
examples just like
how a child learns
his or her native language.
natural language processing
breaks down those
elemental pieces
of human language.
NLP includes
many different approaches
and techniques
for analyzing
interpreting human language
ranging from
machine learning methods
from statistical methods
to rules-based
and algorithmic models.
tools.
To do practical
natural language processing
you need several different
approaches as the text
and voice-based data
commonly vary.
Basic natural language
processing tasks
break down human language
into elemental pieces
when trying to understand
those relationships
between the pieces
as well as to explore
how these pieces work
together to create
some meaning.
There are some
underlying tasks
marked higher level
natural language
capabilities
such as topic modeling
and discovery
context categorization
sediment analysis
text-to-speech
and speech-to-text conversion
context categorization
and machine translation.
Context categorization
is a linguistic-based
summary of documents
that includes indexing
search
duplication detection
and content alerts.
Topic modeling
and discovery
involves accurately
capturing the themes
and meanings
into text collections
and applying advanced
analytics to textual data
like forecasting
and text optimization.
Natural language processing
is also used
for contextual extraction
which automatically
pulls structured information
from textual data.
NLP techniques
are used for sediment analysis
which identifies
the subject options
and moods
within large textual data
including opinion mining
and advantaged
sediment analysis.
In addition,
NLP is used
for transforming
text commands
into voice
and vice versa
into text-to-speech
and speech-to-text conversion.
With natural language processing
you can perform
document summarization
which automatically
generates different
synopses
of large bodies
of textual data.
NLP is also used
for machine translation
which is automatic
translation of speech-to-text
from one language
to another.
In these cases
we have just been discussing
they are all instances
where natural language
processing methods
can be used
to take raw language input
then algorithms
and linguistics
are applied
to enrich
or transform
the textual data
in such a way
to bring greater value.
Natural language processing
applications
Natural language processing
in fact
goes hand-in-hand
with text analytics
that groups,
counts,
and categorizes words
to extract relevant structure
and meaning
from large volumes
of textual data.
Text analytics
is mainly used
for exploring
large textual content
and for deriving
new variables
to raw textual data
which may be filtered,
visualized,
or used
as input
to various statistical
and predictive models.
Text analytics
and natural language
processing
together
are used
for many applications
that include
subject matter expertise
which is classified
textual context
into some meaningful topic
for discovering trends
and tracking actions.
NLP and text analytics
are involved also
in investigative discovering
used for identifying clues
and patterns
for detecting
and solving crimes
and social media analytics
that tracks sediment
and awareness
about different topics
and identifies
key influencers.
Natural language processing
models
are commonly based
on deep learning
and machine learning algorithms.
Instead of relying
on those traditional
hand-coding
large data sets
of rules,
natural language processing
can rely on deep learning
and machine learning techniques
to automatically detect
and learn those rules
just by analyzing
numerous sets of examples.
Natural language processing
models
are commonly based
on deep learning
and machine learning algorithms.
Instead of relying
on those traditional
hand-coding
large data sets
of rules,
natural language processing
can rely on deep learning
and machine learning techniques
to automatically detect
and learn rules
just by analyzing
numerous sets of examples.
Analyzing these examples,
NLP can easily make
a statistical inference.
Here,
more data analyzed
means more accurate models.
Developers can use
natural language processing models
for summarizing blocks of text.
Using Summarizer,
it is very easy
to extract
the most important
central idea
from the textual data
while ignoring
that irrelevant information.
Using NLP,
developers can also
create a chat box,
automatically generate
keywords tags
using auto-tag
that leverages LDA
that is a technique
for discovering topics
that are contained
within the text body.
Developers with NLP
can identify
the kind of entity
extracted
such as it being
a place,
person,
or organization
using methods
named entity recognition.
Developers also
commonly use
NLP sediment analysis
to identify
certain sediments
of strings
of textual data
from neutral
to negative.
With NLP,
it is very easy
to reduce words
to their stems
or roots
with Porter Stemmer
or
to break up
textual data
into tokens
using Tokenizer.
There are also
open source
natural language
processing libraries
like NLTK,
which is a Python
library that provides
various modules
for tokenizing,
stemming,
tagging,
classifying,
and analyzing text.
We will be using
NLTK in this book.
Natural language
processing used
to be very difficult,
but
with improvement
in artificial intelligence
and deep learning models
can effectively
interpret NLP problems.
These improvements
have implications
for both NLP methods
and types of data
which can be analyzed.
As more and more
pieces of information
are created
on the Internet
every day,
more and more
NLP techniques
are introduced
to analyze it online.
In fact,
advances in
natural language processing
make it possible
to analyze
and learn
from that greater range
of different data sources,
helping businesses
and companies
to enhance
their performance.
Therefore,
stepping deeper
into this world
of natural language
processing
is a greater idea
for everyone
who wants to expand
their processing capabilities
and learn more
and learn more about data.
Chapter 1
Text Pre-Processing
According to the latest
industry studies,
only around 21%
of the data available
to us
is present
in the entirely
structured form.
As you know,
data is generated
always and all the times.
as we tweet,
as we speak,
as we write,
as we send messages
and through the countless
other activities
we connect
and join
into online
every day.
However,
a majority of this data
is present
in the textual form
which makes it
a highly unstructured form.
Several more
prominent examples
of unstructured data
include our posts
on social media,
our chat conversations,
articles,
news blogs,
patient records
and other textual data
generated every day.
Despite having
this high proportion
of data,
the information
within the data
is not accessible
directly unless
it is processed.
It must be understood
manually
or analyzed
by an automatic system
before it can be available.
To produce
some actionable
and significant insights
for textual data,
it is very important
to get deeper
into natural language
processing techniques.
As you already know,
natural language processing
consists of
systematic processes
for data analyzing
and deriving
valuable information
from the textual data
in an efficient
and smart way.
By utilizing
natural language processing,
you can easily organize
the massive amount
of textual data,
perform various
automated tasks
as well as solve
a wide range
of different problems
like machine translation,
automatic text summarization,
name entity recognition,
sentiment analysis,
topic segmentation,
relationship extraction,
and many others.
Further in the book,
we are going to use
Python's
open source library
NLTK
that comes with
many language
processing modules
which will help us
perform various tasks.
Therefore,
to start,
you must install
and import
NLTK data
as follows.
Pseudo
easy underscore
install
PIP
next line
Pseudo
PIP
install
dash
U
NLTK
NLTK
NLTK
Here,
we installed
PIP
and run
in terminal
and we also
installed
NLTK
and run
in terminal.
After this is done,
you must download
NLTK data
and run
your Python shell
also in terminal
as follows.
Import
NLTK
NLTK dot
download
open parenthesis
close parenthesis.
Once you download
NLTK data,
you must follow
your screen instructions
and download
the desired collection
or package.
Other Python libraries
can be easily
installed using PIP.
Noise removal
As you already know,
text comes in the form
of unstructured data
obtained from
the available data.
Therefore,
there are different types
of noise present
in the data
we must get rid of.
Before text
or data
pre-processing,
data must be
accurately analyzable.
Therefore,
we must perform
text pre-processing
which is the process
of standardization
and cleaning of text
making it
completely noise-free
and completely ready
for further analysis.
Text pre-processing
involves three main steps
including noise removal,
lexicon normalization
and object standardization.
Therefore,
the initial process
starts with some
raw text
we just obtained.
Then,
during text pre-processing
you must perform
noisy entities removal,
word normalization
and word standardization
to get clean text
completely free of noise.
With noise removal,
you can easily
clean up
any piece of text
which is not relevant
to the data content.
For instance,
language stop words,
links,
URLs,
various social media entities,
industry-specific words
and punctuations
are commonly considered
as noise
as they are not relevant
to the data content.
During noise removal process,
you deal with the removal
of all types
of these noisy entities
that are present
in the text.
The most common approach
for noise removal
is to prepare your dictionary
of noisy entities
and then iterate
the text objects
by words or token.
You can then easily
eliminate those tokens
that are present
in your noisy dictionary.
We are going to use tokens
in this book a lot.
Tokens are entities
or words present
in the textual data
while tokenization
is the process
of converting some text
into different tokens.
For noise removal
in Python,
you must run the code
as follows.
with your noise list dictionary
including non-relevant tokens
or word entities.
Noise underscore list
equals
open bracket
double quote
a-n
double quote
comma
double quote
a
double quote
comma
double quote
the
double quote
comma
double quote
dot dot dot
double quote
close bracket.
Def underscore remove
underscore noise
open parentheses
input underscore text
close parentheses
colon
words equal
input underscore text
dot split
open parentheses
close parentheses
noise underscore
free words
equals
open bracket
w
space
ord
for word
in words
or
if word not in noise
underscore list
close bracket
noise underscore
free
underscore text
equals
double quote
space
double quote
dot
join
open parentheses
noise
underscore
free
underscore
words
close parentheses
return
noise
underscore
free
underscore
text
underscore
remove
underscore
noise
open parentheses
double quote
your sample text
double quote
close parentheses
greater than greater than greater than double quote your sample text double quote
there is also another approach to noise removal this approach includes the regular expression
as you deal with certain patterns of noise following this python code you can easily
remove a regex pattern from any input text input space input re def underscore remove
regex open parenthesis input underscore text comma regex underscore pattern close parenthesis
colon urls equal re dot f i n d i t e r open parenthesis regex underscore pattern comma
input underscore text close parenthesis for i in urls colon input underscore text equals
re dot sub open parenthesis i dot group open parenthesis close parenthesis dot strip open parenthesis
close parenthesis comma single quote single quote comma input underscore text close parenthesis
return input underscore text regex underscore pattern equals double quote pound sign open bracket
backslash w backslash w close bracket asterisk double quote underscore remove underscore regex open parenthesis
double quote remove this pound sign hashtag pound sign hashtag from sample text double quote comma regex underscore pattern
close parenthesis close parenthesis greater than greater than greater than greater than double quote
remove this from sample text double quote besides noise removal for the text pre-processing
we also use lexicon normalization another common type of noise present in textual data concerns multiple representations
which are exhibited by just single words for example swim swimming and swimmer are only different variations
of the same word which is swim even though they mean different things they are contextually very similar
performing lexicon normalization you convert all these disparities of words into their lemma or normalized forms
text normalization in fact is a fundamental step for text engineering as it helps you convert
those high dimensional features to a low dimensional space with only one feature instead of several
different features which is an ideal for any deep learning model the most commonly used lexicon
normalization techniques include lemmatization and stemming lemmatization is step-by-step processing of
obtaining the stem or the root of the words in addition to words words morphological analysis
stemming is a rule-based process of stemming suffix from words like s ing s etc to perform stemming and
lemmatization with nltk you must run the following code from mltk dot stem dot wordnet import wordnet lemmatizer
next line lem equals wordnet lemmatizer open parenthesis close parenthesis
from mltk dot stem dot porter import porter stem next line stem equals porter stem open parenthesis close parenthesis
next line
next line
another common step for text preprocessing is object standardization as you know text preprocessing is object standardization
as you know textual data commonly contains phrases and words that are not present in those standard lexical dictionaries
therefore these unstandardized pieces are not recognized by search models and engines
some examples are hashtags with attached words acronyms or colloquial slangs
thanks to the help of manually prepared data dictionaries and regular expressions these kinds of noise can be easily fixed
you can also use dictionary lookup methods when you want to replace social media slang to standardized text as follows
lookup underscore dict equals open squiggly bracket single quote rt single quote colon single quote retweet single quote comma single quote dm single quote colon single quote direct message single quote comma double quote
double quote l u v double quote colon double quote awsm double quote comma double quote def underscore lookup underscore words open parenthesis input underscore text
close parenthesis colon words equal input underscore text dot split open parenthesis close parenthesis
нак
kuin
next line
word in words
if word dot lower
ca
open parenthesis close parenthesis in lookup underscore setzt colon word equals lookup underscore
di
kick
op
bracket
c
new words dot append open parenthesis word close parenthesis new underscore text equals double quote double quote dot join open parenthesis new underscore words close parenthesis
next line return new underscore text underscore lookup underscore words open parenthesis double quote rt this is a retweeted tweet double quote close parenthesis double quote retweet this is a retweeted tweet close parenthesis double quote
apart from these common text preprocessing techniques there are some other techniques used such as grammar checker encoding decoding noise and spelling correction however these three text preprocessing methods are the most commonly used when dealing with unstructured data
chapter two feature engineering to analyze preprocessed text data you must convert that data into features depending on the text usage your text features are constructed using several different methods such as engrams syntactual parsing entities word embedding statistical features and word based features synthetic parsing
syntactic parsing involves the complete analysis of words contained in sentences for their arrangement and grammar in a way which shows the specific relationship between the words one of the most important attributes of text syntactic are part of speech tags and dependency grammar
sentences are composed of different words that are sown together the specific relationship between these words in sentences is purely determined by the dependency grammar which is a class of syntactic text analysis dealing with relations between two words
every relationship can be easily represented in the form of a dependent relation and governor the relationship between different words is a grammar dependency tree
the tree the tree you get representing different relationships also shows you the root word for every sentence that is linked with other object subtrees every subtree itself is a dependency tree with specific relationships as well
this tree when you parse it recursively gives you grammar triplets as the output that you can use for feature extractions for various natural language processing problems like text classification entity wise
entity wise sediment analysis or actor and entity identification you can use NLTK dependency grammar to generate your dependency tree
part of speech tagging in addition to the grammar relationships every word within a sentence is also associated with a pass or part of speech tag including adverb noun or adjective the part of speech tagging defines the function and usage of the word and sentences
following this coding following this coding you can use the NLTK to perform post tagging annotations on any input text the NLTK has several different implementations but the default is set on perceptron tagger from NLTK import word underscore tokenize comma POS underscore tag text equals double quote I am learning natural language processing
natural language processing double quote Token equals word underscore tokenize open parentheses text close parentheses print POS underscore tag open parentheses tokens close parentheses open bracket open parentheses single quote I single quote comma single quote PRP single quote close parentheses comma
end criterium
open parenthesis
single quote
N single quote
single quote
single quote
comma
single quote
VB P single quote
close parentheses
comma
confusing
usstQuery
single quote
learning
single quote
comma
single quote
VBG
single quote
closed parentheses
single quote
open parenthesis
Single quote
natural
single quote
comma
single quote, NNP
single quote, close parenthesis
single quote,
open parenthesis
single quote, language
single quote, comma
single quote, NNP
single quote, close parenthesis
comma
open parenthesis
single quote, processing
single quote, comma
single quote, NNP
single quote, close bracket
Part of speech tagging is commonly used for several natural language processing purposes like
Improving the world-based feature, word sense disambiguation, limitization and normalization or efficient stop word removal
Part of speech tagging is very useful when it comes to word sense disambiguation
As you know, some language words regularly have multiple meanings based on their usage
So we use POS for efficient word sense disambiguation
Part of speech is also used for improving word-based features
A natural language processing learning model can easily learn different contexts of words when they used words as the features
However, when you use part of speech linked with words, you preserve the words context making stronger features
POS tags are also the fundamental step of limitization
Which is the process of converting words into their base form also known as lemma
POS tagging is also very useful when it comes to removing stop words efficiently
Entity Extraction
As you already know, entities are defined as the most important part of sentences
Like verb phrases or noun phrases
Entity detection language, processing algorithms are usually enabled models or dictionary lookups
Rules-based parsing, dependency and POS tagging parsing
The main applicability of entity detection algorithms is seen in content analyzer, chatbots and consumer insights
The main entity detection method include named entity recognition and topic modeling
The overall process of detecting the name entity like location names, person names, company names and other forms of textual data is called named entity recognition or simply NER
The most common name entity recognition models have three blocks including entity disambiguation
Phrase classification and noun phrase identification
Noun phrase identification is dealing with extracting all the noun phrases from the text using part of speech tagging and dependency parsing
Phrase classification is the main classification step in which all obtained and extracted noun phrases are carefully classified into correct categories such as names or locations
For instance, Google Maps API has a great tool for disambiguating locations
You can also use the open database from Wikipedia to identify company or person names
Apart from this, you can easily create your own lookup dictionaries and tables just by combining information from several sources
Entity disambiguation is also named entity recognition block
Sometimes entities are misclassified so creating a validation layer on top of your results is very useful
Using knowledge of graphs can be easily exploited for this step
Therefore, for entity disambiguation, you can use other popular knowledge graphs such as IBM Watson, Google Knowledge Graph or Wikipedia
Topic modeling
Topic modeling is an important process of automatically identifying all topics present in a given corpus or a collection of texts
Topic modeling derives many hidden patterns within the words in any quantity of text in a purely unsupervised manner
Topics, in fact, are defined as those concurring or those repeating patterns in a body of text
The most popular topic modeling methods are Latent Dirichlet association or LDA
You can easily implement LDA in Python by using the following code
Next Line
Dock 1
Equals
Double quote
Sample
Text
1
Double quote
Next Line
Dock 2
Equals
Double quote
Next Line
Dock 3
Equals
Double quote
Sample
Text
3
Double Quote
Next Line
Next line, doc underscore complete equals
Next line, doc underscore clean equals
GenSim from GenSim. Next line, import corpora.
After that, you must create the term dictionary of your text corpus in which unique terms will be assigned an index.
Then you must convert your list of documents into your document term matrix.
Dictionary equals corpora dot dictionary open parentheses doc underscore clean
close parentheses doc underscore term underscore matrix equals
open bracket dictionary dot doc to bow open parentheses doc
close parentheses for doc in doc underscore clean close bracket
The next step is to create an object with your LDA model with GenSim library as follows
LDA equals GenSim dot models dot LDA model dot LDA model
Then you can run and train your LDA model on your document term matrix and finally print your results
LDA model equals LDA open parentheses doc underscore term underscore matrix comma
num underscore topics equals three comma
ID to word equals dictionary comma passes equal 50 close parentheses print open parentheses LDA model dot print underscore topics
close parentheses close parentheses close parentheses close parentheses
N-grams
N-grams are combinations of N-words. N-grams are generally more informative as features than N-word features.
In addition, biograms in which N is equal to 2 are commonly the most important features of all others.
You can create your biogram on any text as follows.
Def generate underscore N-grams
open parentheses text comma N
close parentheses colon
words equal text dot split
open parentheses close parentheses
output equals
open bracket close bracket
close bracket
for I in range
open parentheses
len
open parentheses
words
close parentheses
minus N plus 1
close parentheses
colon
output dot
append
open parentheses
words
open bracket
I
colon
I
plus N
close bracket
close parentheses
return output
generate underscore N-grams
open parentheses
single quote
single quote
saving
yourivalismusample seal
single quote
common
two
close
open bracket
open bracket
open bracket
sch
rele-quote
Dit
may
tracking
common
couple
single
Comma, single quote, text, single quote, close bracket, close bracket.
Text Data Statistical Features
Textual data can be quantified directly into numbers thanks to several different techniques, such as inverse document frequency or IDF, term frequency or TF.
Inverse document frequency and term frequency are weighted models, which are commonly used for different types of information retrieval tasks.
Inverse document frequency and term frequency aim to convert the textual documents into certain vector models obtained based on the concurrence of specific words in textual documents without taking any consideration of the exact word ordering.
For instance, if you have a data set of n textual documents, your term frequency is defined as the count of term T in your document sample,
while IDF is defined as algorithm ratio of total documents you have available in your corpus and the number of all documents in your corpus, which have your term T.
Your term frequency and inverse document frequency formula can give you the relative importance of the terms in your text corpus, using the following code.
You can easily convert your text into term frequency and inverse document frequency using scikit-learn Python package.
From sklearn.feature underscore extraction dot text import tfidf vectorizer
Next line, object equals tfidf vectorizer open parenthesis close parenthesis.
Next line, corpus equals, this is your sample document dot, single quote, comma, single quote, comma, single quote, comma, single quote, comma, single quote, your third sample document text, single quote, close bracket.
Next line, print x, next line, open parenthesis, 0 comma, 1, close parenthesis, 0 point 3 4 5 2 0 5 0 1 6 8 6 5, next line, open parenthesis,
open parenthesis, 0 point 3 4 5 2 0 0 0 0 1 6 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0. 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0
two comma four close parentheses zero point four four four five one four three one one five three
seven once you have run the code your model will create a vocabulary dictionary as well as assign
indexes to every word in your text sample every row in your output will contain a tf.idf value
and a tuple of words in the index in your document you can also use density or count readability
features in your analysis these features may seem not so important but they show an amazing
impact on the natural language processing learning models some of the most important density and count
features include punctuation counts word count industry specific word counts and sentence count
there are also other types of measurement for readability such as flesh reading ease syllable
counts and smog index to create these features you must refer to text that library word embedding
word betting is the way of representing words as vectors the main goal of word embedding is to
redefine those high dimensional word features into low dimensional feature vectors as you preserve that
contextual similarity present in the text corpus word embedding or text vectors are commonly used in deep
learning models as recurrent neural networks and convolutional neural networks two of the most popular
methods of creating text word embedding include glow v and word2vec these models take corpus and produce
certain word vectors as outputs word2vec is a commonly used model for word embedding composed of an already
pre-processed module which is a shallow neural network named continuous bags of words there is also another
shallow network called skipgram these models are commonly used for numerous natural language processing
problems as they can easily construct the vocabulary from every training corpus and then learn all word
embedding representations implementing the following code you can use gensim package to perform word
embedding as the vectors from gensim dot models import word2vec sentence equals open bracket open bracket
single quote natural single quote comma single quote language single quote comma single quote processing single
quote quote quote quote quote quote quote quote quote close bracket close bracket next you must train your
model on the text corpus you obtained model equals word2vec open parentheses sentence comma min underscore count
equals one close parentheses print model dot similarity open parentheses comma open parentheses single quote natural single quote
comma single quote language single quote language single quote processing single quote close
parentheses 0.11222489293 next line print model open bracket single quote learning single quote close bracket
bracket array open parentheses open bracket 0.00459356 space 0.00303564 dash 0.00467622 space 0.00209638 comma space dot dot dot dot dot space
close bracket close bracket close parentheses you can use word embedding as your feature vector for countless
deep learning models to measure text similarities using text clarification similarity and word clustering techniques
chapter three text classification text classification is one of the most common neural language processing problems
some of the most common text classification problems include sediment classification
email spam identification organization of website by search engines and topic classification of news
text object sentences or documents in one of many fixed categories
text classification is of great importance when you are dealing with massive amounts of data
especially when it comes to information organizing information filtering and information storage
the most well-known natural language processing classifier consists of prediction and training
first in the training process your text input is processed and all text features are created
then once your deep learning model acquires text features it uses them to make predictions when
compared to new text that gets inputted you can build a natural language processing
classifier using a naive base classifier from the text blob library which is built on top of nltk
text blob is a very easy to use natural language processing tool api commonly used in many nlp problems
from text blob dot classifier import naive base classifier as nbc from text blob
import text blob training underscore corpus equals open bracket open parenthesis single quote text one
single quote comma single quote class underscore b single quote close parentheses comma single quote
casino quote
single quote
Class  Kurt
Class  Kurt
Class Finger
Class
A
Class
A
Class
A
Class
A
Class
A
A-S-A-A-S-A-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T-E-T
8 single quote comma SIGLE QUOTE CLASS UNDER SCORE A
SIGLE QUOTE CLOSE PARENTHESESCOMMA
OPEN PARENTHES DOUBLE-QUOTE TEXT NINE
DOUBLE-QUOTE SAMPLE TEXT NINE
DOUBLE-QUOTE COMMA SIGLE QUOTE CLASS UNDER SCORE A
SIGLE QUOTE CLOSE PARENTHESES SIGLE QUOTE
OPEN-PARENTHESES SIGLE QUOTE TEXT
9 comma single quote class underscore B single quote close parentheses close bracket next line
test underscore corpus equals open bracket open parentheses double quote text sample text double
quote comma single quote class underscore B single quote close parentheses comma open parentheses
double quote sample text double quote comma single quote class underscore a single quote close
parentheses comma open parentheses double quote sample text double quote comma single quote class
underscore B single quote close parentheses comma open parentheses single quote sample text
single quote comma single quote class underscore a single quote close parentheses comma open
parentheses single quote sample text single quote comma single quote class underscore b
single quote close parentheses close bracket model equals NBC open parentheses training
underscore corpus, close parenthesis, print, open parenthesis, model dot classifier, open
parenthesis, double quote, sample text, double quote, close parenthesis, close parenthesis.
Next line, double quote, class underscore A, double quote, next line, print, open parenthesis,
open parenthesis, double quote, sample, text, double quote, close parenthesis, close parenthesis.
Next line, double quote, class underscore B, double quote, next line, print, open parenthesis,
model dot accuracy, open parenthesis, test, underscore corpus, close parenthesis, close parenthesis,
0.83.
Python's sci-learn library also provides a pipeline natural language processing framework you can
use for text classification as follows.
From sklearn.feature underscore extraction dot text import tfidf vectorize from sklearn.metrics
import classification underscore report from sci-learn import svm.
Once done, you must prepare your data for svm model using the same text corpus and training
corpus from the naive Bayes example from above.
Open parenthesis, Bayes example, close parenthesis, next line, train underscore data equals open
bracket, close bracket, next line, train underscore labels equals open bracket, close bracket, next
line, for now in training underscore corpus, colon, train underscore data dot append, open parenthesis,
test underscore labels equals open bracket, close bracket, close parenthesis, test underscore data equals open
bracket, close bracket, next line, test underscore labels equals open bracket, close bracket.
For now in test underscore corpus, colon, next line, test underscore data dot append, open parenthesis, row, open bracket,
open bracket, zero, close bracket, close parenthesis, test underscore label dot append, open parenthesis, row, open bracket, one, close bracket, close parenthesis.
After that is complete, you must create and train your feature vectors.
Vectorizer equals tfidf vectorizer, open parenthesis, min underscore df equals four, comma, max underscore df equals 0.9, close parenthesis.
Train underscore vectors.
Train underscore vectors equal vectorize dot fit underscore transform open parenthesis, train underscore data, close parenthesis.
The next step is to apply a model on your test data and perform classification with SVM.
Then you can print your results.
Test underscore vectors equal vectorize dot transform open parenthesis, test underscore data, close parenthesis,
model equals SVM dot SVC, open parenthesis, kernel equals, single quote, linear, single quote, close parenthesis,
ізE with open parenthesis, model dot fit, open parenthesis, train underscore vectors, comma, train underscore,
labels, close parenthesis, prediction equals, model dot predict, open parenthesis, test underscore vectors, close parenthesis,
Next line.
The text classification model like these are dependent upon the quantity and quality of your model features.
While applying deep learning models, it is always a good idea to include more training data.
You can always improve your natural language processing classifier using several different techniques like text similarity, matching, or coherence resolution.
Text matching.
One of the most important areas of natural language processing is the matching of several text objects to find similarities between them.
Some of the most important text matching or text similarity applications include data deduplication, automatic spelling, correction, and genome analysis.
There are several text matching techniques available like Levenstein Distant, Phonetic Matching, Cosine Similarity, and Flex String Matching.
The method you choose to use depends upon your requirements.
For learning instance, Levenstein Distance Method represents the distance between two strings.
This is defined as the minimum number of edits required for transforming one string into another.
It also includes the number of allowable edit operations including substitution, insertion, or deletion of a single character.
Levenstein Distance.
You can easily implement Levenstein Distance to perform efficient memory calculations as follows.
DEF.
DEF.
LEVENSTEIN
S1, S2
If
LEN
S1
LEN
S2
S1, S2
S2
S2
S1
S1
S1
S2
S1
S2
S2
S2
S1
S2
S2
S1
S1
S1
S1
S2
S1
S2
S1
S1
S2 S1
S1 S1 S1 S2
For index 1, comma, char 1 in enumerated, open parenthesis, s1, close parenthesis, colon,
if char 1 equals equals char 2, colon, new distances dot append, open parenthesis, distances, open bracket, index, 1, close bracket, close parenthesis.
Next line, or, colon.
New distances dot append, open parenthesis, 1 plus min, open parenthesis, open parenthesis, distances, open bracket, index 1, close bracket, comma, distances, open bracket, index 1, plus 1, close bracket, comma, distances equal, new distances.
Next line, return distances, open bracket, dash 1, close bracket.
Next line, print, open parenthesis, Levenstein, open parenthesis, double quote, analyze, double quote, comma, double quote, analyze, double quote, close parenthesis, close parenthesis.
Phonetic matching.
Another commonly used text matching method is phonetic matching.
A typical phonetic matching model takes keywords such as location name or person's name as input and produces character strings, which identify collections of words that are phonetically similar.
Phonetic matching is a very useful technique when it comes to searching through large text kippurah, matching relevant names and correcting spelling errors.
Two main phonetic matching algorithms are Metaphone and Soundex.
You can also use the popular Python module, fuzzy, to complete certain Soundex strings for words as follows.
Import fuzzy.
Next line, Soundex equals fuzzy dot Soundex, open parenthesis, 4, close parenthesis.
Next line, print Soundex, open parenthesis, single quote, A-N-K-I-T, single quote, close parenthesis.
Next line, double quote, A-N-K-I-T, single quote, close parenthesis.
Next line, double quote, A-N-K-I-T, double quote, A-N-K-I-T, double quote, A-N-K-I-T, single quote, close parenthesis.
Flexible string matching.
Flexible string matching is another commonly used text matching technique.
A complete text matching system always includes different models that are pipelined together to compute a variety of different text corpora.
Here those regular expressions are very useful.
Other common text matching techniques include limitization matching, extract matching, and compact matching which processes punctuation, slangs, and spaces.
When you have text represented as a vector notation, you must use a text matching technique named cosine similarity to measure those vectorized similarities.
You can easily convert your text to vectors using term frequency and apply cosine similarities between the two text pieces you obtained as follows.
Import math from collection import counter.
Next line, def get underscore cosine, open parenthesis, VEC1, comma, VEC2, close parenthesis, colon,
common equals set
open parenthesis
VEC1 dot keys
open parenthesis
closed parenthesis
ampersand
set
open parenthesis
VEC2 dot keys
open parenthesis
closed parenthesis
close parenthesis
Numerator equals
sum
open bracket
VEC1
open bracket
asterisk
VEC2
open bracket
closed bracket
closed parenthesis
next line
sum equals sum
open parenthesis
open bracket
VEC1
open bracket
asterisk
asterisk
2 for x
in VEC1
dot
keys
open parenthesis
closed parenthesis
closed bracket
closed parenthesis
next line
sum2
equals sum
open parenthesis
open bracket
VEC2
open bracket
X
closed bracket
def get
def get
underscore
cos
open parenthesis
VEC1
comma
VEC2
closed parenthesis
colon
common equals
set
open parenthesis
VEC1
dot
keys
open parenthesis
close parenthesis
close parenthesis
ampersand
set
open parenthesis
VEC2
dot
VEC2
dot
keys
open parenthesis
Next line.
Numerator equals sum
Next line.
Sum 1 equals sum
Next line.
Sum 2 equals sum
Next line.
Denominator equals math dot sqrt open parenthesis sum 1 close parenthesis asterisk math dot sqrt open parenthesis sum 2 close parenthesis.
Next line.
If not denominator colon.
Next line.
Return 0 dot 0.
Next line.
Or colon.
Next line.
Return float open parenthesis numerator close parenthesis forward slash denominator.
Next line.
Def text underscore to underscore vector open parenthesis text close parenthesis colon.
Next line.
Words equal text dot split open parenthesis close parenthesis.
Next line.
Return counter open parenthesis words close parenthesis.
Next line.
Text 1 equals single quote learning natural language process.
Single quote.
Next line.
Text equals single quote natural language processing in python guide.
Single quote.
Next line.
Next line.
Vector 1 equals text underscore to underscore vector open parenthesis text 1 close parenthesis.
Next line.
Vector 2 equals text underscore to underscore vector open parenthesis 2 close parenthesis.
Next line.
Cosine equals get underscore cosine.
Open parenthesis.
Open parenthesis.
Vector 1 comma.
Vector 2 close parenthesis 0.62.
Coreference resolution.
Another common text matching method is coreference resolution, which is a process of finding rational links between the words within sentences in textual data.
In fact, conference resolution is the component of natural language processing, which does the job of finding relations between the words automatically.
Hence, it is commonly used in question answering, document summarization, and information extraction.
For commercial purposes, there is a Python wrapper provided by Stanford Core NLP in which you can practice conference resolution on your text data.
Chapter 4.
Working with language data using NLTK.
As already mentioned in the book, text-based communication has become one of the most common expression forms we use for communicating with each other.
We text message, post on our social media platforms, we email daily.
Because of this text-based communication, unstructured data has become very common, and analyzing great quantities of unstructured data has become a key way of understanding what people think.
For instance, reviews on Amazon help users in making decisions about purchasing products, tweets on Twitter help people find trending news topics, etc.
These examples of structured knowledge gained from textual data represents the main task of natural language processing.
This field of computer science focuses on the relationship and interaction between humans and computers while natural language processing methods and techniques help us analyze text.
Thus, by utilizing NLP, we can provide new ways for computers to understand our human language.
Later in the book, we are going to use Python's very powerful natural language processing toolkit, NLTK, to solve some common natural language processing tasks.
Importing NLTK
The first step is to import the NLTK module.
Once you import it, make sure it is installed properly.
You can check its status by running the code as follows.
Python dash C double quote import NLTK double quote.
If you installed NLTK properly, this command will complete without any errors.
Once done with the NLTK installation, you must make sure you have the latest version 3.2.1.
You must install this version as it contains the NLTK Twitter package we are going to use later for text analysis.
Python dash C double quote import NLTK print open parenthesis NLTK dot underscore version underscore close parenthesis double quote.
If you did not install NLTK properly, you will receive the error message which indicates that NLTK is not installed as expected.
In this case, you can download the library we need using pip as follows.
Pip install NLTK
Downloading NLTK's tagger and data
In this section of the book, we are going to use an NLTK Twitter corpus you can download through NLTK.
We are going to work with Twitter samples.
Before we dig deeper into this problem, the first step is to download the Twitter corpus by running the following code on the command line.
Python dash M NLTK dot downloader Twitter underscore samples.
If you execute this command line properly, you will get the output as follows.
Open bracket NLTK underscore data
Close bracket
Downloading package Twitter underscore samples to
Next line
Open bracket
NLTK underscore data
Close bracket
Forward slash
Users
Forward slash
NLTK underscore data
Dot dot dot
Open bracket
NLTK underscore data
Close bracket
Close bracket
Unzipping
Corpa
Forward slash
Twitter
Underscore samples
Dot
Zip
Dot
The next step is to download the POS tagger or part of speech tagger.
As you already know, part of speech tagging is the process of labeling words in text data as it corresponds to a certain part of speech tagging such as adjectives, nouns, verbs, etc.
Here, we are going to use NLTK's POS average perceptron tagger which uses the perceptron algorithm to predict which part of speech tagging is most likely to occur.
You can download the NLTK tagger running the command as follows.
Python dash M NLTK dot downloader average underscore perceptron underscore tagger
If you execute the command properly, you will get the following outcome.
Open bracket
NLTK underscore data
Close bracket
Downloading package
Averaged
Underscore
Perceptron
Underscore
Tagger
2
Next line
Open bracket
NLTK
Underscore
Data
Close bracket
Forward slash
Users
Forward slash
NLTK
Underscore
Data
Unbracket
NLTK
Underscore
Data
Close bracket
Unzipping
Taggers
Forward slash
Averaged
Underscore
Perceptron
Underscore
Tagger
Dot
Zip
Dot
The next step is to double check if you downloaded the corpus correctly.
You can do it on your terminal as you open your Python interactive environment.
Once you import it, you must import the Twitter samples as follows.
Python
From NLTK
Dot
Corpus
Import
Twitter
Underscore
Samples
This NLTK
Twitter corpus contains a sample of 20,000 tweets that were received from the popular
Twitter streaming API.
Full tweets here are stored as JSON or line separated.
You can see how many JSON files in total there are in your Twitter corpus by running the
command as follows.
Twitter Underscore
Samples
Dot
Fields
Close
Parenthesis
To execute the command properly, you will get the following output.
Open
Bracket
You
Single Quote
Negative
Underscore
Tweets
Dot
JSON
Single Quote
Comma
You
Single Quote
Positive Underscore
Tweets
Dot
Jason
Single Quote
Comma
You
Single Quote
Tweets
Dot
20150430
Dash
223406
Dot
Jason
Single Quote
Close
Bracket
You can easily return the tweet strings by using the file IDs you obtained in the
previous step.
Twitter Underscore
Samples
Dot
Strings
Single Quote
Tweets
Dot
20150430
Dash
223406
Dot
Jason
Single Quote
Close
Parenthesis
Running this code, you will get output that looks like the following.
Open Bracket
You
Single Quote
RT
At
Kirk Cuss
Colon
Indirect Cost of the United Kingdom being in the European Union is estimated to be
the cost of Britain
Backslash
XA3170
Billion Per Year
Exclamation Point
Hashtag
Better Off
Out
Hashtag
UK
IP
Single Quote
Dot Dot Dot
Close Bracket
If you get this, you can move on as this means you downloaded the Twitter corpus properly.
Now you can exit your Python interactive environment.
Once done, you have access to your Twitter samples corpus and you can begin writing a code to process those imported tweets.
The main goal of your script here will be to find and count how many nouns and adjectives appear in the positive subset of your Twitter samples corpus.
Counting nouns can help you determine how many topics are being discussed, while counting adjectives can help you determine what kind of language is being used.
You can also extend your script to count all positive adjectives such as happy or great versus negative adjectives such as sad or lame that can be used when it comes to analyzing the sentiment of reviews or tweets about a product or a movie.
Tokenizing sentences
To tokenize sentences, you must create the script in the text editor of your choice.
You can call it nlp.py.
Then, in your file, you must import the Twitter corpus.
First, create your tweets variable to assign it to the list of strings from all the positive JSON file tweets as follows.
From nltk.corpus import twitter underscore samples tweets equal twitter underscore samples dot strings open parentheses single quote positive underscore tweets dot JSON single quote close parentheses.
When you load your first list of tweets, every tweet will be represented as one string.
Prior to determining which word in your tweets are nouns or adjectives, you must tokenize your sentences.
In this case, tokenize your tweets.
As you know by now, tokenization is the process of breaking up sequences of strings into smaller pieces like phrases, keywords, words, symbols, and other elements that are called tokens.
To tokenize your Twitter samples, you must create a new variable named tweets tokens.
To that you will assign all the tokenized lists of tweets as follows.
From nltk.corpus import twitter underscore samples tweets equal twitter underscore samples dot strings open parentheses single quote positive underscore tweets dot
json single quote close parentheses.
Next line, tweets underscore tokens equal twitter underscore samples dot tokenized open parentheses single quote positive underscore tweets dot json single quote close parentheses.
In fact, the new variable you have created contains all the elements in your Twitter list as a list of tokens.
Once you get the tokens from every tweet, you can tag those tokens with the proper part of speech tags.
Tagging Sentences.
To access the nltk's part of speech tagger, you must import it.
All parts of speech statements must go at the beginning of the code as follows.
From nltk.corpus import twitter underscore samples from nltk dot tags import pos underscore tag underscore sense.
tweets, tweets equal twitter underscore samples dot strings open parentheses single quote positive underscore tweets dot json single quote close parentheses,
tweets underscore tokens equal twitter underscore samples dot tokenized open parentheses single quote positive underscore tweets dot json single quote close parentheses.
Then you can tag every tweet token.
NLTK allows you to perform it all at once.
To do so, you must create a new variable named tweetsTag that you are going to use for storing your tagged list.
tweets underscore tagged equal pos underscore tag underscore cents
tweets underscore tokens close parentheses
Your first element in your tweet tagged variable will look like this
open bracket open parentheses you single quote hashtag follow Friday single quote comma single quote JJ single quote close parentheses comma
open parentheses you single quote at France underscore INTE single quote comma single quote NNP single quote close parentheses
close parentheses comma you single quote at P K U C H L Y 5 7 single quote comma single quote NNP single quote
end parentheses comma open parentheses close parentheses comma open parentheses you single quote at M I L I P O L underscore Paris single quote comma
single quote inches single quote quickly
in distance you single quote human being single quote comma single quote VBG single quote end parentheses comma
open parentheses you single quote being single quote comma single quote VBG single quote end parentheses comma
SINGLE QUOTE
open parenthesis, you, single quote, members, single quote, comma, single quote, nns, single quote, end parenthesis, comma, open parenthesis, you, single quote, in, single quote, end parenthesis, comma, single quote, open parenthesis, close parenthesis,
comma
open parenthesis
you
single quote
my
single quote
comma
single quote
PRP
dollar sign
single quote
close parenthesis
single quote
open parenthesis
you
single quote
community
single quote
comma
single quote
MN
single quote
close parenthesis
comma
open parenthesis
you
single quote
this
single quote
comma, single quote, DT, single quote, end parenthesis, close parenthesis, single quote,
open parenthesis, U, single quote, weak, single quote, comma, NN, single quote, close parenthesis,
single quote, open parenthesis, U, single quote, colon, close parenthesis, single quote, comma, single quote, NN, single quote, close parenthesis, close bracket.
Here you can see the tweets you import are characterized as a list representing every token you have information about as a part of speech tag.
Moreover, every token pair or tag is saved as a tuple.
In NLTK, adjectives are denoted as JJ, while singular nouns are denoted as NN and plural nouns are denoted as NNS.
In the next section, we are going to count only singular nouns.
Counting part of speech tags.
In this section of the book, we are going to count how many times adjectives JJ and singular nouns NN appear in our Twitter samples corpus.
We are going to keep track of how many times these parts of speech tags appear in the corpus.
To do this, we will be using a count variable whereby we are going to add every time we find a new part of speech tag.
To count part of speech tags, you must create your count variable set to zero from the beginning.
From NLTK dot corpus import twitter underscore samples from NLTK dot tag import POS underscore tag underscore sense.
Tweets equal twitter underscore samples dot strings open parenthesis single quote positive underscore tweets dot JSON single quote close parenthesis.
Next line, JJ underscore count equals zero.
Next line, NN underscore count equals zero.
Once done with creating the count variables, you must create two for loops.
The initial loop is used for iterating through every tweet contained in the list, while the second loop is used for iterating through every tag or token pair in every tweet.
For every tag pair, you must look up the tag with the proper tuple index.
Once completed, you must check if the part of speech tag matches the string NN or HH using conditional statements.
Once you find a matching tag, you must add plus equals one to the proper accumulator.
Next line, NLTK dot corpus import twitter underscore samples.
Next line, NLTK dot tag import POS underscore tag underscore sense.
Next line, tweets equal twitter underscore samples dot strings open parenthesis single quote positive underscore tweets dot JSON single quote close parenthesis.
Next line, tweets underscore token equals twitter underscore samples dot tokenized open parenthesis single quote positive underscore tweets dot JSON single quote close parenthesis.
Next line, JJ underscore count equals zero.
Next line, NN underscore count equals zero.
Next line, for tweet in tweets underscore tagged colon.
Next line, tag equals pair open bracket one close bracket.
Next line, if tag equals equals single quote JJ single quote colon.
Next line, JJ underscore count plus one equals one.
JJ underscore count plus equals one.
Next line, E-L-I-F tag equals equals single quote NN single quote colon.
Next line, NN underscore count plus equals one.
After you have completed your two loops, you should have a total count for nouns and adjectives in your twitter samples corpus.
To see how many nouns and adjectives are in your corpus, you must print statements at the end of your script as follows.
For tweets in tweet underscore tagged colon.
Next line, for pair in tweets colon.
Next line, tag equals pair open bracket one close bracket.
Next line, if tag equals equals single quote JJ single quote colon.
Next line, JJ underscore count plus equals one.
Next line, E-L-I-F tag equals equals single quote NN single quote colon.
Next line, NN underscore count plus equals one.
Next line, print open parentheses single quote total number of adjectives equals single quote comma JJ underscore count.
Close parentheses.
Next line, print open parentheses single quote total number of nouns equals single quote comma NN underscore count close parentheses.
Once you get here, your program will be able to accurately output the total number of nouns and adjectives which are found in your twitter samples corpus.
Running natural language processing script.
The last step is to run your natural language processing script.
To do so, you must save your NLP.py file and run it to see how many nouns and adjectives there are in your twitter samples corpus.
Python NLP.py
Notably, this process will take several minutes to finish.
If all went well, once you run the command, you should get the output as follows.
Total number of adjectives equals 6094.
Next line, total number of nouns equals 13180.
If your output looks like this, it means that you have successfully counted the part of speech tags in your corpus.
Here, we use the twitter samples corpus downloaded with NLTK, but these steps will work for many other data sets you import.
You can also extend the code to count both plural and singular nouns.
Visualize your data with matplotlib or perform sediment analysis of contained objectives.
In this section of the book, we are going to build a text summarizer algorithm to reduce bodies of text while we keep its original meaning, which can give you great insight into the original text.
Although there are many other NLP libraries we could use during this example, we are going to remain consistent and use NLTK.
There are four main steps included in text summarization.
1. Removing stopwords
2. Creating the frequency table of words
3. Assigning scores to every sentence depending on the frequency table and words it contains
4. Building a summary of adding and giving each sentence a specific score threshold
The first step is to remove stopwords
As you know by now, stopwords are all the words that do not add any value to the overall meaning of sentences
By removing stopwords, we can easily narrow the number of words contained in any textual data while we preserve the original meaning
We generally remove stopwords from analyzed text since we know that these words do not give any insight into the original body of text
To build our text summarizer, we must use two NLTK libraries which are necessary. To start, import them as follows
From NLTK underscore tokenize, import word underscore tokenize, comma, send underscore tokenize
As you already know, a corpus is a collection of text. It might be the dataset containing poems, Twitter samples, etc.
Here we are going to use already predetermined stopwords for quicker removal
We may remove tokenizers as well, if needed
There are three main types of tokenizers including rejects, word, and sentence tokenizer
However, here we are going to use only sentence and word tokenizers
Removing stopwords
To remove stopwords, first you must create two arrays
One for each word in the text body and another for stopwords
Stopwords equal
Set
Open parenthesis
Stopwords
Dot
Words
Open parenthesis
Double quote
English
Double quote
Close parenthesis
Close parenthesis
Next line
Words
Equal
Word
Underscore
Tokenize
Open parenthesis
Text
Close parenthesis
Next step is to create a dictionary you are going to use for your word frequency table
For this step we are going to use only those words that are not part of our stopwords array
Freq table equals dict
open parenthesis
close parenthesis
Next line
For word in words
colon
Next line
Word equal word dot lower
open parenthesis
close parenthesis
Next line
If word in stopwords
stopwords
colon
Next line
Continue
Next line
If word in freak table
colon
Freq table
open bracket
word
close bracket
plus equal one
Next line
else
colon
Next line
Freq table
open bracket
word
close bracket
equals one
Next you can create a frequency table dictionary over each sentence to know which sentences
have the most important insight to the overall meaning of the text
Assign scores to sentences
Since you already have your sentences tokenizer here you must run the sent tokenize argument to create an array of your sentences
Once this is done you need a dictionary to assign and keep the score of every sentence so that later you can use your dictionary to create the summary
Sentence equals sent underscore tokenize open parenthesis text close parenthesis
Sentence value equals dict open parenthesis close parenthesis
At this point you must go through every sentence and assign it a score depending on the words contained
There are many algorithms available for assigning scores but we are going to use a basic algorithm which adds the frequency of each non-stop word in your sentence
For sentence in sentences colon
Next line
For word value in
Freq table
colon
Next line
If value
open bracket
0
close bracket
in sentence
dot
lower
open parenthesis
close parenthesis
colon
If sentence
open bracket
colon
12
close bracket
in sentence value
colon
Sentence
open bracket
sentence
open bracket
colon
12
close bracket
close bracket
plus equals
word value
open bracket
1
close bracket
next line
OR
colon
Next line
Sentence value
open bracket
sentence
open bracket
colon
12
close bracket
close bracket
It being understood that an index 0 of word value always returns the word itself while
the index of value 1 always represents the number of total instances present.
One common issue with score algorithms is that those long sentences always have the
advantage when compared to shorter sentences.
To solve this problem, you must divide each sentence score by the total number of words
present in the sentence.
Then you must use values you get to compare your score.
The simplest approach to this problem is to find the average score of every sentence.
Once you get it, you can easily find a threshold.
Next line.
Summary values equal 0.
Next line.
For sentence in sentence value colon.
Next line.
Summaries plus equal sentence value open bracket sentence close bracket.
Next line.
Average equals int open parenthesis sum values forward slash len open parenthesis sense value
close parenthesis close parenthesis.
Now you are wondering what a good threshold is.
Note that the wrong value can easily give a summary which is too small or too big.
The average can be a good threshold.
Here you can go with a shorter summary thereby getting a threshold you can use.
After that you must apply your threshold and store your sentences so you get your summary.
Summary equals single quote space single quote.
Next line.
For sentence in sentences colon.
Next line.
If sentence open bracket colon 12 close bracket in sentence value and sentence value open
bracket sentence open bracket colon 12 close bracket close bracket greater than open parenthesis
1.5 asterisk average close parenthesis colon next line summary plus equals double quote space double quote plus sentence.
Once completed you can print to generate your summary and check its quality.
Frequency table enhancement.
You can make smarter frequency tables using a stemmer algorithm which bring words back to their root words.
This is very useful when you want to add importance to the words with similar meaning.
To implement stemmer you can use the stemmers already provided in the NLTK library.
There are many different stemmers provided in this library for finding the root words.
It doesn't matter which one you choose.
They are all very good.
From NLTK dot stem import ps equals porter stemmer open parenthesis close parenthesis.
Once done with importing stemmer you can pass each word by the stemmer before you add it to your frequency table.
This is a good practice to stem each word before you add scores of the words when you are going through every sentence.
Chapter 6 Sentiment Analysis
In this section of the book we are going to perform sentiment analysis which is one of the common natural language processing problems.
Sentiment analysis is the process of automatically identifying and categorizing opinions.
These opinions and attitudes are normally negative, neutral, or positive and are expressed in a piece of text by the writer towards a topic, problem, or product.
Here again we are going to use NLTK.
The Python 3 environment comes with many analytics libraries already installed including NLTK that we will need in this next section.
Please ensure you are running in this environment.
Loading NLTK Packages
To start with sentiment analysis you must load the package that you will be working on.
There are different packages available like linear algebra, data processing, and CSV file.
You can easily load as follows.
Import numpy as np.
Next line Import pandas as pd.
Next line From sklearn dot model underscore selection import train underscore test underscore split.
Next line Import nltk Next line From nltk dot corpus import stopwords Next line From nltk dot classify import sklearn classifier From word cloud import word cloud comma stopwords import matplotlib dot pyplot as plt.
Next line Percent mapplotlib inline Once you have loaded your input data it is available on your input directory.
Pressing shift plus enter you will get the list of all files you have in your input directory.
From subprocess import check underscore output Next we are going to perform sediments analysis on the Twitter samples dataset.
The first step is to drop any unnecessary columns and keep only text and sediment.
Data equals pd dot read underscore csv open parentheses single quote dot dot forward slash input forward slash sediment dot csv single quote end parentheses.
Next line Data equals data open bracket open bracket single quote text single quote comma single quote sentiment single quote close bracket close bracket close bracket.
The next step here is to split your dataset into a testing and training set.
The test set will be 10% of your original dataset.
For this sediment analysis problem you should also drop all neutral tweets as we want to focus only on positive and negative opinions.
Therefore split the dataset into test and train sets and remove all neutral sediments as follows.
Train comma test equals train underscore test underscore split open parentheses data comma test underscore size equals 0.1 close parentheses.
Next line train equals train open bracket train dot sediment exclamation equals double quote neutral double quote close bracket.
Cleaning text data.
After those steps are completed you will then need to separate the negative and positive tweets in your training set to easily visualize all contained words.
However prior to doing that you must clean your texts from links mentions and hashtags.
Once you have cleaned it it is ready for word cloud visualization which will only show you the most emphatic words from the negative and positive tweets.
The responses link has separated lasung about false Kiawaadean.
For that if you don't want to improve the negative of this effective end the number you will have given that correct
the difference in aのは.
Presenta.
Aren't the words?
We, the ones you look at the negative one.
We'll find.
historians learn as a surprised detail through the evidence.
Off the results.
Tторr.
surprises
farmers are a poor choice.
Nara nostro centralize exam .
Your examples have been asked for Hasnard.
open bracket data single quote color equals single quote black single quote
close parentheses colon next line words equal single quote space single quote dot join open
parentheses data close parentheses next line clean underscore word equals double quote space
double quote dot join open parentheses open bracket word for word in words dot split open parentheses
close parentheses next line if single quote HTTP single quote not in word next line and not word
dot starts with open parentheses single quote at single quote close parentheses and not word dot
starts with single quote open parentheses single quote hashtag single quote close parentheses any
word exclamation point equals single quote RT single quote close bracket close parentheses word cloud
equals word cloud open parentheses stop words equals stop words single quote background underscore color
color equals color comma next line width equals 2500 comma next line height equals 200 close
parentheses dot generate open parentheses clean underscore word close parentheses PLT dot figure open
parentheses 1 comma fig size equals open parentheses 13 comma 13 close parentheses close parentheses close
parentheses PLT dot I am show open parentheses word cloud close parentheses PLT dot access open parentheses
open parentheses single quote off single quote close parentheses PLT dot show open parentheses close
parentheses close parentheses next line print open parentheses double quote positive words double quote close
You will see that most of the positive tweets contain words
such as Together, String, Live, and Truth, while most of the negative tweets contain words
like Disappointing, Trying, and Influence.
After text visualization, you will remove the mentions, links, hashtags, and stopwords
from your training set as follows.
Tweets ==
Next line, stopwords underscore set equals set, open parenthesis, stopwords dot, words,
open parenthesis, double quote, English, double quote, close parenthesis, close parenthesis.
Next line, for index, comma, row in train dot, enter rows, open parenthesis, close parenthesis,
colon, words underscore filtered equal, open bracket, e dot, lower, open parenthesis, close parenthesis,
for, e in row, dot, text dot, split, open parenthesis, close parenthesis, if len, open parenthesis,
e, close parenthesis, greater than, equals, three, close bracket, words, underscore, cleaned,
or cleaned, equals, open bracket, word for word in words, underscore, filtered, if, single quote,
HTTP, single quote, and not word, dot, starts with, open parenthesis, single quote, at sign,
single quote, close parenthesis, and not word, dot, starts with, open parenthesis, single quote,
quote, hashtag, single quote, close parenthesis, and word, exclamation point, equals, single
quote, RT, single quote, close bracket, words, underscore, without, underscore, stopwords, equal,
open bracket, word for word, in words, underscore, cleaned, if not word in stopwords, underscore, set, close bracket,
close bracket, tweets, dot, append, open parenthesis, open parenthesis, underscore, cleaned, comma, row, dot,
sentiment, close parenthesis, close parenthesis, close parenthesis, test, underscore, POS, equals, test, open
bracket, test, open bracket, single quote, sentiment, single quote, close bracket, equals, equals, single quote, positive, single
single quote, close bracket, TEST, underscore, POS, equals, test, underscore, POS,
open bracket, single quote, text, close, single quote, close bracket, TEST, underscore, NEG, equals, TEST,
open bracket, TEST, open bracket, single quote, SEDMENT, single quote, close bracket, equals,近 inhale,
CLOSEDione
single quote, text, single quote, closed bracket, closed bracket.
Extracting Features
The next step is to extract features with NLTK lib.
To do so, you must first measure a frequent distribution and then select the resulting keys.
def get underscore words underscore in underscore tweets
open parenthesis tweets, closed parenthesis, colon, all, equals, open bracket, closed bracket, closed bracket.
Next line, for, for, open parenthesis, words, comma, sediment, closed bracket, in tweets, parenthesis.
Next line, return all, def get underscore word underscore features, open parenthesis, word list, closed parenthesis,
colon, word list equals, NLTK, dot, freakdist, open parenthesis, word list, closed parenthesis,
features equal, word list, dot, keys, open parenthesis, closed parenthesis.
Next line, return features.
w underscore features equal get underscore word underscore features, open parenthesis, get underscore words, underscore in tweets, in underscore tweets, open parenthesis, tweets, closed parenthesis, close parenthesis.
def extract underscore features, open parenthesis, document, close parenthesis, colon, document underscore words, equals, set, open parenthesis, document, close parenthesis,
Features equal
Features equal
OpenSquiggly bracket
CloseSquiggly bracket
Next line
For Word in W underscore features colon
Features open bracket single quote contains
Open parenthesis percentage S
Close parenthesis single quote percent word
Close bracket equals
Open parenthesis word in document underscore words
Close parenthesis
Next line
Return Features
Once done you can plot the most frequently used words as follows
WordCloud underscore draw
Open parenthesis
W underscore features
Close parenthesis
With NLTK Naive Bayes Classifier
You can easily classify the extracted tweet word features
Training underscore set equals
NLTK dot
Classify dot
Apply features
Open parenthesis
Extract underscore features
Comma
Tweets
Close parenthesis
Classifier equals
NLTK dot
Naive Bayes Classifier dot
Train
Open parenthesis
Train underscore set
Close parenthesis
You can also measure to see how your score algorithm scored
Neg underscore cnt equals zero
Next line
POS underscore cnt equals zero
Next line
For OBJ in test underscore neg
For OBJ in test underscore neg colon
Next line
Res equals
Classifier dot
Classify
Open parenthesis
Extract
Underscore
Features
Open parenthesis
ObJ dot
Split
Open parenthesis
Close parenthesis
Close parenthesis
Close parenthesis
Close parenthesis
Next line
If
Open parenthesis
Res equals
Equals
Single quote
Negative
Single quote
Close parenthesis
Colon
Neg underscore cnt
Equals
Neg underscore cnt
Plus
1
For OBJ in test
Underscore
POS
Colon
Res equals
Classifier
Dot
Classify
Open parenthesis
Extract
Underscore
Features
Open parenthesis
ObJ dot
Split
Open parenthesis
Close parenthesis
Close parenthesis
Close parenthesis
If
Open parenthesis
Res equals
Equals
Single quote
Positive
Single quote
Close parenthesis
Colon
POS
Underscore
Cnt
Equals
POS
Underscore
Cnt
Plus
One
Next line
Print
Open parenthesis
Single quote
Open bracket
Negative
Close bracket
Colon
Percent S
Over percent S
Single quote
Percent
Open parenthesis
Len
Open parenthesis
Test
Underscore
Neg
Close parenthesis
Comma
Neg
Underscore
Cnt
Close parenthesis
Close parenthesis
Close parenthesis
Next line
Print
Open parenthesis
Single quote
Open bracket
Positive
Close bracket
Colon
Percentage
S
Over percentage
S
Single quote
Percentage
Open parenthesis
Len
Open parenthesis
Test
Underscore
POS
Close parenthesis
Comma
POS
Underscore
Or cnt
Close parenthesis
Close parenthesis
Close parenthesis
Here you have learned how to perform sediment analysis on tweets you import using
NLTK
This algorithm performs well, but there may be some issues when tweets are sarcastic, ironic, or gas reference, or something with their own original context.
Either way, using by the steps presented here you can perform sediment analysis on other datasets like movie reviews, product reviews, news comments, etc.
Chapter 7.
Stemming and Limitization
Very often, different word inflections may have the same meaning, at least when it comes to data analysis.
Therefore, it may be very useful to group these words together.
For instance, instead of handling the words swimmer, swimming, and swim individually, we can treat them as the same word, which is swim.
There are two common approaches to this treatment of the similar meaning words, stemming and limitization.
Limitization takes those inflected word forms and returns them in their base form, root, or lemma.
To achieve this, you need some context of the word to use, like when it is an adjective or a noun.
Unlike lemmatization, stemming is a cruder attempt at generating the word root form.
It commonly returns a word that is simply the first several characters which consists in any word form.
There are several different versions available, but stemming and limitization are most commonly used.
To perform stemming and limitization, you must import word net lemmatizer and porter stemmer as follows.
From nltk.stem import porter stemmer comma word net lemmatizer.
Next line, stemmer equals porter stemmer open parenthesis, close parenthesis.
Test underscore word equals double quotes swimming, double quote.
Next line, word underscore stem equals stemmer dot stem open parenthesis test underscore word close parenthesis.
word underscore lemmatize equals lemmatizer dot lemmatize open parentheses test underscore word
close parentheses word underscore lemmatize underscore verb equals lemmatizer dot lemmatize
open parentheses test underscore word comma pos equals double quote v double quote close
Word underscore lemmatize underscore ADJ equals lemmatizer dot lemmatize open parenthesis test underscore word comma POS equals double quote A double quote close parenthesis.
Next line print open parenthesis word underscore stem comma word underscore lemmatize comma word underscore lemmatize underscore verb comma word underscore lemmatize underscore ADJ close parenthesis.
You will get different forms of the word swim like swimmer, swimming, etc. Here you can see that the word stemmer, in this case, has output the word swim as this is a stemmed version of your input word.
When it comes to lemmatizer, it needs to know whether the input word has been used as an adjective or as a verb to properly lemmatize your input word.
The overall process of assigning these tags is known as part of speech tagging we previously covered in the book.
You might want to consider that even though stemming is a less thorough approach in practice, it often performs better than lemmatization.
You can also lemmatize the same words using WordNet as follows.
From NLTK dot stem import WordNet lemmatizer.
Next line, lemmatizer equals WordNet lemmatizer, open parenthesis, close parenthesis.
Next line, print, open parenthesis, lemmatizer dot, lemmatize, open parenthesis, single quote, your input, stemmed, word, single quote, close parenthesis, close parenthesis.
There are several differences between lemmatization and stemming.
For instance, stemming can work on words without knowing their context, so it has lower accuracy, but it is faster than lemmatization.
In addition, word lemmatizing returns you a real word even in those cases when it is not the same word.
It may be a synonym, or it may be a real word.
Therefore, if you care about the speed and not about accuracy, use stemming, but if accuracy is more important to you, use lemmatization.
Stemming non-English words
Take into consideration here that you can stem non-English words using the NLTK stemmer.
It can stem 13 languages besides English as follows.
From NLTK.stem, import, snowball stemmer, print, open parenthesis, snowball stemmer, dot, languages, close parenthesis.
You can also use the NLTK stem function to class some non-English words as follows.
From NLTK.stem, import, snowball stemmer, French underscore stemmer, equals, snowball stemmer, open parenthesis, single quote, French, single quote, close parenthesis.
Next line, print, open parenthesis, French underscore stemmer, equals, French underscore stemmer, dot, stem, open parenthesis, double quote, French, word, double quote, close parenthesis, close parenthesis.
Getting synonyms and antonyms from WordNet
As you can perform lemmatization using WordNet, you can also use it for getting synonyms and antonyms.
As you know, we installed NLTK packages with NLTK download arguments and one of the packages we got is WordNet.
It is a large database built for natural language processing including brief definitions and collections of synonyms.
You can easily get WordNet definitions by running the following code.
From NLTK.Corpus.import, syn, equals, WordNet, dot, syn, sets, open parenthesis, double quote, pane, double quote, close parenthesis.
Next line, print, open parenthesis, syn, open bracket, zero, close bracket, dot, definition, open parenthesis, close parenthesis, close parenthesis.
Next line, print, open parenthesis, syn, open bracket, zero, close bracket, dot, examples, open parenthesis, close parenthesis, close parenthesis.
You will get your definition as follows.
A symptom of some physical disorder or hurt.
Open bracket, single quote, the patient develops severe pain and distention.
Single quote, close bracket.
Consider that WordNet has many definitions from an assortment of terms you can easily get.
From NLTK.Corpus.import, WordNet.
Syn equals, WordNet, dot, syn, sets, open parenthesis, double quote, NLP, double quote, close parenthesis.
Next line, print, open parenthesis, syn, open bracket, zero, close bracket, dot.
Definition, open parenthesis, open parenthesis, close parenthesis.
Syn equals, WordNet, dot, syn, sets, open parenthesis, double quote, Python, double quote, close parenthesis.
Print, open parenthesis, syn, open bracket, zero, close bracket, dot, definition, open parenthesis, close parenthesis, close parenthesis.
Running this code will get you the following output.
The branch of information science that deals with natural language information.
You can also use WordNet to obtain synonym words by running the code as follows.
From nltk.corpus import WordNet.
Next line, synonyms equal open bracket, close bracket.
Next line, for syn in WordNet dot syn sets.
Open parentheses, single quote, computer, single quote, close parentheses, colon.
Next line, for lemma in syn dot lemmas open parentheses, close parentheses, colon.
Synonyms dot append, open parentheses, lemma dot name, open parentheses, close parentheses, close parentheses.
Next line, print, open parentheses, synonyms, close parentheses.
Your input will look like this.
In a similar manner, you can now get some antonyms from WordNet
by running the code as follows.
However, before you add your words to the array,
make sure you check their lemmas.
From nltk.corpus import WordNet
next line antonyms equal
open bracket close bracket
for syn in WordNet dot
syn sets open parenthesis
double quote big double quote close parenthesis colon
next line for one in syn dot lemmas
open parenthesis close parenthesis colon
next line if one dot antonyms
open parenthesis close parenthesis colon
antonyms dot append open parenthesis
one dot antonyms open parenthesis close parenthesis
open bracket zero close bracket dot
name open parenthesis close parenthesis close parenthesis
next line print open parenthesis
antonyms close parenthesis
last words
natural language processing is one of the most important methodologies of this digital age
implementing and understanding those complex language utterances
is one of the most crucial parts of deep learning and artificial intelligence
application of natural language processing
are everywhere
and we communicate with each other in language emails
language translation
advertisements
customer service
etc
if you are interested in analyzing large data sets
developing applications
or maybe documenting languages
this book will help you kickstart your NLP journey
as you work on more challenging and more complex tasks in the future
the book will be your best companion
offering you a highly accessible introduction to NLP in Python
using one of the most popular natural language processing libraries
NLTK
the book is perfect for any beginner out there
who is looking forward to learning more about using data
from the world we live in
and how to gain knowledge by utilizing it
there are many underlying tasks
and deep learning models
in recent times deep learning approaches have also obtained amazingly high performances across
many natural language processing tasks
so those common traditional NLP challenges and difficulties are finally being solved
these deep learning natural language processing models can easily solve numerous tasks
with just single end-to-end models without the need for old-style feature engineering
by having followed this step-by-step guide
you know how to implement innumerable NLP tasks in NLTK
how to use your skills for performing text classification
sediment analysis and much more
as you finish with this book you have gained practical NLP skills using NLTK and Python
you can then use these skills for yourself in solving an endless number of tasks in the future
this has been natural language processing with Python
natural language processing using NLTK
written by Frank Milstein
narrated by John Wilkins
copyright 2018 by Frank Milstein
production copyright 2018 by Frank Milstein
Audible hopes you have enjoyed this program
