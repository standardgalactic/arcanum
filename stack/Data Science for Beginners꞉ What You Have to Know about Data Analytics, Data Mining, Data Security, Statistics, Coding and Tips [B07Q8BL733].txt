This is Audible. Data science for beginners. What you have to know about data analytics.
Data mining, data security, statistics, coding, and tips. Written by Travis Goleman. Narrated by
Austin Stoller. Introduction. Data is an important resource.
However, if you do not have the right means to process it,
then there is not much that you can benefit from regarding its value. Data science is one of those
multidisciplinary areas whose major focus is to derive value from data in all means. This book
will explore the field of data science using data and its structure. In addition, it will describe
high-level processes that one uses to change data into value. You know that data science
is a process. However, this does not mean that it lacks creativity. The steps that one follows in
transforming raw data into value also vary. Data comes in different forms, but at an advanced level,
it exists in three major categories. Those categories are structured, semi-structured,
and unstructured. Data scientists are experts responsible for gathering, analyzing, and interpreting
large amounts of data to help businesses and organizations. Data is an important resource.
However, if you do not have the right means to process it, then there is not much that you can
benefit from regarding its value. Data science is one of those multidisciplinary areas whose major focus
is to derive value from data in all means. This book will explore the field of data science using data
and its structure. In addition, it will describe high-level processes that one uses to change data into value.
Chapter 1 Introduction to Data Science
What is Data Science?
Besides this, both organizations and enterprises are required to build a framework and develop a solution
to store data. Therefore, Hadoop and other frameworks were developed to solve this problem.
Once this issue was solved, the focus shifted to how data could be processed.
This is why it is important to understand what data science is and how it can add value to a business.
How do you go about doing this? You need to take a bunch of decisions to actually do the purchase.
So we start with which website, which portal, or which website you should use.
So we try to find out, let's say you want to buy the furniture. Obviously, you don't go to an online
grocery store because you need furniture because there are several websites. So that is the first decision
you need to take which website I should use. So once we have multiple websites, you kind of discard all
the websites which don't sell furniture and you stick to those websites which sell furniture.
Now, within that, we try to find out what is the ratings of these websites. If the ratings is more,
that means they are reliable quality, probably is good, and so on and so forth. So only then you want
to buy from that particular website. So anything that does not satisfy these criteria, you close all
those websites. Close in the sense you close the browser, right? So you still are left with maybe a
few of these, which satisfy your criteria, that is which sell the pages or websites that sell furniture.
They may have a rating of 4 and above. And then you look for discount, is somebody providing discount
greater than 20%. Then again, you filter out some of them which probably are not providing any discount,
and zero down to one or two websites which are probably providing those discounts,
and go ahead and select the furniture and purchase it.
So this is a very basic example, probably don't follow this always exactly the same way,
but just to illustrate and drive home the point. We can answer many questions using data science.
For example, when we take a cab and we book a cab now to go from location A to location B,
what is the best route that the cab can take to reach the fastest way or in the least amount of time?
There could be several factors. There could be traffic. There could be a bad road. There could be weather.
Now all these come as inputs, and a decision needs to be taken as to which is the best route.
Another example is TV shows. So Netflix, and maybe even a lot of other TV channels,
they have to perform this analysis to find out what kind of shows people are viewing,
what kind of shows people are liking, and so on and so forth, so that they can then sell this info to
advertisers because their main source of revenue is advertising.
Predictive maintenance. We need to find out, will my car break down? Will my refrigerator break down
in the next year or two years? Should I be prepared to buy a new refrigerator? You can potentially apply
data science here as well. Then in politics, a lot of data science is applied in politics you must have
seen on TV about US elections or in the UK or even in India. Nowadays, everybody is applying data science
in elections and trying to capture the words or rather the voters influence the water's personalized
messages by providing personalized messages and so on and so forth, and that is what not only that.
People use data science to even predict who is going to win the elections. It's a different matter
that probably not all predictions about to be true, but then yes, this is why they use data science to do
these predictions. Why is data science important? Traditionally, data was structured in a small size.
This means that there was no problem if you wanted to analyze data. Why? There were simple BI tools that
you could use to analyze data, but modern data is unstructured and different from traditional data.
Therefore, you need to have advanced methods of data analysis. The image below indicates that before
the year 2020, more than 80% of the data will be unstructured. Have you ever thought of having the
ability to understand the exact requirements of your customers from existing data such as purchase history,
past browsing history, income, and age? The truth is, now it is possible.
Let's use a different example to demonstrate the role of data science in decision making.
What if your car is intelligent enough to drive you home? That would be cool.
Well, that is how the self-driving cars have been designed to work.
These cars gather live data from sensors to build a map of the surroundings.
Based on this data, the car can make decisions such as when to slow down,
when to overtake, and when to take a turn.
These cars have complex machine learning algorithms that analyze the data collected to develop a meaningful
result. Data science is further applied in predictive analytics.
This includes places such as weather forecasting, radars, and satellites.
Models have been created that will not only forecast weather, but also predict natural calamities.
This helps an individual to take the right measures beforehand and save many lives.
Data science refers to a combination of several tools, machine learning principles,
and algorithms whose purpose is to discover hidden patterns from raw data.
One might wonder how different it is from statistics.
Therefore, data science helps an individual predict and make decisions by taking advantage of
prescriptive analytics, machine learning, and predictive casual analytics.
Prescriptive analytics.
If you need a model that has the intelligence and capability to make its own decisions,
then prescriptive analytics is the best to use.
This new field delivers advice. It doesn't just predict,
but it also recommends different prescribed actions and related outcomes.
The best example to illustrate this is the Google self-driving car.
Data that is collected by the vehicle is used to train the cars.
You can further mine this data by using algorithms to reveal intelligence.
We will talk about what is the need for data science, and then what exactly is data science,
some definitions, and understand the differences between data science and business intelligence.
Then, we will talk about the prerequisites for learning data science,
and then what does the data scientist do?
What are the activities performed by a data scientist as part of his daily life,
and then we will talk about the data science life cycle with a quick example,
and briefly touch upon the demand over increasing demand for data scientists, alright?
Automobile.
So, let us get started now. You must have already heard about autonomous cars,
so I'm sure you must be excited to have a car driving by itself,
which will take you from home to office or office to home.
That's where one of the examples where data science is used,
and the car needs to take a lot of decisions in this whole process, whether to speed up,
whether to apply the brake, take a left turn, right turn, or slow down.
So, all these decisions are a part of data science,
and there is a study that says that self-driving cars will minimize accidents.
In fact, it will root out more than 2 million deaths caused by car accidents annually.
There is a lot of research, and there is a lot of testing going on,
and not a lot of cars are yet in production in terms of usage,
but it is going to happen every automotive company that its name is investing in self-driving cars.
Therefore, in about 10 to 15 years, some of the studies say that most of the cars will be autonomous,
or self-driving cars.
Aviation.
Where else there are issues, for example, if we take air?
This is another area where data science contributes in a big major way.
Flights get delayed due to weather conditions because the weather is not predicted in time,
and the demand of passengers is not probably seen ahead of time.
For all these, you need data science.
Then this could be improper route planning, and some customers might miss some flights,
that again needs data science.
Similarly, it could be incorrect decisions in selecting the right equipment.
So which plane should fly in which route, that's the equipment it's being mentioned here,
if that is not planned properly, and you might end up in a situation where the plane is not available.
It has you got turn for a flight in a particular route.
So these are some of the challenges,
can one of the representative industries we are talking about, which is airlines.
Therefore, if we use data science properly,
all of these, or most of these problems can be avoided,
and that will help in reducing the pain both for the airlines and for the passengers.
A few more examples.
What else can we do?
There are some of the other things that we can do,
and we will stick to the airline industry.
We can do better route planning, so that there are less cancellations,
and less frustrated people.
We can predict, use predictive analytics,
and predict any delays, so that there are some sweats,
can be rescheduled ahead of time, and that there are no last minute changes.
Data science can also be used to make promotional offers.
And last, but not least, is what kind of planes should be used,
or the different classes of planes that should be used in different routes for better performance.
Therefore, these are some examples of how data science can be used in airlines,
and another example for the industry where data science can be used,
and benefited, would be in logistics.
So, companies like FedEx, they use data science models to increase their efficiency
drastically to optimize the roads, and cut costs, and so on.
Therefore, before their delivery truck actually sets out,
they determine which is the best possible route to ship their items to the customers,
and based on various inputs.
They also predict, or come up with the best suited time to deliver,
and finally, yet importantly, they determine what the best mode of transport for this delivery is as well.
Data science.
It's advantage.
These are some of the main areas where data science is used.
Better decision making.
There are always tricky decisions to be made, so which is the right decision?
Which way to go, so that is one area.
Predicting.
For example, can we predict delays, like in the case of airlines?
Can we predict demand for certain products?
Let's say, any commerce that is the pattern discovery or pattern recognition.
Is there a pattern in which people are buying items?
For example, it could be seasonally.
If you take the data sales data for multiple years, there may be a pattern in a way people are buying,
that is a buying pattern.
Certain months, probably the sales will increase.
Certain months of the sales will come down certain quarters.
Traditionally, the sales will be higher certain quarters.
Therefore, that is a pattern, and this pattern discovery is another area where data science is applied.
Process and data science.
Asking the right question.
Basically, you want to know what exactly is the problem you're trying to solve,
so that is asking the right question, so that this circle out.
Exploring the data.
The next step is after exploring the data, so this is the first step you will ask some questions.
What exactly is the problem you're trying to solve?
And then obviously, you will have some data for that as input,
and you perform some exploratory analysis on the data.
For example, you need to clean the data to make sure everything is fine,
and so on and so forth, so that is all is a part of exploratory analysis.
And then you need to do modeling.
Let's say, if you have to perform machine learning,
you need to decide which algorithm to use, and which model to use,
and then you need to train the model, and so on and so forth.
So that's all part of the modeling process,
and then you run your data through the, and then through this process,
and then you come out with the final results of this exercise,
which involves visualizing the results,
and preparing a way to communicate the results to the concerned people.
So it could be in the form of PowerPoint slides,
or it could be in the form of a dashboard,
which is basically what we call as a visualization.
And so, all the insights that have been gathered through this exercise,
that has to be communicated in a proper way,
in an easy-to-understand way,
which is again, a key part of this whole exercise communicating the results.
Chapter 2. How to think about data.
According to the Webster Dictionary of 1973, data refers to information that is factual.
It is used to discuss, reason, and calculate.
It can also be defined as information that is needed for analysis.
This information can be in numerical form, ready to be processed and transmitted digitally.
From the definitions we have listed above, we can practically define data as characters, numbers,
images, or any kind of recording that is in a format that can be assessed for a decision on an action to be arrived at on.
There is a belief that goes around that data that stands alone does not have any meaning,
and it only gets its meaning when it is interpreted to become information.
When data is examined closely, some patterns can be identified, which can later be used as knowledge.
When you look at disciplines such as nutrition, public health, education, management, and nursing,
It is evident that the quantity and quality of data, be it descriptive or statistical, is required to have baselines,
get actions that are effective, set targets and goals, evaluate impacts, and monitor progress.
Before presentation and interpretation of information, there have to be procedures for collecting and filtering data.
For instance, if you have the number 2146, that is just a number, but is basically a raw number without meaning.
Just like cabbage or crop that is picked from the farm is raw and not yet prepared.
That is how we view data as a raw material that has the potential of becoming information.
During data collection, it is important to identify the best data to collect.
One needs to understand that there are several forms by which data can be collected.
These forms include pictures, numbers, articles, tweets, words, and maps.
When you try to look at the best form of data collection, the concept of quantitative and qualitative approach is brought up,
which can lead to a more difficult program planning.
It is at times difficult to come to a conclusion on the qualitative and quantitative data differences,
since the minds of many people, the disparity between the two is based on the notion that one is preferred over the other.
Types of data
Even though there has been a huge debate over the advantages and disadvantage of quantitative data and qualitative data,
we need both types of data to have a high quality program.
There are instances that a qualitative approach will be preferred over quantitative data and vice versa.
The thing is that both approaches have differing logics and strengths, which address different purposes and questions.
It is important to know that there are times when having both worlds makes total sense.
These situations will prove that combining these approaches will tackle a question and allow for better decisions to be made.
Qualitative data
If data is represented in a narrative or verbal format, we describe it as qualitative data.
Focus groups
Questionnaires that are open-ended
Interviews and other situations that are less structured are how this type of data are collected.
A simpler way of looking at data that is qualitative is thinking of words as a way of qualitative data presentation.
Quantitative data
This is data expressed in the form of numeric, where numerical values are either small or large.
Values that are numerical may respond to a label or specific category.
Contrasting types of data
Qualitative data
Newspaper articles
Advisory group minutes
Logs of social service
Agreements of family partnerships
Minutes for the policy council
Qualitative data
Tracking system for a child's performance
Tracking system for health data
Mixed data
Transaction and enrollment record survey
Transaction and enrollment record survey, which includes parents
Teachers
Farmers
Data strategies
Different strategies of qualitative and quantitative data analysis can provide a data analysis with a way of approaching data in an organized way
This allows the data analysis to form a sequence of different processes, which can be used
Data visualization
This involves the creation of a picture or graphic data display
This is with the intention of starting the process of analysis for aid to reporting or findings representation
This means the creation of a visualized display of data
This is not ideally analysis, nor is it a substitute for analysis
However, data visualization can be used as a start before data analysis
Analysis that is exploratory
This entails going through data that has low levels of knowledge about a certain indicator
like the first and second language acquisition
A relationship between an indicator and its cause can also be included
Analyses
That is exploratory
This involves looking at data to describe what is happening
by creating a baseline for the analysis of the future
Use of trends
This involves looking at collected data that occurs at different times
with the intention of identifying and estimating change
The basic goal of trend analysis is the overview of data over some time
For instance, to discover if a certain indicator
like the number of children who have disabilities has decreased or increased over time
And if that is the case, how fast or slow as the decrease or increase occurred
One can compare two different time periods
This kind of analysis of trends is conducted to assess the indicator's level before and after a particular event
Estimation
This involves the use of real data values that can predict the future value
This is with the aim of countering boredom after mastering all other strategies listed above
When working with qualitative or quantitative data, these procedures can occur
The use of data about poverty levels can be combined with interviews from providers who serve families
who are low-income earners to assist in determining families in the area that are eligible for income
Planning for the future can be assisted by estimation
Forecasting quantities closely related to eligible families and children
Demographic characteristics and social services can work well using estimation
Ideally, estimation refers to the combination of info that originates from various sources of data
to project information that is not available in a singular source
Chapter three
Importance of data in business
Here are just a few examples of data playing an important role in business success
Transactional data
This could be pulled out of your ledger
Sales report and web payment transactions
If you have a customer relationship management system in place
You will also be able to take stock of how your customers are spending on your products
Online engagement reporting
This is data based on the interaction of customers on your website
There are tools available such as crazy egg and google analytics which can help you collect data from your website
Improving your marketing strategies
Based on the data collected
It is easier for a company to come up with innovative and attractive marketing strategies
It is easier for a company to alter existing marketing strategies and policies in such a fashion that they are in line with the current trends and customer expectations
Identifying pain points
If your business is driven by predetermined processes and patterns
Then data can help you identify any deviations from the usual
These small deviations could be the reason behind the sudden decrease in sales
Increase in customer complaints
Or decrease in productivity
With the help of data you will be able to catch these little mishaps early and take corrective actions
Detecting fraud
When you have the numbers in hand
It will be easier for you to detect any fraud that is being committed
For instance
When you have the purchase invoice of 100 units of TINs
And then you see from your sales report that only 90 TINs have been sold
Then you are missing 10 TINs from your inventory
And you know where to look
Most companies are being silent victims of fraud because they are unaware of the fraud being committed in the first place
One important reason for this is because of the absence of proper data management
Which could have helped detect fraud easily in the early stages
Improving customer experience
Based on customers feedback you will be able to work on areas that can help you improve the quality of your product or service
And therefore satisfy the customer
Similarly when you have a repository of customer feedback you will be able to customize your product or service in a better fashion
For instance
There are companies that send out customized personal emails to their customers
This sends out a message that the company genuinely cares about its customers and would like to satisfy them
This is possible solely because of effective data management
Hiring process
Using data and selecting the right personal items to be a neglected practice in corporate life
It's critical to position the most qualified person in the right job in your company
You want your business to be highly successful in every facet of operation
Using data to hire the right person is a sure way to put the best person in the job
What kind of data would you use to hire a professional?
Big companies which have astronomical budgets use big data to locate and select the most skilled individuals for the right jobs
Startups and small companies would benefit immensely from using big data to hire the right group of people to make their recruitment successful from the start
This avenue for gathering data for hiring purposes has proven to be a successful avenue for hiring the right fit for organizations of all sizes
Again companies can use their data scientists to extract and interpret the precise data needed by human resource departments
Using social media platforms to recruit
Social media platforms
Facebook
Twitter and LinkedIn to name a few
Are hidden gold mines as data sources for finding high profile candidates for the right positions within companies
Take twitter for instance
Company job recruiters can follow people who tweet specifically about their industry
Through this process a company can find and recruit the ideal candidates based on their knowledge of a particular industry or job within that industry
Do their tweets inspire new thoughts and possibly new innovations for their industry?
If so you have a whole pool of potential job applicants
Facebook is another option for data gathering for potential job candidates
Remember these avenues are virtually free and corporations could use them as part of a major cost-effective strategy
Facebook is all about collecting social networking data for companies looking to expand their workforce or replace an existing open position or positions
Company recruiters can join industry niche groups or niche job groups
Liking and following group members comments will establish the company's presence within the group
Thus allowing highly focused job ads to be posted within the group
The company can increase views
Thereby widening the pool of potential job candidates
It is easy to establish a timeline to brand the company as an innovative and cutting-edge place to work
You establish your presence by engaging with friends or followers who are in the same industry as your company
For a minimal advertising fee you can promote your job ad
By doing this you geometrically increase your reach among potential job seekers
If your company releases highly effective job data posts you will reel in a higher yield of highly skilled job searchers
Therefore greatly increasing the percentages of people who are the perfect fit for your job
Niche social groups
Niche social groups are specialized groups that you can join on social and web platforms that can help you find specific skill sets
For example, if you're looking to hire a human resource manager
What better place to find a potential recruit than by joining a specific human resources social group?
Locate social connections within that group and then post descriptive but alluring job data posts
And you may find the right person to fit your human resources position
Even if your company doesn't find the right person group members will surely have referrals
Again approaching these groups is a very cost-effective way to advertise your job posts
Innovative data gathering methods for the hiring process
Why not think outside the hiring process box and try new methods of data collection to hire the right professional?
Use social collecting data sites that gather data such as facebook
google plus
linkedin
and twitter
Your company can search on these sites
Extracting pertinent data from posts made by potential job candidates
Such data can be used to help your company connect with highly efficient job applicants
An overlooked but very good data pool to use is keywords
Keywords are used on the internet for every type of search imaginable
Why not use the most visible keywords in your online job descriptions?
By doing this your company can widely increase the number of views that your job posting will attract
You can also use computers and software to find the right job candidate for your company
Traditionally these data sources have been used to either terminate a company's employee
Or analyze whether an existing employee is a right fit for another job within the company
Gamification
This is a unique data tool that isn't currently in widespread use
It does motivate candidates to press in and put forth their best effort in the job selection process
You provide people with badges and other virtual goods that will motivate them to persevere through the process
In turn their skills and being able to perform the job requirements will be readily obvious
This also makes the job application a fun experience instead of a typical tedious task
Job previews
Pre-planning the job hiring process with accurate data about what the job requirements are will prepare the job seeker to know
What is to expect if he or she is hired for the position?
It is said that a lot of learning on the job is by trial and error
Which prolongs the learning process?
This takes more time to get the employee up to speed to function efficiently as a valuable resource within the company
Incorporating the job preview data into the hiring process reduces the learning curve and helps the employee become efficient much quicker
These are some innovative data gathering methods companies can use to streamline the hiring process
They also help human resource departments pick the most skilled people to fill their employment needs
Hence data is crucial in aiding businesses to make effective decisions
These are some of the reasons why data is crucial for the effective functioning of a business
Now that we have had a glance at the importance of data
Let us get into the other aspects of data analysis in the upcoming chapters
Improving marketing strategies
Based on the types of data collected it can be easier to find attractive and innovative marketing strategies
If a company knows how customers are reacting to current marketing techniques
It will allow them to make changes that will fall in line with trends and expectations of their customers
Identifying data breaches
With the availability of data streams ever increasing it creates another problem when it comes to fraudulent practices
Although comprehensive yet subtle the impacts of data breaches can negatively affect accounting
Payroll
Retail and other company systems
Making decisions
Many important decisions about a business require data about market trends
Customer basis
And prices offered by competitors for the same or similar products or services
If data does not influence the decision making process it could cost the company immensely
For example launching a new product in the market without considering the price of a competitor's product might cause your product to be overpriced
Therefore creating problems when trying to increase sales
Data should not only apply to decisions about products or services
But also to other areas of business management
Certain data sets will provide information on how many employees it will take to foster the efficient functioning of a department
This will allow you to determine where you are understaffed or overstaffed
Chapter four
The art of data science
Data science is an art
It is not a concept that one can teach a computer
Data analysts use different tools to achieve their task
Right from linear regression to classification trees
Even though all these tools are known to the computer
It is the role of the data analysis to figure out a way in which he or she can gather all the tools
And integrate them to data to develop the correct answer to a question
When it comes to data analysis
It is not as easy as it looks
One of the reasons why it is difficult is because only a few people have mastered the art of data analysis
This means that only a few people can explain how it is done
Surprisingly, many people try to analyze data daily
But the majority fails in their efforts
This is because experts in this field haven't taken time to explain how they think while analyzing data
However, the process of data analysis has not been written down properly
While there are many books written about statistics
None of them tries to address how one can create a real-world data analysis solution
On the other hand, coming up with an important framework involves classifying elements of data analysis using an abstract language
In some cases, this language might be mathematics
Conversely, the same details of the analysis are what make each analysis complex and interesting
The cycle of analysis
You might look at data analysis and think that it follows a linear step-by-step process that has a well-developed result
However, data analysis is an iterative and non-linear approach that is depicted by a series of epicycles
In this approach, information is learned at each step which then decides whether to redo or refine the next step that is already performed or proceed to the next step
When it comes to analyzing data, the iterative process is used in all steps of the data analysis
Besides this, certain data analysis might appear fixed and linear because of the algorithms encapsulated in the different software
Therefore, it is important that one understands what it means by the term data analysis
Although a study of data involves creating and implementing a plan for gathering data
data analysis assumes that data is already gathered
Most importantly, a study will involve the creation of a hypothesis
designing of a data collection procedure
gathering of data
and interpretation of the data
However, since data analysis assumes that data should be collected already
it involves the development and refinement of a question and process of analyzing and interpreting data
While there are many different types of activities that one can engage in while performing data analysis
each aspect of the whole process can be undertaken through an interactive process
Most importantly, for each of the above five activities, it is advised that you include the following steps
Define or set the expectations
Collect information and compare the data to your expectations
If the data fails to match your expectations
Revise or fix the data so that both your expectations and data match
Going through all the three steps above is what is referred to as the cycle of the data analysis
While you navigate through every stage of the analysis, you will be required to go through the epicycle to constantly revise your question
Formal models
Interpretation
and communication
A repeated cycle through each of these five major activities forms the largest part of data analysis
Define the expectations
In this step, you intentionally lay down what you expect before you can do anything such as performing a procedure
inspecting your data or typing a command
For the experienced data analyst creating expectations might be automatic or a subconscious process
Despite this, it is important to think about it
For instance, if you are going to shop with friends and you have to stop by an ATM to withdraw some money
You need to decide on the amount of money that you want to withdraw
You need to have some expectations of the price of the things you are going to buy
This could be something that you have no problem with if you know the price or prices of the product or products you are going to buy
This is an example of prior knowledge
Another example of prior knowledge would be to know the time that a specific restaurant closes
Using that information, you can schedule your time and activity so that you show up for dinner before it closes
You can also find out additional information from your friends that will help you come up with expectations
Or Google a restaurant to learn more about their working hours
This procedure that you apply on prior information to develop expectation or implement analysis procedure
is the same used in every main activity of the analysis process
Information collection
This step requires one to collect information required to the question or data
For questions, one collects information by doing a literature research or finding out from experts
For the data, once you have developed some expectations about what the result can be when the data is inspected
It is okay to go ahead and carry out the operation
The results of this activity include data that you need to collect and determine whether the collected data matches your expectations
Comparison of expectations
Once you have the data in your hands, the next step is to compare your expectations to the data
Here, there are two possible results
Your cost estimations match the amount on the check
Your cost estimations fail to match
If both the cost estimations and amount match, then you can move on to the next activity
Alternatively, if your expectations cost $60, but the check is $30, then your expectations and data are different
In this case, there could be two possible reasons for the difference
The first is that you may have wrong expectations and you need to revise
And second, the check may be wrong and contains errors
One key indicator that can hint on the status of your data analysis is the easiness or difficulty
to match the data you collected to your original expectations
Volume, Velocity, and Variety
Big data has various V's
The major ones include velocity, variety, and volume
Big data surpasses the storage capacity of normal databases
The scale of data generated is massive
As of today, a huge amount of data is generated
One reason for this is because of the increase in interaction
Interaction is a new phenomenon besides just transaction of data
Data interaction comes from activities of the browser
Personal digital recorders
And geolocation
With the advent of the Internet of Things, massive data is produced that humans spend their entire time trying to analyze
A good data scientist should know how to control volume
He or she should know how to create algorithms that can intelligently use the size of the data effectively
Things acquire a new direction when you have a gargantuan data because each similarity becomes important
And one can easily make false conclusions
In most business applications, extraction of correlation is enough
However, the right data science uses techniques that determine the cause based on these correlations
Data velocity will always accelerate
There is an increase in Facebook posts
Tweets and financial information generated by many users at a higher speed
Velocity increases the volume of data and reduces the time of data retention
For example, a high-frequency trading activity depends on data streams and fast information
But the authenticity of the data reduces rapidly
Lastly, data variety has gone deep
Models which depend on just a handful of variables can now produce hundreds of variables because of the increase in computing power
The rate of change in volume
Velocity and data variety is currently possible for new economic metrics and various tools
Machine Learning
Machine Learning refers to how systems learn from various types of data they process
It is possible to train a system based on particular data to make decisions
The training process occurs continuously to enable systems to make updates and enhance decision-making ability
Systems that use spam filters are a great example to demonstrate how machine learning is applied
These systems use a bias and filter to change decisions
Therefore, it will continue to stay ahead of spammers
The ability to dynamically learn is important because it helps prevent spammers from gaming the filter
Credit approvals use neutral nets and are a great example of machine learning technique
Besides that machine learning refers to data compared to judgments
Hence, a good data scientist should have a variety of both
Machine learning has helped in finding answers to questions of interest
And it has further proved to be a game changer
What makes machine learning very interesting is the four characteristics of machine intelligence
It is built on a strong foundation of a theoretical breakthrough
It redefines the current economic paradigm
The final result is a commoditization
It unearths new data from data science
Supervised and unsupervised learning
There are two broad ways that a system can learn
Supervised and unsupervised learning
Supervised learning is where a system makes decisions depending on the type of data entered
Automated credit card approvals and spam filters apply supervised learning to achieve their functions
The system is supplied with a historical data sample of outputs and inputs
Based on this type of data the system establishes the relationship between the two using machine learning techniques
You will need to use your judgment to choose the best technique to handle the task
Unsupervised learning happens when you only have to input data
x without a corresponding output variable
Predictions and forecasts
Data science involves making forecasts and predictions
However, there is a distinction between the two
Predictions focus on highlighting a single outcome
If a person says that it will be cold tomorrow, he or she has predicted
But if they say that the chance of tomorrow being cold is 40%, they shall have made a forecast
This is because a forecast provides outcomes in the form of probabilities
Chapter 5 big data and its value
Across the globe data and technology are interwoven into society and the things we do
Like other production factors such as human capital and hard assets
There are many parts of the modern economic activity that couldn't happen without data
Big data is in short the large amounts of data that are gathered in order to be analyzed
From this data we can find patterns that will better inform future decisions
This data and what can be learned from it will become how companies compete and grow in the near future
Productivity will be greatly improved as well
Significant value will be created in the economy of the world because of increase in the quality of services and products while reducing waste
While this data has been around it is only excited people that are already interested in data
As times have changed we are getting more and more excited by the amount of data that we're generating
Mining and storing
This data is now one of the most important economic factors for so many different people
In the present we can look back at the trends in it innovation and investment
We can also see the impact on productivity and competitiveness that have resulted from those trends and how big data can make large changes in our modern lives
Like the previous it enabled innovations big data has the same requirements to move productivity further
For example, if you see innovations in current technology
Then there will need to be a close following after of complementary management innovations
Big data technology supplies and analytic capabilities are so advanced now that it will have just as much of an impact on productivity as suppliers of other technologies
Businesses around the world will need to start taking big data seriously because of the potential it has to create some real value
There are already retail companies that are putting big data to work because of the potential it has to increase the operating margins
Big data a new advantage
Since it has come to light big data is becoming an incredibly important way that companies are outperforming each other
Even new entrants into the market are being able to leverage statistics that data has found in order to compete
innovate and attain real value
This will be the way that all the different companies new and established will compete on the same level
There are other industries that are using the sensors in their products to gain data that they can use
This can be seen in children's toys
Large-scale industrial goods and so many others
The data that they gather show how the products are used in real life
With this data companies can make improvements on the products based on how people are really using them
This will make these products so much better for the future users
Big data is going to help create new growth opportunities and create new companies that specialize in aggregating and analyzing data
There's a good proportion of companies that will sit right in the middle of the flowing information
They'll be receiving information and data that comes from many sources just to analyze it
Managers and company leaders that are thinking ahead need to start creating and finding new ways to make their companies capable of dealing with big data
People that do so will need to be especially aggressive about it
The high frequency of the data will allow users to try to test theories and analyze the results in ways that they were incapable of before
There have been major studies of major industries that have found ways that big data can be used
Big data can unlock serious value for industries because it makes information transparent
There's a lot of data that isn't being recorded and stored
There is still a lot of information that cannot be found as well
There are people that are spending a quarter of their time looking for extremely specific data and then storing it
Sometimes in a digital space
There's a lot of inefficiency in this work right now
More and more companies are storing data from transactions online
These people are able to collect tons of accurate and detailed information about everything
They can find out inventory and even the number of sick days that people are taking
Some companies are already using this data collection and analysis to do experiments and see how they can make better informed management decisions
Big data allows companies to put their customers into smaller groups
This will allow them to tailor the services and products that they are offering
More sophisticated analytics are also allowing for better decision making to happen
There are fewer risks and bring light to information and insights that might not have seen the light of day
Big data can be used to create a brand new generation of services and products that wouldn't have been otherwise possible
Some manufacturers are already using the data that has been collected from their sensors to figure out more efficient and useful after sales services
Big data creates value
Using the u.s healthcare system as an example, we can look at ways that big data can really create good value
If the healthcare system used big data to use the efficient and quality of their services
They would actually create 300 billion dollars of value every year
70% of that value would have been seen from a cut in expenditures
These expenditures that would be cut are only 8% of the current expenditures
If you look at european developed economies instead, you can see a different way that big data creates value
The government administrations could use big data in the right way to improve operational efficiency
That would result in about 100 billion pounds worth of value every year
This is just one area
If the governments used advanced analytics and boosted tax revenue collection
They would create ever more value just from cutting down on errors and fraud in the system
Even though we've been looking at companies and governments so far
They aren't the only ones that are going to benefit from using big data
A consumer will benefit from this system as well
Using location data in specific services people could find a consumer surplus of up to 600 billion dollars
This can be seen especially in systems and apps that use real-time traffic information to make smart routing
These systems are some of the most used on the market and they use location data
There are more and more people using smartphones
If big data was used to its full potential
Then the annual productivity of the healthcare system could be improved around 0.7 percent
But it would take the combination of data from all these different sources to create that improved efficiency
The unfortunate part is that some of the data would need to come from places that do not share their data at scale right now
Data like clinical claims and patient records would need to somehow be integrated into the system
The patient in turn would have better access to more of their healthcare information and will be able to compare physicians
treatments and drugs
This would allow patients to pick out their medications and treatments based on the statistics that are available to them
However, in order to get these kinds of benefits patients would have to accept a trade for some of their privacy
Data security and privacy are two of the biggest roadblocks in the way of this
We must find a way around them if we really ever want to see the true benefits of using big data
The most prevalent challenge right now is the fact that there is a shortage of people that are skilled in analyzing big data properly
By 2018 the us will be facing a shortage of 140 000 and 190 000 people with training in deep analysis
They'll also be facing a shortage of roughly 1.5 million people that have quantitative skills and managerial experience needed to interpret the analyses correctly
These people will be basing their decisions off of the data
There are many technological issues in the way as well that will need to be resolved before big data can be used effectively by more companies
There are so many incompatible formats and standards that are floating around as well as legacy systems
That are stopping people from integrating data and from using sophisticated analytical tools to really look at the data sets
Ultimately, there will have to be technology made for computing and storage through to the application of visualization and analytical software
All this technology will have to be available in a stack so that it is more effective
In order to take true advantage of big data, there has to be better access to data and that means all of it
There are going to be so many organizations that will need to have access to data stores and maintained by third parties to add that data to their own
These third parties could be customers or business partners
This need for data will mean that companies that really need data will have to be able to come up with interesting proposals for suppliers
Customers and possibly even competitors in order to get their hands on that data
As long as big data is understood by governments and companies the potential it has to deliver better productivity
Will ensure that there will be some incentive for companies to take the actions that they have to get over the barriers that are standing in the way
In getting around these barriers companies will find new ways to be competitive in their industries and against individual companies
There will be greater productivity and efficiency all around which will result in better services even when money is tight
Big data brings value to businesses worldwide
Big data has been bringing value to businesses internationally for a while
The amount of value it will continue to bring is almost immeasurable
There are several ways that the big data has impacted the world so far
It has created a brand new career field in data science
Data interpretation has been changed drastically because of big data
The healthcare industry has been improving quickly and considerably since they added predictive analytics into the part of their business
Laser scanning technology is changing and has changed the way that law enforcement officers reconstruct crime scenes
Predictive analytics are changing how caregivers and patients interact
There are even data models that are being built now to look at business problems and help find solutions
Predictive analytics has had an impact on the way that the real estate industry conducts business
Big data is a big deal
Besides the fact that data is bringing so much value to so many different companies and industries
It is also opening up a whole new path of management principles that companies can use
Early on in professional management corporate leaders discovered that one of the key factors for competitive success was a minimum scale of efficiency
Comparatively one of the modern factors for competitive success is going to be capturing higher quality data and using that data with more efficiency at scale
For the current company executives that might be doubting how much big data is going to help them
There are these five questions that will really help them figure out how big data is going to benefit them and their organizations
What can we expect to happen in a world that is transparent meaning that data is readily available?
Over time information is becoming more accessible in all sectors
The fact that data is coming out of the shadows means that organizations which have relied heavily on data as a competitive asset are potentially going to feel threatened
This can be seen especially in the real estate industry
The real estate industry has typically provided a gateway to transaction data and a knowledge of bids and buyer behaviors that haven't been available elsewhere
Gaining access to all of that requires quite a bit of money and even more effort
In recent years online specialists are bypassing the agents to create a parallel resource for real estate data
This data is gotten directly from buyers and sellers and available to those same groups
Pricing and cost data has also been a spike in availability for several industries
There are even companies using satellite imagery that is available at their fingertips
They're using processing and analysis to look at the physical facilities of their competitors
That information can provide insights into what expansion plans or physical constraints that their competitors are facing
But with all that data there comes a challenge
The data is being kept within departments
Engineering
R&D
Service operations and manufacturing will have their different information and it will be stored in different ways depending on the department
However, the fact that all this information is kept in these little pockets means that the data cannot be used and analyzed in a timely manner
This can cause all sorts of problems for companies
For example, financial institutions don't share data across departments like money management
Financial markets or lending
This segmentation means that the customers have been compartmentalized
They don't see the customer across all these different areas, but just the separate images
Some companies in the manufacturing business are trying to stop the separation of data
They're integrating data from their different systems and asking their smaller units to collaborate in order to help their data flow
They're even looking for data and information outside of their groups to see if there's anything else out there that might help them figure out better products and services
The automotive industry has suppliers all around the world making components that are then used in the cars that they're making
Integrating data across all these would allow the companies and their supply chain partners to work together at the design stage instead of later on
Can testing decisions change the way that companies compete?
Gaining the ability to really test decisions will cut down on costs and improve a company's competitiveness
These automotive companies that will be able to test and experiment with the different components
By going through this process, they'll be able to gain results and data that will guide their decisions about operational changes and investments
Really experimentation will allow companies and their managers to really see the difference between correlation and causation while also boosting financial performance and producing more effective products
Companies that deal with the public have been dividing and targeting specific customers for quite a while now
Big data is taking that further than it ever by making it possible for real-time personalization to become part of these companies
Retailers may become able to track individual customers and their behaviors by monitoring their internet click streams
Knowing this they will be able to make small changes on websites that will help move the customer in a direction to buy
They will be able to see when a customer is making a decision on something they might purchase
From here they will be able to nudge the customer towards buying
They could offer bundled products
benefits and reward programs
This is real-time targeting
Real-time targeting also brings in data from loyalty groups
This can help increase higher-end purchases made by the most valuable customers
The retail industry is likely to be the most driven by data because they're keeping track of internet purchases
Conversations taking place on social media and location data pulled from smartphones
They've got tons of data at their fingertips
Besides the data they have better analytical tools now that can divide customers into smaller segments for even better targeting
Will big data just help management or will it eventually replace it?
Big data opens up new ways for algorithms and analysis
Meditated by machines to be used
Manufacturers are using algorithms to analyze the data that is being collected from sensors on the production line
This data and analysis helped the manufacturers regulate the process
Reduce their waste
Increase outfit and even cut down on potentially expensive and dangerous human intervention
There was another global company that was learning a lot by looking at their own data
From doing the analysis for themselves
They eventually decide to branch out and create a business that analyzes data for other organizations
The business aggregates supply chain and chop floor data for manufacturers
It also sells relevant software tools that a company will need to improve their own performance
This side business that the company opened is outperforming the manufacturing business and that is because of the value of big data
Big data is creating a whole new support model for the markets that already exist today
Companies have all sorts of new data needs and they need qualified people to support that data
As a result, if you own a business, then you may need an outside firm to analyze and interpret any data you're producing for you
These specialized firms can take large amounts of data in various forms and break it down for you
These firms exist because there is a need for support for larger companies in many different industries
The employees they hire are trained to locate and capture data in systems and processes
They're allowing larger companies to focus on their work and doing the data aggregation for the company
They assimilate
Analyze and interpret trends in the data and then they report to the company about any notifications that they have
For a company that doesn't want to hire out a firm
They have the option to create a data support department within their own company
This will be more cost effective than hiring an entire outside firm
But it does require very specific and specialized skills within the company
The department would focus on taking the data flow and analyzing
interpreting and finding new ways to use the data
These new applications and the new data department would monitor existing data for fraud
triggers or issues
Big data has created a whole new field of studies in colleges and higher institutions of learning
People are training in the latest methods of big data gathering
analyzing and interpreting
This path will lead people to critical positions in the newly trending data support companies
Big data has created all sorts of changes and it will continue to make even more
In education areas big data will influence and change the way that teachers are hired
The data will be able to look at recruiting processes and predictive analytics will be able to look at the traits
that most effective teachers are going to need to most properly maximize the learning experience
Big data is a term that is used for data that is so large that traditional applications for analyzing data are useless
However, the term big data can also be used when talking about specific advanced methods of collecting data
And often does not refer to the size of the data being collected
Discovering patterns and taking action
Discovering the patterns of customers is quite interesting
But it cannot be put to use unless we are able to take some sort of action that will lead to an increase in sales or a reduction of costs
Many people have heard the story of beer and diapers
It is a story about one large retailer that through data mining found that when men came into the store on Friday nights to purchase diapers
There was a high percentage of them purchasing beer as well
This allowed the company to change the layout of the store so they could place the beer next to the diapers
When the company did this they found that the amount of beer they sold increased dramatically
Once said to be a true story, it is believed to be an urban legend
But it can still help us today to understand how important data mining really is
You see although this little nugget of information could have easily been overlooked a pattern was discovered through big data
Inaction was taken and profits increased
In the example above the information that was obtained was used to identify a previously unknown group of customers
Men who purchased diapers on a Friday night
Then use the information gathered about this specific group and increase sales
Today the same type of information is being gathered and used
However, with the data that is available today
Companies are able to focus on very narrow groups of people to ensure their profits are boosted
Inventing process
Another way that big data adds value to companies is it allows them to invent processes or improve upon existing processes to improve their business
One example of this is when businesses used to have to track all of the product by hand
And after all of this tracking they would have to place orders for more product
Then came the time the data was used to place the order
It became apparent to companies that it was easy for them to track what was being purchased and in what quantities through what was being scanned on the register
Of course because the item was sold it was taking away from what they had in stock
Therefore they were able to use the numbers they obtained from registers to place their orders
Today many companies do not even have to do this
They simply decide how much of which product they need to keep in stock
And when it comes time to order their computer spits out a paper telling them exactly what to order and in what quantity
Other companies never have to worry about ordering anymore because it is all automated and they receive the amount of product
They need based off of their previous sales
All this took place because of the data that was able to be collected
A process was improved upon and now we have much simpler processes than we once did
At least in terms of retail business
Advantages of using big data
You may be sold on the idea of big data helping you to understand your customers
But any data can do that
This may leave you asking what the advantages of big data are
Allows a dialogue with customers
Today's customers are tough
They do a lot of research before they make a purchase
They ask their friends about their purchases and they want to be treated a certain way
And they want companies to truly be thankful for what they choose to purchase from them
Big data allows businesses to have an almost one-on-one conversation with their customers
Of course, this does not mean that businesses are going around having a one-on-one conversation with every single customer they have
But take for example company A
This company has a member's reward card that members scan before making their purchases
This allows the workers to immediately know what the customers purchase on a regular basis
And it helps the business know what type of up sales can be made to any specific customer
This can also allow companies to know that they may be able to sell a specific product to their customer
Based on what they have been searching for online as well as what they have been posting on social media
For example, if one customer posts that they want a phone with great signal
But they're not willing to pay the high prices that comes with a contract phone
This information might allow a company to offer this specific customer a prepaid cell phone the next time they come into the store
Redevelopment of your products
By using big data
You are going to be able to understand what aspects of your product people like and what aspects people think need improved upon
You can also find out how different groups of your customers feel about your products
For example, a stay-at-home mom may not feel that your product is meeting her needs
While the mom who works outside of the home feels that your product makes her life much easier
Perform a risk analysis
By using big data you'll be able to understand what your risk will be if you for example change a product
Big data also allows you to look at the data in real time
This means you do not have to rely on information that could be days or weeks old
But instead you can keep up with the trends as they are happening
Allows you to have the ability to identify important information that is going to help improve the quality of the decisions being made
Although it can seem complex and hard to understand big data can provide many benefits for many benefits
The benefits mentioned above prove that big data delivers the value companies are looking for
However, there are still a few issues like when it comes to how big data is going to evolve
But that does not mean that this is the time to question the value of big data
In fact, that time has come and gone
Many companies have proven that big data is able to successfully help to reduce costs
improve decision making and even help companies improve upon already existing products while helping them to create new products
It is clear that this era of big data is going to provide huge business opportunities
And it is important that businesses do not wait too long before taking advantage of big data
Chapter six data science as a change agent
Data science is a change agent
Even though it has not been realized yet. This should be the perfect time for organizations to be ready
Companies that plan to change in the next couple of years should position themselves by gathering the correct data and investing in analytics capacity
The main thing to consider while building a predictive model is to establish
Whatever it is that you want to predict and gather massive data sets which will permit you to do so
Even though it is still far to realize the change in management by applying predictive models
Organizations can remain on the right path by adopting the right tools and collecting accurate data
Use digital engagement tools
There are always new systems created to allow organizations to create real-time feedback from employees
As of now organizations can take advantage of these tools and help enhance their services
Most of these tools have been developed and installed with more functionality to reveal more information about an employee
These are tools which will indicate more than just what employees think every time
Most of these tools provide a relevant change to the management and can further let one determine whether a change is received equally across many locations
By working with large travel tourism firms, it helps create a system for real-time employee feedback
This provides a chance to experiment with various change strategies in a particular population company
The real-time feedback implies that one has to learn quickly how communications and interaction tactics are received
Therefore, it optimizes the actions in days instead of weeks
This particular data is then fed into a predictive model that will help one to accurately identify
Actions that can improve the adoption of a new practice and behavior by a specific group of employees
Commercial tools that are available include iq polls and sampling groups of employees through a smartphone app on a weekly basis to extract real-time insights
Many of these tools generate a big change, but the data stream produced is even more important
As a result, it is essential to deploy these tools to achieve the success that comes with a change in data-driven processes
Social media analytics and stakeholder sentiment
Managers of change have another option to see past the limits of enterprise to draw insights related to the impact of change programs
Customers, investors, and supplies are major stakeholders in the change programs
They have a likely chance to participate in social media and comment about the changes which a company plans to make
Advances in linguistic text analysis imply that one can detect clues linked to actions and behavior from the choices of words that people use
Besides that the use of certain phrases articles and pronouns can help an individual discover how the other party feels
By applying such tools to analyze emails of a company it eases the process of finding out insights and reactions of employees to various changes implemented
Therefore insights generated after analyzing the internal conversation in an organization will be stronger when it is combined with external social media data
Recording a reference data related to change in the current projects
You will discover that most organizations focus on measuring fractional shifts in the inventory turns
Operational performance and manufacturing efficiency
However issues dealing with a change have a low track performance
Even if projects have unique characteristics, there are many similarities when it comes to changing a system
Improving a process and reorganizing projects
There is a chance to record information related to the team
How long it takes to implement and the tactics applied
Building a reference data set similar to this may not guarantee an instant gain
But the entire data set increases
Therefore it becomes easy for one to build an accurate predictive model that can be helpful in changing an organization
How data is important in change
For many years companies relied on data driven techniques to select candidates fit for senior roles
Nowadays retail businesses have also adopted predictive analytics to hire their employees
As a result these tools have improved the performance of a project and helped develop a new data set
Let's assume that every leader and member of a team was subjected to psychometric testing and evaluation before a project
The data produced from this testing can be useful in the identification of a casual model that can generate the correct change project
Creation of a dashboard
Each organization is imagined to have a bespoke dashboard created together with the firm's leadership team
This reflects the priorities of the organization
Future plans and competitive position
By doing it this way dashboards generate significant insights into a particular transformation investment created by the organization
Most data that contributes to these signs are already available but not collected
There are clients of change logic that have developed a dashboard to identify attrition and recruitment in a must-win talent
This is not hard but it helps to enlighten the managerial team on how to use data to make intuitive decisions
While it might take some time for such tools to be created organizations need to begin to create dashboards
If possible they should automate them
As of today most of these dashboards are vulnerable to version control problems
internal politics and human error
Computerizing the dashboards makes it objective and transparent
While companies gather data and use data scientists to build accurate models
All managers of change must be confident to call data scientists to come and interpret procedures that allow firms to meet their goals
Creation of metrics can't be easy because metrics aren't one and done installations
Instead it is a commitment cycle to capture data
Refine dashboards and create models
To develop a reliable data set takes time
Even if it might end up taking a lot of time
It will finally end the casual loop and produce a reliable prediction about how an action can alter a given metric
This will increase investment and change from particular action to another data informed perception
Change management helps shift a struggling project and recommends significant business results and how you can realize them
Innovation mindset
While organizations continue to shift to digital transformation methods
More teams that are knowledgeable about data science are being formed
So far the major challenge for chief data scientists involves positioning the data science function properly
Where a given organization need is to increase its current and future activities
This means that there is a need to recruit data science teams to completely interact with the business and adopt the operational backbone of the firm
Data science has no formal description
Its goal is to understand and analyze actual phenomena using data
This can highly vary from one industry to another
However, with experiences from both worlds
One can define data science as an integration of skills and programming
Mathematics and communication with the application of scientific method to a particular domain of knowledge
This practice can be summarized into different strategies
Skill sets
This requires an individual to measure the readiness of analytics
Talent management
Spreading evidence-based culture
And application of data science processes
Data sets
This often covers data governance
Infrastructures
And strategic data sources
Tool sets
This deals with the selection of the right data science tools
And use of the best practices in the company or firm
Mindset
This will collect all the animating principles that support the ethos of a data science function to create value
And innovate at the center of a digital transformation
Mindset is the main force that changes investments in skill sets
Data sets
And tool sets
Into a cultural and economic impact
Below are a set of principles that act as a blueprint for how one can develop the mindset of innovation in data science
Culture before technology
In the modern era of technological push it is easy for anyone to be dragged into the technology trap
Some of these tools may result in distractions and short-term cosmetic results
Similar to other innovative teams technology does not attract or retain talent in the data science
Instead what drives people is the healthy culture which ensures everyone is satisfied
Supported and challenged in his or her jobs
Stories and not spirits
In the early stages projects should have a buy-in from the leadership so that they can deal with anti-change agents and handle unrealistic expectations
With the advent of agile methodologies most firms have turned to sprints
However, innovation calls for both time and patience
Stories are an amazing way that one can create the right focus over time and build patience for teams to help in the execution
Demos
Visualizations and other types of stories motivate data science teams to wait
Ask questions and concentrate on visions then planning for a sprint
Narratives deliver the right chance to think deeply and consider a given business via interactions
System dynamics as well as how the change in analytical capability might affect an organization
The material generated furthermore acts as a link to shared knowledge outside the organization and across disciplines
It champions for a positive and creative spirit
Ethics and not profits
Since the practice of data science is popular teams should develop a moral direction and techniques that predict the limits of their discoveries
In addition, it is part of their leadership duty to assist in determining a social mission of the developed algorithm and analytics
It calls for collaboration to ensure the use of data science
For instance at the massachusetts institute of technology
MIT
Urban demos provided a means where one has to explore whether the concepts are culturally or socially acceptable before they were potentially profitable
By championing for incentives that would result in social good and at the same time align people before profit
It helps enrich the analytical capacities with members that may not have participated in the commercial ventures
Polymath before expertise
Data science includes a multidisciplinary concept and cannot work in isolation
To ensure that you can collaborate with other parties
Specialists must learn beyond their domain expertise
There are certain organizations which may fall into the trap that is a must to have a phd to do data science
This is not true because this practice calls for one to have an enhanced sense of curiosity to help learn the language of other disciplines and an in-depth appetite for collaborative learning
There are some of the traits of a polymath that makes a team ready to interact with people with diverse roles in the organization from design
product management
communication
finance
and many more
Of late terms such as ai designer and product scientist have risen in the industry to emphasize the importance of connecting different disciplines with the scientific method
At the same time specific views from interdisciplinary domains such as science
Technology data bias and algorithmic have become part of the major themes of data science
A generalized specialist will deliver an interdisciplinary knowledge that champions for creativity and a deeper understanding of what an organization or society needs
A team made up of generalized specialists presents a better general perspective for complex
Deep and unconventional areas compared to a team of experts
Chapter seven how to use data science right
While there is a lot that you can do with data science
You must remember that it is mainly just a tool that you use in business
If you know how to use it properly and you make sure to stay efficient with it
Data science can be a great tool that helps limit your risk and even make you more money
However, if you do not use it properly, it could easily cause a lot more harm to your business than it does good
It is easy to become captivated with all the possibilities that can come with data science
But if your business can't afford it or if you just try to use it without the right experience or knowledge
Then you will end up costing your business a lot of money
To get as much out of the wealth of data that a business has and information on the internet
management must think of the data analytically
If management is not able to do this then they will become completely dependent of the results from data mining
And they won't think for themselves
There's a ton of information that comes from the data mining process
But you must think it through and combine your knowledge and expertise to get the best results
Of course, this is not to say that the management needs to be data scientists to understand the information and to use it
It just means that the managers of an organization at least need to know some of the basics to appreciate the different opportunities that it will provide
You do not want to waste the vulnerable resources that data science can provide simply because you don't understand how it works or what all it can do for you and your company
As a manager, there are a few things that you should be able to do
Even if you are not a data scientist
You should be able to appreciate all the opportunities that this information provides
Make sure that your data science team has the resources that it needs to get the job done
And be willing to invest your time and money so that data experimentation occurs
Finally, you must be able to work with your team to ensure that they stay on track and help you get information to help move the business forward
How data science gives a competitive advantage?
Data science as long as it is used correctly can give a business a big competitive edge in their market
To have an advantage over the competition you must make sure that you are always one to two steps ahead of them
This can be done through a willingness and the act of investing in new data assets and also the development of new capabilities and techniques
It also requires that you not only treat the investment and the results from this as an asset
But you must also treat your data science team and the field of data science in the same way
With the best data science team you will be able to gain the useful insights that you need to help move your business into the future
There are so many businesses that will rely on experience and knowledge to help them
And if you have been in the industry for a long time, you will probably do well
Most of those who are new to an industry will end up failing with this though
However, even if you are doing well
Data science could provide you with some useful information and open up new doors that you may not have thought about in the past
Chapter 8 Data Security
A security data scientist refers to a specialist in data analysis for fraud and security
They have a different range of specialties which might consist of one or more of the following
Security metrics
Malware analysis
Insider threat detection
Fraud and loss analytics
Computer and network forensics
And many more
Security data science
This phrase refers to the application of complicated analytics to access and discover unknown risks
In general data science refers to the method of extracting important insights from data
When it comes to security important data insights help reduce risks
Data science has emerged to fulfill the challenges of processing extensive data sets
Big data and the exploration of new data produced by smart devices
Social media and the web
Data science has a long and rich history of fraud and security
Both information security and fraud monitoring fields have been going through changes to deal with problems and draw insights from extensive data
Why security data science?
This is focused on upgrading information security via practical applications of statistics
Data analysis
Machine learning and data visualization
While the tools and techniques are not different compared to those applied in data science
This field has a major focus on decreasing risk and identification of fraud
It is believed that domain knowledge and experience is important in the successful application of analytics to cut down on the risk in fraud losses
Unmasking information security using data science
It is not easy to find good resources written about security data science on the web
What is the reason for increased ransomware attacks and data breaches?
There are quite a number of reasons to explain the rise in ransomware attacks and data breaches
Attackers discover an efficient way to generate quick cash using ransomware
One reason for this is that you can find ransomware as a service on the dark web
As a result attackers can choose to leverage on the ransomware service and concentrate on the ransom extortion
The attack surface has increased and the network perimeter is dissolved as a result of cloud and mobile
Attackers have increased the number of tools as a means to escape the current information security tools
The information security team has insufficient cameras to monitor movements of an intruder in the network enterprise
Therefore adversaries have an advantage because they can move in any direction within the network of an enterprise
Challenges of information security
There are many points of vulnerability that an attacker can use to enter into a network enterprise
It is not easy to provide total security because the tools that you use to enforce security of a network aren't 100 percent perfect
Some may fail to distinguish between a genuine user and an intruder
Information security is skewed
The security team has a responsibility to write down more than 10 000 lines of code to fix a point of intrusion and enforce system security
However adversaries just need to identify a point of weakness to attack and create a security patch
The adversaries apply the same commands tools and scripts that a system administrators use
Based on the skill set of the attacker they can choose to use a tool such as nmap
Metasploit and powersploit
Why information security should have a data scientist
Once attackers get close to an enterprise network they need to determine the point at which they are
Once they identify the locations they approach the targets and perform the attack
While carrying out some of these operations they may leave behind certain traces to reveal their signals
These signals can be found in data and their presence can be unearthed with the help of a data scientist
In the beginning all the data used to be transferred to a security data lake such as security information and management
SIEM
Challenges of data science in the information security
Challenges experienced in the information security are multi-dimensional
What this means is that many features exist in tons of data sources
That is why it is critical to identify the presence of an attacker through mining data found in machine logs
This is one of the most complex problems because both the signal to noise ratio is very low
In addition to set up a link between independent and attack sequences is a very big endeavor
But the industry has chosen to deal with this problem by producing class labels
For instance detection of malware and ranking of dns domains is completely done by application of machine learning techniques
Another way these cases are applied in security data science is in the development of a baseline for every network
Then making comparisons to discover anomalies
The evolution of security data science
The security data science has undergone the evolution of three phases
Rule-based and detection of an anomaly by systems
Data science has been playing an important role especially in information security
This started with rule-based techniques that helped an individual discover strange activities in the intrusion detection system and prevention
Rules are defined and set up by security experts
In case of an intrusion an alert is sent out
For instance, if an attacker tries to break into a system and reaches the maximum count of attempts
An alert is sent out to the security experts
These anomaly detection systems usually depend on the normal behavior of network and hosts
This means that when something extends beyond the normal behavior an alert is generated
Fortunately, there are anomaly detection algorithms to detect any unusual activity
Anomaly-based algorithms are built on networks to facilitate
Hosts and users that have abnormal behavior
Excessive dns failures
Anomalous ports
Unusual traffic from a host
However, many of the ad systems generated false alarms and call for security analysis to confirm the alerts
Security data lakes
It is important to transfer an alert and combine multiple data sources in the security data lake
Malware detection
Big data frameworks have a new security data science technique
This makes it possible to use row logs in real time and generates alerts
As a result, new user and entity behavior have been created to leverage on spark and detection of an anomaly
So far enterprises can quickly notice when there is an inside attack because of the new solutions invented
However, there are still some issues with the same means to detect an anomaly
Another area that has increasingly gained attention is the endpoint security where there is deep learning applied in the detection and classification of malware
Supervised ML algorithms are applied in the classification of malicious scripts
Detection of dns tunnels
Malware detection
Application of attacks and a lot of known threats that include labels present in the training system
Deception triggered data science
In the modern world a new paradigm shift for information security field has evolved
In the security defense deception is applied first in the entire enterprise network
Next data science is used to profile the behavior of an adversary and their actions in the network
Deception triggered data science is not the same with normal data security data science
For the normal data security it mainly depends on techniques to detect a glitch in the network traffic
Even if this method starts with a real attack a glitch revealed by a deception activity does not need an anomaly detection algorithm
Deception alerts represents a critical alert
Data science is similar to other security events which have a high fidelity alert to draw insights related to the adversary behavior
In this situation context is collected and described around a deception alert rather than searching for anomalies
This type of data science can dwell most on capturing everything that is linked to an attack
Three trends in security data science in 2018
Machine learning for response automation
Machine learning has proven to be an important tool when it comes to detecting evidence of threats used in compiling a report
Security experts can subconsciously train themselves to respond to evidence of an event in a given way
The key to information security depends on the information security analysis plus a majority of the knee-jerk responses that can be automated
In most cases the response may be machine learning automated
The information overload pane is not a recent problem for machine learning
However, various pressures call for a more widespread application of ml to simplify response via machine learning distillation instead of complex additional evidence
Some of them include market pressure to maximize workflows
Diminishing returns on reduction time
Machine learning to handle automation
There have been trials to reveal how ai can scale digital attacks
Tool sets are making the barrier to entry low
There are several economic drivers to facilitate bypassing of captcha
The current security risks and exploits are much more embarrassing and complex for advanced adversaries to succeed
Model compliance
Global laws determine the design
Engineering and operational cost of security data science solutions
The laws provide strict guidelines on the handling of data
Movement and model building constraints
Model compliance is not a one-time investment
Privacy laws can change depending on the political landscape
Creating models which adhere to compliance laws is critical
Some of the actions which you can do include
End customers should audit data and tag it correctly
Perform tiered modeling
Every geographic region should be modeled separately
And then the results need to be scrubbed and transferred to a global model
Chapter 9 data in the cloud
Data science refers to an intersection of many important concepts
To be a data scientist you need to have solid programming skills
Even though you might not be familiar with all programming concepts such as general production software architecture and infrastructure
You are required to have some basic skills in computer science concepts
Before you start a data science class
It is a must that you install our language in python on your computer
While advanced analytics continues to expand data science teams continue to evolve
This calls for a collaborative solution such as a recommendation system
Predictive analytics and so on
Research and notebook tools integrated with code source control are an example of a collaboration solution
Collaborative requires inclusion of those working outside especially when data science sets out to accomplish business goals
What is a cloud?
While it sounds hypothetical and abstract a cloud has a concrete meaning
Before moving on here are some definitions of a few concepts that you need to know
Network
This refers to a collection of computers connected together that share resources
The internet is a good example of a network
Home networks like wi-fi service set and local area network have multiple computers connected
Shared resources include media, web pages, app servers, data storage, and printers
Computers in a network are referred to as nodes
Intercommunication between computers is through internet protocols such as http
TCP
and IP
Some of these communication protocols can help update a status
Make a request and many other uses
Additionally, it is hard for computers to be located on premise
In other words, both data and applications are found on computers in the data center
A data center is useful because it has the necessary infrastructure to support security and protection among others
Since the cost of computers and storage has continued to reduce over time
Many solutions now involve multiple computers that work together
And they aren't that costly when one wants to scale
This is very different from scaling solutions which include buying a powerful and expensive computing device
The reason for collaboration is to facilitate continuous operation even if a single computer breaks down
Besides this collaboration enhances the system so that it can automatically scale and handle any load that is applied
Popular social networking sites such as YouTube, Netflix, and Facebook are good examples of cloud applications that must be scaled
It is rare to see these applications failing
This is because they have their systems hosted in the cloud
A cluster refers to a group of computers connected to the same network and all work to accomplish a similar task
You can consider it as a single computer that can improve performance, scalability, and availability
Next, let's define the cloud
In simple terms, cloud refers to the process where a single entity can have total control over a group of network computers and generate software solutions
Depending on how the cloud is defined, the internet is said to be a network and not a cloud
Data science in the cloud
Anyone who is familiar with data science is aware that the process takes place in the local machine of the data scientist
The computer is already installed with a programming language
This can be R or Python
The data scientist further installs relevant development packages using a manager such as Anaconda or installs individual packages manually
Once the development environment is ready, then the usual data science workflow starts
With data as the only element needed
Sometimes it is not a must to carry out all the data science and big data related tasks in a different local development environment
Here are the reasons why
The development environment processing power fails to carry out tasks in a reasonable amount of time
Presence of large data sets that cannot be contained in the development environment's system memory
Deliverables must be arrayed into a production environment and incorporated as a component in a large application
It is advised to use a machine that is fast and powerful
If such instances arise, there are many options to take
Instead of using a local environment of a data scientist
People deploy the computing task to an on-premise machine or even a cloud-based virtual machine
The advantages of using virtual machines and auto scaling clusters are one can span up and discard it at any time they need
Also, it is customized to fulfill one's computing power and storage needs
The process of deploying a deliverable into a production environment so that it can be used in a large data pipeline
Data pipeline has many challenges that an individual must consider
Besides the custom developed cloud-based solutions
There are still many clouds and service-based offerings available from specific vendors
Software Architecture and Quality Attributes
Software architects design a software system that is cloud-based
This system may represent a product
Service or a task dependent on the computing system
One of the tasks involved in building software architecture includes selecting the right language to program
This may call for much consideration, especially centered on the purpose of the system
This part of software architecture requires a person who is experienced and skilled
Cloud computing enhances an agile culture
It is a mature market and several large companies have started to build effective and elastic cloud environments
Some of these environments have been deployed on pools of server networking resources and storage
They are more cost effective and this seems to increase the roi of advanced analytics
Generally applications built to support and use cloud embrace and allow fast moving as well as enhance
Creativity from teams
Additionally teams are taking advantage of the big space in the cloud to store more data and discover a lot of use cases for data
With cloud computing it is possible to release a feature now and have it tested immediately to reveal whether it adds value
Sharing big data in the cloud
Traditionally it was risky to let business process wire data into silos
Teams had a big problem when they wanted to share insights
Collaboration used to be a big problem
Not forgetting to mention the difficulty in transferring large amounts of data
Cloud computing has reduced most of these problems and made it easy for teams to work together across different distances
It is very difficult to ignore the benefits of cloud computing in the big data field
In fact, most companies across the world rely on the cloud
Take uber and airbnb for example
These companies adopt cloud computing as a means to improve information sharing and data
Getting big data insights fast using cloud
There was a time when big data was considered expensive and overwhelming
During this period efforts in big data needed to be reactionary and generate insights
Cloud computing removed the need of a data warehouse
This means that you don't need to worry about analyzing compiling and collecting data because you will have the best tools to use in big data
Take the example of gathering customer analytics and data science
If you choose to use cloud and big data technology
It will make the whole process easy and allow you to collect information from various sources and sales
marketing and web analytics
Even without the need of massive servers companies can get data and quickly analyze it before making use of it
Cloud and big data governance
Cloud is a great thing, right?
However, most companies are scared about how to regulate governance
privacy and control
Big data comes with its challenges and implementation of cloud data brings with its issues on privacy and security
This is the reason why it is critical to develop a solid governance plan in your cloud solutions
Make sure that it is an open architecture and forward compatible
This will ensure that your cloud solution remains robust and governable
Why do data scientists need cloud tools to deliver the value of data for businesses?
Data scientists help organizations to begin using data for transformative purposes
Data scientists continue to be in great demand today because of the massive data that organizations have and need to deal with
There's about 80 percent of unstructured data that organizations receive in the form of social media
Emails videos and images
With the growth in cloud computing data scientists need to deal with new workloads from IoT services
AI and analytics
Accessing data in the cloud is important to any data scientists today and they require a centralized and accessible platform across all teams
As digital growth continues to power many companies and industries around the world
There is an increasing need to record and manage new and legacy data
Once a data scientist has easy access to this particular data
He or she is already equipped with the right skills to analyze the increasing volumes via cloud technology
And turn information into insights which can change the industries and businesses
Most companies employ a data scientist to build an algorithm and machine learning model
Which is a part of their favorite job?
Data scientists spend about 80 percent researching
Cleaning and organizing data
This leaves only 20 percent to analyzing of the data
As a result organizations must create new cloud services and technology to enable data scientists with the tools needed to search and organize massive data quickly
This will set aside more time to concentrate on where their skills are most valuable
Such areas include analyzing and working with the growing data set generated by sensors and users
Cloud is the main ground that will allow data scientists to save
access and extend models
The biggest part of data science is assessing a model to make sure that it is strong and reliable
In addition, data science modeling is highly associated with building information feature set
It involves different processes which make sure that the data at hand is harnessed in the best way
Robust data model
Robust data models are important in creating the production
First, they must have better performance depending on different metrics
Usually a single metric can mislead the way a model performs because there are many aspects in the classification problems
Sensitivity analysis describes another important aspect of data science modeling
This is something that is important for testing a model to make sure it is strong
Sensitivity refers to a condition which the output of a model is meant to change considerably if the input changes slightly
This is very undesirable because it must be checked since the robust model is stable
Lastly, interpretability is an essential aspect even though it is not always possible
This is usually related to how easy one can interpret the results of a model
But most modern models resemble black boxes
This makes it hard for one to interpret them
Besides that it is better to go for an interpretable model because you might need to defend the output from others
For a model to work best it must require information that has a rich set of features
The latter is developed in different ways
Whichever the case cleaning the data is a must
This calls for fixing issues with the data points
Filling missing values where it is possible and in some situations removing noisy elements
Before the variables are used in a model you must perform normalization on them
This is achieved using a linear transformation or making sure that the variable values rotate around a given range
Usually normalization is enough for one to turn variables into features once they are cleaned
Binning is another process which facilitates featurization
It involves building nominal variables which can further be broken down into different binary features applied in a data model
Lastly some reduction methods are important in building a feature set
This involves building a linear combination of features that display the same information in fewer dimensions
Important considerations
Besides the basic attributes of data science modeling
There are other important things that a data scientist must know to create something valuable
Things such as in-depth testing using specialized sampling
Sensitivity analysis
And different aspects of the model performance to improve a given performance aspect along to data science modeling
The future of data science and predictive modeling
There's no question on how predictive analytics is important
The new algorithm-based discipline has empowered us to deliver insights on the probability of a given outcome
Use cases of data science and predictive modeling
Appear like trip planning tools to help customers define locations
Dates
Hotel needs
And other factors which impact travel details
These products represent an evolution towards a user-friendly data science tool that can change a customer into a data scientist
From predictive to prescriptive
Data scientists are shifting towards a practical application of prescriptive instead of predictive modeling
While the latter applies historical data to predict the probability of future events
The former makes an assumption that an active human agent is capable of affecting outcomes
Quick definitions
Descriptive analytics
This represents the first stage of business analytics where you look at historical data and performance
Prescriptive analytics
This represents the third stage of business analytics where one determines the best course of action to take
Predictive analytics
This is the second stage of business analytics where you choose the best course of action
Data scientists can create a prescriptive uplift model to determine the chances of converting a lead with a given offer
For example to determine whether sending a sign-on bonus can affect a prospective employee's chances of accepting a job
This way it becomes important for proactive planning
In addition, it creates a line difference between machine and human interactions
Older predictive models for example provide end users with insight into the likelihood of an event
It is then left to human agents to decide on what they can do with that information
However, the prescriptive uplift model can determine the events of an outcome if the end users decide to follow a given course of action
Therefore, it helps determine if a marketing campaign can win over a given demographic or provide solutions for a political campaign to win votes
The prospective applications of this type of proactive data are unmeasured
They are especially important when a person wants to predict results in retail
marketing
politics
and charitable donations
Deep learning
Machines performing the processing patterns
The response to prescriptive modeling is a significant development as a result of the popularity of machine learning technologies
Modern data scientists are called to closely work with software engineers to develop new automated tools
Most of these tools that they create use machine learning technology
The chances are that you might have heard of technological development
It is very popular in the data science field
But the thing that is most popular is machine learning because of how it relates to the big data and data science
Machine learning provides powerful applications of big data and statistical probability
It helps break down divisions that exist between data processing and data collection
In this case the machine continuously adjusts its behavior and response depending on the environmental data
A good example of this is amazon's recommendation tool
The more data that is fed into the tool by users via buying and browsing the more precise the recommendation
Tools such as these represent evidence of real-time interaction between devices and humans
While humans respond to results machines customize their next offerings depending on the feedback
This is like a conversation compared to a chain of commands
Deep learning is a branch of machine learning
It models the structure and processes patterns of the human mind
Deep learning computers are created using neural networks that have layers of nodes developed on top of one another
This kind of architecture creates connections with another non-linear fashion
Technologies in deep learning are important at emulating the type of visual processing carried out by the brain when it responds to signals from the eyes
For example google translate app has deep learning to translate unembedded text from images into another language
A user that is traveling into a different country can take a picture of a box of cereal and upload it to google translate
Google can then identify text on the box and return a translation in the native language
In the same way there's another google project called google sunroof which takes images from the google earth application and builds 3d models of rooftops using a solar installation
By using deep learning neural networks
The sunroof tool can distinguish roof surfaces from cars or trees
Even when obscuring factors exist such as tree coverage and shadow
Deep learning tools such as these permit data scientists to develop predictive models depending on the unstructured data like audio
Video and images instead of depending on structured information such as numbers and text
Data science converging with AI
For example the autodesk's project dreamcatcher uses AI to produce 3d product designs depending on criteria generated by designers such as functional objectives
manufacturing methods and budget
The distinction between this kind of project and other tools is that it doesn't only let users create designs
However, it provides different sets of data depending on solutions and allows designers to create prototypes that focus on customized problems
Such tools expect a future where data science shifts from a strict analytical function and delivers insights and recommendations to product development and R&D applications
Apart from AI tools performing human functions, they also do it most effectively
Machines like these can determine insights that only humans can feel
For instance, the effective applies AI equipment to identify human emotions such as joy, anger, and surprise
End of human-driven data science
The success of automated machine learning tools has led to the argument of whether there will be a time when human experts won't be necessary at all
The advanced nature of neural networks might very well automate the role of neural networks
And this could also automate the work of data scientists
Chapter 11
Mining techniques data scientists require for their own toolbox
One of the major strengths of data scientists is a strong background in math and statistics
Mathematics help them create complex analytics
Besides this, they also use mathematics to create machine learning models and artificial intelligence
Similar to software engineering, data scientists must interact with the business side
This involves mastering the domain so that they can draw insights
Data scientists need to analyze data to help a business and this calls for some business acumen
Lastly, the results need to be assigned to the business in a way that anyone can understand
This calls for the ability to verbally and visually communicate advanced results and observations in a manner that a business can understand as well as work on it
Therefore, it is important for any wannabe data scientist to have knowledge about data mining
Data mining describes a process where raw data is structured in such a way where one can recognize patterns in the data via mathematical and computational algorithms
Below are five mining techniques that every data scientist should know
Map reduce
The modern data mining applications need to manage vast amounts of data rapidly
To deal with these applications one must use a new software stack
Since programming systems can retrieve parallelism for a computing cluster
A software stack has a new file system called a distributed file system
The system has a larger unit than the disk blocks found in the normal operating system
A distributed file system replicates data to enforce security against media failures
In addition to such file systems a higher level programming system has also been created
This is referred to as Map reduce
It is a form of computing which has been implemented in different systems such as Hadoop and Google's implementation
You can adopt a Map reduce implementation to control large scale computations such that it can deal with hardware faults
You only need to write three functions
That is map and reduce
And then you can allow the system control parallel execution and task collaboration
Distance measures
Distant measure basically refers to a technique that handles this problem
It searches for the nearest neighbors in a higher dimensional space
For every application it is important to define the meaning of similarity
The most popular definition is the jacquard similarity
It refers to the ratio between intersection sets and union
It is the best similarity to reveal textual similarity found in documents in certain behaviors of customers
For example, when looking for identical documents, there are different instances for this particular example
There might be very small pieces of one document appearing out of order
More documents for comparisons
And documents that are so large to fit in the main memory
To handle these issues, there are three important steps to finding similar documents
Shingling
This involves converting documents into sets
Min hashing
It involves converting a large set into short signatures while maintaining similarity
Locality sensitive hashing
Concentrate on signature pairs that might be from similar documents
The most powerful way you can represent document assets is to retrieve a set of short strings from the document
A kshingle refers to any k characters that can show up in a document
A min hash functions on sets
Locality sensitive hashing
Link analysis
Traditional search engines did not provide accurate search results because of spam vulnerability
However, Google managed to overcome this problem by using the following technique
Page rank
It uses simulation
If a user serving a web page starts from a random page
Page rank attempts to congregate in case it had monitored specific outlines from the page that users are located
This whole process works iteratively
Meaning pages that have a higher number of users are ranked better than pages without users visiting
The content in a page was determined by the specific phrases used in the page and linked with external pages
Although it is easy for a spammer to modify a page that they are administrators
It is very difficult for them to do the same on an external page which they aren't administrators
In other words page rank represents a function which allocates a real number to a web page
The intention is that a page with a higher page rank becomes more important than a page that does not have a page rank
There's no fixed algorithm defined to assign a page rank
But there are of different variety
For powerfully connected web graphs page rank applies the principle of transition matrix
This principle is useful for calculating the rank of a page
To calculate the behavior of a page rank it stimulates the actions of random users on a page
There are different enhancements that one can make to page rank
The first one is called topic sensitive page rank
This type of improvement can weigh certain pages more heavily as a result of their topic
If you're aware of the query on a particular page, then it is possible to be biased on the rank of the page
Data streaming
In most of the data mining situations, you can't know the whole data set in advance
There are times when data arrives in the form of a stream
And then gets processed immediately before it disappears forever
Furthermore, the speed at which data arrives very fast
And that makes it hard to store in the active storage
In short, the data is infinite and non-stationary
Stream management, therefore, becomes very important
Streams can be archived into a store
But this will make it impossible to reply to queries from the archival store
This can later be analyzed under special cases by using a specific retrieval method
Furthermore, there is a working store where summaries are placed so that one can use to reply to queries
The active store can either be a disk or main memory
It all depends on the speed at which one wants to process the queries
Whichever way, it does not have the right capacity to store data from other streams
Data streaming has different problems as highlighted below
Filtering streams
To accept tuples that fit a specific criterion
Accepted tuples should go through a separate process of the stream while the rest of the tuples are eliminated
Bloom filtering is a wonderful technique that one can use to filter streams to allow elements in a given set to pass through while foreign elements are deleted
Members in the selected set are hashed into buckets to form bits
The bits are then set to one
If you would like to test an element of a stream
You must hash the element into a set of bits using the hash function
Count specific elements in a stream
Consider stream elements chosen from a universal set
If you wanted to know the number of unique elements that exist in a stream
You might have to count from the start of the stream
Flagellate martin is a method which often hashes elements to integers
Described as binary numbers
By using a lot of the hash functions and integrating these estimates you finally get a reliable estimate
Frequent item set analysis
The market-based model features many relationships
On one side there are items and on the opposite side there are baskets
Here data is similar to a file that has a series of baskets
In reference to the distributed file system
Baskets represent the original file
Each basket is of type set of items
As a result a popular family technique to characterize data depending on the market basket model is to discover frequent item sets
These are sets of items that reveal the most baskets
Market basket analysis was previously applied in supermarket and chain stores
These stores track down the contents of each market basket that a customer brings to the checkout
Items represent products sold by the store while baskets are a set of items found in a single basket
That said this same model can be applied in many different data types such as
Similar concepts
Let items represent words and baskets documents
Therefore a document or basket has words or items available in the document
If you are able to search for words that are repeated in a document sets will contain the most words
Plagiarism
You can let the items represent documents and baskets to be sentenced
Properties of frequent items sets to know
Association rules
These refer to implications in case a basket has a specific set of items
Monotonicity
One of the most important properties of item sets is that if a set is frequent then all its subsets are frequent
Chapter 12 tips for data science
Getting started in data science is a great idea when you want to make improvements in your business
But you want to make sure that you are making decisions that will be smart
Rather than just taking leaps and not knowing what you are doing
Having some tips to make it easier
Can make a big difference in the results that you see
Let's take a look at some of the best tips that you can use when you get started with data science
Understand the business before starting to solve any problems
While the data scientists may be excited to get started
You have to understand what you are looking for before you can do the work
Otherwise, you may use the wrong method or algorithm
Or you are going to just end up with a lot of information that looks like a mess
It is best to understand the business before you take up the project
If you already work for that company and you do this in-house, then it shouldn't be an issue
Some of the things that you should explore about the business to help you out include
Customer level information
You need to have some ideas about the customers the company has
This could be a month-on-month customer attrition
A number of active customers and more
Business strategies
This would be a look at the way that the company gets new customers
And how they work to keep their valuable customers
Product information
You also need to have some information on the product or services that the company offers
You can ask how the customer will interact with the products
And how they earn money through the product
Learn as much about the product as possible before starting
If you can go through and answer these questions, then you have a good start to working on the project
Figure out the right evaluation method you should use
This is not meant to be a difficult puzzle to solve for you as an analyst
But this is also a trap that some will find themselves in
Let's say that you are doing the data science to come up with a targeting model for a new marketing campaign
You need to know which model you are going to use to get the right information out of your data set
Some types of data are going to lend themselves better to one method over another
And you will see this pretty quickly
Other times you will have to try a few of the methods to see which one gives you the best results
Or at least the results that look the least confusing
Break out of the industry silos to get alternate solutions
Analytics is being used in almost all business industries
So instead of staying in traditional approaches that are found with your particular business
Why not go beyond that and see if other industries have found the solution that you're looking for?
A good example of this is a recommended video solution that was implemented in the ecommerce industry and can be used when you're doing a blogging portal
However, the only way that you're going to get this done is to interact with those who are working in the other industry
This can help you to learn how to make it happen and learn from them
If you just sit there in your own industry and try to get things done, you may see some success
But you're missing out on some great opportunities
Our world is changing quickly and many industries are using the same technology in different ways
Learning how some of these industries use data science can end up helping your own business even if they are not really related
Engage with your business counterparts
You should not be doing the whole analysis on your own
This will make you miss out on many important things
You must interact with other business partners and discuss what they are looking for
Some of the important things about their business and so on
As you go through the process, you should make sure that you keep in touch with them
Sometimes this is hard
When you do the analysis for a business
They often want to stay away from the technical details because they are worried that these details would be too complicated
They would be just happy to receive the results at the end and then go through them and make decisions
However, if you want to do the best analysis possible
You must have a constant stream of interaction between you and the people you do the work for
This helps you to stay on track through it
Find the right information and even find some patterns that you may miss out on if you do the whole project on your own
Keep the language simple
You do not need to dumb down the information so that it is watered out
But some statisticians like to use complex formulations that those people outside of the field cannot understand
Moreover, this is even easier to do when you work with data science
However, what you need to do is look at the output of variables that you have
And then try to find a simple way to help the business understand what you're presenting to them
Let's take a look at how this can work
You are looking through the data that you have to find out which agents would be the top performers once they got on board the team
You may come up with the right stratified population and the way that you expect them to perform based on the data
In the process you had to go through and choose a lever which may have changed the population mix
What you would do here is simple
You would just need to implement a differential fee strategy so that you could change the application mix and then this would change the population mix
During this process you would also want to make sure that you learn the business language when you are presenting your findings to business leaders
The project may be easy, but sometimes you may have trouble selling it back to a business
And often the reason for this is because of the gap in understanding the internal discussion with the business
It is really important for you to speak the language of your audience
It is possible to have times when the smartest models are rejected
And the simple models are the ones that the company likes
The only reason for this is because the analysis can speak business to the company while presenting their models
Follow up on the chosen implementation plan
So after you've gone through and talked to a business about the model you want to use for this process
There's still more work to be done
You need to come up with monthly or more often if needed follow-ups with the business to help understand how the project was implemented
And that it is being used in the right manner
You want to make sure that the business is on board with what you are doing and that they are being presented with the most up-to-date information possible
They will not want to receive the information just once and then call that good forever
The world of business is changing so fast that information they find valuable today may not count in a few weeks or months
A constant flow of new data will come in and setting up meetings with the business and those in charge on a regular basis
Will make it easier to ensure that they get the best and newest information to make important business decisions
Read about the industry
The industry is always changing and growing
While something may have been difficult to do in the past
In a few months, it may be really easy because a new technique has developed
You can learn from others in the field and even rely on some of the other industries which use this science to provide you with the solutions that you need
As you get started with the industry of data science
Make sure that you read as much as possible to help you out
You can look at books
Look online
Look at magazines
And more
The more information that you can learn about the industry
The better you can be at providing data science services to your clients
Never stop learning
This industry will change a lot in the near future and having a lot of knowledge readily available
And ensuring that you keep up to date can be really valuable when you are first getting started
You never know what you can learn along the way that could help make your job a whole lot easier
Find new ways to improve
The field of data science is growing by leaps and bounds
It is a relatively new field
But it is really helping many businesses to grow and do well
The only issue is that since it is so new
It is growing so quickly and you will find that many new techniques and even new methods are going to come out in the future
These can really improve what you can do in data science
But it means that you will always need to update your skills along the way
If you are working on a project and find that none of your techniques from the past seem to be just right
Then you may want to consider doing some research
There are always new ways that you can try out
And this is certain that more will be introduced in the near future
Never stop learning about the industry and what it has to offer and continue to learn more of the techniques along the way
This will ensure that you are providing your clients with the best information possible
And it can even make your work so much easier
Do not make the decisions for the company
Unless you are one of the managers in the company who has started doing data science
You do not get to make decisions for the company
And you do not get to push what ideas you think would be the best
Your job is to provide information for the company efficiently and quickly
You will of course write a report on the information that you find
And in a way that those in charge of decision making can read through and see what the best course of action is
But you must only write down what is actually there
Without any swaying or changing of the information and without giving your opinion
The company that hires you is not there to hear your opinion about the market or about what they should do next
They can get opinions all over the place if they want
They want you to go through a large amount of data and information to help them figure out what steps they should take to better their business in the future
If you can do this with a data set and present it in a clear manner, you will do well with the business
Getting started in data science can be a rewarding and exciting career choice
Many companies are starting to see the value of hiring individuals
Or at least training ones in their own company who can go through all this information to help them make informed decisions
Moreover when these companies find someone who can give them accurate information
They can combine it with their own experience and knowledge about the industry to help move their company into the future
Chapter 13
Difference between business intelligence and data science
Business intelligence was one of the initial phases where people wanted to make some sense out of data
So for some of you who may not be aware there were multiple phases of this it devolution
So initially there was all automations
You had automation of your selling process manufacturing process
You had your erp systems your crm systems and so on
So then people started saying that okay, we need to understand or get some information out of this data
That's how business intelligence started. So if we take from a data source perspective
Let's compare these from with each of these criteria
The criteria are what does the data source what is the method what are the skills and what is the focus
And if we compare business intelligence with data science
deserve it looks
As far as the data source is concerned business intelligence was primarily used structured data
So you had all of your enterprise applications like erp
crm
And so on and they were working out of pretty much our dbms
relational database management systems like oracle m my school and so on
So all this data was structured in neat form of table rows and columns
And then they were all brought into a centralized place because then it remembers these were still different applications
So they were working off different databases and silos
If you wanted to get a combined view you needed to create what is known as a data warehouse and bring all this data together
And then look at it in a uniform way
This is what business intelligence was doing pretty much
It was structured data and it had reports and dashboards that was pretty much of what was there in business intelligence
Now with data science in addition to structured data, we also use a lot of unstructured data
Example weblogs or comments if we are talking about customer feedback
There is a structured part and there is an unstructured part where people write three text data science includes that as well and brings everything and then performs analysis
Therefore data science that is the different methods in business intelligence pretty much
It is analytical in the sense that okay, you have some data
We are just trying to present the truth and mostly what has happened historical data. That is it
In case of data science, we go beyond what we go deeper in terms of finding why certain behavior has occurred and go beyond just providing a report
There's a deeper statistical analysis that is done
That is what the scientific part is and deeper insights are gathered not just reporting
So that is from a method perspective
I'm a skills perspective business intelligence needs a little bit of statistics
But more visualization because they primarily consisted of dashboards that are primarily consist of dashboards and reports
Whereas in data science the visualization of course is there
But there's a lot of statistics involved because we are looking at things like correlation
We are looking for example if we perform machine learning we try to do regression try to predict what will be the sales
Maybe in the future and so on and so forth
Therefore, it is much more involved in case of data science compared to business intelligence
The skills are many more compared to business intelligence
Finally yet importantly, what is the focus?
Focus of business intelligence is pretty much historical data
So the sales have happened based on the sales still today
You try to come up with the report what was my sales maybe for this whole year or maybe for the last five years and so on and so forth
In data science you take historical data
But you also combine that with maybe some other required information and you also try to predict the future
So we try to extrapolate maybe the sales and say okay sales as of now as of today
This is the sales is five million and if we based on the historical information
We see that sales increase on our you don't know monthly basis ten percent. That is what history says
Therefore our sales for next month will be this much so that is the focus of data science
It goes beyond just reporting
Chapter 14 prerequisites for data science
What are the prerequisites for data science?
There are three essential traits required for you to be a data scientist
Curiosity
Ask questions
You need to be able to ask questions. The first step in data science is asking question
What is the problem we are trying to solve if you ask the right question only then you will get the right answer?
This is a very crucial step where a lot of data science projects fail because you may be asking the wrong question
And then obviously when you get done, so that's not down
So you're looking for so it's very important that you ask the right question
Common sense
The second part or the second trait is common sense. So you need to be creative
You need to come up with ways to use the data that you have and try to solve the business problem on hand
In many cases, you may not have all the data that you need in many cases
As the data may be incomplete, so that is where you need to come up with what are the best ways to fill these gaps
Wherever this is missing and that's where common sense comes into play
Communication
Finally, yet importantly after doing all this analysis if you're able to communicate the results in the right way
The whole exercise will fail
Therefore communication is a key trait for a data scientist
Maybe technically you may be a genius, but then if you're unable to communicate those results with a proper way
Once again, that will not help
These are the three main traits
Curiosity
Common sense
And communication skills in a way you can say these are the three C's
So what are the other prerequisites?
Machine learning
Machine learning is the backbone of data science
Data science involves quite a bit of machine learning in addition to the basic statistics that we do
Therefore a data scientist needs to have a good hang or need to be very good at data science
Modeling
Modeling is also a part of machine learning in a way
But you need to be good at identifying what are the algorithms that are most suitable to solve a given problem
What models can be used and how do we train these models and so on and so forth
Statistics
Statistics is the core foundation of data science
So you need to understand statistics
And you need to have a good hang of statistics in order to be a good data scientist
And this will also help in getting good results
Programming is to some extent required
At least some program or other would be required as a part of executing a data science project
The most common programming languages are
Python
and R
Python specifically is becoming a very popular programming language in data science because of its ease of learning
Because of the multiple libraries that it supports for performing data science and machine learning
Chapter 15
Tools and skills in data science
Now coming to the tools and skills that are used in data science
These are some of the skills from a language perspective. It is python or R
And from a skills perspective in addition to some of the programming languages
It would help if you have a good knowledge or good understanding of statistics
In addition, what are the tools that are used in data analysis?
SAS is one of the most popular tools
It has been there for a very long time
That is the reason it is very popular
And however, this is compared to most of the other tools
It is a proprietary software whereas python and R mostly are open source
The other tools are like Jupyter, Jupyter, Norbergs
You have RStudio
These are more development environments and development tools
So Jupyter Notebooks as an interactive development environment
Similarly, RStudio is for performing or writing our code and performing analytics
And performing data analysis and machine learning activities
You can perform in RStudio
It is a very nice UI
And initially it was not so popular primarily because it did not have user interface
And RStudio is a relatively new addition
And after the advent of RStudio became extremely popular
There are other tools like MATLAB
And of course some people do with Excel as well
As far as data warehousing is concerned
Some of the skills that are required are ETL
So in order to extract data and transform load
ETL stands for extraction transform load
So you have data and databases like your ERP system
RAC graming system
You need to extract that and then do some transformations
And then load it into your warehouse so that all the data from various sources look uniform
Then you need some SQL skills
Which are basically querying the data writing SQL queries
So that those are excellent skills for data warehousing
And there are some standard tools that are available like informatics data stage talent
And AWS Redshift
If you want to do some on the cloud I think AWS Redshift is again a good tool
Data visualization tools or data visualization some of the skills that would be required
Or let's say R
R provides some very good visualization capabilities especially for developing during development
Then you have python libraries
matpotlib
And so on
Which provides very powerful visualization capabilities and that is from skills perspective
Whereas tools that can be used are TableLoop
Is a very popular visualization tool again that is proprietary tool
So it's a little expensive maybe but excellent capabilities from a visualization perspective
Then there are tools like Kongos
Which is an IBM product
Which provides very good visualization capabilities as well
And then coming to the machine learning part of it
The skills required there are python
Which is more for programming part
And then you will need some mathematical skills like algebra
Linear algebra specifically
And then statistics
And maybe a little bit of calculus and so on
The tools that are used for machine learning are
Spark MLlib
And Apache
Mahout
And OneCloud
If you want to do something
You can use Microsoft Azure ML Studio as well
Therefore
These are by no means an exclusive list
There are actually many tools
And probably a few more skills also may be there
But this is discussing a quick overview
Like a summarizing of summarization of the tools and skills
What does a data scientist during the course of his work?
Now moving on to the life of a data scientist
What does a data scientist during the course of his work?
So let us see
So typically a data scientist is given a problem
A business problem that he needs to solve
And in order to do that
If you remember from the previous slide
He asks the question as to what is the problem that he needs to solve
So that is the first thing he has got the problem
And the next thing is to gather the data that is required to solve this problem
So he goes about looking for data from anywhere
It could be the enterprise
Very often the data is not provided
In the nice format that he would like to have it
Or we would like to have it
First step is to get whatever data is possible
What is known as raw data in whatever format
It could be enterprise data
It could be it is there probably a requirement to go
And get some public data
And in some cases so all the raw data is collected
Then that is processed and analyzed
And then prepared into a format in which it can be used
And then it is fed into the analytic system
Be it a machine learning algorithm or a statistical model
And we get the output
And then he puts these output in a proper format
For presenting it to the stakeholders
Feeding this into that analysis system
That has been designed
Be it mathematical models
Machine learning models
And then get the results
The insights
And then present it in a nice way
So that the stakeholders can understand
How the machine learning algorithms
Chapter 16
Life cycle of a data science concept study
In this step it involves understanding the business problem
Asking questions
Get a good understanding of the business model
Meet up with all the stakeholders
Understand what kind of data is available
And all that is a part of the first step
So here are a few examples we want to see
What are the various specifications
And then what is the end goal
What is the budget
Is there an example of this kind of a problem
That has been maybe solved earlier
So all this is a part of the concept study
And another example could be a very specific one
To predict the price of a 1.25 carat diamond
Moreover there may be relevant information inputs
That are available
And we want to predict the price
Data gathering and data preparation
The next step in this process is
Data preparation data gathering
Also known as data manipulation
Therefore what happens here is
The raw data that is available
May not be usable in its current format
For various reasons
So that is why in this step
A data scientist would explore the data
He will take a look at some sample data
If we pick there are millions of records
Pick a few thousand records
And see how the data is looking
Are there any gaps?
Is the structure appropriate
To be fed into the system?
Are there some columns?
Which are probably not adding value
May not be required for the analysis very often
These are like the names of the customers
They will probably not add any value
Or much value from an analysis perspective
The structure of the data
May be the data is coming from multiple data sources
And the structures may not be matching
What the other problems there may be gaps in the data
So the data all the columns
All the cells are not filled
If you're talking about structured data
There are several blank records or blank columns
So if you use that data directly
You'll get errors
Or you'll get inaccurate results
Either how do you get rid of the data
Or how do you fill those gaps
With something meaningful
All that is a part of data munging
Or data manipulation
These are some additional subtopics within
That so-it integration is one of them
There are any conflicts in the data
There may be data
May be redundant
Data redundancy
Data redundancy is another issue
There may be you have
Let's say data coming from two different systems
And both of them have customer table
For example customer information
Therefore when you merge them
There is duplication issues
So how do we resolve it
So that is what?
Data transformation
Data transformation
As I said
There will be situations
Where data is coming from multiple sources
And then when we merge them together
They may not be matching
So we need to do some transformations
To make sure everything is similar
We may have to do some data reduction
If the data size is too big
You may have to come up with ways
To reduce it meaningfully
Without losing information
Data cleaning
There will be wrong values
Use null values
Or the missing values
So how do you handle all of that?
A few examples of very specific stuff
So there are missing values
How do you handle missing values
Or null values here?
In this particular slide
We have seen three types of issues
One is missing value
Then you have null value
You see the difference between the two right
So in the missing value
There is nothing blind
Null value
It says null
No system can handle
If there are null values
Similarly
There is improper data
So it's supposed to be numeric value
But there is a string of non-numeric value
So how do we clean and prepare the data
So that our system can work flawlessly?
So there are multiple ways
And there is no one common way of doing this
It can vary from project to project
It can vary from what exactly is the problem we are trying to solve
It can vary from data scientist to data scientist
Organization to organization
So these are like some standard practices people come up with
And of course there will be a lot of trial and error
Somebody would have tried out something
And it worked
And we'll continue to use that mechanism
So that's how we need to take care of data cleaning now
What are the various ways of doing
You know if values are missing
How you take care of that now
If the data is too large
And only a few records have some missing values
Then it is okay to just get some of those entire rows for example
So if you have a million records
And out of which 100 records don't have full data
So there are some missing values in about 100 records
Therefore it is fine because it is a small percentage of the data
So you can get rid of the entire records
Which are missing values
However that is not a very common situation very often
You will have multiple or at least a large number of data set
For example out of a million records
You may have 50,000 records which are like having missing values
Now that's a significant amount
You cannot get rid of all those records
Your analysis will be inaccurate
How do you handle such situation?
So there are again multiple ways of doing it
One is you can probably four particular values are missing
In a particular column
You can probably take the mean value for that particular column
And fill all the missing values with the mean value
That first of all you don't get errors because of missing values
And second you don't get results that are way off
Because these values are completely different from what is there
So that is one way
Then either a few other could be taken the median value
Or depending on what kind of data we are talking
So something meaningful we will have to put in there
So typically you take the entire data set
The input data set
And split it into two parts
And again the ratio can vary from person to person
Individual preferences
Some people like to split it into 50-50
Model planning
Now this models can be statistical models
This could be machine learning models
So you need to decide what kind of models you are going to use
Again it depends on what is the problem you are trying to solve
If it is a regression problem
You need to think of a regression algorithm
And come up with a regression model
So it could be linear regression
If you are talking about classification
Then you need to pick an appropriate classification algorithm
Like logistic regression
Or decision tree
Or SVM
And then you need to train that particular model
So that is the model building
Or model planning process
And the cleaned up data has to be fed into the model
And apart from cleaning
You may also have in order to determine what kind of model
You will use you have to perform some exploratory data analysis
To understand the relationship between the various variables
And see if the data is appropriate
And so on
So that is the additional preparatory step that needs to be done
Model building
We have done the planning part
We said
What is the algorithm you are going to use?
What kind of model are we going to use?
Now we need to actually train this model
Or build the model rather
So that it can then be deployed
So what are the various ways
Of what are the various types of model building activities?
So it could be
Let's say in this particular example
That we have taken
You want to find out the price of 1.35 carat diamond
So this is let's say
A linear regression problem
You have data for various carats of diamond
And you use that information
You pass it through a linear regression model
Or you create a real linear regression model
Which can then predict your price
For the 1.35 carat
Communicate
This results to the appropriate stakeholders
Therefore
Which is basically taking this result
And preparing like a presentation
Or a dashboard
And communicating these results
To the concerned people
So finishing
Or getting the results of the analysis
Is not the last step
But you need to do
As a data scientist
Take these results
And present it to the team
That has given you this problem
In the first place
And explain your findings
Explain the findings of this exercise
And recommend maybe what steps they need
In order to overcome this problem
Or solve this problem
If everything is fine
Your data scientist presentations are accepted
Then they put it into factors
And thereby they will be able to improve
Or solve the problem they stated in step 1
The raw data needs to be manipulated
You need to do data munging
So that you have the data
In a certain proper format
To be used by the model
Or our analysis system
And then you need to do the model planning
What kind of a model?
What algorithm you will use for a given problem
And then the model building
So the exact execution of that model
It happened in step 4
And you implement and execute that model
And put the data through the analysis
In this step
And then you get the result
This results are then communicated
Packaged
And presented
And communicated to the stakeholders
And once that is accepted
That is operationalized
Conclusion
The practice of data science
Can be best explained
As a combination of statistical analysis
Big data
And data mining
Businesses
Companies
And large organizations
Have problems that data scientists need to solve
Usually most of these problems
May not be part of the standard data mining tasks
Therefore data scientists
May need to divide the problem into smaller chunks
That are solvable
Often beginning with existing tools
For some tasks
You may well not know how to solve it
That is where data mining comes into play
By carefully applying data mining techniques
Learned in this book
You can easily overcome
Some of the challenges that you face
Don't forget
That to become the best data scientist
You need to ensure that you have
A strong analytical mind
To help you create effective business solutions
Although you may not have
All the skills and knowledge
That the best data scientists have
Everything has its starting point
If you can ensure
What you spend most of your time
Learning and mastering
What the best data scientists know
Then you can be sure
To achieve a lot in your journey
Of data analysis
We also talked about
The various tools that are available
Like python and R
And we did a comparison
Between business intelligence
And data science
In addition
We did a detailed discussion
About the life cycle
Of a data science project
With an example
And finally yet importantly
We talked about
The demand for data scientists
The global demand
There's a huge demand
For data scientists
This has been
Data science for beginners
What you have to know
About data analytics
Data mining
Data security
Statistics
Coding
And tips
Written by Travis Goleman
Narrated by Austin Stoller
Copyright 2019
By Travis Goleman
Production copyright
By Travis Goleman
Audible hopes you have enjoyed this program
