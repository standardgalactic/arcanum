Welcome to the ACTIMF podcast, where we will present short, digestible segments clipped
from the Active Inference Lab weekly livestreams.
If you like what you hear and you want to learn more, check out the entire livestream
at the Active Inference Lab YouTube channel.
The link to the livestream is provided in the episode description.
My name is Blue Knight and I will be guiding you through this podcast episode, which is
clipped from ACTIMF Lab livestream number 6.1.
This discussion will be loosely structured around the paper, A Tale of Two Densities,
Active Inference is Inactive Inference by Maxwell Ramstead, Michael Kirchhoff, and Carl Friston.
Maxwell Ramstead will start us off by providing some perspective on the paper, and Daniel
Friedman will jump in to facilitate this discussion about the relationship between the recognition
model and the generative model.
Yeah, so I'm Maxwell Ramstead, I'm based in Montreal, where I'm talking to you from at
McGill University, and I'm also the first author on the paper that we'll be discussing today.
We started off with a much stronger position than we ended up with, and it's by engaging
seriously with the arguments that Alex and Jakob had prepared.
you know, with a few other people, Pavel Godzievsky and so on, had been arguing for this kind
of stuff.
But I really think that there is something to that argument.
And the whole point is to keep the baby, you know, while getting rid of the bathwater.
And, you know, the baby is this sort of idea that there is something like representations
of the external world that, you know, carry semantic content and so on.
Like, it's just that it's not the generative models.
It's the recognition densities.
That was sort of the tweak that we wanted to bring to the table, but...
Cool.
Alex, and then anyone else who raises their hand.
Hi, I'm Alex Kiefer.
I'm in the philosophy department of Monash University, currently in New York City.
Like Maxwell says, we've had a lot of interesting questions about this.
I guess the point, the point on which I remain not convinced yet is that I think, I still think
of the generative model, even in the FEP as a representation.
So we can, we can maybe talk about that.
I think to me what this brings to the table, and this is like transformed how I see this stuff,
is the importance of understanding the generative model as a control system.
I just think you can do that in a way that also grants that it's a structural representation.
And that's what I've been, that's still the framework in which I work.
So I think there's another question raised here about whether the generative model is
sort of encoded.
But I do understand where the authors are coming from on that question.
And I think it's not so straightforward in the FEP as it would be in some earlier models
like the Helmholtz machine.
So I don't know.
I'm curious to see if Maxwell and I still disagree.
I have the sense that I maybe still want to call the generative model a representation
more than Maxwell does, but we can get to that.
Can I ask Alex, and again, anyone can raise their hand and have a thought here too, is
what is the advantage of wanting something to be a representation or what is the alternative
to something being structural?
Right.
So I mean, so I don't think in those terms, in terms of the advantage of it, I just think,
is it a representation or not?
I think it is based on what our representation is and how this thing functions in this model.
So I know oftentimes people talk about like, what's the explanatory value of talking in terms
of representations?
But I mean, to answer more directly, what's the alternative to it being structural?
That's another good question.
In my view, this structural representationalist paradigm really isn't anything new.
It's kind of just like the core notion of representation that's been at work in serious cognitive science
since like Turing.
So like if you go back to papers like there's a paper from 1980 by Alan Newell on sort of foundations
of cognitive science, and he described how what makes a particular symbolic physical symbol
system able to get what gives it its universal computational capacity is its capacity to simulate
other systems.
So I really think this notion of simulation is at the heart of what representation is supposed
to be in cognitive science, and that really just is structural representation.
And if you go back last, I won't go on forever, if you go to Cummins's early work on structural
representationalism, the S representation can also be read as simulation.
So it's not that I think that you need to call it the generative model, a structural representation.
I just think once you understand how it functions, I would say even under the FVP, that's like
what it is.
But there's still a question about how it's encoded or whether it's encoded.
I think that's a separate question.
Awesome.
Thanks for that response.
We'll go to Lee and then anyone else that can raise their hand.
Yeah.
I'm Lee.
I'm based in London, but I'm actually studying at the University of York, studying embodied
cognition.
I was just trying to make sure that I was understanding terms in the same way that they were being used.
But Alex, you just talked about the notion of representation of simulation.
And one of the ways that I've kind of arrived at active inference is via the work of Barsali,
symbol theory and simulation.
Does that, is that overlapping with what you're meaning by simulation?
Or does it have a different meaning if you're familiar with this work at all?
Probably.
I'm not, I'm not as familiar with that work as with the new old paper that I just, that
I mentioned.
I am and it is just a, yeah.
Great.
No, just kidding.
I want to read it.
Um, okay.
I'll, I'll accept that.
I'll, I'll take that for the work work.
Yeah.
I mean, essentially, uh, especially these deep generative models are, they allow you to entertain.
Um, well, I mean, the, there's a sense in which the, like the Barsali, you know, idea of metaphor
is just what the generative models do.
Uh, you know, if we're talking about a kind of loose associative structure that redeploys
inferences about one domain in another, I mean, uh, this is the kind of thing that you
would expect the generative model would be able to do.
Uh, so these things are definitely, I think, strongly aligned, uh, in effect.
Uh, I mean, didn't Steven suggest something similar?
I think, uh, like the, yeah, metaphors are generative models in a strong sense.
Cool notion.
We'll go to Alex Kiefer and then anyone else.
Yeah.
So, um, that's interesting.
I don't know what to say about metaphors, but, um, uh, I think the way I see it, the way
I see this now is that, um, I don't think what Maxwell at all are saying is, is it all,
of course, um, maybe not, of course, I don't think it's at all wrong, but I just think that
our, uh, the sort of the structural representation of this reading, my view is that it doesn't
quite say enough, but not that, not that it's wrong in any particular respect as applied to
the FEP.
Like, so, so after further conversations with Maxwell and also with, um, with Michael and
the other authors on this, um, it seems to me that you still, there's still a need for
a sort of neuronally realized generative model.
Uh, in addition to considering the entire phenotype to be a generative model.
So if you have a fancy organism that can do, you know, deep temporal kind of modeling and
plan for the future and things like that, um, I think there's still a need for something
that looks like a structural representation, um, in the brain that, that is the generative
model can be construed that way, as well as there's this sort of larger, more encompassing
system.
Um, and maybe this could be some somewhat hashed out by going to the multilevel active
infant stuff.
Um, but last piece of this, um, uh, in discussions with, with, um, Michael Kirchhoff, I hope I'm
saying his name correctly.
We've never said it out loud to each other.
Um, it, it seems as though he, at least he is thinking of top down sort of propagation
of signals as a form of action.
So the stuff that we were saying about, um, top down generative models might just be sort
of a special case of the sort of action that, um, this paper is talking about.
Cool.
It really reminds me about Alejandra's point one or two weeks ago about what are the similarities
and the differences between the systems that can at least appear to do these deep simulations,
the counterfactuals like the brain versus a embodied phenotype.
The cell perimeter is not simulating other cell perimeters.
Um, that's a little bit different than the way that the brain might be able to do something
strategic.
We hope you enjoyed this week's episode.
Stay tuned for next time where we will keep discussing this paper and the relationship
between the recognition density and the generative density.
