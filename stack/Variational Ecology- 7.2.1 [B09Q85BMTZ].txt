Welcome to the ACTIV podcast, where we will present short digestible segments clipped
from the Active Inference Lab weekly live streams. If you like what you hear and you
want to learn more, check out the entire live stream at the Active Inference Lab YouTube
channel. The link to the live stream is provided in the episode description. My name is Blue
Knight, and I will be guiding you through this podcast episode, which is clipped from
Active Inflab live stream number 7.2. This discussion will be loosely structured around
the paper Variational Ecology and the Physics of Sentient Systems by Maxwell Ramstad, Axel
Constant, Paul Badcock, and Carl Friston. Maxwell Ramstad, the first author of the paper, will
start us off by explaining Markov blankets in multi-scale systems. What we basically do
is we're going to take a bunch of states that comprise our system, and then partition those
states into Markov blankets, and then from there reconstruct the system at the superordinate
scale on the basis of these Markov blankets. So basically, in new work, especially in the
particular physics monograph, we speak of this in terms of two operators that we use, G and
R. G is a grouping operator effectively, and it allows us to construct Markov blankets from
the base states that we're considering. And then R takes these Markov blankets and then
reduces their dimensionality. So I mean, effectively, we're going to be dropping some of the states
to construct the states at the scale above. So we start with a bunch of states, and these
are all the states that are of interest to us in the system. So, you know, these can be literally,
any states whatsoever. These techniques have been used recently to partition fMRI data. So when
you're dealing with fMRI data, each state is a voxel, so a 3D pixel effectively. And so what we do from
there is you have all of your states, and you have this continuous time series data typically. And so what you do
is you construct an adjacency matrix. So an adjacency matrix is essentially you're plotting all of your
states by all of your states. So the diagonal is basically like, you know, your degree of self-coupling.
And what you do is you populate this matrix with coefficients that express how tightly this state and that state are coupled.
And then so from this adjacency matrix, what you do is you take the Jacobian and the Hessian of these adjacency matrices.
So the Jacobian and the Hessian are basically the same matrix. But rather than having the coupling coefficient in each entry, what you have is the partial derivatives of each entry.
So effectively, this is a very technical and complicated way of saying what we're looking at is the relative rates of change of every variable with respect to every other variable.
The reason we're doing this is that a zero in these, you know, matrices that contain the partial derivatives means that effectively there is no relative change.
Meaning that like you can, you can vary one of the variables and the other one doesn't change.
So this is a way of testing for conditional independence is the short story.
So basically using these mathematical techniques, what you do is you read off the conditional independencies from this coupling matrix, this adjacency matrix.
And from there, you're able to rebuild the, well, you're able to construct Markov blankets in terms of the states that are independent of each other, conditioned on other states.
So, I mean, to recap, we started with a bunch of states that just compose the system that we're interested in explaining at one scale.
We put these states through a grouping operator that chunks it into Markov blanketed particles based on the conditional independencies.
So what we're left with after applying the grouping operator is a bunch of Markov blankets effectively, Markov blanketed particles.
So then in order to get to the next scale, what we do is we reduce the dimensionality of this partitioned set of states.
So effectively what we're doing is we're dropping the internal states and the fast stable modes of interaction at every scale in order to construct the states at the level above.
And this gets us basically from the other half of the image, basically moving from these partitioned states back to a new set of states at the superordinate scale.
So conceptually what we're saying is that states at a superordinate scale are literally constituted by the slow metastable modes of interaction between states at the level.
between states at the level below.
So just conceptually, this means that you take, say, an organ like the heart.
Well, the heart itself is constituted by the slower modes of interaction among heart cells.
So, for example, like, you know, the main compartments of the heart, for example, are literally constituted by the modes of interaction between the cells that are components of those,
of those components effectively, of those compartments.
Yeah, so then the reduction operator, we're dropping the internal states because they're effective.
All of the information that you need to know about the internal states are already, is already summarized in the blanket.
So you drop the internal states and you drop the fast, stable modes of oscillation, and that gets you from the partitioned system back to a set of states at the next scale.
So hopefully that explains what's going on.
But again, that might sound very technical, but the idea, I think, is pretty simple.
And it's just that we're constructing superordinate layers of the system from the slow modes of interaction of components at the subordinate scale.
Now, Daniel Friedman is going to jump in for a little bit of Q&A with Maxwell.
Very nice explanation, Maxwell.
Really, thanks a lot for that.
Pleasure.
Yeah, super helpful.
So let's go over that because I think it's really critical.
And I think for some people, it's like, wait, weren't they just talking about ecology?
So what just happened there?
How did we go from the big questions in ecology to this grouping operator loop that was just described?
And so the main pivot, I think, has to do with multiscale systems and how we're going to be modeling across different scales.
And so what the question is, is, again, thinking about understanding and modeling these dynamics of multiscale systems.
And there's an unprincipled way.
And this is a principled, not the only principled way, but a principled way to do this multiscale integration.
So an unprincipled way would be, for example, you use method A to go from the fast dynamics to the next level.
So you have a heart cell simulator, proprietary program, heart cell sim 4.
And then you make just another software pipeline that connects the outcome of that simulation to a model of the whole heart physiology.
And then you kind of run the results from these two different models back and forth.
So maybe it works. Maybe it's functional.
So no one's saying it isn't.
It's just that the way that those two pieces were joined together was a bit a priori.
It was a little bit just chosen.
And it would be better if there was a way that we could, for example, take measurements across levels and use information that they provide to constitute our better understanding of the whole system.
So not just throw away the model that went information that doesn't fit into each little bottleneck of information, but how can we have this full variational in the constituting variance across multiple timescale as well as variational from that physics side,
which brings along all these really positive benefits related to computability and tractability and relationship with Bayesian statistics and foreign factor graphs and all these kind of cool areas.
It ends on bridging. So if there's a hinge with multiscale systems having a useful description, then we're in the category of the physics models like spin graphs and game of life.
And then also at higher levels, at the very least, the tools for dealing with the kinds of things that those kinds of simulations like the game of life were invented to describe.
So that's sort of the main key point.
And so this is what Maxwell has run through and represented here is describing just like you could use model selection techniques to figure out which multilevel statistical regression model or which multiple ANOVA was the most appropriate or whether to include an interaction term or not,
or a multi scale model from a Bayesian framework, whichever modeling framework you're working within.
Hopefully there's a principled way to go about doing model selection and doing model comparison.
If I can just interject, Carl Fristen calls this the particular partition.
And there's a play on words here, there's a double entendre.
By particular partition, he means partitioning the system into particles, which are the internal states of a system plus the Markov blanket.
But he also means it's a particular partition, as in there are other ways of doing this.
Yeah, there's not necessarily one unique way of cutting things up.
And so the partition itself is named to reflect that fact. I thought it was kind of fun to bring up.
Yes. Now, let me connect that point right there about one particular choice relating to dimensional reduction techniques and where degrees of freedom come into play for the experimenter.
So in real data sets, you can do techniques like principal component analysis and you can do things like linear discriminant analysis, which looks basically for group separation as an additive combination of principal components.
So it tries to explain the most variation with these orthogonal axes through the data.
That's principal components analysis. Now, there's some data sets where the first principal component explains 80 and the next one explains 5% of the variance and has a long tail.
Now, whether you need the first principal component and you're just going to take only the first one or whether you take 99% of the variance being explained by 500 principal components,
where the explainability of principal component 74 on your Netflix consumption doesn't really map to like a real world understandable trait,
it still could reduce your uncertainty about a machine learning model, but it doesn't like map onto something that's actual about the group anymore and maybe misaligned with all these other things.
Those kinds of analyses are always up to the experimenter to decide how many dimensions they want to reduce to.
Do you want to reduce down to a two dimensional space just upon the data points in the principal component one, principal component two,
just remap onto the two dimensions that explain the most variance linearly or do you want to map into a 500 dimension space?
That's where a lot of the freedom comes into play. So just like there's a particular principal component analysis is presented in a paper, but principal component analysis is a general technique.
A particular system will have a particular decomposition statistically and in this way.
And that will also reflect degrees of experimental freedom and modeling it and in the statistics.
But the framework, just like the ANOVA framework, goes a little bit beyond and isn't necessarily about any specific system.
Well, one thing to point out is that there's a kind of recursivity at play where, I mean, what you can actually see what is the best way to partition the system just kind of recursively.
And that like what you can do is take your system and then write alternative generative models of how you think the system is actually behaving.
And then you can use free energy as a metric to see which out of these alternative models of the system structure is the one that best explains its behavior.
And we're not only just like modeling the system as if it were an active inference agent, but we can use generative models to assess which out of the possible ways of cutting up the states of carving up the system is the most appropriate.
And again, it's using the same metrics. So model evidence. It's always model evidence. I had a call with Carl yesterday and he was saying, well, you know, whatever the question, the answer is always model evidence.
Let me connect that to one more niche idea. So we talked about the skilled intentionality framework and how that's like part of, you know, the niche is the affordances and that can reflect a lot of aspects that are learned or that are cultural.
But the niche can also include mercury that's influencing you and you can't taste it or detect it, but it's changing your phenotype or something like that.
So, again, maybe some niche concepts are more affordance oriented, like the ecological psychology niche is more oriented towards action.
Unsurprisingly, however, for many ecologists, when you talk about the niche, that means like how much light is hitting the ground, what temperature is it at night?
So that is also reflected by the realized versus the fundamental niche, because especially when you're talking about those abiotic factors, you're sometimes talking about, oh, yeah, rocks that you can knock over.
But a lot of times, again, people just think of rocks as being in the physical niche.
So things can influence and not be reflected by sensory states. And then all we have is the model.
Do you have the model of decision making? It doesn't know that it's being slowly drifting off into a non-adaptive state space because of the cup that has led into it.
All it is experiencing slash reducing its uncertainty is about its generative model of its niche.
And that's more on the ecological psychology side. And that can even be extended to like a molecular level, like chemicals that can't be tasted.
They can still influence you. So it's not only about how the action is enabled in the world, but that is the primacy when you're coming from an organism or a system of interest perspective.
And that's where you make this initial like, what are the measurements to make?
Like Maxwell started off with, you start with states. Those are measurements.
You could say we could measure this. We could measure this thing over here. We could do it over here.
But you got to start with at least the idea that measurements are going to be made or at least that there are certain distributions or types of measurements that could be made.
One other thing was the grouping operator that you mentioned that kind of reminded me of click detection of conditional independencies.
And so the other way of saying that is click detection of the conditional dependencies, because in order to find out which ones are zeros, you got to find out which ones are non-zero.
And then when you think about there's this representation ability to transform between the matrix and the network, like through the adjacency matrix or more advanced relation related matrices, then you can do click detection on the matrix.
The matrix is going to be able to make a network that has the exact same data in it, whether it's just one zero, like edges or not edges, or whether it's a weighted edge or a different type of edge.
There's such an equivalence between matrices and networks that click detection on a social network or something like that is going to be also detectable on the matrix form.
It's not something to keep in mind when we're talking about like a matrix coming in here, but then there's like maybe a network of interactions coming out another side.
We hope you enjoyed this week's episode.
Stay tuned for next time where we'll have one last discussion of this paper.
And we're going to talk specifically about sharing a generative model and model selection by evolution.
How these data highlights aredag ciek doesn't have all this material generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation as generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generation generationæŽ¥ sovereignty
