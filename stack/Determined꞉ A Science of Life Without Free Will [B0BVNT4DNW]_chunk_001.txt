Evolution tinkers, improvising, elegantly or otherwise, with what's on hand.
Our insulin neurons don't distinguish between disgusting smells and disgusting behaviors,
explaining metaphors about moral disgust leaving a bad taste in your mouth,
making you queasy, making you want to puke.
You sense something disgusting, and unconsciously it occurs to you that it's disgusting and wrong
when those people do X.
And once activated this way, the insula then activates the amygdala,
a brain region central to fear and aggression.
Naturally, there is the flip side to the sensory disgust phenomenon.
Sugary versus salty snacks make subjects rate themselves as more agreeable and helpful individuals,
and rate faces and artwork as more attractive.
Ask a subject,
Hey, in last week's questionnaire you were fine with behavior A,
but now, in this smelly room, you're not.
Why?
They won't explain how a smell confused their insula and made them less of a moral relativist.
They'll claim some recent insight caused them,
bogus free will and conscious intent ablaze,
to decide that behavior A isn't okay after all.
It's not just sensory disgust that can shape intent in seconds to minutes.
Beauty can as well.
For millennia, sages have proclaimed how outer beauty reflects inner goodness.
While we may no longer openly claim that,
beauty is good still holds sway unconsciously.
Attractive people are judged to be more honest, intelligent, and competent,
are more likely to be elected or hired, and with higher salaries.
They're less likely to be convicted of crimes, then getting shorter sentences.
Jeez, can't the brain distinguish beauty from goodness?
Not especially.
In three different studies, subjects in brain scanners alternated between rating the beauty of something,
for example, faces, or the goodness of some behavior.
Both types of assessments activated the same region,
the orbitofrontal cortex, or OFC.
The more beautiful or good, the more OFC activation,
and the less insula activation.
It's as if irrelevant emotions about beauty gum up cerebral contemplation of the scales of justice,
which was shown in another study.
Moral judgments were no longer colored by aesthetics
after temporary inhibition of a part of the PFC
that funnels information about emotions into the frontal cortex.
Interesting, the subject is told.
Last week, you sent that other person to prison for life.
But just now, when looking at this other person who had done the same thing,
you voted for them for Congress.
How come?
And the answer isn't, murder is definitely bad,
but OMG, those eyes are like deep limpid pools.
Where did the intent behind the decision come from?
The fact that the brain hasn't had enough time yet
to evolve separate circuits for evaluating morality and aesthetics.
Next, want to make someone more likely to choose to clean their hands?
Have them describe something crummy and unethical they've done.
Afterward, they're more likely to wash their hands or reach for hand sanitizer
than if they've been recounting something ethically neutral they'd done.
Subjects instructed to lie about something,
rate cleansing, but not non-cleansing, products,
is more desirable than do those instructed to be honest.
Another study showed remarkable somatic specificity,
where lying orally, via voicemail, increased the desire for mouthwash,
while lying by hand, via email, made hand sanitizers more desirable.
One neuroimaging study showed that when lying by voicemail
boosts preference for mouthwash,
a different part of the sensory cortex activates
than when lying by email boosts the appeal of hand sanitizers.
Neurons believing, literally, that your mouth or hand, respectively, is dirty.
Thus, feeling morally soiled makes us want to cleanse.
I don't believe there's a soul for such moral taint to weigh on,
but it sure weighs on your frontal cortex.
After disclosing an unethical act,
subjects are less effective at cognitive tasks
that tap into frontal function.
Unless they got to wash their hands in between.
The scientists who first reported this general phenomenon
poetically named it
the Macbeth effect,
after Lady Macbeth,
washing her hands of that imaginary damn spot
caused by her murderousness.
Reflecting that,
induced disgust in subjects,
and if they can then wash their hands,
they judge purity-related norm violations less harshly.
Our judgments, decisions, and intentions
are also shaped by sensory information
coming from our bodies.
That is, interoceptive sensation.
Consider one study concerning the insula
confusing moral and visceral disgust.
If you're ever on a ship in rough waters
and are heaving over the rail,
it's guaranteed that someone will sidle over
and smugly tell you
that they're feeling great
because they ate some ginger,
which settles the stomach.
In the study,
subjects judge the wrongness of norm violations.
For example,
a morgue worker touching the eye of a corpse
when no one is looking,
drinking out of a new toilet.
Consuming ginger beforehand lessened disapproval.
Interpretation?
First,
hearing about that illicit eyeball touching
pushes your stomach toward lurching
thanks to your weird human insula.
Your brain then decides your feelings
about that behavior
based in part on lurching severity.
Less lurching,
thanks to ginger,
and funeral home shenanigans
don't seem as bad.
Particularly interesting findings
regarding interoception
concern hunger.
One much-noted study suggested
that hunger makes us less forgiving.
Specifically,
across more than a thousand judicial decisions,
the longer it had been
since judges had eaten,
the less likely they were
to grant a prisoner parole.
Other studies also show
that hunger changes
pro-social behavior.
Changes.
Decreasing pro-sociality,
as with the judges,
or increasing it.
It depends.
Hunger seems to have
different effects
on how charitable subjects
say they are going to be
versus how charitable
they actually are.
or where subjects
have either only one
or multiple chances
to be naughty or nice
in an economic game.
But as the key point,
people don't cite
blood glucose levels
when explaining why,
say,
they were nice just now
and not earlier.
In other words,
as we sit there,
deciding which button to push
with supposed
freely chosen intent,
we are being influenced
by our sensory environment,
a foul smell,
a beautiful face,
the feel of vomit goulash,
a gurgling stomach,
a racing heart.
Does this disprove
free will?
Nah.
The effects are typically mild
and only occur
in the average subject,
with plenty of individuals
who are exceptions.
This is just the first step
in understanding
where intentions come from.
Minutes to days before.
The choice you'd seemingly
freely make
about the life-or-death
button-pressing task
can also be
powerfully influenced
by events
in the preceding minutes
to days.
As one of the most
important routes,
consider the scads
of different types
of hormones
in our circulation,
each secreted
at a different rate
and affecting the brain
in varied ways
from one individual
to the next,
all without our control
or awareness.
Let's start with
one of the usual suspects
when it comes to
hormones-altering behavior,
namely,
testosterone.
How does testosterone,
T,
in the preceding minutes
to days,
play a role
in determining
whether you kill
that person?
Well,
testosterone causes
aggression,
so the higher the T-level,
the more likely you'll be
to make the more
aggressive decision.
Simple.
But as a first complication,
T doesn't actually
cause aggression.
For starters,
T rarely generates
new patterns of aggression.
Instead,
it makes pre-existing patterns
more likely to happen.
Boost a monkey's T-levels,
and he becomes
more aggressive
to monkeys,
already lower ranking
than him,
in the dominance hierarchy,
while brown-nosing
his social betters
as per usual.
Testosterone makes
the amygdala
more reactive,
but only if neurons
there are already
being stimulated
by looking at,
say,
the face of a stranger.
Moreover,
T lowers the threshold
for aggression
most dramatically
in individuals
already prone
toward aggression.
The hormone
also distorts judgment,
making you more likely
to interpret
a neutral facial expression
as threatening.
Boosting your T-levels
makes you more likely
to be overly confident
in an economic game,
resulting in being
less cooperative.
Who needs anyone else
when you're convinced
you're fine
on your own?
Moreover,
T tilts you
toward more risk-taking
and impulsivity
by strengthening
the ability
of the amygdala
to directly activate
behavior
and weakening
the ability
of the frontal cortex
to rein it in.
Stay tuned
for the next chapter.
Finally,
T makes you
less generous
and more self-centered
in,
for example,
economic games,
as well as
less empathic
toward entrusting
of strangers.
A pretty crummy picture.
Back to your deciding
which button to press.
If T is having
particularly strong
effects in your brain
at the time,
you become more likely
to perceive threat,
real or otherwise,
less caring about
others' pain,
and more likely
to fall into
aggressive tendencies
that you already have.
What factors determine
whether T has strong
effects in your brain?
Time of day matters,
as T levels are
nearly twice as high
during the daily
circadian peak
as during the trough.
Whether you're sick
or injured,
just had a fight,
or just had sex,
all influence
T secretion.
It also depends
on how high
your average
T levels are.
They can vary
five-fold
among healthy
individuals
of the same sex,
even more so
in adolescence.
Moreover,
the brain's
sensitivity to T
also varies,
with T receptor
numbers in some
brain regions
varying up to
tenfold among
individuals.
And why do
individuals differ
in how much T
their gonads make,
or how many
receptors there are
in particular
brain regions?
Genes and fetal
and postnatal
environment matter.
And why do
individuals differ
in the extent
of their pre-existing
tendencies toward
aggression?
That is,
how the amygdala,
frontal cortex,
and so on
differ.
Above all,
because of how
much life has
taught them
at a young age
that the world
is a menacing
place.
Testosterone is
not the only
hormone that can
influence your
button-pressing
intentions.
There's oxytocin,
acclaimed for
having pro-social
effects among
mammals.
Oxytocin enhances
mother-infant
bonding in mammals,
and enhances
human-dog bonding.
The related
hormone,
vasopressin,
makes males
more paternal
and the rare
species where
males help
parent.
These species
also tend to
form monogamous
pair bonds.
Oxytocin and
vasopressin strengthen
the bond in females
and males,
respectively.
What's the nuts-and-bolts
biology of why
males in some
rodent species
are monogamous
and others not?
Monogamous species
are genetically
prone toward higher
concentrations of
vasopressin receptors
in the dopaminergic
reward part of
the brain.
The nucleus
accumbens.
The hormone is
released during
sex.
The experience
with that female
feels really,
really pleasurable
because of the
higher receptor
number,
and the male
sticks around.
Amazingly,
boost vasopressin
receptor levels
in that part of
the brain in
males from
polygamous
rodent species,
and they become
monogamous.
Wham-bam-think!
Weird.
I don't know
what just came
over me.
But I'm going
to spend the
rest of my
life helping
this female
raise our
kids.
Oxytocin
and vasopressin
have effects
that are the
polar opposite
of T's.
They decrease
excitability in
the amygdala,
making rodents
less aggressive
and people
calmer.
Boost your
oxytocin levels
experimentally,
and you're more
likely to be
charitable and
trusting in a
competitive game.
And showing how
this is the
endocrinology of
sociality,
you wouldn't
have the
response to
oxytocin if
you thought you
were playing
against a
computer.
As an immensely
cool wrinkle,
oxytocin doesn't
make us warm and
fuzzy and pro-social
to everyone,
only to in-group
members, people who
count as an
us.
In one study in the
Netherlands, subjects
had to decide if it
was okay to kill one
person to save five.
Oxytocin had no
effects when the
potential victim had
a Dutch name, but
made subjects more
likely to sacrifice
someone with a
German or Middle
Eastern name, two
groups that evoke
negative connotations
among the Dutch, and
increased implicit
bias against those two
groups.
In another study,
while oxytocin made
team members more
cooperative in a
competitive game, as
expected, it made
them more preemptively
aggressive to
opponents.
The hormone even
enhances gloating over
strangers' bad luck.
Thus, the hormone
makes us nicer, more
generous, empathic,
trusting, loving, to
people who count as
an us.
But if it is a them,
who looks, speaks,
eats, prays, loves
differently than we
do, forget singing
kumbaya.
On to individual
differences related to
oxytocin.
The hormone's levels
vary manifold among
different individuals,
as do levels of
receptors for oxytocin
in the brain.
Those differences
arise from the
effects of everything
from genes and
fetal environment, to
whether you woke up
this morning next to
someone who makes you
feel safe and loved.
Moreover, oxytocin
receptors and
vasopressin receptors
each come in different
versions in different
people.
Which flavor you were
handed at conception
influences parenting
style, stability of
romantic relationships,
aggressiveness,
sensitivity to
threat, and
charitableness.
Thus, the decisions
you supposedly make
freely in moments that
test your character,
generosity, empathy,
honesty, are
influenced by the
levels of these
hormones in your
bloodstream and the
levels and variance
of their receptors in
your brain.
One last class of
hormones.
When an organism is
stressed, whether
mammal, fish, bird,
reptile, or amphibian,
it secretes from the
adrenal gland hormones
called glucocorticoids,
which do roughly the
same things to the
body in all these
cases.
They mobilize energy
from storage sites in
the body, like the
liver or fat cells, to
fuel exercising muscle.
Very helpful if you are
stressed because, say,
a lion is trying to
eat you, or if you're
that lion and will
starve unless you
predate something.
Following the same
logic, glucocorticoids
increase blood pressure
and heart rate,
delivering oxygen and
energy to those
life-saving muscles
that much faster.
They suppress
reproductive physiology.
Don't waste energy,
say, ovulating, if
you're running for
your life.
As might be expected
during stress,
glucocorticoids
altered the brain.
Amygdala neurons
become more
excitable, more
potently activating
the basal ganglia,
and disrupting the
frontal cortex, all
making for fast,
habitual responses
with low accuracy
in assessing what's
happening.
Meanwhile, as we'll
see in the next
chapter, frontal
cortical neurons
become less excitable,
limiting their
ability to make the
amygdala act
sensibly.
Based on these
particular effects in
the brain,
glucocorticoids have
predictable effects
on behavior during
stress.
Your judgments
become more
impulsive.
If you're reactively
aggressive, you
become more so.
If anxious, more so.
If depressive, ditto.
You become less
empathic, more
egoistic, more
selfish in moral
decision-making.
The workings of
every bit of this
endocrine system will
reflect whether you've
been stressed recently
by, say, a mean
boss, a miserable
morning's commute, or
surviving your village
being pillaged.
Your gene variance
will influence the
production and
degradation of
glucocorticoids, as
well as the number and
function of glucocorticoid
receptors in different
parts of your brain.
And the system would
have developed
differently in you
depending on things
like the amount of
inflammation you
experienced as a
fetus, your parents'
socioeconomic status, and
your mother's parenting
style.
Thus, three different
classes of hormones work
over the course of
minutes to hours to
alter the decision you
make.
This just scratches the
surface.
Google list of human
hormones, and you'll
find more than 75 most
affecting behavior, all
rumbling below the
surface, influencing your
brain without your
awareness.
Do these endocrine
effects over the course
of minutes to hours
disprove free will?
Certainly not on their
own, because they
typically alter the
likelihood of certain
behaviors rather than
cause them.
On to our next turtle,
heading all the way
down.
Weeks to years before.
So, hormones can change
the brain over the course
of minutes to hours.
In those cases, change
the brain isn't some
abstraction.
As a result of a
hormone's actions, neurons
might release packets of
neurotransmitter when they
otherwise wouldn't.
Particular ion channels
might open or close.
The number of receptors
for some messenger might
change in a specific
brain region.
The brain is structurally
and functionally
malleable, and your
pattern of hormone
exposure this morning
will have altered your
brain now, as you
contemplate the two
buttons.
The point of this
section is that such
neuroplasticity is small
potatoes compared with how
the brain can change in
response to experience over
longer periods.
Synapses might permanently
become more excitable, more
likely to send a message from
one neuron to the next.
Pairs of neurons can form
entirely new synapses, or
disconnect existing ones.
Branchings of dendrites and
axons might expand or
contract.
Neurons can die.
Others are born.
Particular brain regions
might expand or atrophy so
dramatically that you can
see the changes on a brain
scan.
Some of this neuroplasticity is
immensely cool, but
tangential to free will
squabbles.
If someone goes blind and
learns to read braille, her
brain remaps.
That is, the distribution and
excitability of synapses to
particular brain regions
change.
Result?
Reading braille with her
fingertips, a tactile
experience, stimulates neurons
in the visual cortex, as if
she were reading printed
text.
Blindfold the volunteer for a
week, and his auditory
projections start colonizing
the snoozing visual cortex,
enhancing his hearing.
Learn a musical instrument, and
the auditory cortex remaps to
devote more space to the
instrument's sound.
Persuade some wildly invested
volunteers to practice a five-
finger exercise on the piano two
hours a day for weeks, and
their motor cortex remaps to
devote more space to controlling
finger movements in that hand.
Get this, the same thing happens if
the volunteer spends that time
imagining the finger exercise.
But then there's neuroplasticity
relevant to free willlessness.
Developing post-traumatic stress
disorder after trauma transforms the
amygdala, synapse number increases
along with the extent of the
circuitry by which the amygdala
influences the rest of the brain.
The overall size of the amygdala
increases, and it becomes more
excitable, with a lower threshold for
triggering fear, anxiety, and
aggression.
Then there's the hippocampus, a brain
region central to learning and memory.
Suffer from major depression for
decades, and the hippocampus shrinks,
disrupting learning and memory.
In contrast, experience two weeks of
rising estrogen levels, that is, be in
the follicular stage of your ovulatory
cycle, and the hippocampus beefs up.
Likewise, if you enjoy exercising
regularly, or are stimulated by an
enriching environment.
Moreover, experience-induced changes
aren't limited to the brain.
Chronic stress expands the adrenal
glands, which then pump out more
glucocorticoids, even when you're not
stressed.
Becoming a father reduces testosterone
levels.
The more nurturing you are, the bigger
the drop.
How's this for how unlikely the
subterranean biological forces on your
behavior can be over weeks to months?
Your gut is filled with bacteria, most of
which help you digest your food.
Filled with is an understatement.
There are more bacteria in your gut than
cells in your own body, of hundreds of
different types, collectively weighing more
than your brain.
As a burgeoning new field, the makeup of the
different species of bacteria in your gut over
the previous weeks will influence things like
appetite and food cravings, and gene
expression patterns in your neurons, and
proclivity toward anxiety, and the ferocity with
which some neurological diseases spread through
your brain.
Clear out all of a mammal's gut bacteria with
antibiotics, and transfer in the bacteria from
another individual, and you'll have transferred
those behavioral effects.
These are mostly subtle effects, but who would have
thought that bacteria in your gut were influencing
what you mistake for free agency?
The implications of all these findings are
obvious.
How will your brain function as you contemplate
the two buttons?
It depends, in part, on events during previous
weeks to years.
Have you been barely managing to pay the rent each
month?
Experiencing the emotional swell of finding love or
of parenting?
Suffering from deadening depression?
Working successfully at a stimulating job?
Rebuilding yourself after combat trauma or sexual
assault?
Having had a dramatic change in diet?
All will change your brain and behavior, beyond your
control, often beyond your awareness.
Moreover, there will be a meta-level of differences
outside your control, in that your genes and
childhood will have regulated how easily your brain
changes in response to particular adult experiences.
There is plasticity, as to how much and what kind of
neuroplasticity each person's brain can manage.
Does neuroplasticity show that free will is a myth?
Not by itself.
Next turtle.
Back to adolescence.
As will be familiar to any reader who is, was, or will be an
adolescent, this is one complex time of life.
Emotional gyrations, impulsive risk-taking and sensation-seeking,
the peak time of life for extremes of both pro- and
antisocial behavior, for individuated creativity, and for
peer-driven conformity.
Behaviorally, it is a beast unto itself.
Neurobiologically, as well.
Most research examines why adolescents behave in adolescent ways.
In contrast, our purpose is to understand how features of the
adolescent brain help explain button-pushing intentions in
adulthood.
Conveniently, the same hugely interesting bit of neurobiology is
relevant to both.
By early adolescence, the brain is a fairly close approximation of
the adult version, with adult densities of neurons and synapses, and the
process of myelinating the brain already achieved.
Except for one brain region, which, amazingly, won't fully mature for
another decade.
The region?
The frontal cortex, of course.
Maturation of this region lags way behind the rest of the cortex, to some
degree in all mammals, and dramatically so in primates.
Some of that delayed maturation is straightforward.
Starting with fetal brain building, there's a steady increase in myelination up to
adult levels, including in the frontal cortex, just with a huge delay.
But the picture is majorly different when it comes to neurons and synapses.
At the start of adolescence, the frontal cortex has more synapses than in the adult.
Adolescence and early adulthood consist of the frontal cortex pruning synapses that
turn out to be superfluous, pokey, or plain wrong, as the region gets progressively leaner
and meaner.
As a great demonstration of this, while a 13-year-old and a 20-year-old may perform equally on
some test of frontal function, the former needs to mobilize more of the region to accomplish
this.
So, the frontal cortex, with its roles in executive function, long-term planning, gratification
postponement, impulse control, and emotion regulation, isn't fully functional in adolescence.
Hmm.
What do you suppose that explains?
Just about everything in adolescence.
Especially when adding the tsunamis of estrogen, progesterone, and testosterone flooding the
brain then.
A juggernaut of appetites and activation, constrained by the flimsiest of frontal cortical breaks.
For our purposes, the main point about delayed frontal maturation isn't that it produces kids
who got really bad tattoos, but the fact that adolescence and early adulthood involve a massive
construction project in the brain's most interesting part.
The implications are obvious.
If you're an adult, your adolescent experiences of trauma, stimulation, love, failure, rejection,
happiness, despair, acne, the whole shebang, will have played an outsized role in constructing
the frontal cortex you're working with as you contemplate those buttons.
Of course, the enormous varieties of adolescence experiences will help produce enormously varied
frontal cortexes in adulthood.
A fascinating implication of the delayed maturation is important to remember when we get to the section
on genes.
By definition, if the frontal cortex is the last part of the brain to develop, it is the
brain region least shaped by genes, and most shaped by environment.
This raises the question of why the frontal cortex matures so slowly.
Is it intrinsically a tougher building project than the rest of the cortex?
Are there specialized neurons, neurotransmitters unique to the region, that are tough to synthesize?
Distinctive synapses that are so fancy that they require thick construction manuals?
No, virtually nothing unique like that.
Thus, delayed maturation isn't inevitable, given the complexity of frontal construction,
where the frontal cortex would develop faster if only it could.
Instead, the delay actively evolved, was selected for.
If this is the brain region central to doing the right thing when it's the harder thing to do,
no genes can specify what counts as the right thing.
It has to be learned the long, hard way, by experience.
This is true for any primate, navigating social complexities as to whether you hassle or kowtow
to someone, align with them, or stab them in the back.
If that's the case for some baboon, just imagine humans.
We have to learn our culture's rationalizations and hypocrisies,
thou shalt not kill, unless it's one of them, in which case, here's a medal.
Don't lie, except if there's a huge payoff, or it's a profoundly good act.
Nope, no refugees hiding in my attic, no siree.
Laws to be followed strictly, laws to be ignored, laws to be resisted.
Reconciling acting as if each day is your last, with today being the first day of the rest of your life.
On and on.
Reflecting that, while frontocortical maturation finally tops out around puberty in other primates,
we need another dozen years.
This suggests something remarkable.
The genetic program of the human brain evolved to free the frontocortex from genes as much as possible.
Much more to come about the frontocortex in the next chapter.
Next turtle.
And childhood.
So adolescence is the final phase of frontocortical construction,
with the process heavily shaped by environment and experience.
Moving further back into childhood,
there are massive amounts of construction of everything in the brain.
A process of a smooth increase in the complexity,
or neuron-neuronal circuitry,
and of myelination.
Naturally, this is paralleled by growing behavioral complexity.
There is maturation of reasoning skills,
and of cognition and affect,
relevant to moral decision-making.
For example,
transitioning from obeying laws
to avoid punishment
to obeying.
Because where would society be without people obeying them?
There's maturation of empathy,
with growing capacities to empathize with someone's emotional
rather than physical state.
About abstract pain.
About pains you've never experienced.
About pain for people totally different from you.
Impulse control is also maturing.
From successfully restraining yourself
for a few minutes from eating a marshmallow,
in order to then be rewarded with two marshmallows,
to staying focused on your 80-year project
to get into the nursing home of your choice.
In other words,
simpler things
precede more complicated things.
Childhood development researchers
have typically framed these trajectories
of maturation
as coming in stages.
For example,
Harvard psychologist
Lawrence Kohlberg's
canonical stages
of moral development.
Predictably,
there are huge differences
as to what particular maturational stage
different kids are at,
the speed of stage transitions,
and the stage carried stably
into adulthood.
Speaking to our interests,
you have to ask
where individual differences
in maturation come from,
how much control
we have over that process,
and how it helps generate
the you that is you,
contemplating the buttons,
what sorts of influences
affect maturation.
An overlapping list
of the most usual suspects
with incredibly brief summaries.
1. Parenting, of course.
Differences in parenting styles
were the focus
of highly influential work
originating with
Berkeley psychologist
Diana Baumreind.
There's authoritative parenting,
where high levels of demands
and expectation
are placed on the child,
coupled with lots of flexibility
in responding
to the child's needs.
This is usually the style
aspired to
by neurotic,
middle-class parents.
Then there's
authoritarian parenting,
high demand,
low responsiveness.
Do this because I said so.
Permissive parenting,
low demand,
high responsiveness.
And negligent parenting,
low demand,
low responsiveness.
And each tends to produce
a different sort of adult.
As we'll see in the next chapter,
parental socio-economic status,
SES,
is also enormously important.
For example,
low familial SES
predicts stunted maturation
of the frontal cortex
in kindergartners.
2. Peer socialization,
with different peers
modeling different behaviors
with varying allure.
The importance of peers
has often been
underappreciated
by developmental psychologists,
but is no surprise
to any primatologists.
Humans invented
a novel way
to transmit information
across generations,
where an adult expert
intentionally directs
information at young'uns,
that is,
a teacher.
In contrast,
the usual among primates
is kids learning
by watching
their somewhat
older peers.
3. Environmental influences.
Is the neighborhood park safe?
Are there more bookstores
or liquor stores?
Is it easy to buy
healthy food?
What's the crime rate?
All the usual.
4. Cultural beliefs
and values,
which influence
these other categories.
As we'll see,
culture dramatically influences
parenting style,
the behaviors modeled
by peers,
the sorts of
physical and social
communities
that are constructed,
cultural variability
in overt
and covert
rites of passage,
the brands
of places of worship,
whether kids aspire
to earn lots
of merit badges
versus getting skilled
at harassing
out-group members.
A pretty straightforward list.
And, of course,
there are loads
of individual differences
in childhood patterns
of hormone exposure,
nutrition,
pathogen load,
and so on.
All converging
to produce a brain
that,
as we'll see
in Chapter 5,
has to be unique.
The huge question
then becomes,
how do different
childhoods produce
different adults?
Sometimes,
the most likely pathway
seems pretty clear
without having to get
all neuroscience-y.
For example,
a study examining
more than a million people
across China
and the U.S.,
showed the effects
of growing up
in clement weather.
That is,
mild fluctuations
around an average
of 70 degrees.
Such individuals
are,
on the average,
more individualistic,
extroverted,
and open
to novel experience.
Likely explanation?
The world is a safer,
easier place
to explore growing up
when you don't have
to spend significant
chunks of each year
worrying about dying
of hypothermia
and or heatstroke
when you go outside,
where average income
is higher
and food stability
greater.
And the magnitude
of the effect
isn't trivial,
being equal to
or greater than
that of age,
gender,
the country's GDP,
population density,
and means of production.
The link between
weather clemency
in childhood
and adult personality
can be framed
biologically
in the most
informative way.
The former influences
the type of brain
you're constructing
that you will carry
into adulthood,
as is almost
always the case.
For example,
lots of childhood stress
by way of glucocorticoids
impairs construction
of the frontal cortex,
producing an adult
less adept
at helpful things
like impulse control.
Lots of exposure
to testosterone
early in life
makes for the construction
of a highly reactive
amygdala,
producing an adult
more likely to respond
aggressively to provocation.
The nuts and bolts
of how this happens
revolves around
the massively trendy
field of epigenetics,
revealing how
early life experience
causes long-lasting
changes in gene expression
in particular
brain regions.
Now,
this is not experience
changing genes
themselves,
that is,
changing DNA sequences,
but instead,
changing their regulation,
whether some gene
is always active,
never active,
or active in one context
but not another.
A lot is known
by now
about how this works.
As one celebrated example,
if you're a baby rat
growing up
with an atypically
inattentive mother,
epigenetic changes
in the regulation
of one gene
in your hippocampus
will make it harder
for you to recover
from stress as an adult.
Where do differences
in rodential mothering
style come from?
Obviously,
from one second,
one minute,
one hour
before in that rat
mom's biological history.
Knowledge about
epigenetic bases of this
has grown at breakneck speed,
showing, for example,
how some epigenetic changes
in the brain
can have
multi-generational
consequences.
For example,
helping to explain
why being a rat,
monkey,
or a human
abused in childhood
increases the odds
of being an abusive parent.
Just to show
the scale
of epigenetic complexity,
differences in
mothering styles
in monkeys
cause epigenetic changes
in more than
a thousand genes
expressed in the
offspring's
frontal cortex.
If you had to
compress the variability
in all those facets
of childhood influences
into a single axis,
it would be easy.
How lucky was
the childhood
you were handed?
This massively
important fact
has been formalized
into an adverse
childhood experience,
ACE score.
What count as
adverse experiences
in this measure?
A logical list.
Abuse.
Physical.
Emotional.
Sexual.
Neglect.
Physical.
Emotional.
Household dysfunction.
Mental illness.
Mother treated violently.
Divorce.
Incarcerated relative.
Substance abuse.
For each of these
experienced,
you get a point
on the checklist
where the unluckiest
have scores
approaching an
unimaginable 10
and the luckiest
luxuriating around 0.
This field
has produced
a finding
that should floor
anyone holding out
for free will.
For every step
higher in one's
ACE score,
there is roughly
a 35% increase
in the likelihood
of adult
antisocial behavior,
including violence,
poor frontocortical
dependent cognition,
problems with
impulse control,
substance abuse,
teen pregnancy
and unsafe sex,
and other risky
behaviors,
and increased
vulnerability to
depression
and anxiety
disorders.
Oh, and also
poorer health
and earlier death.
You'd get the same
story if you flipped
the approach
180 degrees.
As a child,
did you feel loved
and safe in your
family?
Was there good
modeling about
sexuality?
Was your neighborhood
crime-free?
Your family
mentally healthy?
Your socioeconomic
status reliable
and good?
Well, then,
you'd be heading
toward a high
RLCE score,
ridiculously lucky
childhood experiences,
predictive of all
sorts of important
good outcomes.
Thus,
essentially every
aspect of your
childhood,
good,
bad,
or in between,
factors over which
you had no control,
sculpted the adult
brain you have
while contemplating
those buttons.
How's this for an
example outside of
someone's control?
Because of the
randomness of
month of birth,
some kids can be
as much as six
months older or
younger than the
average of their
peer group.
Older kindergarteners,
for example,
are typically more
cognitively advanced.
Result?
They get more
one-on-one attention
and praise from
teachers,
so that by first
grade,
their advantage is
even greater,
so that by second
grade...
And in the UK,
which has an
August 31st cutoff
for kindergarten,
this relative
age effect
produces a major
skew in
educational attainment.
See figure 2.1
in the PDF file
that accompanies
this audiobook.
Luck evens out
over time,
my ass.
Does the role
of childhood
invalidate free will?
Nope.
The likes of ACE
scores are about
adult potential
and vulnerability,
not inevitable
destiny,
and there are
plenty of people
whose adulthoods
are radically
different from
what you'd expect,
given their
childhoods.
This is just
another piece
of the sequence
of influences.
Back to the womb.
If you couldn't
control what family
you landed in
at birth,
you sure had
no control
over which womb
you hung out in
for nine
influential months.
Environmental
influences begin
long before birth.
The biggest
source of these
influences
is what's in
the maternal
circulation,
which will help
determine what's
in the fetus,
levels of a huge
array of different
hormones,
immune factors,
inflammatory
molecules,
pathogens,
nutrients,
environmental toxins,
illicit substances,
all which regulate
brain function
in adulthood.
Not surprising,
the general themes
echo those of
childhood.
Lots of
glucocorticoids
from mom
marinating
your fetal
brain,
thanks to
maternal stress,
and there's
increased
vulnerability
to depression
and anxiety
in your
adulthood.
Lots of
androgens
in your
fetal circulation,
coming from
mom.
Females
secrete
androgens,
though to a
lesser extent
than do
males.
Makes you
more likely
as an
adult of
either sex
to show
spontaneous
and reactive
aggression,
poor emotion
regulation,
low empathy,
alcoholism,
criminality,
even
lousy
handwriting,
a shortage
of nutrients
for the
fetus,
caused by
maternal
starvation,
and there's
increased risk
of schizophrenia
in adulthood,
along with
a variety
of metabolic
and cardiovascular
diseases,
the implications
of fetal
environmental
effects,
another route
toward how
lucky or
unlucky you're
likely to be
in the world
that awaits
you.
Back to
your very
beginning.
Genes.
Down to
the next
turtle.
If you
didn't choose
the womb
you grew
in,
you certainly
didn't choose
the unique
mixture of
genes you
inherited from
your parents.
Genes have
plenty to do
with decision-making
crossroads,
and in more
interesting ways
than commonly
believed.
We start with
an unbelievably
superficial
primer on
genes,
to position us
to appreciate
things when
we get to
genes and
free will.
First,
what are
genes and
what do
they do?
Our bodies
are filled
with thousands
of different
types of
proteins doing
dizzyingly
varied jobs.
Some are
cytoskeletal
proteins that
give different
cell types
their distinctive
shapes.
Some are
messengers.
Many
neurotransmitters,
hormones,
and immune
messengers are
proteins.
It's proteins
that make up
enzymes that
construct those
messengers and
that tear them
apart when
they're obsolete.
Virtually all
receptors for
messengers throughout
the body are
made of protein.
Where does all
this proteinaceous
versatility come
from?
Each type of
protein is
constructed from
a distinctive
sequence of
different types of
amino acid
building blocks.
The sequence
determines the
shape of the
protein.
The shape
determines
function.
A gene is
the stretch of
DNA that
specifies the
sequence shape
function of a
particular protein.
Each of our
approximately 20,000
genes codes for
the production of
a unique protein.
How does a gene
decide when to
initiate the
construction of the
protein it codes
for, and whether
there will be one
or 10,000 copies
made?
Implicit in this
question is the
popular view of
genes as the be-all
and end-all, the
code of codes in
regulating what
goes on in your
body.
As it turns out,
genes decide
nothing, are out
at sea.
Saying that a
gene decides when
to generate its
associated protein
is like saying
that the recipe
decides when to
bake the cake
that it codes
for.
Instead, genes
are turned on
and off by
environment.
What is meant
here by
environment?
It can be the
environment within
a single cell.
The cell is
running low on
energy, which
generates a
messenger molecule
that activates
the genes that
code for proteins
that boost energy
production.
Environment can
encompass the
entire body.
A hormone is
secreted and is
carried in the
circulation to
target cells at
the other end of
the body, where
it binds to its
distinctive receptors.
As a result,
particular genes
are turned on
or off.
Or environment
can take the
form of our
everyday usage,
namely events
happening in the
world around us.
These different
versions of
environment are
linked.
For example,
living in a
stressful, dangerous
city will produce
chronically elevated
levels of
glucocorticoids
secreted by your
adrenal glands,
which will activate
particular genes
and neurons in the
amygdala, making
those cells more
excitable.
how do differently
environmentally
activated messengers
turn on different
genes?
Not every stretch of
DNA contributes to
the code in a
gene.
Instead, long
stretches don't
code for anything.
Instead, they are
the on-off switches
for activating
nearby genes.
Now, for a wild
fact, only about
5% of DNA
constitutes genes.
The remaining
95%?
The dizzingly
complex on-off
switches, the
means by which
various environmental
influences regulate
unique networks of
genes, with
multiple types of
switches on a
single gene and
multiple genes being
regulated by the
same type of
switch.
In other words,
most DNA is
devoted to gene
regulation rather
than to genes
themselves.
Moreover,
evolutionary changes
in DNA are
usually more
consequential when
they alter on-off
switches rather
than the gene.
As another measure
of the importance
of the regulation,
the more complex
the organism, the
greater the
percentage of its
DNA is devoted
to gene
regulation.
Where have we
gotten in this
primer?
Genes code for
workhorse proteins.
Genes don't decide
when they are
active, but are
instead regulated
by environmental
signals.
The evolution of
DNA, the
is disproportionately
about gene
regulation rather
than about genes.
So, environmental
signals have
activated some
gene, leading to
the production of
its protein.
The newly made
proteins then do
their usual thing.
As a next key
point, the same
protein can work
differently in
different environments.
Such gene-slash-
environment
interactions are
less important in
species that inhabit
only one type of
environment.
environment, but
there are plenty
relevant in
species that inhabit
multiple types of
environments, species
like, say, us.
We can live in
tundra, desert, or
rainforest, in an
urban megalopolis of
millions, or in
small hunter-gatherer
bands, in capitalist
or socialist societies,
polygamous, or
monogamous cultures.
When it comes to
humans, it can be
silly to ask what a
particular gene does,
only what it does in
a particular
environment.
What might gene-slash-
environment
interactions look like?
Suppose someone has a
gene variant related to
aggression.
Depending on the
environment, that can
result in an increased
likelihood of street
brawling or of playing
chess really aggressively,
or a gene related to
risk-taking, that
depending on environment
will influence whether
you rob a store or
gamble on founding a
startup, or a gene
related to addiction,
that depending on
environment produces a
brahmin drinking too
much scotch in his club
or someone desperately
stealing to get money
for heroin.
Final bit of the
primer.
Most genes come in
more than one flavor,
with people inheriting
their particular variants
from their parents.
Such gene variants
code for slightly
different versions of
their protein, with
some being better at
their job than others.
Where have we gotten?
People differing in the
flavors of genes they
possess, those genes
being regulated
differently in different
environments, producing
proteins whose effects
vary in different
environments.
We now consider how
genes relate to this
free-will obsession of
ours.
It's button time.
How will your brain be
influenced in that
moment by the flavors of
particular genes you
inherited?
Consider the neurotransmitter
serotonin.
Differing profiles of
serotonin signaling among
people help explain
individual differences
related to mood, levels of
arousal, tendency toward
compulsive behavior,
ruminative thoughts, and
reactive aggression.
And how can individual
differences in gene
variants contribute to
differences in serotonin
signaling?
Easily, different flavors
exist for the genes coding
for the proteins that
synthesize serotonin, that
remove it from the
synapse, and that
degrade it, plus
variants in the genes that
code more than a dozen
different types of
serotonin receptors.
Same story with the
neurotransmitter
dopamine.
To barely scratch the
surface, individual
differences in dopamine
signaling are relevant to
reward, anticipation,
motivation, addiction,
gratification postponement,
long-term planning, risk
taking, novelty seeking,
salience of cues, and
ability to focus.
You know, things pertinent
to our judging say whether
someone could have
transcended their dire
circumstances if only they
could have shown some
self-discipline.
And the genetic sources of
dopaminergic differences
among people?
Genetic variants related to
dopamine's synthesis,
degradation, and removal
from the synapse, as well as
in the various dopamine
receptors.
We could go on now to the
neurotransmitter, norepinephrine,
or enzymes that synthesize
and degrade various hormones
and hormone receptors, or
pretty much anything pertinent
to brain function.
There's usually extensive
individual variation in every
relevant gene, and you
weren't consulted as to
which you'd choose to
inherit.
What about the flip side?
A bunch of people all have
the identical gene variant,
but live in different
environments?
You get precisely what was
just discussed, namely,
dramatically different effects
of the gene variant
depending on environment.
For example, one variant of
the gene whose protein
breaks down serotonin will
increase your risk of
antisocial behavior, but
only if you were severely
abused during childhood.
A variant of a dopamine
receptor gene makes you
either more or less likely
to be generous, depending on
whether you grew up with or
without secure parental
attachment.
That same variant is
associated with poor
gratification postponement if
you were raised in poverty.
One variant of the gene that
directs dopamine synthesis is
associated with anger, but
only if you were sexually
abused as a kid.
One version of the gene for
the oxytocin receptor is
associated with less
sensitive parenting, but
only when coupled with
childhood abuse.
On and on, and with many of
the same relationships being
seen in other primate species
as well.
Dang, how can environment
cause genes to work so
differently, even in
diametrically opposite ways?
just to start to put all the
pieces together, because
different environments will
cause different sorts of
epigenetic changes in the
same gene or genetic switch.
Thus, people have all these
different versions of all of
these, and these different
versions work differently,
depending on childhood
environment.
Just to put some numbers to
it, humans have roughly 20,000
genes in our genome.
Of those, approximately 80% are
active in the brain.
16,000.
Of those genes, nearly all
come in more than one
flavor, are polymorphic.
Does this mean that in each of
those genes, the polymorphism
consists of one spot in that
gene's DNA sequence that can
differ among individuals?
No.
There are actually an average of
250 spots in the DNA sequence of
each gene, which adds up to
there being individual
variability in approximately
4 million spots in the
sequence of DNA that codes for
genes active in the brain.
Does behavior genetics disprove
free will?
Not on its own.
As a familiar theme, genes are
about potentials and
vulnerabilities, not
inevitabilities, and the effects
of most of these genes on
behavior are relatively mild.
Nonetheless, all these effects on
behavior arise from genes you
didn't choose, interacting with
the childhood you didn't choose.
Back centuries.
The sort of people you come from.
The Libetian buttons beckon.
What does your culture have to do
with the intent you will act upon?
Tons.
Because from your moment of birth,
you were subject to a universal,
which is that every culture's values
include ways to make their
inheritors recapitulate those
values, to become the sort of
people you come from.
As a result, your brain reflects
who your ancestors were and what
historical and ecological
circumstances led them to invent
those values surrounding you.
If a fairly tunnel-visioned
neurobiologist became dictator of
the world, anthropology would be
defined as the study of the ways
that different groups of people
attempt to shape brain construction
in their children.
Cultures produce dramatically
different behaviors with
consistent patterns.
One of the most studied contrasts
concerns individualist versus
collectivist cultures.
The former emphasize autonomy,
personal achievement,
uniqueness, and the needs and rights
of the individual.
It's looking out for number one,
where your actions are yours.
Collectivist cultures, in contrast,
espouse harmony, interdependence,
and conformity, where the needs of
the community guide behavior.
The priority is that your actions
make the community proud,
because you are theirs.
Most studies of these contrasts compare
individuals from the poster child of
individualist cultures,
the United States,
with those from the textbook
collectivist cultures of East Asia.
The differences make sense.
People from the U.S. are more likely
to use first-person singular pronouns
to define themselves
in personal rather than relational terms.
I'm a lawyer,
versus I'm a parent.
To organize memory around events,
rather than social relations.
The summer I learned to swim,
versus the summer we became friends.
Ask subjects to draw a sociogram,
a diagram with circles
representing themselves
and the people who matter in their lives.
Connected by lines,
Americans typically place themselves
in the biggest circle,
in the center.
Meanwhile,
an East Asian's circle
typically is no bigger than the others,
and is not front and center.
The American goal
is to distinguish yourself
by getting ahead of everyone else.
The East Asian
is to avoid being distinguishable.
And from these differences
come major differences
as to what count as norm violations
and what you do about them.
Naturally,
this reflects different workings
of the brain and body.
On average,
in East Asian individuals,
the dopamine reward system
activates more
when looking at a calm
versus excited facial expression.
For Americans,
it's the opposite.
Show subjects a picture
of a complex scene.
Within milliseconds,
East Asians typically scan
the entire scene as a whole,
remembering it.
Americans focus on the person
in the center of the picture.
Force an American to tell you
about times
that other people influenced them,
and they secrete glucocorticoids.
Someone East Asian
will secrete the stress hormone
when forced to tell you
about times
they influenced other people.
Where do these differences
come from?
The standard explanations
for American individualism
include,
A,
not only are we a nation
of immigrants,
as of 2017,
approximately 37% immigrants,
or children of.
But it's not random
who emigrates.
Instead,
emigrating is a filtering process
selecting for people
willing to leave their world
and culture behind,
sustain an arduous journey
to a place with barriers
impeding their entry,
and labor at the most shit jobs
when granted admission.
And B,
most of American history
has been spent
with an expanding western border
settled by similarly tough,
individualist pioneers.
Meanwhile,
the standard explanation
for East Asian collectivism
is ecology dictating
the means of production.
Ten millennia of rice farming,
which demands massive amounts
of collective labor
to turn mountains
into terraced rice paddies,
collective planting
and harvesting
of each person's crops
and sequence,
collective construction
and maintenance
of massive
and ancient irrigation systems.
A fascinating exception
that proves the rule
concerns parts
of northern China
where the ecosystem
precludes rice growing,
producing millennia
of the much more
individualistic process
of wheat farming.
Farmers from this region,
and even their university student
grandchildren,
are as individualistic
as westerners.
As one finding
that is beyond cool,
Chinese from rice regions
accommodate
and avoid obstacles.
In this case,
walking around two chairs
experimentally placed
to block the way
in Starbucks.
People from wheat regions
remove obstacles.
That is,
moving the chairs apart.
Thus,
cultural differences
arising centuries,
millennia ago,
influence behaviors
from the most subtle
and minuscule
to dramatic.
Another literature
compares cultures
of rainforest
versus desert dwellers,
where the former
tend toward inventing
polytheistic religions,
the latter
monotheistic ones.
This probably reflects
ecological influences
as well.
Life in the desert
is a furnace-blasted,
desiccated,
singular struggle
for survival.
Rainforests
teem with a multitude
of species,
biasing toward the invention
of a multitude
of gods.
Moreover,
monotheistic desert dwellers
are more warlike
and more effective conquerors
than rainforest polytheists,
explaining why
roughly 55% of humans
proclaim religions
invented by
Middle Eastern
monotheistic shepherds.
Shepherding raises
another cultural difference.
Traditionally,
humans make livings
as agriculturalists,
hunter-gatherers,
or pastoralists.
The last are folks
in deserts,
grasslands,
or plains of tundra,
with their herds
of goats,
camels,
sheep,
cows,
llamas,
yaks,
or reindeer.
Such pastoralists
are uniquely vulnerable.
It's hard to sneak in
at night
and steal someone's
rice field
or rainforest,
but you can be
a sneaky varmint
and rustle someone's herd,
stealing the milk
and meat
they survive on.
This pastoralist
vulnerability
has generated
cultures of honor
with the following features.
A.
Extreme but temporary
hospitality
to the stranger
passing through.
After all,
most pastoralists
are wanderers themselves
with their animals
at some point.
B.
Adherence to strict
codes of behavior
where norm violations
are typically interpreted
as insulting someone.
C.
Such insults
demanding
retributive violence,
the world of feuds
and vendettas
lasting generations.
D.
The existence
of warrior classes
and values
where valor in battle
produces high status
and a glorious afterlife.
Much has been made
of the hospitality,
conservatism,
as in strictly
conserving cultural norms,
and violence
of the traditional
culture of honor
of the American South.
The pattern
of violence
tells a ton.
Murders in the South,
which typically
has the highest rates
in the country,
are not about
stick-ups gone wrong
in a city.
They're about
murdering someone
who has seriously
tarnished your honor
by conspicuously
bad-mouthing you,
failing to repay a debt,
coming on
to your significant other,
particularly
if living
in a rural area.
Where does
the Southern culture
of honor
come from?
A widely accepted
theory among historians
makes this paragraph's
point perfectly.
While colonial
New England
filled with pilgrims
and the Mid-Atlantic
with mercantile folks
like Quakers,
the South
was disproportionately
peopled by
wild-assed
pastoralists
from Northern England,
Scotland,
and Ireland.
One last
cultural comparison
between
tight cultures
with numerous
and strictly
enforced norms
of behavior
and loose ones.
What are some
predictors
of a society
being tight?
A history
of lots
of cultural crises,
droughts,
famines,
and earthquakes,
and high rates
of infectious diseases.
And I mean it
with history.
In one study
of 33 countries,
tightness
was more likely
in cultures
that had high
population densities
back in 1500.
500 years ago?
How can that be?
Because
generation
after generation,
ancestral culture
influenced the likes
of how much
physical contact
mothers had
with their children,
whether kids
were subject
to scarification,
genital mutilation,
and life-threatening
rites of passage,
whether myths
and songs
were about vengeance
or turning
the other cheek.
Does the influence
of culture
disprove
free will?
Obviously not.
As usual,
these are tendencies
amid lots
of individual variation.
Just consider
Gandhi,
Anwar Sadat,
Yitzhak Rabin,
and Michael Collins,
atypically inclined
toward peacemaking,
assassinated
by co-religionists,
atypically inclined
toward extremism
and violence.
Oh, why not?
Evolution.
For various reasons,
humans were sculpted
by evolution
over millions of years
to be,
on the average,
more aggressive
than bonobos,
but less so
than chimps,
more social
than orangutans,
but less so
than baboons,
more monogamous
than mouse lemurs,
but more polygamous
than marmosets.
Nuff said.
Seamless.
Where does intent
come from?
What makes us
who we are
at any given minute?
What came
before?
This raises
an immensely
important point
first brought up
in chapter 1,
which is that
the biology
and environment
interactions
of, say,
a minute ago
and a decade ago
are not separate
entities.
Suppose we are
considering the genes
someone inherited,
back when they were
a fertilized egg,
and what those genes
have to do
with that person's
behavior.
Well then,
we are being
geneticists
thinking about
genetics.
We could even
make our club
more exclusive
and be
behavior geneticists,
publishing our
research only
in a journal
called,
well,
Behavior Genetics.
But if we are
talking about
the genes inherited
that are relevant
to the person's
behavior,
we're automatically
also talking about
how the person's
brain was constructed.
Because brain
construction is
primarily carried
out by the
proteins coded
for, by genes
implicated in
neurodevelopment.
Similarly, if we
are studying the
effects of
childhood adversity
on adult
behavior, often
best understood
on the
psychological or
sociological level,
we're implicitly
also considering
how the molecular
biology of
childhood epigenetics
helps explain
adult personality
and temperament.
If we are
evolutionary
biologists
thinking about
human behavior,
by definition,
we're also
being behavior
geneticists,
developmental
neurobiologists,
and neuroplasticians.
Spellcheck just
went crazy.
This is because
evolving means
changes in what
variants of genes
you find in
organisms, and
thus the ways in
which they shape
brain construction.
Study hormones
and behavior,
and we're also
studying what
fetal life had
to do with the
development of
the glands that
secrete those
hormones, so on
and so on, each
moment flowing from
all that came
before, and
whether it's the
smell of a
room, what
happened to you
when you were a
fetus, or what
was up with your
ancestors in the
year 1500, all
are things that
you couldn't
control.
A seamless
stream of
influences that,
as said at the
beginning, precludes
being able to
shoehorn in this
thing called free
will that is
supposedly in the
brain, but not
of it.
In the words of
legal scholar
Pete Alces,
there is no
remaining gap
between nature
and nurture
for moral
responsibility to
fill.
Philosopher
Peter Tse hits
the nail on the
head when
referring to the
biological turtles
all the way
down as a
responsibility
destroying
regress.
This seamless
stream shows why
bad luck doesn't
get evened
out, why it
amplifies instead.
Have some
particular unlucky
gene variant, and
you'll be unluckily
sensitive to the
effects of
adversity during
childhood.
Suffering from
early life
adversity is a
predictor that
you'll be spending
the rest of your
life in
environments that
present you with
fewer opportunities
than most, and
that enhanced
developmental
sensitivity will
unluckily make you
less able to
benefit from those
rare opportunities.
You may not
understand them, may
not recognize them
as opportunities, may
not have the tools to
make use of them, or
to keep you from
impulsively blowing
the opportunity.
Fewer of those
benefits make for a
more stressful adult
life, which will
change your brain into
one that is
unluckily bad at
resilience, emotional
control, reflection,
cognition.
Bad luck doesn't get
evened out by good.
It is usually
amplified until you're
not even on the
playing field that
needs to be leveled.
This is the view
forcefully argued by
philosopher Neil
Levy in his 2011
book, Hard Luck,
How Luck Undermines
Free Will and
Moral Responsibility,
Oxford University
Press.
He focuses on two
categories of luck.
One, present luck,
examines its role in
the difference between
driving while so drunk
that when coupled
with events in the
seconds to minutes
before, you would
have killed someone if
they had happened to
be crossing the
street, and the bad
luck of being in that
state and actually
killing someone.
As we saw, whether
this distinction is
meaningful is often
the domain of legal
scholars.
More meaningful to
Levy is what he
calls constitutive luck,
the fortune, good or
bad, that sculpted you
up to this moment.
In other words, our
world of one second
before, one minute
before.
Although he only
passingly frames the
idea biologically.
And when you recognize
that that is all there
is to explain who we
are, he concludes,
it is not ontology
that rules out free
will, it is luck, his
emphasis.
In his view, not only
does it make no sense
to hold us responsible
for our actions, we
also had no control
over the formation of
our beliefs, about the
rightness and
consequences of that
action, or about the
availability of
alternatives.
You can't successfully
believe something
different from what you
believe.
In the first chapter, I
wrote about what is
needed to prove free
will, and this chapter
has added details to
that demand.
Show me that the thing
a neuron just did in
someone's brain was
unaffected by any of
these preceding
factors, by the
goings-on in the 80
billion neurons
surrounding it, by any
of the infinite number
of combinations of
hormone levels percolated
that morning, by any
of the countless types
of childhoods and
fetal environments were
experienced, by any of
the 2 to the 4
millionth power different
genomes that neuron
contains, multiplied by
the nearly as large
range of epigenetic
orchestrations possible,
etc.
All out of your
control.
Turtles all the way
down is a joke because
the confident claim
presented to William
James is not just
absurd, but immune to
every challenge he
raises.
It's a highbrow version
of the insult battles
that would go on in
schoolyards in my youth.
You're a sucky baseball
player.
I know you are, but
what am I?
Now you're being
annoying.
I know you are, but
what am I?
Now you're indulging in
lazy sophistry.
I know you are.
If the old woman going
at James were, at some
point, to report that
the next turtle down
floats in the air, the
anecdote wouldn't be
funny.
While the answer is
still absurd, the
rhythm of the infinite
regress has been
broken.
Why did that moment
just occur?
Because of what came
before it.
Then, why did that
moment just occur?
Because of what came
before that.
Forever.
Isn't absurd, and is
instead, how the
universe works.
The absurdity amid
this seamlessness is
to think that we have
free will, and that it
exists because at some
point, the state of the
world, or of the
frontal cortex, or
neuron, or molecule of
serotonin, that came
before that, happened
out of thin air.
In order to prove
there is free will, you
have to show that some
behavior just happened
out of thin air, in the
sense of considering all
these biological
precursors.
It may be possible to
sidestep that with some
subtle philosophical
arguments, but you can't
with anything known to
science.
As noted in the first
chapter, the prominent
compatibilist philosopher,
Alfred Mealy, judged this
requirement of free will
as setting the bar
absurdly high.
Some subtle semantics
come into play.
What Levy calls
constitutive luck, is
luck that is remote
to Mealy.
Remote as in so
detached in time, a
whole million years
before you decide, a
whole minute before you
decide, that it doesn't
preclude free will and
responsibility.
This is supposedly
because the remoteness
is so remote as to not
be remotely relevant, or
because the consequences of
that remote biological and
environmental luck, are
still filtered through some
sort of immaterial you, at
the end picking and
choosing among the
influences, or because
remote bad luck, a la
Dennett, will be balanced
out by good luck in the
long run, and can thus be
ignored.
This is how some
compatibilists arrive at the
conclusion that someone's
history is irrelevant.
Levy's wording of
constitutive luck suggests
something very different,
namely, that not only is
history relevant, but, in
his words, the problem of
history is a problem of
luck.
It is why it is anything
but an absurdly high bar or
straw man to say that free
will can exist only if
neurons' actions are
completely uninfluenced by
all the uncontrollable
factors that came before.
It's the only requirement
there can be, because all
that came before, with its
varying flavors of
uncontrollable luck, is
what came to constitute
you.
This is how you became
you.
4.
Willing Willpower
The Myth of Grit
The last two chapters were
devoted to how you can
believe in free will by
ignoring history.
history.
And you can't.
To repeat our emerging
mantra, all we are is the
history of our biology, over
which we had no control, and
of its interaction with
environments, over which we
also had no control, creating
who we are in the moment.
However, not all free will
fans deny the importance of
history, and this chapter
dissects two ways in which it
is invoked.
The first, which we'll blow
over relatively quickly, is a
silly effort by some serious
scholars to incorporate history
into the picture as part of a
larger strategy of saying,
yes, of course free will
exists, just not where you're
looking.
It happened in the past.
It'll happen in your future.
It happens wherever you're not
looking in the brain.
It happens outside you, floating
on interactions between people.
We'll look at the second
misuse of history more deeply.
Those last two chapters were
about the damage caused if you
decide that punishment and
reward are morally justifiable
because history doesn't matter
when explaining someone's
behavior.
This chapter is about how it's
just as destructive to conclude
that history is relevant only to
some aspects of behavior.
Wasness
Suppose you have some guy in a
tough situation, being threatened
by a stranger who's coming at him
with a knife.
Our guy pulls out a gun and
shoots once, leaving the assailant
on the ground.
What does our guy then do?
Does he conclude, it's over, he's
incapacitated, I'm safe?
Or does he keep shooting?
What if he waits 11 seconds before
attacking the assailant further?
In the final scenario, he is charged
with premeditated murder.
If he had stopped after the first
shot, it would have counted as
self-defense.
But he had 11 seconds to think
about his options, meaning that his
second round of shots was freely
chosen and premeditated.
Let's consider the guy's history.
He was born with fetal alcohol
syndrome due to his mother's
drinking.
She abandoned him when he was
five, resulting in a string of
foster homes featuring physical
and sexual abuse.
A drinking problem by 13,
homeless at 15, multiple head
injuries from fights, surviving by
panhandling and being a sex
worker, robbed numerous times,
stabbed a month earlier by a
stranger.
An outreach psychiatric social
worker saw him once and noted that
he might well have PTSD.
D, you think?
Someone has tried to kill you, and
you have 11 seconds to make a
life-or-death decision.
There's a well-understood
neurobiology as to why you readily
make a terrible decision during
this monumental stressor.
Now, instead, it's our guy with a
neurodevelopmental disorder due to
fetal neurotoxicity, repeated
childhood trauma, substance abuse,
repeated brain injuries, and a
recent stabbing in a similar
situation.
His history has resulted in this
part of his brain being enlarged,
this other part atrophied, this
pathway disconnected.
And as a result, there's like zero
chance that he'll make a prudent,
self-regulated decision in those 11
seconds.
And you'd have done the same thing if
life had handed you that brain.
In this context, 11 seconds to
premeditate is a joke.
Despite that, the compatibilist
philosophers, and most prosecutors,
and judges, and juries, don't think
it's a joke.
Sure, life has thrown awful things at
the guy, but he's had plenty of time
in the past to have chosen to not be
the sort of person who would go back
and put another bullet in the
assailant's brain.
A great summary of this viewpoint is
given by philosopher Neil Levy, one
that he does not agree with.
Agents are not responsible as soon as
they acquire a set of active
dispositions and values.
Instead, they become responsible by
taking responsibility for their
dispositions and values.
Manipulated agents are not immediately
responsible for their actions,
because it is only after they have
had sufficient time to
reflect upon and experience the
effects of their new dispositions
that they qualify as fully
responsible agents.
The passing of time under normal
conditions offers opportunities for
deliberation and reflection, thereby
enabling agents to become responsible
for who they are.
Agents become responsible for their
dispositions and values in the course
of normal life, even when these
dispositions and values are the
product of awful constitutive luck.
At some point, bad constitutive luck
ceases to excuse, because agents
have had time to take responsibility
for it.
Sure, maybe no free will just now, but
there was relevant free will in the
past.
As implied in Levy's quote,
the process of freely choosing what
sort of person you become, despite
whatever bad constitutive luck you've
had, is usually framed as a gradual,
usually maturational process.
In a debate with Dennett,
incompatibilist Greg Caruso outlined
chapter 3's essence.
We have no control over either the
biology or the environment thrown at us.
Dennett's response was,
so what?
The point I think you are missing is
that autonomy is something one grows
into, and this is indeed a process that
is initially entirely beyond one's
control.
But as one matures and learns, one
begins to be able to control more and
more of one's activities, choices,
thoughts, attitudes, etc.
This is a logical outcome of Dennett's
claim that bad and good luck average out
over time.
Come on, get your act together.
You've had enough time to take
responsibility, to choose to catch up
to everyone else in the marathon.
A similar view comes from the
distinguished philosopher Robert Kane
of the University of Texas.
Free will, in my view, involves more
than merely free of action.
It concerns self-formation.
The relevant question for free will
is this.
How did you get to be the kind of
person you now are?
Roskies and Shadlin write,
It is plausible to think that agents
might be held morally responsible,
even for decisions that are not
conscious, if those decisions are due
to policy settings which are
expressions of the agent.
In other words, acts of free will
in the past.
Not all versions of this idea require
gradual acquisition of past tense
free will.
King believes that choose what sort
of person you're going to be
happens at moments of crisis,
at major forks in the road,
at moments of what he calls
self-forming actions.
And he proposes a mechanism by which
this supposedly occurs,
which we'll touch on briefly in
chapter 10.
In contrast, psychiatrist Sean Spence
of the University of Sheffield
believes that those
I had free will back then moments
happen when life is at its optimal
rather than in crisis.
Whether that free will
wasness was a slow maturational
process or occurred in a flash of
crisis or propitiousness,
the problem should be obvious.
Was, was once, now.
If the function of a neuron right now
is embedded in its neuronal
neighborhood, effects of hormones,
brain development, genes, and so on,
you can't go away for a week
and then show that the function a week
prior wasn't embedded after all.
A variant on this idea is that you
may not have free will now about
now, you have free will now about
who you are going to be in the future.
Philosopher Peter Tse, who calls this
second order free will,
writes how the brain can cultivate and
create new types of options for itself
in the future.
Not just any brains, however.
Tigers, he notes, can't have this
sort of free will.
For example, choosing that they're
going to become vegans.
Humans, in contrast, bear a degree of
responsibility for having chosen to
become the kind of chooser who they
now are.
Combine this with Dennett's
retrospective view, and we have
something akin to the idea that
somewhere in the future, you will have
had free will in the past.
I will freely choose.
Rather than there being free will,
just not when you're looking,
there's free will, just not where
you're looking.
You may have shown that free will
isn't coming from the area of the
brain you're studying.
It's coming from the area you aren't.
Roskies writes,
It is possible that an indeterministic
event elsewhere in the larger system
affects the firing of neurons in
brain region X, thus making the system
as a whole indeterministic, even
though the relation between neuronal
activity in brain region X and
behavior is deterministic.
And neuroscientist Michael Gazzaniga
moves the free will outside the brain
entirely.
Responsibility exists at a different
level of organization, the social level,
not in our determined brains.
There are two big problems with this.
First, it isn't free will and
responsibility just because, on the
social level, everyone says it is.
That's a central point of this book.
Second, sociality, social interactions,
organisms being social with each other,
are as much an end product of biology
interacting with environment as is the
shape of your nose.
Throw down the gauntlet from chapter 3.
Present me with the neuron, right here,
right now, that caused that behavior,
independent of any other current or
historical biological influence.
The answer can't be, well, we can't,
but that happened before.
Or, that's going to occur, but not yet.
Or, that's occurring right now, but not here.
Instead, over there.
There are no cracks in the process by which
was generates is in which to squeeze free will.
We move now to probably the most important
topic in this half of the book, a way to
erroneously see free will that isn't there.
What you were given and what you do with it
Cato and Finn, names changed to protect
their identities, have a good thing going,
backing each other in a fight and serving
as each other's wingman in the sex department.
Each has a fairly dominant personality,
and working together, they're unstoppable.
I'm watching them racing across a field.
Cato got the head start, but Finn is catching up.
They're trying to run down a gazelle,
which is tearing away from them.
Cato and Finn are baboons, intent on a meal.
If they do catch the gazelle, which seems
increasingly likely, Cato will eat first,
as he is number two in the hierarchy,
Finn, number three.
Finn is still catching up.
I note a subtle shift in his running,
something I can't describe,
but having observed Finn for a long time,
I know what's coming next.
Idiot, you're going to blow it, I think.
Finn has seemingly decided,
screw it with this waiting for the leftovers.
I want first dibs on the best parts.
He accelerates.
What fools these baboons be, I think.
Finn leaps on Cato's back,
biting him, knocking him over,
so that Finn can get the gazelle himself.
Naturally, he trips over Cato in the process,
and sprawls ass over a tea kettle.
They get up, glowering at each other,
the gazelle long gone,
end of their cooperative coalition.
With Cato no longer willing to back him up in a fight,
Finn is soon toppled by Bodhi,
number four in the hierarchy,
followed by being trounced by number five,
Chad.
Some baboons are just that way.
They're full of potential.
Big, muscular, with sharp canines,
but go nowhere in the hierarchy
because they never miss an opportunity
to miss an opportunity.
They break up their coalition with an impulsive act,
like Finn did.
They can't keep themselves from challenging
the alpha male for a female
and get pummeled.
They're in a bad mood
and can't stop themselves
from displacing aggression
by biting the wrong nearby female,
then get chased out of the troop
by her irate, high-ranking relatives.
Major underachievers
that can resist anything
except temptation.
We are replete with human examples,
always featuring the word squander.
Athletes who squander their natural talents
by partying.
Smart kids squandering their academic potential
with drugs or indolence.
Dissipated jet-setters
who squander their families' fortunes
on crackpot vanity projects.
According to one study,
70% of family fortunes
are lost by the second generation of inheritors.
From Finn on, squanderers all.
And then there are the people
who overcame bad luck
with spectacular tenacity and grit.
Oprah, growing up wearing potato sack dresses.
Harlan Sanders,
eventually the colonel,
who failed to sell his fried chicken recipe
to 1,009 restaurants
before striking gold.
Marathoner Eliud Kipchoge,
who collapsed a few meters
from the finish line
and crawled to the end.
Fellow Kenyan,
Yvonne Ngetik,
who crawled the final 50 meters
of her marathon.
Japanese runner,
Rei Ida,
who fell,
fracturing her leg,
and crawled the final 200 meters
to the finish line.
Nobel laureate geneticist,
Mario Capecchi,
who was a homeless street kid
in World War II Italy.
Then, of course,
there's Helen Keller
and Anne Sullivan
with the W-A-T-E-R.
Desmond Doss,
an unarmed conscientious objector medic
who returned under enemy fire
to carry 75 injured servicemen
to safety
in the Battle of Okinawa.
Five-foot-three Muggsy Bogues,
playing in the NBA.
Madeleine Albright,
future Secretary of State,
who was a teenage Czechoslovakian refugee,
sold bras
in a Denver department store.
The Argentinian guy
working as a janitor and bouncer
who put his nose to the grindstone
and became the Pope.
Whether considering Finn
and the squanderers
or Albright selling bras,
we are moths pulled to the flame
of the most entrenched free will myth.
We've already examined versions
of partial free will,
not now, but in the past.
Not here, but where you're not looking.
This is another version
of partial free will.
Yes, there are our attributes,
gifts, shortcomings,
and deficiencies
over which we had no control.
But it is us,
we,
agentic,
free,
captain of our own fate selves,
who choose
what we'd do
with those attributes.
Yes,
you had no control
over that ideal ratio
of slow-to-fast-twitch fibers
in your leg muscles
that made you
a natural marathoner.
But it's you
who fought through the pain
at the finish line.
Yes,
you didn't choose
the versions of glutamate
receptor genes
you inherited
that give you
a great memory,
but you're responsible
for being lazy
and arrogant.
Yes,
you may have inherited genes
that predispose you
to alcoholism,
but it's you
who commendably
resists the temptation
to drink.
A stunningly clear statement
of this compatibilist dualism
concerns Jerry Sandusky,
the Penn State football coach
who was sentenced
to 60 years in prison
in 2012
for being a horrific
serial child molester.
Soon after this,
a provocative CNN piece
ran under the title
Do Pedophiles Deserve Sympathy?
Psychologist James Cantor
of the University of Toronto
reviewed the neurobiology
of pedophilia.
The wrong mix of genes,
endocrine abnormalities
in fetal life,
and childhood head injury
all increased the likelihood.
Does this raise the possibility
that a neurobiological die
is cast,
that some people
are destined
to be this way?
Precisely.
Cantor concludes correctly,
one cannot choose
to not be a pedophile.
But then he does
an Olympian leap
across the Grand Canyon-sized
false dichotomy
of compatibilism.
Does any of that biology
lessen the condemnation
and punishment
that Sandusky deserved?
No.
One cannot choose
to not be a pedophile,
but one can choose
to not be a child molester.
My emphasis.
The following table
formalizes this dichotomy.
On one side
are things that most people
accept as outside our control.
Biological stuff.
Sure,
sometimes we have trouble
remembering that.
We praise,
single out,
the chorus member
who is an anchor
of reliability
because of their perfect pitch,
which is a biologically
heritable trait.
We praise a basketball
player's dunk,
ignoring that being
seven foot two
has something to do with it.
We smile more
at someone attractive,
are more likely to vote
for them in an election,
less likely to convict
them of a crime.
Yeah, yeah,
we agree sheepishly
when this is pointed out.
They obviously didn't choose
the shape of their cheekbones.
We're usually pretty good
at remembering
that the biological stuff
is out of our control.
Biological stuff.
Do you have grit?
Having destructive
sexual urges?
Do you resist
acting upon them?
Being a natural marathoner?
Do you fight
through the pain?
Not being all that bright?
Do you triumph
by studying extra hard?
Having a proclivity
toward alcoholism?
Do you order
ginger ale instead?
Having a beautiful face?
Do you resist
concluding that
you're entitled
to people being nice
to you because of it?
And then paired
with those
is the free will
you supposedly exercise
in choosing
what you do
with your biological attributes.
The you who sits
in a bunker
in your brain
but not
of your brain.
Your you-ness
is made of
nanochips,
old vacuum tubes,
ancient parchments
with transcripts
of Sunday morning sermons,
stalactites
of your mother's
admonishing voice,
streaks of brimstone,
rivets made out
of gumption.
Whatever that real
you is composed of,
it sure ain't
squishy
biological brain
yuck.
When viewed
as evidence
or free will,
the questioning
side of the chart
is a compatibilist
playground
of blame
and praise.
It seems so hard,
so counterintuitive,
to think that willpower
is made of neurons,
neurotransmitters,
receptors,
and so on.
There seems a much
easier answer.
Willpower
is what happens
when that non-biological
essence of you
is bespangled
with fairy dust.
And as one of the
most important points
of this book,
we have as little
control over the
statements
as we do the
responses.
Both sides
are equally the outcome
of uncontrollable
biology
interacting with
uncontrollable
environment.
To understand
the biology,
time to focus
on the fanciest
part of the brain,
the frontal cortex,
which was lightly
touched on
in the last
two chapters.
Doing the right
thing
when it's the
harder thing
to do.
Bragging
for the frontal
cortex,
it's the newest
part of the brain.
We primates
have,
proportionately,
more of it
than other mammals.
When you examine
gene variants
that are unique
to primates,
a disproportionate
percentage of them
are expressed
in the frontal
cortex.
Our human
frontal cortex
is proportionately
bigger and or
more complexly
wired than that
of any other
primate.
As noted in
the last chapter,
it's the last
part of the brain
to fully mature,
not being fully
constructed,
until your mid-twenties.
This is outrageously
delayed,
given that most
of the brain
is up and running
within a few
years of birth.
And as a major
implication of this
delay,
a quarter century
of environmental
influences shape
how the frontal
cortex is being
put together.
It's one of the
hardest-working
parts of the brain
in terms of
energy consumption.
It has a type
of neuron found
nowhere else
in the brain.
And the most
interesting part
of the frontal
cortex,
the prefrontal
cortex,
PFC,
is proportionately
even larger
than the rest
of the frontal
cortex,
and more
recently evolved.
As a reminder,
the PFC is
central to
executive function,
decision-making.
We saw this
in chapter 2,
where,
way up in the
chain of
Libetian commands,
there was the
PFC making
decisions up to
10 seconds
before subjects
first became
aware of that
intent.
What the PFC
is most about
is making
tough decisions
in the face
of temptation,
gratification
postponement,
long-term
planning,
impulse control,
emotional
regulation.
The PFC
is essential
for getting
you to do
the right
thing when
it is the
harder thing
to do,
which is so
pertinent to
that false
dichotomy
between what
attributes fate
hands you
and what you
do with them.
The Cognitive
PFC
As a warm-up,
let's examine
doing the
right thing
in the
cognitive realm.
It's the
PFC that
inhibits you
from doing
something
wrong.
