powerbroker, his sweeping biography of the urban planner Robert Moses. Assigned in many college
courses, the book has sold more than 400,000 copies and has never been out of print since
its publication in 1974. For the past four decades, Caro has been writing about the mid-century
political maestro Lyndon B. Johnson, four volumes and counting, including Master of the Senate,
another Pulitzer winner, and Means of Assent. In total, he has written more than 4,000 well-honed,
fact-rich pages of prose over the course of an acclaimed career. But at first, Caro struggled
even to wrap his head around his subjects. While researching and reporting the powerbroker,
he was overwhelmed by the volume of information he had collected.
It was so big, so immense, he has said. I couldn't figure out what to do with the material.
Caro's books are too colossal to be held entirely in mind, even their author's mind. Nor is the space
of a typewritten page, Caro does not use a computer, nearly big enough to contain the full sweep of his
storytelling. In order to complete these massive projects, Caro has to extend his thinking into
physical space. One entire wall of his office on Manhattan's Upper West Side is taken up by a cork
board four feet high and ten feet wide. The board is covered with a detailed outline of Caro's current
work in progress, plotting its trajectory from beginning to end. So thorough is Caro that he must
know the last sentence of a book before he starts composing its first lines. As he writes, the stretch
of wall becomes another dimension in which to think. I can't start writing a book until I've thought it through
and can see it whole in my mind, he told a visitor to his office. So before I start writing, I boil the book
down to three paragraphs, or two, or one. That's when it comes into view. That process might take
weeks. And then I turn those paragraphs into an outline of the whole book. That's what you see up
here on my wall now. In another interview, Caro explained how the outline wall helps him stay in
the zone. I don't want to stop while I'm writing, so I have to know where everything is, he explained.
It's hard for me to keep in the mood of the chapter I'm writing if I have to keep searching
for files. Out of necessity, Caro found his way to a mode of thinking and working that would not have
been possible had he tried to keep his voluminous material entirely in his head. When thought
overwhelms the mind, the mind uses the world, psychologist Barbara Tversky has observed. Once we
recognize this possibility, we can deliberately shape the material worlds in which we
learn and work to facilitate mental extension, to enhance the cognitive congeniality of a space,
in the words of David Kirsch, a professor at the University of California, San Diego.
To understand how this works, let's take a closer look at what Caro's wall is doing for his mind.
On the most basic level, the author is using physical space to offload facts and ideas.
He need not keep mentally aloft these pieces of information or the complex structure in which
they are embedded. His posted outline holds them at the ready, granting him more mental resources to
think about that same material. Keeping a thought in mind while also doing things to and with that
thought is a cognitively taxing activity. We put part of this mental burden down when we delegate the
representation of the information to physical space, something like jotting down a phone number
instead of having to continually refresh its mental representation by repeating it under our breath.
Caro's wall turns the mental map of his book into a stable external artifact.
This is the second way in which the corkboard in Caro's office extends his ability to think.
Looking it over, he can now see, far more clearly and concretely than if the map had remained inside
his head, how his ideas relate to one another, how the many paths taken by his narrative twist and turn,
diverge and converge. Although Caro tailored his longtime method to suit his particular style of working,
the strategy he came up with is similar to one that has received substantial empirical support from
psychology, an approach known as concept mapping. A concept map is a visual representation of facts
and ideas and of the relationships among them. It can take the form of a detailed outline, as in
Robert Caro's case, but it is often more graphic and schematic in form. Research has revealed that the act of
creating a concept map on its own generates a number of cognitive benefits. It forces us to reflect on
what we know and to organize it into a coherent structure. As we construct the concept map,
the process may reveal gaps in our understanding of which we were previously unaware. And having gone
through the process of concept mapping, we remember the material better because we have thought deeply
about its meaning. Once the concept map is completed, the knowledge that usually resides inside the head
is made visible. By inspecting the map, we're better able to see the big picture and to resist
becoming distracted by individual details. We can also more readily perceive how the different parts
of a complex whole are related to one another. Joseph Novak, now a professor emeritus of biology and
science education at Cornell University, was investigating the way children learned science when he pioneered
the concept mapping method in the 1970s. Although the technique originated in education, Novak notes that
it is increasingly being applied in the world of work, where, he says, the knowledge structure necessary
to understand and resolve problems is often an order of magnitude more complex than that which is
required in academic settings. Concept maps can vary enormously in size and complexity,
from a simple diagram to an elaborate plan featuring hundreds of interacting elements.
Robert Caro's map, for example, is big, big enough to stand in front of, to walk along,
to lean into and stand back from. The sheer expansiveness of his outline allows Caro to bring to bear on his
project not only his purely cognitive faculties of reasoning and analysis, but also his more
visceral powers of navigation and wayfinding. Researchers are now producing evidence that these ancient,
evolved capacities can help us think more intelligently about abstract concepts, an insight that showed up
first in, of all places, in, of all places, a futuristic action film.
The scene from the 2002 movie Minority Report is famous because, well, it's just so cool.
Chief of pre-crime John Anderton, played by Tom Cruise, stands in front of a bank of gigantic computer
screens. He is reviewing evidence of a crime yet to be committed, but this is no staid intellectual
exercise. The way he interacts with the information splayed before him is active, almost tactile.
He reaches out with his hands to grab and move images as if they were physical objects. He turns
his head to catch a scene unfolding in his peripheral vision. He takes a step forward to inspect a picture
more closely. Cruise, as Anderton, physically navigates through the investigative file as he would
through a three-dimensional landscape. The movie, based on a short story by Philip K. Dick and set in the year
2054, featured technology that was not yet available in the real world, yet John Anderton's use of the
interface comes off as completely plausible, even, to him,
unexceptional. David Kirby, a professor of science, technology, and society at California
Polytechnic State University, maintains that this is the key to moviegoers' suspension of disbelief.
The most successful cinematic technologies are taken for granted by the characters in a film,
he writes, and thus communicate to the audience that these are not extraordinary,
but rather everyday technologies. The director of Minority Report, Steven Spielberg,
had an important factor working in his favor when he staged this scene. The technology employed by his
lead character relied on a human capacity that could hardly be more every day or taken for granted,
the ability to move ourselves through space. For added versimilitude, Spielberg invited computer
scientists from the Massachusetts Institute of Technology to collaborate on the film's production,
encouraging them to take on that design work as if it were an R&D effort, says John Undercoffler,
one of the researchers from MIT. And, in a sense, it was. Following the release of the movie,
Undercoffler says, he was approached by countless investors and CEOs who wanted to know,
is that real? Can we pay you to build it if it's not real? Since then, scientists have succeeded at
building something quite similar to the technology that Tom Cruise engaged to such dazzling effect.
John Undercoffler is now himself the CEO of Oblong Industries, developer of a Minority Report-like
user interface he calls a spatial operating environment. What's more, researchers have begun
to study the cognitive effects of this technology, and they find that it makes real a promise of
science fiction. It helps people to think more intelligently. The particular tool that has
become the subject of empirical investigation is the large high-resolution display, an oversized
computer screen to which users can bring some of the same navigational capacities they would apply
to a real-world landscape. Picture a bank of computer screens three and a half feet wide and nine feet
long, presenting to the eye some 31.5 million pixels. The average computer monitor has fewer
than 800,000 pixels. Robert Ball, an associate professor of computer science at Weber State
University in Utah, has run numerous studies comparing people's performance when interacting
with a display like this to their performance when consulting a conventionally proportioned screen.
The improvements generated by the use of the supersize display are striking. Ball and his collaborators
have reported that large, high-resolution displays increase by more than tenfold the average speed
at which basic visualization tasks are completed. On more challenging tasks, such as pattern finding,
study participants improved their performance by 200 to 300 percent when using large displays.
Working with the smaller screen, users resorted to less efficient and more simplistic strategies,
producing fewer and more limited solutions to the problems posed by experimenters.
When using a large display, they engaged in higher-order thinking, arrived at a greater number of
discoveries, and achieved broader, more integrative insights. Such gains are not a matter of individual
differences or preferences, Ball emphasizes. Everyone who engages with a larger display finds that their
thinking is enhanced. Why would this be? Large, high-resolution displays allow users to deploy their
physical embodied resources, says Ball, adding, with small displays, much of the body's built-in
and functionality is wasted. These corporeal resources are many and rich. They include peripheral vision, or the ability
to see objects and movements outside the area of the eye's direct focus. Research by Ball and others shows that the
capacity to access information through our peripheral vision enables us to gather more knowledge and insight at one time,
providing us with a richer sense of context. The power to see out of the corners of our eyes also allows us to be more
efficient at finding the information we need, and helps us to keep more of that information in mind as we think about the
challenge before us. Smaller displays, meanwhile, encourage a narrower visual focus, and consequently, more limited
thinking. As Ball puts it, the availability of more screen pixels permits us to use more of our own
brain pixels to understand and solve problems. Our built-in embodied resources also include our spatial memory,
our robust capacity exploited by the method of loci to remember where things are. This ability is often wasted,
as Ball would have it, by conventional computer technology. On small displays, information is
contained within windows that are of necessity, stacked on top of one another or moved around on the screen,
interfering with our ability to relate to that information in terms of where it is located. By contrast,
large displays, or multiple displays, offer enough space to lay out all the data in an arrangement that
persists over time, allowing us to leverage our spatial memory as we navigate through that information.
Researchers from the University of Virginia and from Carnegie Mellon University reported that study
participants were able to recall 56% more information when it was presented to them on multiple monitors
rather than on a single screen. The multiple monitor setup induced the participants to orient their own
bodies toward the information they sought, rotating their torsos, turning their heads, thereby generating
memory-enhancing mental tags as to the information's spatial location. Significantly, the researchers noted,
these cues were generated without active effort. Automatically noting place information is simply something we
humans do, enriching our memories without depleting precious mental resources.
Other embodied resources engaged by large displays include proprioception, or our sense of how and
where the body is moving at a given moment, and our experience of optical flow or the continuous stream of
information our eyes receive as we move about in real-life environments. Both these busy sources of input
fall silent when we sit motionless before our small screens, depriving us of rich dimensions of data that
could otherwise be bolstering our recall and deepening our insight. Indeed, the use of a compact display
actively drains our mental capacity. The screen's small size means that the map we construct of our
conceptual terrain has to be held inside our head rather than fully laid out on the screen itself. We must devote some
portion of our limited cognitive bandwidth to maintaining that map in mind. What's more, the mental version
of our map may not stay true to the data, becoming inaccurate or distorted over time. Finally, a small screen
requires us to engage in virtual navigation through information, scrolling, zooming, clicking, rather than the
more intuitive physical navigation our bodies carry out so effortlessly. Robert Ball reports that as display
size increases, virtual navigation activity decreases, and so does the time required to carry out a task.
Large displays, he has found, require as much as 90 percent less window management than small monitors.
Of course, few of us are about to install a 30-square-foot screen in our home or office, although large
interactive displays are becoming an ever more common sight in industry, academia, and the corporate world.
But Ball notes that much less dramatic changes to the places where we work and learn can allow us to
garner the benefits of physically navigating the space of ideas. The key, he says, is to turn away from
choosing technology that is itself ever faster and more powerful toward tools that make better use of our
own human capacities, capacities that conventional technology often fails to leverage. Rather than
investing in a lightning-quick processor, he suggests, we should spend our money on a larger monitor or on
multiple monitors to be set up next to one another and used at the same time. The computer user who makes
this choice, he writes, will most likely be more productive because she invested in the human component
of her computer system. She has more information displayed at one time on her monitor, which in turn
enables her to take advantage of the human side of the equation.
The technology that allows us to explore the space of ideas need not be digital. Sometimes the most
generative tools are the simplest—a pencil, a notebook, an observing gaze. For a young Charles Darwin,
such modest equipment provided the key to developing a theory that would change the world. In 1831, Darwin was 22
years old, recently graduated from Christ's College, Cambridge, uncertain whether to pursue a conventional
career as a doctor or parson or to follow his burgeoning interest in natural history. In August of that
year, he received a letter from his former tutor at Cambridge asking if he would be interested in
serving as a naturalist on a two-year expedition aboard the HMS Beagle. Darwin accepted, and in December he
began his seaborne apprenticeship to Captain Robert Fitzroy. The young man carefully observed and emulated the
actions of the experienced captain. Darwin had never kept a journal before coming aboard the Beagle, for example,
but he began to do so under the influence of Fitzroy, whose naval training had taught him to keep a precise record of
every happening aboard the ship and every detail of its ocean-going environment. Each day, Darwin and
Fitzroy ate lunch together. Following the meal, Fitzroy settled down to writing, bringing both the formal
ship's log and his personal journal up to date. Darwin followed suit, keeping current his own set of papers,
his field notebooks, in which he recorded his immediate observations, often in the form of drawings and sketches,
his scientific journal, which combined observations from his field notebooks with more integrative and
theoretical musings, and his personal diary. Even when Darwin disembarked from the ship for a time,
traveling by land through South America, he endeavored to maintain the nautical custom of noting down
every incident, every striking sight he encountered. The historian of science and Harvard University
professor Janet Brown has remarked upon the significance of this activity of Darwin's.
In keeping such copious records, he learned to write easily about nature and about himself.
Like Fitzroy, he taught himself to look closely at his surroundings, to make notes and measurements,
and to run through a mental checklist of features that ought to be recorded, never relying entirely on memory,
and always writing reports soon after the event. She adds, although this was an ordinary practice in
naval affairs, it was for Darwin a basic lesson in arranging his thoughts clearly and an excellent
preparation for composing logical scientific arguments that stood him in good stead for many years afterwards.
But Darwin's careful note-keeping did not simply help him learn to
arrange his thoughts clearly and compose logical scientific arguments. The projection of the
internal workings of his mind onto the physical space of his journal created a conceptual map he was
able to follow all the way to his theory of evolution. Some 25 years before the publication of his epical
book The Origin of Species, the entries in the journal Darwin kept throughout his expedition moved his
thinking forward, step by tentative step. On October 10, 1833, for example, Darwin discovered a fossilized
horse tooth on the banks of the Rio Parana in northeastern Argentina. Alongside the tooth,
he found the fossilized bones of a megatherium, a giant ground sloth. Writing in his journal,
Darwin puzzled over the fact that although these fossilized remains were apparently of the same age,
horses still populated the earth in great numbers, while the megatherium was long extinct.
Eighteen months later, on April 1, 1835, Darwin came upon a fossil forest high in the Andes,
what he described in a letter to his tutor in Cambridge as,
a small wood of petrified trees. Again, he pondered the implications of this discovery in his journal,
noting that one possible explanation for the existence of the forest was a long-ago subsidence,
or a sinking of the land under the sea, where the trees would have become calcified by marine sediment.
Darwin knew that such dramatic topographical shifts, down and then up again to the mountainous
elevation at which he encountered the fossil forest, were not endorsed by the thinking of the day,
which assumed geological stability since the time of earth's creation.
I must confess, however, that I myself cannot quite banish the idea of subsidence,
enormous as the extent of movement required assuredly is, he confided in his journal.
Such precise and yet open-minded observations helped keep Darwin moving on a steady path toward
a conclusion that now appears inevitable, but which hardly seems so at the time. In 1849,
at 40 years of age, his voyage on the HMS Beagle behind him, but the publication of The Origin of
Species still to come, Darwin advised those who would follow in his footsteps to
acquire the habit of writing very copious notes, not all for publication, but as a guide for himself.
The naturalist must take precautions to attain accuracy, he continued, for the imagination is
apt to run riot when dealing with masses of vast dimensions and with time during almost infinity.
When thought overwhelms the mind, the mind uses the world, and researchers have reported some
intriguing findings about why this use of the physical-spatial world is so beneficial for our
thinking. As with the creation of concept maps, the process of taking notes in the field,
whether that field is a sales floor, a conference room, or a high school chemistry lab, itself confers
a cognitive bonus. When we simply watch or listen, we take it all in, imposing few distinctions on the
stimuli streaming past our eyes and ears. As soon as we begin making notes, however, we are forced to
discriminate, judge, and select. This more engaged mental activity leads us to process what we're
observing more deeply. It can also lead us to have new thoughts. Our jottings build for us a series of
ascending steps from which we can survey new vistas. Eric Green, a professor of ecology and evolutionary
biology at the University of Montana, has relied on his field notebooks throughout his long career.
His stacks of spiral-bound notebooks contain descriptions of macaws and parrots flying at
dusk to roost in palm swamps in Peru, of the Wahoo alarm calls of olive baboons in the Okavango
delta of Botswana, warning one another of approaching lions, of teenage male sperm whales flipping up their
tails as they begin their hour-long dives to catch giant squid in a deep-water trench off New Zealand.
But his notes are not merely a record of what he has observed and experienced. They are, he says,
the main source of ideas that take my research in new directions.
Seeking to give his students a sense of this process, Green assigned a field notebook exercise
in the upper-level ecology class he teaches to UM undergraduates. He asked them to pick one thing
and observe it carefully over the entire semester. The object of study could be a single tree,
a bird feeder, a beaver dam, the student's own garden. This was not, he stressed to his class,
a rote exercise in recording, but a highly generative activity, the starting point of scientific discovery.
One of the main things I wanted to get across is that one of the hardest parts of science is coming
up with new questions, says Green. Where do fresh new ideas come from? Careful observations of nature
are a great place to start. In addition to making observations of their chosen site over time,
students were required to come up with at least 10 research questions inspired by what they saw.
As Green's students discovered, the very act of noticing and selecting points of interest to put
down on paper initiates a more profound level of mental processing. Things really get interesting,
however, when we pause and look back at what we've written. Representations in the mind and
representations on the page may seem roughly equivalent, when in fact they differ significantly in
terms of what psychologists call their affordances, that is, what we're able to do with them.
External representations, for example, are more definite than internal ones.
Picture a tiger, suggests philosopher Daniel Dennett in a classic thought experiment. Imagine in detail its
eyes, its nose, its paws, its tail. Following a few moments of conjuring, we may feel we've summoned up a
fairly complete image. Now, says Dennett, answer this question. How many stripes does the tiger have?
Suddenly the mental picture that had seemed so solid becomes maddeningly slippery. If we had drawn the
tiger on paper, of course, counting its stripes would be a straightforward task.
Here, then, is one of the unique affordances of an external representation. We can apply one or more of
our physical senses to it. As the tiger example shows, seeing an image in our mind's eye is not the same
as seeing it on the page. Daniel Reisberg, a professor emeritus of psychology at Reed College in Oregon,
calls this shift in perspective the detachment gain, the cognitive benefit we receive from putting a bit of
distance between ourselves and the content of our minds. When we do so, we can see more clearly what
that content is made of, how many stripes are on the tiger, so to speak. This measure of space also
allows us to activate our powers of recognition. We leverage these powers whenever we write down two or
more ways to spell a word, seeking the one that looks right. The curious thing about this common practice
is that we do tend to know immediately which spelling appears correct, indicating that this is
knowledge we already possess but can't access until it's externalized. A similar phenomenon has been
reported by researchers investigating science learning. In a study published in 2016, experimenters asked
8th grade students to illustrate with drawings the operation of a mechanical system, a bicycle pump,
and a chemical system, the bonding of atoms to form molecules. Generating visual explanations of how
these systems work led the students to achieve a deeper level of understanding. Without any additional
instruction, participants were able to use their own drawings as a check for completeness and coherence,
as well as a platform for inference, the researchers note. Turning a mental representation into shapes and
lines on a page supported the students' growing understanding, helping them to elucidate more fully what they
already knew about these scientific systems. At the same time, the explicitness, the definiteness,
of the drawings they made revealed with ruthless rigor what the students did not yet know or understand,
leading them to fill in the gaps thus exposed. So external representations are more definite than internal
ones. And yet, in another sense, they are also more usefully ambiguous. When a representation remains
inside our heads, there's no mystery about what it signifies. It's our thought, and so there can be
neither doubt nor ambiguity about what is intended, notes Daniel Riesberg. Once we've placed it on the page,
however, we can riff on it, play with it, take it in new directions. It can almost seem as if we
ourselves didn't make it. And indeed, researchers who have observed artists, architects, and designers
as they create report that they often discover elements in their own work that they did not put
there, at least not intentionally. Gabriela Goldschmidt, professor emeritus of architecture at the Technion Israel
Institute of Technology, explains how this works. One reads off the sketch more information than was
invested in its making. This becomes possible because when we put down on paper dots, lines,
and other marks, new combinations and relationships among these elements are created that we could not
have anticipated or planned for. We discover them in the sketch as it is being made. Architects,
artists, and designers often speak of a conversation carried on between eye and hand. Goldschmidt makes
the two-way nature of this conversation clear when she refers to the backtalk of self-generated sketches.
Research by Goldschmidt and others shows that those who are skilled at drawing excel in managing this
lively dialogue. Such studies find, for example, that expert architects are far more adept at identifying
promising possibilities within their existing sketches than our relative novices. In one in-depth analysis
of an experienced architect's methods, researchers determined that fully 80 percent of his new ideas
came from reinterpreting his old drawings. Expert architects are also less likely than beginners to get
stuck perseverating on a single unproductive concept. They are proficient at recombining disparate elements
found in their sketches into new and auspicious forms. From these observations of expert draftsmen,
some promising prescriptions can be drawn. When setting out to generate new ideas, we should begin with
only the most general plan or goal. Early on in the process, vagueness and ambiguity are more generative
than explicitness or definition. Think of the task not in linear terms, tracing a direct line from point A
to point B, but rather as a cycle. Think, draw, look, rethink, redraw. Likewise, don't envision the mind telling the pencil
what to do. Instead, allow a conversation to develop between eye and hand, the action of one informing the
other. Finally, we ought to postpone judgment as long as possible, such that this give and take between
perception and action can proceed without becoming inhibited by preconceived notions or by critical self-doubt.
Across the board in every field, experts are distinguished by their skillful use of externalization.
As cognitive scientist David Kirsch has written a video game, Virtuosos, better players use the world
better. Skilled artists, scientists, designers, and architects don't limit themselves to the two-dimensional
space of the page. They regularly reach for three-dimensional models, which offer additional
advantages. Users can manipulate the various elements of the model, view the model from multiple
perspectives and orient their own bodies to the model, bringing the full complement of their
embodied resources to bear on thinking about the task and the challenges it presents.
David Kirsch has made close observations of the way architects use physical mock-ups of the buildings
they are designing. When they interact with the models they have constructed, he maintains,
they are literally thinking with these objects. Interactions carried out in three dimensions, he says,
enable forms of thought that would be hard, if not impossible, to reach otherwise.
Kirsch calls this the cognitive extra that comes from moving concrete objects through physical space,
a mental dividend that made the difference for one scientist struggling with a seemingly insoluble problem.
A dreary day in February 1953 found James Watson in low spirits. He and his collaborator,
Francis Crick, both young scientists at the Cavendish Laboratory in Cambridge, England, had been working for
months to determine the structure of DNA, the molecule that contains the genetic code for living things.
That morning, a colleague had urged him not to waste any more time with my harebrained scheme,
as Watson later recounted in his autobiography. Hoping to demonstrate that his proposed arrangement of the
four chemical bases that make up DNA, adenine, guanine, cytosine, and thymine, was true to life,
he had asked the machinists at the Cavendish workshop to solder models of the bases out of tin. The models were
taking too long to be finished, however, and Watson felt as if he had run into a stone wall. Finally, in desperation,
he took on the job himself and spent the afternoon fashioning models out of stiff cardboard.
Watson continues the story.
When I got to our still empty office the following morning, I quickly cleared away the papers from my
desktop so that I would have a large flat surface on which to form pairs of bases held together by
hydrogen bonds. At first, he tried fitting his cardboard bases together in a fashion dictated by
his latest thinking about how the elements of DNA might be arranged. But, Watson related,
I saw all too well that they led nowhere. So, he began shifting the bases in and out of various
other pairing possibilities. Then realization dawned. Suddenly, I became aware that an adenine-thymine
pair held together by two hydrogen bonds was identical in shape to a guanine-cytosine pair held together by
at least two hydrogen bonds. Moving around the pieces of his cardboard model, Watson began to
envision the chemical bases embedded in a double helix structure. All the hydrogen bonds seemed to
form naturally, he noted. No fudging was required to make the two types of base pairs identical in shape.
His morale skyrocketed, he recalled, as the pieces quite literally came together before his eyes.
Just then, his partner, Crick, made his appearance, and Watson wasted no time in announcing the
breakthrough. Upon his arrival, Francis did not get more than halfway through the door before I
let loose that the answer to everything was in our hands. The final step of Watson and Crick's long
journey of discovery demonstrates the value of what psychologists call interactivity, the physical
manipulation of tactile objects as an aid to solving abstract problems. The fact that Watson had to
make his models himself is telling. Outside the architect's studio, or the kindergarten classroom,
interactivity is not widely employed. Our assumption that the brain operates like a computer has led us to
to believe that we need only to input the necessary information in order to generate the correct
solution. But human minds don't work that way, observes Frédéric Vallée-Torangeau, a professor of psychology at
Kingston University in the UK. The computer analogy implies that simulating a situation in your head while you
think is equivalent to living through that situation while you think, he writes. Our research strongly
challenges this assumption. We show instead that people's thoughts, choices, and insights can be
transformed by physical interaction with things. In other words, thinking with your brain alone, like a
computer does, is not equivalent to thinking with your brain, your eyes, and your hands.
A series of studies conducted by Vallée-Torangeau and his colleagues all follow a similar pattern.
Experimenters pose a problem. One group of problem solvers is permitted to interact physically with the
properties of the problem. A second group must think through the problem in their heads.
Interactivity inevitably benefits performance, he reports. This holds true for a wide variety of problem
types, from basic arithmetic, to complex reasoning, to planning for future events, to solving creative
insight problems. People who are permitted to manipulate concrete tokens representing elements of
the problem to be solved, bear less cognitive load and enjoy increased working memory. They learn more and are
better able to transfer their learning to new situations. They are less likely to engage in
symbol pushing or moving numbers and words around in the absence of understanding. They are more
motivated and engaged and experience less anxiety. They even arrive at correct answers more quickly.
As the title of one of Vallée-Torangeau's studies puts it,
moves in the world are faster than moves in the head. Given the demonstrated benefits of interactivity,
why do so many of us continue to solve problems with our heads alone? Blame our entrenched cultural bias in
favor of brain-bound thinking, which holds that the only activity that matters is purely mental in kind.
Manipulating real-world objects in order to solve an intellectual problem is regarded as childish or
uncouth. Real geniuses do it in their heads. This persistent oversight has occasionally been the cause of some
irritated impatience among those who do recognize the value of externalization and interactivity.
There's a classic story, for example, concerning the theoretical physicist Richard Feynman,
who was as well known for authoring popular books such as Surely You're Joking, Mr. Feynman as for
winning the Nobel Prize awarded to him and two colleagues in 1965. In a post-Nobel interview with
the historian Charles Weiner, Weiner referred in passing to a batch of Feynman's original notes and sketches,
observing that the materials represented a record of the day-to-day work done by the physicist.
Instead of simply assenting to Weiner's remark, Feynman reacted with unexpected sharpness.
I actually did the work on the paper, he said. Well, Weiner replied, the work was done in your head,
but the record of it is still here. Feynman wasn't having it. No, it's not a record, not really.
It's working. You have to work on paper, and this is the paper, okay? Feynman wasn't just being crotchety.
He was defending a view of the act of creation that would be codified four decades later in Andy
Clark's theory of the extended mind. Writing about this very episode, Clark argues that, indeed,
Feynman was actually thinking on the paper. The loop through pen and paper is part of the physical
machinery responsible for the shape of the flow of thoughts and ideas that we take nonetheless to be
distinctively those of Richard Feynman. We often ignore or dismiss these loops, preferring to focus
on what goes on in the brain, but this incomplete perspective leads us to misunderstand our own minds.
Writes Clark, it is because we are so prone to think that the mental action is all, or nearly all,
on the inside, that we have developed sciences and images of the mind that are, in a fundamental sense,
inadequate. We will begin to see ourselves aright, he suggests, only when we recognize the role of
material things in our thinking, when we correct the errors and omissions of the brain-bound perspective,
and put brain, body, and world together again.
Part 3. Thinking with Our Relationships
Chapter 7. Thinking with Experts
Germany has long been Europe's economic powerhouse. Among the nation's many strengths,
observers often single out its distinctive system of apprenticeships. Every year, about half a million
young Germans move directly from high school into well-designed apprentice programs operated inside
companies, where they learn technical skills such as welding, machining, and electrical engineering.
This deeply entrenched system has for decades enabled Germany's manufacturing sector to thrive.
But as in other Western countries, the dominance of industry in Germany is giving way to a more
information-centric economy, creating demand for skills like computer programming. This change has brought
new challenges, and students and instructors have struggled to adapt. At the University of Potsdam,
a school of some 20,000 students located outside Berlin, a key step for undergraduates who want a career
in tech is a course on theoretical computer science. Yet year after year, the rate at which students
failed the course was stunning, as high as 60 percent. The problem seemed related to the course's highly
abstract content. Sitting passively in lectures, students simply weren't grasping the meaning of
concepts like parsing algorithms, closure properties, and linear-bounded automata. Then a group of computer
science professors hit upon a solution, one that harked back to Germany's historical strength. Led by
Potsdam professor Christoph Kreitz, the faculty members reimagined the class as an apprenticeship,
albeit of a special sort. The course was reorganized around making the internal thought processes of
computer scientists visible to students, as visible as a carpenter fitting a joint or a tailor cutting a bolt of
cloth. This is what's known as a cognitive apprenticeship, a term coined by Alan Collins, now a professor emeritus
of education at Northwestern University. In a 1991 article written with John Seeley Brown and Ann Holum,
Collins noted a crucial difference between traditional apprenticeships and modern schooling. In the former,
learners can see the processes of work, while in the latter, the processes of thinking are often invisible
to both the students and the teacher. Collins and his co-authors identified four features of
apprenticeship that could be adapted to the demands of knowledge work, modeling or demonstrating the task
while explaining it aloud, scaffolding or structuring an opportunity for the learner to try the task
herself, fading or gradually withdrawing guidance as the learner becomes more proficient,
and coaching or helping the learner through difficulties along the way. Christoph Kreitz and his colleagues
incorporated these features of traditional apprenticeships into their course redesign,
reducing the amount of time students spent in lectures and increasing the length and frequency of small group
sessions led by tutors. In these sessions, students didn't listen to a description of computer science
concepts or engage in a discussion about the work performed by computer scientists. They actually
did the work themselves under the tutors' close supervision. The results of these changes were dramatic. The
proportion of students failing the course shrank from above 60 percent to less than 10 percent.
The kind of shift Kreitz and his colleagues made at Potsdam is one that many of us will contemplate in
coming years. All over the world, in every sector and specialty, education and work are less and less about
executing concrete tasks and more and more about engaging in internal thought processes. As Alan Collins observed,
these processes are largely inaccessible to both novice and expert. The novice doesn't yet know the material well enough,
while the expert knows it so well that it has become second nature. This reality means that if we are to
extend our thinking with others' expertise, we must find better ways of effecting an accurate transfer of
knowledge from one mind to another. Cognitive apprenticeships are one such method. We'll explore
several others in this chapter, starting with an approach that boasts both a long historical pedigree and an
increasingly robust foundation in scientific research. So what if it makes us a bit uneasy?
Inside the Hôpital Universitaire Pitié-Saint-Petrière in Paris, a young man stares vacantly into space.
A twitch contorts his mouth and a tremor runs through his body like an electric shock. Nearby,
another young man accepts help getting up from his chair. His right arm is bent at an awkward angle,
and his right leg drags stiffly behind him. Across the room, a young woman is asked if she can touch
her nose with her index finger. She tries, but her finger misses the mark and lands on her cheek.
On this day, the often uncanny symptoms of neurological disease are on vivid display.
But these individuals are not patients. They are medical students, doctors in training.
Coached by their professors, they are learning how to mimic the symptoms of the ailments they
will be called upon to treat. Instructors show the students how to arrange their facial features,
how to move their hands, how to sit and stand and walk. Faculty members also guide the responses
of another contingent of students, dressed in white lab coats, who are role-playing the physicians
they will become. After extensive practice, the patients and doctors will perform a series
of clinical vignettes for their peers from the stage of the hospital's amphitheater.
Jean-Martin Charcot, the 19th-century physician known as the father of neurology,
practiced and taught at this very institution. Charcot brought his patients on stage with him
as he lectured, allowing his students to see firsthand the many forms neurological disease could take.
Imitating such forms with one's own face and body is an even more effective means of learning,
maintains Emmanuel Rose, who introduced his mind-based role-play training program to the
students at PTA Salpetriere in 2015. Rose, a consulting neurologist at the hospital and a professor of
neurology at Sorbonne University, had become concerned that traditional methods of instruction were not
supporting students' acquisition of knowledge and were not dispelling students' apprehension in the
face of neurological illness. He reasoned that actively imitating the distinctive symptoms of such maladies,
the tremors of Parkinson's, the jerky movements of chorea, the slurred speech of
cerebellar syndrome, could help students learn while diffusing their discomfort.
And indeed, a study conducted by Rose and his colleagues found that two and a half years after
their neurological rotation, medical students who had participated in the miming program recalled
neurological signs and symptoms much better than students who had received only conventional
instruction centered on lectures and textbooks. Medical students who had simulated their patient
symptoms also reported that the experience deepened their understanding of neurological illness
and increased their motivation to learn about it. In an earlier chapter on sensations,
we heard that our automatic and unconscious mimicry of other people helps us understand them better,
for example. The same is true for more deliberate imitation. Researchers have demonstrated, for instance,
that intentionally imitating someone's accent allows us to comprehend more easily the words the person is
speaking, a finding that might readily be applied to second language learning. When we copy the accent of our
conversation partner, when we produce the sounds that individual is making with our own mouth,
we become better able to predict and thus to make sense of what he or she is saying. As with the medical
students at PTA Salt-Petrier, it's a matter of attaining understanding from the inside, of taking aspects of the
other into ourselves. We also come to feel more positive about the people whose speech we mimic,
an effect that holds true for imitation more generally. Emmanuel Rose has found that the experience of
imitating patients makes the young doctors he trains more empathetic as well as more comfortable with
the signs of their patients' disorders. Imitation permits us to extend to the other some of the familiar
regard we feel for ourselves, as well as some of the insight we gain from inhabiting the role of dynamic
actor in the world rather than that of passive observer. It is a general-purpose strategy with
boundless applications in education, in the workplace, and in the learning we do on our own time.
There's just one problem. As a society, we are suspicious of imitation, regarding it as juvenile,
disreputable, even morally wrong. It's a reaction Rose has come to know well. Despite the demonstrated
benefits of mind-based role-play, many of his fellow medical school professors have expressed apprehension
about implementing the practice. Some of his students, too, initially voiced discomfort at the
prospect of imitating patients. Rose is careful to note that those who participate are in no way
mocking or making fun of their charges. In fact, he says, the act of imitation is imbued with respect.
It's treating patients as the ultimate authority, as the experts on what it's like to have their
condition. The conventional approach to cognition has persuaded us that the only route to more
intelligent thinking lies in cultivating our own brain. Imitating the thought of other individuals
courts' accusations of being derivative or even of being a plagiarist, a charge that can end a writer's
career or a student's tenure at school. But this was not always the case. Greek and Roman thinkers
revered imitation as an art in its own right, one that was to be energetically pursued. Imitation
occupied a central role in classical education, where it was treated not as lazy cheating, but as a
rigorous practice of striving for excellence by emulating the masters. In the Romans' highly structured
system of schooling, students would begin by reading and analyzing aloud a model text. Early in
pupils' education, this might be a simple fable by Aesop. Later on, a complex speech by Cicero or
Demosthenes. The students would memorize the text and recite it from memory. Then they would embark on
a succession of exercises designed to make them intimately familiar with the work in question.
They would paraphrase the model text, putting it in their own words. They would translate the text from
Greek to Latin or Latin to Greek. They would turn the text from Latin prose into Latin verse or even from
Latin prose into Greek verse. They would compress the model into fewer words or elaborate it at greater
length. They would alter its tone from plain spoken to grandiloquent or the other way around. Finally,
they would write their own pieces, but in the style of the admired author. Having imitated the model from
every angle, students would begin the sequence anew, moving on to a more challenging text.
We know about the Roman system largely from the writings of Quintilian, the master teacher of Rome.
Marcus Fabius Quintilianus, born around the year AD 35, headed a school of rhetoric that enrolled students
from the city's most illustrious families, including the Emperor Domitian's two heirs. In his masterwork,
the Institutio Oratoria, subtitle, Education of an Orator in Twelve Books, Quintilian unapologetically
asserted the value of copying. From authors worthy of our study, he wrote, we must draw our stock of
words, the variety of our figures, and our methods of composition, so as to form our minds on the model
of every excellence. The educator continued, there can be no doubt that in art, no small portion of
our task lies in imitation, since although invention came first and is all-important, it is expedient to
imitate whatever has been invented with success, and it is a universal rule of life that we should wish
to copy what we approve in others. This system of education, founded on mimicking the masters,
was remarkably robust, persisting for centuries and spreading throughout Europe and beyond.
Fifteen hundred years after Quintilian, children in Tudor England were still being taught in this
fashion. A scholar and teacher of that time, Juan Luis Vives, explained why imitation was necessary.
While some basic capacities, such as speaking one's native language, seem to come naturally to
humans, nature has fashioned man for the most part strangely hostile to art, he observed. Since she
let us be born ignorant and absolutely skillless of all arts, we require imitation. Vives had intuited a
truth that cognitive science would later demonstrate empirically. Many of the achievements of
human culture do not come naturally do not come naturally but must be painstakingly acquired.
The most effective way to take possession of these skills, Vives and others of his time believed,
was imitation. Then, as the eighteenth century was drawing to a close, the romantics arrived on the
scene. This band of poets and painters and musicians worshipped originality, venerated authenticity. They
rejected all that was old and familiar and time-worn in favor of what was inventive and imaginative and
heartfelt. Their insistence on originality came in response to two major developments of the age. The first of
these was industrialization. As factories rose brick by brick, an aesthetic counter-movement mounted in
tandem. Machines could stamp out identical copies. Only humans could come up with one-of-a-kind ideas.
A particular sort of machine occasioned the second major development of the time,
a flood of texts produced by the newly common printing press. More than any generation before
them, the thinkers of the Romantic era bore what literary critic Walter Jackson Bate called
the burden of the past, as masterworks from earlier eras became widely available for the first time.
Immersed in a sea of their predecessors' words, they felt an urgent need to create something new and
fresh and never before said. William Blake, the English poet and artist born in 1757,
was one of the earliest and most passionately original Romantics. In creating works like Songs
of Innocence and of Experience and Visions of the Daughters of Albion, Blake employed a technique he had
invented himself, relief etching, in which he used acid-resistant chemicals to mark a copper plate,
then applied acid to etch away the untreated areas. The mystical-minded Blake claimed that his deceased
brother Robert had revealed the technique to him in a vision. These works took a form that was also newly
devised by Blake, illuminated books that combined text and drawings, and that were etched, printed, and colored
by Blake himself. No two books were the same. And their content was nothing if not original,
an elaborate invented cosmology featuring allegorical figures with names like Irizin, representing reason,
and Loss, imagination. In his illuminated book Jerusalem, Loss, Blake's alter ego, voiced a sentiment that
might have served as the Romantics' motto. I must create a system, Blake's character declared,
or else be enslaved by another man's. Under the Romantics' influence, Imitation did not merely become
less favored than previously. It came to be actively disdained and disparaged, an attitude that was carried
forward into succeeding decades. The naturalists of the late 19th century described imitation as the
habit of children, women, and savages, and held up original expression as the preserve of European men.
Innovation climbed to the top of the cultural value system, while imitation sank to an unaccustomed low.
This is the cult of originality to which we ourselves subscribe, more so now than ever before.
Our society celebrates pioneers and trailblazers, like, for example, the late Steve Jobs, the Apple
computer founder, famous for offering dazzling onstage introductions of the company's latest inventions.
His company's advertisements glorified those who break the mold, rather than those who allow themselves
to be shaped by it. Here's to the crazy ones, intoned the voiceover of an Apple commercial that aired in
1997. The misfits, the rebels, the troublemakers, the round pegs in the square holes, the ones who see
things differently. They're not fond of rules, and they have no respect for the status quo.
It was the proudest moment of Kevin Leyland's professional career, he says. The prestigious
journal Science published a photograph of Leyland mowing his lawn. In the picture, Leyland's three-year-old
son is walking just behind him, intently pushing his own toy lawnmower. The photo accompanied a
commentary on Leyland's research about the importance of imitation in human culture.
In the same issue, Leyland, a professor of biology at the University of St. Andrews in the UK,
reported on the results of a computerized competition he and his collaborators had set up.
This was a multi-round tournament in which the contenders, bots that had been programmed to
behave in particular ways, battled to victory for a monetary prize. A hundred entrants from around the
world had faced off, each designed to act according to one of three strategies or a combination thereof,
applying original ideas, engaging in trial and error, or copying others.
As for which strategy worked best, there was really no contest. Copying was far and away the
most successful approach. The winning entry exclusively copied others. It never innovated.
By comparison, a player bot whose strategy relied almost entirely on innovation finished 95th out of
the 100 contestants. The result came as a surprise to Leyland and one of his collaborators, Luke Rendell.
We were expecting someone to come up with a really clever way to say,
in these conditions you should copy, and in these conditions you should learn stuff for yourself,
says Rendell. But the winner just copied all the time.
Kevin Leyland acknowledges that imitation has a bad reputation, but he says researchers like him,
in fields from biology to economics to psychology to political science,
are discovering how valuable imitation can be as a way of learning new skills and making intelligent
decisions. Researchers from these very disciplines are using models and simulations, as well as
historical analyses and real-world case studies, to show that imitation is often the most efficient
and effective route to successful performance. And they're elaborating the reasons why this is so,
reasons vividly illustrated by examples from the business world. First on the list, by copying others,
imitators allow other individuals to act as filters, efficiently sorting through available options.
Finance professors Gerald Martin and John Puthenperackle examined what would happen if an investor did
nothing but copy the moves of celebrated investor Warren Buffett. Buffett's investment choices are
periodically made public when his company files a report with the Securities and Exchange Commission.
An individual who simply buys what Buffett is buying, the researchers found, will earn an average
of more than 10 percent above market returns. Of course, investors already do take notice of the
activity of a high-powered investor like Buffett, but even so they're missing out on the benefits of
imitating his selections more closely. Investors are leaving money on the table, Martin suggests,
because everyone likes to think they've got an innovative strategy or an overlooked gem.
Although it can feel good to chart our own course, he says, we often perform better when we copy someone
more experienced and more knowledgeable than ourselves. Second, imitators can draw from a wide variety of
solutions instead of being tied to just one. They can choose precisely the strategy that is most
effective in the current moment, making quick adjustments to changing conditions. That sums up
the business model of Zara, a worldwide chain of clothing stores based in the industrial city of Artiso,
Spain. At the headquarters of its parent company, Inditex, Zara's designers cluster around tables
covered with pages ripped from fashion magazines and catalogs, with photographs snapped of stylish
people on streets and in airports, and with the deconstructed parts of other designers' garments
fresh from the runway. Zara is engaged in a permanent quest for inspiration everywhere and from everybody,
says Spanish journalist Enrique Badia, who has written extensively about the company. Zara even copies its own
customers. Store managers from the chain's hundreds of locations are in frequent touch with its designers,
passing along new looks spotted on Zara's fashion-forward clientele.
Kazra Ferdos, a professor of operations and information management at Georgetown University,
notes that Zara's adroit use of imitation has helped make Inditex the largest fashion apparel
retailer in the world. Its success, he and two co-authors concluded in a company profile written
for the Harvard Business Review, depends on a constant exchange of information throughout
every part of Zara's supply chain, from customers to store managers, from store managers to market
specialists and designers, from designers to production staff. Crucially, the information that
flows so freely at the company concerns not new ideas, but ideas good enough to copy.
The third advantage of imitation. Copiers can evade mistakes by steering clear of the errors made
by others who went before them, while innovators have no such guide to potential pitfalls.
A case in point, diapers. Among parents who rely on disposable diapers, Pampers is a household name.
Less familiar is the brand Chucks, and yet Chucks was the first to arrive on the market all the way back in 1935.
The problem, Chucks were expensive, costing about 8.5 cents per diaper, at a time when parents could
wash cloth diapers at a cost of 1.5 cents each. As a result, parents tended to use the product only when
traveling, and Chucks accounted for just 1% of the overall market for diapers.
Procter & Gamble saw an opportunity. It imitated the basic idea behind Chucks, while intentionally
addressing parents' main objection to the product, its high price. When Pampers were rolled out nationwide
in 1966, at a cost of 3 cents per diaper, P&G's version was enthusiastically welcomed by parents.
Gerard Telles and Peter Golder, both professors of marketing, conducted a historical analysis of 50
consumer product categories, including diapers, from which the Pampers vs. Chucks example was taken.
Their results showed that the failure rate of market pioneers is an alarming 47%,
while the mean market share they capture is only 10%. Far better than being first,
Telus and Golder concluded, is being what some have called a fast second, an agile imitator.
Companies that capitalize on others' innovations have a minimal failure rate,
and an average market share almost three times that of market pioneers, they found.
In this category, they include Timex, Gillette, and Ford, firms that are often recalled,
wrongly, as being first in their field. Fourth, imitators are able to avoid being
swayed by deception or secrecy. By working directly off of what others do, copiers get access to the
best strategies in others' repertoires. Competitors have no choice but to display what social scientists
call honest signals as they make decisions for themselves based on their own best interests.
This is the case in every sort of contest, including sporting events like the America's Cup,
the high-profile sailing race. Jan Michael Ross and Dmitry Sharapov, both business professors at
Imperial College London, studied the competitive interactions among yachts engaged in head-to-head races
in the America's Cup World Series. The researchers found that sailors often engaged in covering or
copying the moves made by their rivals, especially when their boat was in the lead. It might seem
surprising that sailors at the front of the pack would imitate those who are trailing, but Ross notes that
such emulation makes sense. As long as the leaders do as their rivals behind them do, their lead will
remain locked in place. Says Ross, our research challenges the common view that it's only the laggards,
the also-rans, who imitate. Last, and perhaps most important, imitators save time, effort, and resources that
would otherwise be invested in originating their own solutions. Research shows that the imitator's costs are
typically 60 to 75 percent of those born by the innovator, and yet it is the imitator who consistently
captures the lion's share of financial returns. Such findings, arriving from many directions, converge on
the conclusion that imitation, if we can get past our aversion to it, opens up possibilities far beyond
those that dwell inside our own heads. Engaging in effective imitation is like being able to think
with other people's brains, like getting a direct download of others' knowledge and experience.
But contrary to its reputation as a lazy cop-out, imitating well is not easy. It rarely entails
automatic or mindless duplication. Rather, it requires cracking a sophisticated code, solving what social
scientists call the correspondence problem, or the challenge of adapting an imitated solution to
the particulars of a new situation. Tackling the correspondence problem involves breaking down an
observed solution into its constituent parts, and then reassembling those parts in a different way.
It demands a willingness to look past superficial features to the deeper reason why the original
solution succeeded succeeded, and an ability to apply that underlying principle in a novel setting.
It's paradoxical, but true. Imitating well demands a considerable degree of creativity.
Adapting solution to problem was the tall task facing graduate nursing student Tess Pape in 1999.
Pape could see the problem clearly enough. Hospital patients were being harmed by medication errors
committed by doctors and nurses. In that year, the Institute of Medicine had released a landmark report
on patient safety to Error is Human. The report found that as many as 98,000 Americans were dying each year
as a result of preventable medical errors occurring in hospitals. More people then succumbed to car accidents,
workplace injuries, or breast cancer. And some significant portion of those deaths involved
mistakes in the dispensing of drugs. But as she investigated ways to address the medication error
crisis, Pape didn't rack her brain for an innovative fix. Instead, she sought to imitate a solution that had
been successfully applied in another industry. That industry was aviation, an enterprise like healthcare in
which people's lives depended on professionals' precision and accuracy. While reading up on
aviation safety, Pape learned that the moments of highest risk occurred during takeoffs and landings,
periods when the plane was under 10,000 feet. She spotted a correspondence in her own field.
For hospital patients who are given medication, the riskiest moments happen during the preparation of the
drug dosage and during administration of the drug to the patient. Delving deeper,
Pape discovered that distractions and interruptions of the pilot by other crew members accounted for a
majority of airline incidents. Another correspondence came into view. Interruptions of healthcare professionals,
she knew, were also to blame for many medication mistakes.
Consider this striking incident reported by a team of researchers observing real-life conditions in
hospitals. One nurse dispensing one medication to one single patient was interrupted 17 times.
Pape also became aware that aviation experts had devised a solution to the problem of pilot
interruption, the sterile cockpit rule. Instituted by the Federal Aviation Administration in 1981,
the rule forbids pilots from engaging in conversation unrelated to the immediate business of flying when
the plane is below 10,000 feet. In her 2002 dissertation, and then in a series of articles published in
medical journals, Pape made a case for imitating this practice. The key to preventing medication errors
lies with adopting protocols from other safety-focused industries, Pape wrote in the journal Med-Surg Nursing
in 2003. The airline industry, for example, has methods in place that improve pilots' focus and provide a
milieu of safety when human life is at stake. Such methods could be adapted to the hospital setting,
she argued, by creating a no-interruption zone around medication preparation areas and by having nurses
who are administering medication wear special vests or sashes, signaling that they are not to be
disturbed. Added Pape pointedly, medication administration should be considered as critical
as piloting a plane because patients place their lives in the hands of healthcare professionals.
Pape wasn't sure if her peers in the healthcare community would be open to
listening to the idea. But listen, they did. Hospitals began following the lead of airlines,
and the change made a dramatic difference. At Kaiser Permanente South San Francisco Medical Center,
for example, the introduction of no-interruption signaling in 2006 led to the virtual elimination
of nurse distractions for those wearing the vests, according to the U.S. government's Agency for
Healthcare Research and Quality. Over a six-month period, medication errors at the hospital fell
by 47%. Nearly two decades after she initiated it, Pape's life-saving act of imitation has spread all
over the country and the world. Tess Pape figured out the correspondence problem on her own, but what if she had
been taught how to imitate? Imitating well is a skill, one that Oded Shankar believes should be
deliberately cultivated. Shankar, a professor of management and human resources at Ohio State
University, studies how companies use imitation to gain a strategic edge in the marketplace.
He maintains that we are living in a golden age of imitation in which access to information about how
other people are addressing problems similar to our own has made it more feasible than ever to copy
effective solutions. Shankar would like to see students in business schools and other graduate
programs taking courses on effective imitation. He imagines companies opening imitation departments
devoted to identifying promising opportunities for copying. And he anticipates a day when successful
imitators are celebrated and admired just as much as innovators are now. Shankar notes that at least
one profession has been taking steps in the direction of his push, healthcare. The urgent need to reduce
medical errors, perceived by Tess Pape and many others, has led hospitals to imitate the practices of a
host of other industries, including the military, railroads, chemical manufacturing, nuclear power,
and, of course, aviation. In addition to the sterile cockpit concept Pape adapted, healthcare professionals have
also borrowed from pilots the onboard checklist, a standardized rundown of tasks to be completed.
In this case, too, imitation has worked wonders. In 2009, researchers from the Harvard School of Public
Health and the World Health Organization reported that after the surgical teams in their study started using a
19-item checklist, the average patient death rate fell more than 40 percent and the rate of
complications decreased by about a third. The medical field has also adopted the peer-to-peer assessment
technique, a common practice in the nuclear power industry. A delegation from one hospital visits another
hospital in order to conduct a structured, confidential, and non-punitive review of the host institution's
safety and quality efforts. Without the threat of sanctions carried by regulators, these peer
reviews can surface problems and suggest fixes, making the technique itself a vehicle for constructive
copying among organizations. Even within healthcare, however, the practice of imitation leaves much room for
improvement. It took 70 years for the checklist concept to migrate from aviation to medicine and 20 years for
the sterile cockpit to make the leap. A more structured and intentional approach to imitation could speed up this
process considerably. In order to elevate the social value of copying, says Shankar, we need not only to promote
new acts of imitation, but also to recognize that imitation is already behind the success of
many of our most admired individuals and organizations, a group that assuredly includes the famed innovator
Steve Jobs. In 1979, Jobs and his colleagues at the fledgling Apple Computer Company were wrestling with how
to turn the crude, clunky computers of the day into sleek personal appliances that were easy and even fun to
use. In December of that year, he got a glimpse of the solution while on a visit to Xerox Park, a research
facility operated by the photocopier giant in Palo Alto, California. Jobs was shown a series of technological
innovations that he knew he could put to use in his own project. A networking platform that allowed computers to
connect to and communicate with one another, a set of visually appealing and user-friendly on-screen
graphics, a mouse that enabled users to point and click. This is it, he shouted to an Apple associate
as their car sped away from Park. We've got to do it! In Oded Shankar's contemplated academic course on
imitation, he might well use Apple as a case study, and he might point out to his class that Jobs had
already taken the first of three steps required to solve the all-important correspondence problem.
Step one, according to Shankar, specify one's own problem and identify an analogous problem that has
been solved successfully. Step two, rigorously analyze why the solution is successful. Jobs and his engineers
at Apple's headquarters in Cupertino, California, immediately got to work deconstructing the marvels
they'd seen at the Xerox facility. Soon they were on to the third and most challenging step,
identify how one's own circumstances differ, then figure out how to adapt the original solution
to the new setting. Xerox had already brought its own computer to market, but it was awkward and difficult
to use. It was designed for the needs of business rather than the individual consumer, and it was
prohibitively expensive, costing more than $16,000. Xerox had found technological solutions that eluded
Apple scientists, but it was Jobs who adapted these solutions to the potential market he saw for
personal devices. An example, the mouse he'd seen at Park had three buttons, which Jobs deemed excessively
fussy. It didn't roll easily, even on smooth surfaces, and it cost a whopping $300. Jobs worked with a local
design firm to produce a one-button mouse that could be operated on any surface, even his blue jeans,
Jobs specified, and that cost only $15. The rest is history, although not history as it is usually told,
a story of solitary geniuses, the ones who see things differently. The lesson of this case study is that
skilled imitation, and not just brilliant innovation, is behind many of the successes we celebrate.
Imitation even appears to be behind our success as a species. Developmental psychologists are
increasingly convinced that infants and children's facility for imitation is what allows them to absorb
so much so quickly. So efficient is imitation as a method of learning, in fact, that roboticists are
studying babies in order to understand how they pull off the trick of observing an adult and then doing
as the grown-up does. Imagine if a robot could watch a human perform an action, say, place a silicon chip
on a circuit board, or make a repair on a space capsule, and then replicate that movement itself.
Elon Musk, founder of Tesla and SpaceX, has invested in research on just such one-shot imitation learning.
But as University of California Berkeley psychologist Alison Gopnik notes, the most sophisticated forms of
artificial intelligence are still far from being able to solve problems that human four-year-olds accomplish with ease.
While it was once regarded as a low-level, primitive instinct, researchers are coming to realize that
imitation, at least as practiced by humans, including very young ones, is a complex and
sophisticated capacity. Although non-human animals do imitate, their mimicry differs in important ways from ours.
For example, young humans copying is unique in that children are quite selective about whom they choose to imitate.
Even preschoolers prefer to imitate people who have shown themselves to be knowledgeable and
competent. Research shows that while toddlers will choose to copy their mothers rather than a person
they've just met, as children grow older they become increasingly willing to copy a stranger if the
stranger appears to have special expertise. By the time a child reaches age seven, mom no longer knows best.
At the same time, children are strikingly unselective about what they imitate, another way in which our
practice of imitation departs from that of animals. Humans are high-fidelity copiers. Our young imitate
adults to the letter, while other animals will make do with a slapdash approximation. This difference can make
apes, monkeys, and even dogs look like the smarter species. Shown a procedure with an extra unnecessary step, like
touching a box with one's forehead before prying it open and retrieving the treat inside. Chimps and canines will skip the
superfluous move to go right for the goods. Children, however, will faithfully imitate every step.
There is sense behind this seemingly irrational behavior. Humans' tendency to over-imitate, to reproduce even the gratuitous
elements of another's behavior, may operate on a copy-now, understand-later basis. After all, there might be good
reasons for such steps that the novice does not yet grasp, especially since so many human tools and practices
are cognitively opaque, not self-explanatory on their face. Even if there doesn't turn out to be a functional
rationale for the actions taken, imitating the customs of one's culture is a smart move for a highly social
species like our own. Indeed, researchers have demonstrated that a four-year-old child is more
likely to over-imitate than a two-year-old, indicating a growing sensitivity to social cues. Our tendency to
engage in over-imitation continues to increase across development all the way into adulthood.
Because so much of human culture is arbitrary in form, why do we clap at the end of a performance,
or eat cake at birthday parties, or wear wedding rings on the fourth finger of the left hand?
It depends on imitation for its perpetuation. Imitation is at the root of our social and cultural
life. It is, quite literally, what makes us human. There is evidence that we are born with a
predisposition to imitate. Several decades ago, University of Washington psychologist Andrew Meltzoff
showed that babies who were days or even hours old responded to his opening his mouth or sticking out
his tongue by forming the same facial expressions in return. At the same time, the capacity to imitate
and to learn from observation can be nurtured. In some present-day cultures, it is cultivated quite
deliberately, with impressive results. In studies comparing European-American children with Mayan
children from Guatemala, psychologists Maricela Correa Chavez and Barbara Rogoff asked children from each
culture to wait to wait while an adult performed a demonstration, folding an origami shape, for another
child nearby. The Mayan youth paid far more sustained attention to the demonstration, and therefore
learned more, than the American kids, who were often distracted or inattentive. Correa Chavez and
Rogoff note that in Mayan homes, children are encouraged to carefully observe older family members so that they can
learn how to carry out the tasks of the household, even at very young ages.
Because our own culture discourages imitation, American children aren't granted similar opportunities
to show how competent they can be. They also lack exposure to inspiring examples or models of the
kind of work that kids their age are capable of producing. For decades, educator Ron Berger toed around
a rolling suitcase filled with hundreds of such models, sketches, poems, and essays created by
children, which he would pull out and share with teachers and students at schools around the country.
Among his favorite examples is a picture he calls Austin's Butterfly, drawn by a first grader in
Boise, Idaho. When he displays the drawing, which depicts a tiger swallowtail butterfly in graceful detail,
students often murmur in awe. Berger's aim is to inspire his audience with examples of excellent
work by their peers, but also to demonstrate to them how such work is made. One by one, he shows
them the six drafts Austin produced on the way to his finished drawing, and tells them about the
constructive critiques Austin received from his classmates at each stage. The contents of Berger's
rolling suitcase are now available in an online archive, but he has found that many teachers
and parents object to the use of models, afraid that it will suppress students' creativity and
originality. In fact, the opposite is true, says Berger, who spent 28 years as a classroom teacher
before becoming chief academic officer of the non-profit organization EL Education.
Seeing examples of outstanding work motivates students by giving them a vision of the possible.
How can we expect students to produce first-rate work, he asks, when they have no idea what first-rate
work looks like? The capacity of models to promote, rather than quash, students' creativity was long
recognized by teachers of composition and rhetoric, subjects that were once a central part of the
curriculum in American schools. One of the field's leading textbooks was authored by literature scholar
Edward P. J. Corbett, who never relinquished the notion that emulating the work of the masters
was the first step towards developing one's own distinctive style.
Imitate that you may be different, Corbett thundered. Even as the use of models has faded from the teaching
of English more generally, it is enjoying a resurgence among educators who are training
students to write within particular academic genres, an activity sometimes called disciplinary
writing. In this context, the emulation of model texts is valued for its capacity to reduce cognitive
load, especially important when students are juggling new concepts and vocabulary while also
trying to construct a coherent argument in writing. Following the contours of a prototype provided by
the instructor permits students to process more deeply the material they are expected to learn.
Marin Robinson, a chemistry professor at Northern Arizona University, leads an undergraduate course aimed
at teaching students to write like a chemist. Those who enroll practice the writing of four forms
central to the discipline—the journal article, the conference abstract, the poster presentation,
and the research proposal. In each case, students follow the scientific and linguistic conventions
on display in the authentic texts they are given—actual articles, abstracts, posters, and proposals.
This act of imitation relieves students of some of their mental burden, Robinson notes,
allowing them to devote the bulk of their cognitive bandwidth to the content of the assignment.
She and her collaborator, English professor Frederica Stoller, have authored a textbook,
Write Like a Chemist, that is used at universities around the country.
A similar movement is emerging in teaching legal writing, another endeavor that requires students
to assimilate a host of new terms and ideas, even as they are learning to write in an unfamiliar genre.
In the course of teaching hundreds of first-year law students, Monty Smith, a professor and dean
at Ohio State University's law school, grew increasingly puzzled by the seeming inability
of his bright, hard-working students to absorb basic tenets of legal thinking and to apply them
in writing. He came to believe that the manner of his instruction was demanding more from them
than their mental bandwidth would allow. Students were being asked to employ a whole new vocabulary
and a whole new suite of concepts, even as they were attempting to write in an unaccustomed style
and an unaccustomed form. It was too much, and they had too few mental resources left over
to actually learn. His solution. At the start of the course, Smith provides students with several
sample legal memorandums, like those written by working lawyers. Guided by a set of instructions
and targeted questions, students are expected to detail their responses to various aspects of the
memos, thereby relieving them of the burden of having to produce their own memos, even as they are
laboring to learn what a memo looks and sounds like. Only after several such structured encounters
with legal documents are students asked to author their own memorandums. As Smith notes,
the emulation of model texts was once a standard feature of instruction in legal writing.
It fell out of favor because of concern that the practice would fail to foster a capacity for
independent thinking. The careful observation of how students actually learn, informed by research
on the role of cognitive load, may be bringing models back into fashion.
Of course, the richest, deepest, and potentially most useful models are people. Yet the very individuals
who are most expert are often least able to share what they know. After years of practice,
much of experts' knowledge and skill has become automatized, so well practiced that they no longer
need to think about it. Automatization allows experts to work efficiently and effectively,
but it also prevents them from offering to others a full account of how they do what they do.
Kenneth Kadinger, a professor at Carnegie Mellon University and the director of its Pittsburgh
Science of Learning Center, estimates that experts are able to articulate only about 30% of what they know.
His conclusion is based on research like the following.
A study that asked expert trauma surgeons to describe how they insert a shunt into a femoral artery,
the large blood vessel in the upper leg, reported that the surgeons neglected to cite nearly 70% of the actions
they performed during the procedure. A study of expert experimental psychologists found that they omitted
or inaccurately characterized an average of 75% of the steps they took when designing experiments
and analyzing data. And a study of expert computer programmers revealed that they enumerated fewer
than half of the tasks they actually carried out when debugging a computer program.
Our systems of academic education and workplace training rely on experts teaching novices,
but they rarely take into account the blind spots that experts acquire by virtue of being experts.
In the era of knowledge work, it's not only the case that learners and novices must become more assiduous
imitators. Instructors and experts must also become more legible models.
This can be accomplished through what philosopher Karsten Stuber calls re-enactive empathy,
an appreciation of the challenges confronting the novice that is produced by re-enacting what
it was like to have once been a beginner oneself. Ting Zhang, an assistant professor of business
administration at Harvard Business School, found a clever way to stage such a re-enactment among
expert musicians. For her experiment, Zhang enlisted a group of experienced guitarists to play their
instruments. Half of them were asked to play as they normally would, while the other half were
instructed to reverse the position of their instrument and play it with their non-dominant
hand. All the musicians were then asked to watch a video clip of a beginning guitar student trying to
form basic chords and to offer him advice. The guidance provided by the guitarists in the reversed
instrument condition, those who had so recently struggled themselves to play in an unfamiliar manner,
was judged to be especially helpful. But re-enacting the experience of being a novice need not be so
literal. Experts can generate empathy for the beginner through acts of the imagination, changing the way
they present information accordingly. An example, experts habitually engage in chunking or compressing
several tasks into one mental unit. This frees up space in the expert's working memory, but it often
baffles the novice for whom each step is new and still imperfectly understood. A math teacher may
speed through an explanation of long division, not remembering or recognizing that the procedures
that now seem so obvious were once utterly inscrutable. Math education expert John Mighton has a suggestion,
break it down into steps, then break it down again into micro-steps if necessary.
Though Mighton now holds a PhD in mathematics, he struggled with math as a child. He succeeded by
teaching himself how to advance one tiny move at a time, the method he now advocates as the founder
of a non-profit educational organization, JUMP Math. The name stands for Junior Undiscovered Math
Prodigies. When instructors make their expertise legible in this way, learners are able to master one small step
