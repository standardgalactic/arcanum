This is Audible.
Mariner Books and Harper Audio present
The Extended Mind
The Power of Thinking Outside the Brain
by Annie Murphy-Paul
Read by the author.
For Sally, Billy, and Frankie
Prologue
When you're writing a book about how to think well,
your sources, the cognitive scientists, psychologists, biologists, neuroscientists,
and philosophers who all have something to contribute on the subject,
will often seem to be speaking, via their work, directly to you.
Yes, you there, writing a book.
They cajole and insist.
They argue and debate.
They issue warnings and pass judgment.
As you lay out their recommendations for the reader,
they inquire pointedly,
Are you taking your own advice?
I entered into one such intimate exchange when I read,
with a jolt of recognition,
a passage written more than 130 years ago.
It was as if the author were reaching through the pages that lay open on my desk.
Making the meeting more intense,
the writer in question was a distinctly intimidating character.
How quickly we guess how someone has come by his ideas,
Nietzsche slyly observed.
Whether it was while sitting in front of his inkwell with a pinched belly,
his head bowed low over the paper,
in which case we are quickly finished with his book, too.
Cramped intestines betray themselves.
You can bet on that.
No less than closet air,
closet ceilings,
closet narrowness.
The room in which I was writing suddenly seemed rather airless and small.
I encountered his words as I was working on a chapter about how bodily movement affects the way we think.
The quote from Nietzsche appears in a book titled
A Philosophy of Walking by the contemporary French philosopher Frédéric Gros.
Gros has his own thoughts to add.
Don't think of a book as issuing only from an author's head, he advises.
Think of the scribe's body,
his hands,
his feet,
his shoulders and legs.
Think of the book as an expression of physiology.
In all too many books,
the reader can sense the seated body,
doubled up,
stooped,
shriveled in on itself.
My seated body shifted guiltily in its chair,
which it had occupied all morning.
Far more conducive to the act of creation,
Gros continues,
is the walking body,
which, he says,
is unfolded and tensed like a bow,
opened to wide spaces like a flower to the sun.
Nietzsche, he reminds us,
wrote that we should
sit as little as possible,
do not believe any idea that was not born in the open air
and of free movement.
The philosophers were ganging up on me.
I closed my laptop and went for a walk.
I was not acting only on their say-so, of course.
By this point in my research,
I had read dozens of empirical studies
showing that a bout of physical activity
sharpens our attention,
improves our memory,
and enhances our creativity.
And, in fact,
I found that the forward movement of my legs,
the flow of images past my eyes,
the slight elevation of my heart rate
did work some kind of change on my mind.
Upon sitting back down at my desk,
I wasted no time resolving a naughty conceptual problem
that had tormented me all morning.
I can only hope that the prose I produced
also retains and expresses the energy,
the springiness of the body
in Gross's formulation.
Could my brain have solved the problem on its own?
Or did it require the assist
provided by my ambulatory limbs?
Our culture insists that the brain
is the sole locus of thinking,
a cordoned-off space where cognition happens,
much as the workings of my laptop
are sealed inside its aluminum case.
This book argues otherwise.
It holds that the mind is something
more like the nest-building bird
I spotted on my walk,
plucking a bit of string here,
a twig there,
constructing a hole out of available parts.
For humans, these parts include,
most notably,
the feelings and movements of our bodies,
the physical spaces in which we learn and work,
and the other minds with which we interact,
our classmates, colleagues, teachers,
supervisors, friends.
Sometimes all three elements come together
in especially felicitous fashion,
as they did for the brilliant intellectual team
of Amos Tversky and Daniel Kahneman.
The two psychologists carried out
much of their groundbreaking work
on heuristics and biases,
the human mind's habitual shortcuts and distortions,
by talking and walking together
through the bustling streets of Jerusalem
or along the rolling hills
of the California coast.
I did the best thinking of my life
on leisurely walks with Amos,
Kahneman has said.
Many tomes have been written
on human cognition,
many theories proposed
and studies conducted,
Tversky's and Kahneman's among them.
These efforts have produced
countless illuminating insights,
but they are limited by their assumption
that thinking happens only inside the brain.
Much less attention has been paid
to the ways people use the world to think,
the gestures of the hands,
the space of a sketchbook,
the act of listening to someone tell a story,
or the task of teaching someone else.
These extra-neural inputs
change the way we think.
It could even be said
that they constitute a part
of the thinking process itself.
But where is the chronicle
of this mode of cognition?
Our scientific journals
mostly proceed from the premise
that the mental organ
is a disembodied,
placeless,
asocial entity,
a brain in a vat.
Our history books spin tales
that attribute
world-changing breakthroughs
to individual men
thinking great thoughts
on their own.
Yet a parallel narrative
has existed in front of us
all along,
a kind of secret history
of thinking outside the brain.
Scientists,
artists,
authors,
leaders,
inventors,
entrepreneurs,
they've all used the world
as raw material
for their trains of thought.
This book aims to exhume
that hidden saga,
reclaiming its rightful place
in any full accounting
of how the human race
has achieved
its remarkable feats
of intellect
and creativity.
We'll learn about
how geneticist
Barbara McClintock
made her Nobel Prize-winning
discoveries
by imaginatively embodying
the plant chromosomes
she studied,
and about how pioneering
psychotherapist
and social critic
Susie Orbach
senses what her patients
are feeling
by tuning into
the internal sensations
of her own body,
a capacity known
as interoception.
We'll contemplate
how biologist
James Watson
determined
the double helix
structure of DNA
by physically manipulating
cardboard cutouts
he'd made himself,
and how author
Robert Caro
plots the lives
of his biographical subjects
on an intricately detailed
wall-sized map.
We'll explore
how virologist
Jonas Salk
was inspired
to complete
his work
on a polio vaccine
while wandering
a 13th century
Italian monastery,
and how the artist
Jackson Pollock
set off a revolution
in painting
by trading his apartment
in frenetic
downtown Manhattan
for a farmhouse
on the verdant
South Fork
of Long Island.
We'll find out
how Pixar director
Brad Bird
creates modern movie
classics like
Ratatouille
and The Incredibles
by arguing
vehemently
with his
longtime producer,
and how physicist
Carl Wieman,
another Nobel Prize winner,
figured out
that inducing
his students
to talk with one another
was the key
to getting them
to think like scientists.
Such stories
push back
against the prevailing
assumption
that the brain
can,
or should,
do it all
on its own.
They are vivid
testimony
to the countervailing
notion
that we think best
when we think
with our bodies,
our spaces,
and our relationships.
But as with
Friedrich Nietzsche's
commendation
of the virtues
of walking,
the evidence
supporting the
efficacy of thinking
outside the brain
is far from
merely anecdotal.
Research emerging
from three related
areas of investigation
has convincingly
demonstrated
the centrality
of extra-neural
resources
to our thinking
processes.
First,
there's the study
of embodied cognition,
which explores
the role
of the body
in our thinking.
For example,
how making
hand gestures
increases the
fluency of our speech
and deepens
our understanding
of abstract concepts.
Second,
there's the study
of situated cognition,
which examines
the influence
of place
on our thinking.
For instance,
how environmental cues
that convey
a sense of belonging
or a sense
of personal control
enhance our performance
in that space.
And third,
there's the study
of distributed cognition,
which probes
the effects
of thinking
with others,
such as how
people working
in groups
can coordinate
their individual
areas of expertise,
a process called
transactive memory,
and how groups
can work together
to produce results
that exceed
their members'
individual contributions,
a phenomenon
known as
collective intelligence.
As a journalist
who has covered
research in psychology
and cognitive science
for more than 20 years,
I read the findings
generated by these fields
with growing excitement.
Together,
they seem to indicate
that it's the stuff
outside our heads
that makes us smart,
a proposition
with enormous implications
for what we do
in education,
in the workplace,
and in our everyday lives.
The only problem,
there was no
together,
no overarching
framework
that organized
these multitudinous
results
into a coherent whole.
Researchers working
within these
three disciplines
published in different
journals
and presented
at different conferences,
rarely drawing
connections among
their areas
of specialization.
Was there some
unifying idea
that could pull together
these deeply
intriguing findings?
Once again,
a philosopher
came to my rescue.
This time,
it was Andy Clark,
professor of cognitive
philosophy
at the University
of Sussex
in England.
In 1995,
Clark had co-written
a paper titled
The Extended Mind,
which opened with
a deceptively simple
question,
where does the mind
stop and the rest
of the world
begin?
Clark and his
co-author,
philosopher David
Chalmers,
noted that we have
traditionally assumed
that the mind
that the mind
is contained
within the head.
But,
they argued,
there is nothing
sacred about
skull and skin.
Elements of the world
outside may
effectively act
as mental
extensions,
allowing us to
think in ways
our brains
could not manage
on their own.
Clark and Chalmers
initially focused
their analysis
on the way
technology can
extend the mind.
A proposal that
quickly made the
leap from
risibly preposterous
to self-evidently
obvious once their
readers acquired
smartphones and
began offloading
large chunks of
their memories
onto their new
devices.
Fellow philosopher
Ned Block likes
to say that Clark
and Chalmers' thesis
was false when it
was written in 1995,
but subsequently
became true,
perhaps in 2007
when Apple
introduced the
first iPhone.
Yet, as early
as that original
paper, Clark
hinted that other
kinds of extensions
were possible.
What about
socially extended
cognition, he
and Chalmers asked.
Could my mental
states be partly
constituted by the
states of other
thinkers?
We see no reason
why not.
In the years that
followed, Clark
continued to enlarge
his conception of
the kinds of
entities that could
serve as extensions
of the mind.
He observed that
our physical
movements and
gestures play
an important
role in an
extended neural
bodily cognitive
economy.
He noted that
humans are
inclined to
create designer
environments,
carefully appointed
spaces that
alter and
simplify the
computational
tasks which
our brains
must perform
in order to
solve complex
problems.
Over the course
of many more
published papers
and books,
Clark mounted
a broad and
persuasive argument
against what he
called the
brain-bound
perspective,
the view that
thinking happens
only inside the
brain, and in
favor of what he
called the
extended perspective,
in which the rich
resources of our
world can and do
enter into our
trains of thought.
Consider me a
convert.
The notion of the
extended mind seized
my imagination and
has not yet released
its grip.
During my many years
reporting, I had
never before
encountered an
idea that
changed so much
about how I
think, how I
work, how I
parent, how I
navigate everyday
life.
It became apparent
to me that Andy
Clark's bold
proposal was not,
or not only, the
esoteric thought
experiment of an
ivory tower
philosopher.
It was a plainly
practical invitation
to think differently
and better.
As I began to
catalog the
dozens of
techniques for
thinking outside
the brain that
researchers have
tested and
verified, I
eagerly incorporated
them into my
own repertoire.
These include
methods for
sharpening our
interoceptive sense
so as to use
these internal
signals to guide
our decisions and
manage our mental
processes.
They encompass
guidelines for the
use of specific
types of gesture
or particular modes
of physical activity
to enhance our
memory and
attention.
This research
offers instructions
on using
time and
nature to
restore our
focus and
increase our
creativity, as
well as
directions for
designing our
learning and
working spaces
for greater
productivity and
performance.
The studies
we'll cover
describe structured
forms of social
interaction that
allow other
people's cognition
to augment our
own.
They also
supply guidance
on how to
offload,
externalize, and
dynamically interact
with our thoughts,
a much more
effective approach
than doing it
all in our
heads.
In time, I
came to recognize
that I was
acquiring a second
education, one
that is increasingly
essential but
almost always
overlooked in our
focus on educating
the brain.
Over many years of
elementary school,
high school, and
even college and
graduate school,
we're never
explicitly taught to
think outside the
brain.
We're not shown how
to employ our bodies
and spaces and
relationships in the
service of
intelligent thought.
Yet this
instruction is
available if we
know where to
look.
Our teachers are
the artists and
scientists and
authors who have
figured out these
methods for
themselves, and
the researchers who
are, at last,
making these methods
the object of
study.
For my own part, I'm
convinced that I
could not have
written this book
without the help of
the practices
detailed within it.
That's not to say
I didn't sometimes
fall back into
our culture's
default position.
Before Friedrich
Nietzsche's
fortuitous
intervention that
morning, I was
in full brain-bound
mode, my head
bowed low over
my keyboard, working
my poor brain ever
harder instead of
looking for
opportunities to
extend it.
I'm grateful for
the nudge my
research supplied.
It's that gentle
push in a more
productive direction
that this book
seeks to offer
its own readers.
Friedrich Gros, the
French philosopher
who brought
Nietzsche's words
to my attention,
maintains that
thinkers ought to
get moving in
a quest for a
different light.
As he observes,
libraries are
always too dark,
and books written
among the stacks
manifest this dull
dimness, while
other books reflect
piercing mountain
light or the sea
sparkling in
sunshine.
It's my hope
that this book
will cast a
different light,
bring a bracing
gust of fresh air
to the thinking
we do as
students and
workers, as
parents and
citizens, as
leaders and
creators.
Our society is
facing unprecedented
challenges, and
we'll need to
think well in
order to solve
them.
The brain-bound
paradigm now so
dominant is
clearly inadequate
to the task.
Everywhere we
look, we see
problems with
attention and
memory, with
motivation and
persistence, with
logical reasoning
and abstract
thinking.
Truly original
ideas and
innovations seem
scarce.
Engagement levels
in schools and
in companies are
low.
Teams and groups
struggle to work
together in an
effective and
satisfying way.
I've come to
believe that such
difficulties result
in large part from
a fundamental
misunderstanding of
how and where
thinking happens.
As long as we
settle for
thinking inside
the brain, we'll
remain bound by
the limits of
that organ.
But when we
reach outside it
with intention
and skill, our
thinking can be
transformed.
It can become as
dynamic as our
bodies, as airy as
our spaces, as
rich as our
relationships, as
capacious as the
whole wide
world.
introduction, thinking
outside the brain.
Use your head.
How many times have
you heard that
phrase?
Perhaps you've even
urged it on someone
else, a son or
daughter, a student,
an employee.
Maybe you've
muttered it under
your breath while
struggling with an
especially tricky
problem, or when
counseling yourself to
remain rational.
Use your head.
The command is a
common one, issued
in schools, in the
workplace, amid the
trials of everyday
life.
Its refrain finds an
echo in culture both
high and low, from
Auguste Rodin's
The Thinker, chin
resting thoughtfully
on fist, to the
bulbous cartoon
depiction of the
brain that festoons all
manner of products and
websites, educational
toys, nutritional
supplements, cognitive
fitness exercises.
When we say it, we
mean, call on the
more than ample
powers of your
brain.
Draw on the
magnificent lump of
tissue inside your
skull.
We place a lot of
faith in that lump.
Whatever the problem,
we believe, the brain
can solve it.
But what if our
faith is misplaced?
What if the
directive to use
your head, ubiquitous
though it may be, is
misguided?
A burgeoning body
of research suggests
that we've got it
exactly backwards.
As it is, we use our
brains entirely too
much, to the
detriment of our
ability to think
intelligently.
What we need to do
is think outside the
brain.
Thinking outside the
brain means skillfully
engaging entities
external to our
heads, the feelings
and movements of
our bodies, the
physical spaces in
which we learn and
work, and the minds
of the other people
around us, drawing
them into our
own mental
processes.
By reaching beyond
the brain to
recruit these
extra neural
resources, we're
able to focus
more intently,
comprehend more
deeply, and
create more
imaginatively, to
entertain ideas
that would be
literally unthinkable
by the brain
alone.
It's true that
we're more
accustomed to
thinking about
our bodies, our
spaces, and our
relationships.
But we can also
think with and
through them by
using the
movements of
our hands to
understand and
express abstract
concepts, for
example, or by
arranging our
workspace in ways
that promote
idea generation, or
by engaging in
social practices
like teaching and
storytelling that
lead to deeper
understanding and
more accurate
memory.
Rather than
exhorting ourselves
and others to
use our heads, we
should be applying
extra neural
resources to the
project of thinking
outside the skull's
narrow circumference.
But wait, you may
be asking, what's
the need?
Isn't the brain on
its own, up to
the job?
Actually, no.
We've been led to
believe that the
human brain is an
all-purpose, all-
powerful thinking
machine.
We're deluged with
reports of
discoveries about
the brain's
astounding
abilities, its
lightning quickness,
and its protean
plasticity.
We're told that the
brain is a
fathomless wonder,
the most complex
structure in the
universe.
But when we clear
away the hype, we
confront the fact that
the brain's capacities
are actually quite
constrained and
specific.
The less heralded
scientific story of
the past several
decades has been
researchers' growing
awareness of the
brain's limits.
The human brain is
limited in its
ability to pay
attention, limited in
its capacity to
remember, limited in
its facility with
abstract concepts,
and limited in its
power to persist at a
challenging task.
Importantly, these
limits apply to
everyone's brain.
It's not a matter of
individual differences
in intelligence.
It's a matter of the
character of the organ
we all possess, its
biological nature, and
its evolutionary
history.
The brain does do a
few things exquisitely
well.
Things like sensing and
moving the body,
navigating through
space, and connecting
with other humans.
These activities it can
manage fluently, almost
effortlessly.
But accurately recalling
complex information,
engaging in rigorous
logical reasoning,
grasping abstract or
counterintuitive ideas,
not so much.
Here we arrive at a
dilemma, one that we all
share.
The modern world is
extraordinarily complex,
bursting with
information, built
around non-intuitive
ideas, centered on
concepts and symbols.
Succeeding in this
world requires focused
attention, prodigious
memory, capacious
bandwidth, sustained
motivation, logical
rigor, and proficiency
with abstractions.
The gap between what our
biological brains are
capable of and what
modern life demands is
large and getting
larger each day.
With every experimental
discovery, the divide
between the scientific
account of the world
and our intuitive folk
understanding grows more
pronounced.
With every terabyte of
data swelling humanity's
store of knowledge, our
native faculties are
further outstripped.
With every twist of
complexity added to the
world's problems, the
naked brain becomes more
unequal to the task of
solving them.
Our response to the
cognitive challenges
posed by contemporary
life has been to
double down on what
the philosopher Andy
Clark calls brain-bound
thinking, those very
capacities that are, on
their own, so woefully
inadequate.
We urge ourselves and
others to grit it out,
bear down, just do it,
to think harder.
But, as we often find to
our frustration, the
brain is made of stubborn
and unyielding stuff, its
vaunted plasticity not
withstanding.
Confronted by its limits,
we may conclude that
ourselves, or our
children, or our
students, or our
employees, are simply not
smart enough, or not
gritty enough.
In fact, it's the way we
handle our mental
shortcomings, which are,
remember, endemic to our
species, that is the
problem.
Our approach constitutes an
instance of, as the poet
William Butler Yeats put it
in another context, the
will trying to do the
work of the imagination.
The smart move is not to
lean ever harder on the
brain, but to learn to
reach beyond it.
In The Middle Class
Gentleman, a comedy
written by the 17th
century French playwright
Moliere, the would-be
aristocrat Monsieur
Jourdain is delighted by a
realization that follows
upon his learning the
difference between prose
and verse.
By my faith, for more
than 40 years I have been
speaking prose without
knowing anything about it,
he exclaims.
Likewise, we may be
impressed to learn that we
have long been drawing
extra neural resources into
our thinking processes, that
we already think outside the
brain.
That's the good news.
The bad news is that we
often do it haphazardly,
without much intention or
skill.
It's no wonder this is the
case, our efforts at
education and training, as
well as management and
leadership, are aimed
almost exclusively at
promoting brain-bound
thinking.
Beginning in elementary
school, we are taught to
sit still, work quietly,
think hard, a model for
mental activity that will
prevail during all the
years that follow, through
high school and college and
into the workplace.
The skills we develop and the
techniques we are taught are
those that involve using our
heads, committing information
to memory, engaging in
internal reasoning and
deliberation, endeavoring to
self-discipline and
self-motivate.
Meanwhile, there's no
corresponding cultivation of
our ability to think outside
the brain.
No instruction, for instance,
in how to tune into the
body's internal signals,
sensations that can
profitably guide our choices
and decisions.
We're not trained to use
bodily movements and
gestures to understand
highly conceptual subjects
like science and
mathematics, or to come up
with novel and original
ideas.
Schools don't teach students
how to restore their
depleted attention with
exposure to nature and the
outdoors, or how to
arrange their study spaces so
that they extend
intelligent thought.
Teachers and managers
don't demonstrate how
abstract ideas can be
turned into physical
objects that can be
manipulated and
transformed in order to
achieve insights and
solve problems.
Employees aren't shown how
the social practices of
imitation and vicarious
learning can shortcut the
process of acquiring
expertise.
Classroom groups and
workplace teams aren't
coached in scientifically
validated methods of
increasing the collective
intelligence of their
members.
Our ability to think
outside the brain has been
left almost entirely
uneducated and
undeveloped.
This oversight is the
regrettable result of what
has been called our
neurocentric bias, that is,
our idealization and even
fetishization of the brain,
and our corresponding blind
spot for all the ways
cognition extends beyond
the skull.
As the comedian Emo
Phillips has remarked,
I used to think that the
brain was the most
wonderful organ in my
body, then I realized
who was telling me this.
Seen from another
perspective, however, this
near-universal neglect
represents an auspicious
opportunity, a world of
unrealized potential.
Until recently, science
shared the larger culture's
neglect of thinking outside
the brain, but this is no
longer the case.
Psychologists, cognitive
scientists and
neuroscientists, are now
able to provide a clear
picture of how extra
neural inputs shape the
way we think.
Even more promising, they
offer practical guidelines
for enhancing our
thinking through the use
of these outside-the-brain
resources.
Such developments are
unfolding against the
backdrop of a broader
shift in how we view the
mind, and by extension,
how we understand
ourselves.
But first, to gain a
sense of where we've
been and where we're
headed, it's worth taking
several steps back in
time to the moment when
our current ideas about
the brain were born.
On February 14, 1946, a
breathless bustle filled
the halls of the Moore
School of Electrical
Engineering in Philadelphia.
On this day, the school's
secret jewel was going to be
revealed to the world, the
ENIAC.
Inside a locked room at
Moore hummed the
electronic numerical
integrator and computer,
the first machine of its
kind capable of performing
calculations at lightning
speed.
Weighing 30 tons, the
massive ENIAC used around
18,000 vacuum tubes,
employed about 6,000
switches, and encompassed
upwards of half a million
soldered joints.
It had taken more than
200,000 man-hours to build.
The bust-sized contraption
was the brainchild of John
Mouchley and J. Presper
Eckert, Jr., two young
scientists at the
University of Pennsylvania,
Moore's parent institution.
With funding from the U.S.
Army, the ENIAC had been
developed for the purpose of
computing artillery
trajectories for American
gunners fighting the war in
Europe.
Compiling trajectory tables,
necessary for the effective
use of new weapons being
introduced by the military,
was a laborious process
requiring the service of
teams of human computers
working in shifts around the
clock.
A machine that could do their
job with speed and accuracy
would give the Army an
invaluable edge.
Now, six months after V-Day,
the demands of wartime were
giving way to the needs of an
expanding economy, and
Mouchley and Eckert had called
a press conference to
introduce their invention to
the world.
The two men had prepared for
the event with deliberate care
and no small amount of
stagecraft.
As the ENIAC chugged away at a
given task, some 300 neon
lights built into the machine's
accumulators flickered and
flashed.
Presper Eckert, known to all as
press, judged the effect of
these small bulbs
insufficiently impressive.
On the morning of the press
conference, he ran out and
purchased an armful of ping-pong
balls, each of which he cut in
half and marked with a number.
The plastic domes glued over
the neon bulbs now cast a
dramatic glow, especially once
the room's overhead lights were
dimmed.
At the appointed hour, the door to
the room that held the ENIAC was
opened, and a gaggle of officials,
academics, and journalists filed in.
Standing in front of the hulking
machine, lab member Arthur Burks
welcomed the group and sought to
impart to them a sense of the
moment's magnitude.
The ENIAC was engineered to carry
out mathematical operations, he
explained, and these operations, if
made to take place rapidly enough,
might in time solve almost any
problem.
Burks announced that he would begin
the day's demonstration by asking
the ENIAC to multiply 97,367 by
itself 5,000 times.
The reporters in the room bent over
their notepads.
Watch closely, you may miss it, he
warned, and pushed a button.
Before the newsmen had time to look
up, the task was complete, executed on
a punch card delivered to Burks' hand.
Next, Burks fed the machine a problem
like those for which it had been
designed.
The ENIAC would now calculate the
trajectory of a shell taking 30
seconds to travel from the gun to
its target.
Such a task would take a team of
human experts three days to
compute.
The ENIAC completed the job in 20
seconds, faster than the shell
itself could fly.
Jean Bartik, one of a group of
pioneering female engineers who
helped program the ENIAC, was on
hand for the demonstration.
She recalled,
it was unheard of that a machine
could reach such speeds of
calculation, and everyone in the
room, even the great mathematicians,
were in complete wonder and awe at
what they had just seen.
The next day, admiring accounts of
the ENIAC appeared in newspapers all
over the world.
Philadelphia, one of the war's top
secrets, an amazing machine which
applies electronic speeds for the
first time to mathematical tasks
hitherto too difficult and
cumbersome for solution, was
announced here tonight by the War
Department, the New York Times
reported on its front page.
The Times reporter, T.R. Kennedy,
Jr., sounded dazzled by what he'd
seen.
So clever is the device, he wrote,
that its creators have given up
trying to find problems so long that
they cannot be solved.
The introduction of the ENIAC was not
just a milestone in the history of
technology.
It was a turning point in the story of
how we understand ourselves.
In its early days, Mouchley and
Eckert's invention was frequently
compared to a human brain.
Newspaper and magazine articles
described the ENIAC as a giant
electronic brain, a robot brain, an
automatic brain, and a brain machine.
But before long, the analogy got
turned around.
It became commonplace that the brain
is like a computer.
Indeed, the cognitive revolution that
would sweep through American
universities in the 1950s and 1960s
was premised on the belief that the
brain could be understood as a flesh
and blood computing machine.
The first generation of cognitive
scientists took seriously the idea that
the mind is a kind of computer, notes
Brown University professor Stephen
Sloman.
Thinking was assumed to be a kind of
computer program that runs in
people's brains.
Since those early days at the dawn of
the digital age, the brain-computer
analogy has become only more
pervasive and more powerful, engaged
not just by researchers and academics,
but by the rest of us, the public at
large.
The metaphor provides us with a model,
sometimes conscious, but often
implicit, of how thinking works.
The brain, according to this analogy,
is a self-contained information
processing machine, sealed inside the
skull as the ENIAC was sequestered in
its locked room.
From this inference emerges a second.
The human brain has attributes akin to
gigabytes of RAM and megahertz of
processing speed that can be easily
measured and compared.
Following on these is the third and
perhaps most significant supposition
of all, that some brains, like some
computers, are just better.
They possess the biological equivalent of
more memory storage, greater processing
power, higher resolution screens.
To this day, the computer metaphor
dominates the way we think and talk
about mental activity, but it's not the
only one that shapes our notion of the
brain.
A half-century after the ENIAC was
unveiled, another analogy rose to
prominence.
New research shows that the brain can be
developed like a muscle, read the headline
of the news article set in bold type.
The year was 2002, and Lisa Blackwell, a graduate
student at Columbia University working with
psychology professor Carol Dweck, was handing
out copies of the article to a classroom full of
seventh graders at a public school in New York
City.
Dweck and Blackwell were testing a new theory,
investigating the possibility that the way we
conceptualize the brain can affect how well we think.
The study's protocol required Blackwell to guide
the students through eight informational
sessions.
In this, the third session in the sequence,
students were to take turns reading the text of
the article aloud.
Many people believe that a person is born either
smart, average, or dumb, and stays that way, one
student began.
But new research shows that the brain is more
like a muscle.
It changes and gets stronger when you use it.
Another student picked up the thread.
Everyone knows that when you lift weights, your
muscles get bigger and you get stronger.
A person who can't lift 20 pounds when they start
exercising can get strong enough to lift 100 pounds
after working out for a long time.
That's because the muscles become larger and stronger
with exercise.
And when you stop exercising, the muscles shrink
and you get weaker.
That's why people say, use it or lose it.
A giggle rippled through the room.
But, a third pupil read on, most people don't know that
when they practice and learn new things, parts of their
brain change and get larger, a lot like muscles do when
they exercise.
Dweck's idea, which she initially called the incremental
theory of intelligence, would eventually become known to
the world as the growth mindset, the belief that concerted mental
effort could make people smarter, just as vigorous physical effort
could make people stronger.
As she and her colleagues wrote in an account of their early research
in schools,
the key message was that learning changes the brain by forming new
connections, and that students are in charge of this process.
From these beginnings, growth mindset became a popular phenomenon,
spawning a book, Mindset, that has sold millions of copies,
and inspiring an untold number of speeches, presentations, and workshops
delivered to corporate and organizational audiences, as well as to
students and teachers.
At the center of it all is a metaphor, the brain as muscle.
The mind in this analogy is akin to a biceps or a quadriceps, a physical
entity that varies in strength among individuals.
The comparison has been incorporated into another hugely popular concept
originating in academic psychology, grit.
Angela Duckworth, the University of Pennsylvania psychologist who defines grit
as perseverance and passion for long-term goals, echoes Dweck in her own book.
Like a muscle that gets stronger with use, the brain changes itself when you
struggle to master a new challenge, she wrote in the best-selling Grit published
in 2016.
The emphasis in grit on mustering more of one's own internal resources makes the
brain as muscle analogy a perfect fit.
The comparison is made even more explicitly by purveyors of so-called
cognitive fitness exercises, which have drawn millions of hopeful users under names
like CogniFit and Brain Gym.
So pervasive is the metaphor that some scientists concerned about the spread of
neuromyths, common misconceptions about the brain, have begun to point out that the
brain is not actually a muscle, but rather an organ made up of specialized cells,
known as neurons.
These two metaphors, brain as computer and brain as muscle, share some key assumptions.
To wit, the mind is a discrete thing that is sealed inside the skull.
This discrete thing determines how well people are able to think.
This thing has stable properties that can easily be measured, compared, and ranked.
Such assumptions feel comfortably familiar.
Indeed, they weren't particularly novel even at the moment they were first proposed.
For centuries, brains had been likened to machines, to whichever appliance of the time
appeared most advanced—a hydraulic pump, a mechanical clock, a steam engine, a telegraph
machine.
In a lecture delivered in 1984, philosopher John Searle noted,
Because we do not understand the brain very well, we are constantly tempted to use the latest
technology as a model for trying to understand it.
In my childhood, we were always assured that the brain was a telephone switchboard.
Teachers, parents, and other adults all proffered the metaphor of brain as switchboard,
recounted Searle, for what else could it be?
Brains had also long been likened to muscles that could be strengthened with exercise, a
theme promulgated, for example, by physicians and health experts in the 19th and early 20th
centuries.
In his first book in physiology and hygiene, published in 1888, Dr. John Harvey Kellogg made
an argument that sounds very much like Carol Dweck's.
What do we do when we want to strengthen our muscles?
We make them work hard every day, do we not?
Kellogg inquired of his intended youthful readership.
The exercise makes them grow large and strong.
It is just the same with our brains.
If we study hard and learn our lessons well, then our brains grow strong and study becomes
easy.
Entrenched historical foundations support these metaphors.
They rest on deep cultural underpinnings as well.
The computer and muscle analogies fit neatly with our society's emphasis on individualism,
its insistence that we operate as autonomous, self-contained beings in possession of capacities
and competencies that are ours alone.
These comparisons also readily conform to our culture's penchant for thinking in terms of
good, better, best.
Scientist and author Stephen Jay Gould once included in his list of the oldest issues and errors
of our philosophical traditions, our persistent inclination to order items by ranking them in a
linear series of increasing worth.
Computers may be slow or fast.
Muscles may be weak or strong.
And so it goes, we assume, with our own and others' minds.
There even appear to be hardwired psychological factors underlying our embrace of these ideas
about the brain.
The belief that some core quantity of intelligence resides within each of our heads fits with a
pattern of thought, apparently universal in humans, that psychologists call essentialism.
That is, the conviction that each entity we encounter possesses an inner essence that makes
it what it is.
Essentialism shows up in every society that has been studied, notes Yale University psychology
professor Paul Bloom.
It appears to be a basic component of how we think about the world.
We think in terms of enduring essences rather than shifting responses to external influences.
Because we find such essences easier to process mentally, as well as more satisfying emotionally,
from the essentialist perspective, people simply are intelligent or they are not.
Together, the historical, cultural, and psychological bases of our assumptions about the mind, that
its properties are individual, inherent, and readily ranked according to quality, give them
a powerful punch.
Such assumptions have profoundly shaped the views we hold on the nature of mental activity,
on the conduct of education and work, and on the value we place on ourselves and others.
It's therefore startling to contemplate that the whole lot of it could be misconceived.
To grasp the nature of this error, we need to consider another metaphor.
On the morning of April 18, 2019, computer screens went dark across a swath of Seoul, South Korea's
largest city.
Lights flickered out in schools and offices across the 234-square-mile metropolis, home to some
10 million people.
Stoplights at street intersections blinked off, and electric-powered trains slowed to a halt.
The cause of the blackout was as small in scale as its effects were widespread, a power outage
caused by magpies, the black- and white-feathered birds who build their nests on utility poles
and transmission towers.
Magpies, members of the Corvid family, which also includes crows, jays, and ravens, are well-known
for making their nests out of whatever is available in the environment.
The birds have been observed using an astonishing array of materials, not only twigs, string, and moss,
but also dental floss, fishing line, and plastic Easter grass, chopsticks, spoons, and drinking straws,
shoelaces, eyeglass frames, and croquet wickets.
During the American Dust Bowl of the 1930s, which eliminated vegetation from huge swaths of the West,
magpies' corvid cousins made nests out of barbed wire.
The densely packed urban neighborhoods of modern-day Seoul feature few trees or bushes,
so magpies use what they can find—metal clothes hangers, TV antennas, and lengths of steel wire.
These materials conduct electricity, and so when the birds build their nests on the city's tall
electrical transmission towers, the flow of electricity is regularly disrupted.
According to KEPCO, the Korea Electric Power Corporation,
magpies are responsible for hundreds of power outages annually in areas all across the country.
Each year, KEPCO employees work to remove upwards of 10,000 nests,
but just as quickly the magpies build them up again.
Magpies may pose a headache for power companies,
but their activity supplies a felicitous analogy for the way the mind works.
Our brains, it might be said, are like magpies,
fashioning their finished products from the materials around them,
weaving the bits and pieces they find into their trains of thought.
Set beside the brain-as-computer and brain-as-muscle metaphors,
it's apparent that the brain-as-magpie is a very different kind of analogy,
with very different implications for how mental processes operate.
For one thing, thought happens not only inside the skull, but out in the world, too.
It's an act of continuous assembly and reassembly that draws on resources external to the brain.
For another, the kinds of materials available to think with
affect the nature and quality of the thought that can be produced.
And last, the capacity to think well, that is, to be intelligent,
is not a fixed property of the individual,
but rather a shifting state that is dependent on access to extra-neural resources
and to the knowledge of how to use them.
This is, admittedly, a radically new way of thinking about thinking.
It may not feel easy or natural to adopt,
but a growing mass of evidence generated within several scientific disciplines
suggests that it's a much more accurate rendering of how human cognition actually works.
Moreover, it's a gratifyingly generative conceptualization,
because it offers so many practical opportunities for improving how well we think.
It has arrived just in time.
Recasting our model of how the mind functions has lately become an urgent necessity,
as we find ourselves increasingly squeezed by two opposing forces.
We need ever more to think outside the brain,
even as we have become ever more stubbornly committed to the brain-bound approach.
First, as to that growing need to think outside the brain,
as many of us can readily recognize,
in the accelerated pace of our days and the escalating complexity of our duties at school and work,
the demands on our thinking are ratcheting up.
There's more information we must deal with.
The information we have to process is coming at us faster,
and the kind of information we must deal with is increasingly specialized and abstract.
This difference in kind is especially significant.
The knowledge and skills that we are biologically prepared to learn have been outstripped
by the need to acquire a set of competencies that come far less naturally
and are acquired with far more difficulty.
David Geary, a professor of psychology at the University of Missouri,
makes a useful distinction between biologically primary and biologically secondary abilities.
Human beings, he points out, are born ready to learn certain things.
How to speak the language of the local community.
How to find their way around a familiar landscape.
How to negotiate the challenges of small group living.
We are not born to learn the intricacies of calculus
or the counterintuitive rules of physics.
We did not evolve to understand the workings of the financial markets
or the complexities of global climate change.
And yet we dwell in a world where such biologically secondary capacities
hold the key to advancement, even survival.
The demands of the modern environment have now met
and exceeded the limits of the biological brain.
For a time, it's true, humanity was able to keep up
with its own ever-advancing culture,
resourcefully finding ways to use the biological brain better.
As their everyday environments grew more intellectually demanding,
people responded by upping their cognitive game.
Continual engagement with the mental rigors of modern life,
along with improving nutrition, rising living conditions,
and reduced exposure to infectious disease and other pathogens,
produced a century-long climb in average IQ score,
as measured by intelligence tests taken by people all over the globe.
But this upward trajectory is now leveling off.
In recent years, IQ scores have stopped rising,
or have even begun to drop,
in countries like Finland, Norway, Denmark, Germany, France, and Britain.
Some researchers suggest that we have now pushed our mental equipment
as far as it can go.
It may be that our brains are already working at near-optimal capacity,
note Nicholas Fitz and Peter Reiner, writing in the journal Nature.
Efforts to wrest more intelligence from this organ, they add,
bump up against the hard limits of neurobiology.
As if to protest this unwelcome truth,
attempts to subvert such limits have received growing attention in recent years.
Commercial brain training regimens like Cogmed, Lumosity, and Brain HQ
have attracted many who desire to improve their memory and increase their focus.
Lumosity alone claims 100 million registered users in 195 countries.
At the same time, so-called neuro-enhancement,
innovations like smart pills and electrical brain stimulation
that claim to make their users more intelligent,
have drawn breathless media coverage,
as well as extensive investment from pharmaceutical and biotechnology companies.
So far, however,
these approaches have yielded little more than disappointment and dashed hopes.
A team of scientists who set out to evaluate all the peer-reviewed intervention studies
cited on the websites of leading brain training companies
could find little evidence within those studies
that training improves everyday cognitive performance.
Engaging in brain training does improve users' performance,
but only on exercises highly similar to the ones they've been practicing.
The effect does not seem to transfer to real-life activities involving attention and memory.
A 2019 study of Cogmed concluded that such transfer is rare or possibly inexistent.
A 2017 study of Lumosity determined that
training appears to have no benefits in healthy young adults.
Similarly, dismal results have been reported for older individuals.
In 2016, Lumosity was forced to pay a $2 million fine for deceptive advertising
to the U.S. Federal Trade Commission.
Smart pills haven't fared much better.
A clinical trial of one nootropic drug popular among Silicon Valley tech workers
found that a cup of coffee was more effective at boosting memory and attention.
Medications and technologies that might someday actually enhance intelligence
remain in the early stages of laboratory testing.
The best way, and at least for now, the only way for us to get smarter
is to get better at thinking outside the brain.
Yet we dismiss or disparage this kind of cognition
to the extent that we consider it at all.
Our pronounced bias in favor of brain-bound thinking
is long-standing and well-entrenched,
but a bias is all it is,
and one that can no longer be supported or sustained.
The future lies in thinking outside the brain.
We can better grasp the future of thinking outside the brain
by taking a look back at the time when the idea first emerged.
In 1997, Andy Clark,
then a professor of philosophy at Washington University in St. Louis, Missouri,
left his laptop behind on a train.
The loss of his usually ever-present computer hit him, he later wrote,
like a sudden and somewhat vicious type of hopefully transient brain damage.
He was left dazed, confused, and visibly enfeebled,
the victim of the cyborg equivalent of a mild stroke.
The experience, distressing as it was,
provided fodder for a notion he'd been pondering for some time.
His computer, he realized,
had in a sense become a part of his mind,
an integral element of his thinking processes.
His mental capacities were effectively extended
by the use of his laptop,
allowing his brain to overachieve,
to think more efficiently and effectively,
more intelligently,
than it could without the device.
His brain, plus his computer,
equaled his mind,
extended.
Two years earlier,
Clark and his colleague David Chalmers
had co-authored an article
that named and described just this phenomenon.
Their paper, titled The Extended Mind,
began by posing a question
that would seem to have an obvious answer.
Where does the mind stop
and the rest of the world begin,
it asked.
Clark and Chalmers went on
to offer an unconventional response.
The mind does not stop
at the standard
demarcations of skin and skull,
they argued.
Rather, it is more accurately viewed
as an extended system,
a coupling of biological organism
and external resources.
A recognition of this reality,
they acknowledged,
will have significant consequences
in terms of philosophical views of the mind,
but also in moral and social domains.
The authors were aware
that the vision they were setting out
would require a thorough reimagining
of what people are like
and how they function,
a reimagining they saw
as necessary and right.
Once the hegemony of skin and skull
is usurped,
they concluded,
we may be able to see ourselves
more truly as creatures of the world.
The world, at first,
was not so sure.
Before being published
in Analysis in 1998,
the paper received rejections
from three other journals.
Once in print,
the extended mind was greeted
with perplexity
and no small amount of derision.
But the idea it proposed
turned out to have surprising power
within the academy
and well beyond it.
What at first appeared radical
and out there
quickly came to seem less so
as daily life in the digital age
provided a continuous
proof-of-concept demonstration
of people extending their minds
with their devices.
Initially derided as wacky,
the notion of the extended mind
came to seem eminently plausible,
even prescient.
In the more than 20 years
since the publication
of the extended mind,
the idea it introduced
has become an essential
umbrella concept
under which a variety
of scientific subfields
have gathered.
Embodied cognition,
situated cognition,
distributed cognition,
each of these takes up
a particular aspect
of the extended mind,
investigating how our thinking
is extended
by our bodies,
by the spaces
in which we learn and work,
and by our interactions
with other people.
Such research
has not only produced
new insights
into the nature
of human cognition,
it has also generated
a corpus of evidence-based
methods
for extending the mind.
That's where this book
comes in.
It aims to operationalize
the extended mind,
to turn this philosophical
Sally into something
practically useful.
In chapter one,
we'll learn how to tune
into our interoception,
the sensations that arise
from within the body,
and how to use these signals
to make sounder decisions.
In chapter two,
we'll find out how moving
our bodies can nudge
our minds towards
deeper understanding.
Chapter three looks at
how the gestures
we make with our hands
can bolster our memory.
Chapter four examines
how time spent
in natural spaces
can restore
our depleted attention.
In chapter five,
we'll see how built spaces,
the interiors we inhabit
at school and at work,
can be designed
to promote creativity.
In chapter six,
we will explore
how moving our thoughts
out of our heads
and into the space of ideas
can lead us to new insights
and discoveries.
Chapter seven probes
how we can think
with the minds of experts.
Chapter eight considers
how we can think
with classmates,
colleagues,
and other peers.
Finally,
in chapter nine,
we'll examine
how groups thinking together
can become more
than the sum
of their members.
Across these varied
instantiations
of the extended mind,
several common themes
are apparent.
The first of these
concerns the source
of Andy Clark's
initial inspiration,
the role of technology
in extending our thinking.
Our devices can
and do extend
our minds,
of course,
but not always.
Sometimes they lead us
to think less intelligently,
as anyone who's been
distracted by clickbait
or misled by a GPS system
can tell you.
The failure of our technology
to consistently
enhance our intelligence
has to do with a metaphor
we encountered earlier
in this introduction,
the computer as brain.
Too often,
those who design
today's computers
and smartphones
have forgotten
that users
inhabit biological bodies,
occupy physical spaces,
and interact
with other human beings.
Technology itself
is brain-bound.
But by the same token,
technology itself
could be extended,
broadened to include
the extra-neural resources
that do so much
to enrich the thinking
we do
in the offline world.
In each of the chapters
that follow,
we'll encounter examples
of such
extended technology,
from an online
foreign language
learning platform
that encourages
its users
to make gestures
and not just repeat words,
to a Waze-like app
that plots
not the fastest route,
but the one
most filled
with nature's greenery,
to a video game
that induces players
to look
not at the screen,
but at one another,
synchronizing their movements
in pursuit
of a shared experience.
A second theme
to emerge
from a review
of research
on the extended mind
is its distinctive take
on the nature
of expertise.
Traditional notions
of what makes an expert
are highly brain-bound,
focused on internal,
individual effort.
Think of the late psychologist
Anders Ericsson's
famous finding
that mastery in any field
requires 10,000 hours
of practice.
The literature
on the extended mind
suggests a different view.
Experts are those
who have learned
how best to marshal
and apply
extra-neural resources
to the task
before them.
This alternative
perspective
has real implications
for how we understand
and cultivate
superior performance.
For example,
although the conventional
take on expertise
highlights economy,
efficiency,
and optimality
of action,
geniuses
and superstars
just do it.
Research in the vein
of the extended mind
finds that experts
actually do more
experimenting,
more testing,
and more backtracking
than beginners.
They are more apt
than novices
to make skillful use
of their bodies,
of physical space,
and of relationships
with others.
In most scenarios,
researchers have found,
experts are less likely
to use their heads
and more inclined
to extend their minds,
a habit that the rest
of us can learn
to emulate
on our way
to achieving mastery.
Finally,
in surveying the study
of the extended mind,
there's one more theme
that is impossible
to ignore,
the matter of what
we might call
extension inequality.
Our schools,
our workplaces,
the very structure
of our society,
are based on the assumption
that some people
are able to think
more intelligently
than others.
The reason for such
individual differences
is taken as self-evident.
Obviously,
it's because those people
are smarter,
because they have more
of the stuff called
intelligence
inside their heads.
Research on the extended mind
points to a different
explanation.
That is,
some people are able
to think more intelligently
because they are better
able to extend
their minds.
They may have more knowledge
about how mental extension
works,
the kind of knowledge
that this book
aims to make accessible.
But it's also indisputable
that the extensions
that allow us
to think well,
the freedom to move
one's body,
say,
or the proximity
of natural green spaces,
control over one's
personal workspace,
or relationships
with informed experts
and accomplished peers,
are far from
equally distributed.
When listening to the chapters
that follow,
we should keep in mind
the way access
or lack of access
to mental extensions
might be shaping
the thinking
of our students,
employees,
co-workers,
and fellow citizens.
Metaphors are powerful
and none more so
than the ones we use
to understand
our own minds.
The value of the approach
described in this audiobook
ultimately lies
in the novel analogy
it offers,
an analogy we can apply
to our everyday efforts
to learn and remember,
to solve problems
and imagine possibilities.
We extend beyond our limits,
not by revving our brains
like a machine
or bulking them up
like a muscle,
but by strewing our world
with rich materials
and by weaving them
into our thoughts.
Part 1
Thinking with Our Bodies
Chapter 1
Thinking with Sensations
During his years
of working as a financial trader
at Goldman Sachs,
Merrill Lynch,
and Deutsche Bank,
John Coates watched it happen
again and again.
Using my best analytical efforts
drawing on my education
Coates has a PhD in economics
from the University of Cambridge
and a wide reading
of economic reports
and statistics,
he would devise
a brilliant trade,
one that was impeccable
in its logic
and unassailable
in its reasoning.
And it would lose money
every time.
Then there were other occasions
equally puzzling.
I would catch a glimpse
with peripheral vision
of another possibility,
another path
into the future.
It showed up
as a mere blip
in my consciousness,
a momentary tug
on my attention,
but it was a flash
of insight
coupled with a gut feeling
that gave it the imprimatur
of the highly probable.
When he obeyed
these gut feelings,
Coates found,
he was usually rewarded
with a profitable outcome.
Against all his assumptions,
all his training,
Coates was forced
to arrive
at an unconventional conclusion.
Good judgment
may require the ability
to listen carefully
to feedback
from the body.
Further,
he observes,
some people may be better
at this than others.
On any Wall Street
trading floor,
you will find
high-IQ,
Ivy League-educated stars
who cannot make
any money at all
for all their convincing analyses.
While across the aisle
sits a trader
with an undistinguished degree
from an unknown university
who cannot keep up
with the latest analytics,
but who consistently
prints money
to the bafflement
and irritation
of his seemingly
more gifted colleagues.
It is possible,
muses Coates,
though odd
to contemplate
that the better judgment
of the money-making trader
may owe something
to his or her ability
to produce bodily signals
and equally
to listen to them.
Coates shares
these reflections
in a captivating book,
The Hour Between Dog
and Wolf,
which draws on his years
as a trader
as well as on his
surprising second career
as an applied physiologist.
Over time,
the questions generated
by his work in finance,
could we tell
whether one person
has better gut feelings
than another?
Could we monitor feedback
from their bodies?
became more compelling
than the work itself,
and Coates left Wall Street
to pursue the answers
in scientific research.
He presented the fruits
of his inquiry
in 2016,
detailing the results
of a collaboration
with academic neuroscientists
and psychiatrists
in the journal
Scientific Reports.
Coates and his new colleagues
examined a group
of financial traders
working on a London
trading floor,
asking each one
to identify
the successive moments
when he felt
his heart beat,
a measure of the individual's
sensitivity
to bodily signals.
The traders,
they found,
were much better
at this task
than were an age
and gender-matched
group of controls
who did not work
in finance.
What's more,
among the traders
themselves,
those who were
the most accurate
in detecting
the timing
of their heartbeats
made more money
and tended
to have longer tenures
in what was
a notably
volatile line of work.
Our results
suggest that
signals from the body,
the gut feelings
of financial lore,
contribute to success
in the markets,
the team concluded.
Confirming Coates'
informal observations,
those who thrived
in this milieu
were not necessarily
people with greater
education or intellect,
but rather,
people with greater
sensitivity
to interoceptive
signals.
Interoception
is,
simply stated,
an awareness
of the inner state
of the body.
Just as we have
sensors that take
in information
from the outside world,
retinas,
cochleas,
taste buds,
olfactory bulbs,
we have sensors
inside our bodies
that send our brains
a constant flow
of data from within.
These sensations
are generated
in places
all over the body,
in our internal organs,
in our muscles,
even in our bones,
and then travel
via multiple pathways
to a structure
in the brain
called the insula.
Such internal reports
are merged
with several other
streams of information,
our active thoughts
and memories,
sensory inputs
gathered from
the external world,
and integrated
into a single snapshot
of our present condition,
a sense of
how I feel
in the moment,
as well as
a sense of
the actions
we must take
to maintain
a state
of internal balance.
All of us
experience
these bodily signals,
but some of us
feel them
more keenly
than others.
To measure
interoceptive awareness,
scientists apply
the heartbeat
detection test,
the one that
John Coates used
with his group
of financial traders.
Test takers
are asked
to identify
the instant
when their heart
beats,
without placing
a hand
on the chest
or resting
a finger
on a wrist.
Researchers
have found
a surprisingly
wide range
in terms
of how people
score.
Some individuals
are interoceptive
champions,
able to determine
accurately
and consistently
when their
heartbeats happen.
Others are
interoceptive duds,
they can't feel
the rhythm.
Few of us
are aware
that this spectrum
of ability
even exists,
much less
where we
fall on it.
So preoccupied
are we
with more
conventionally
brain-bound
capacities.
We may
remember
down to the
point
our SAT
scores
or our
high school
GPA,
but we haven't
given this
particular aptitude
a moment's
thought.
Vivian Ainley
recalls a clear
demonstration
of this
common oversight.
Ainley,
an interoception
researcher
at Royal Holloway
University of London,
was administering
the heartbeat
detection test
to members
of the public
as part of
an exhibit
at London
Science Museum.
Visitors to
the exhibit
were instructed
to place a
finger on a
sensor that
detected their
pulse.
The readout
of the sensor
was visible
only to
Ainley.
Please tell
me when
your heart
beats,
she would
say to
each patron
who stepped
forward.
An elderly
couple who
stopped by
the booth
had very
different
reactions
to
Ainley's
request.
How on
earth would
I know
what my
heart
is doing?
The woman
asked
incredulously.
Her husband
turned and
stared at
her,
equally
dumbfounded.
But of
course you
know,
he exclaimed.
Don't be
so stupid.
Everyone
knows what
their heartbeat
is.
He had
always been
able to
hear his
heart,
and she
had never
been able
to hear
hers,
Ainley
observed in
an interview,
smiling at
the memory.
They had
been married
for decades,
but they
had never
talked of
or even
recognized
this difference
between
them.
Though we
may not
notice such
differences,
they are
real,
and even
visible to
scientists using
brain scanning
technology.
The size and
activity level of
the brain's
interoceptive
hub,
the insula,
vary among
individuals and
are correlated
with their
awareness of
interoceptive
sensations.
How such
differences arise
in the first
place is not
yet known.
All of us
begin life with
our interoceptive
capacities already
operating,
interoceptive
awareness continues
to develop
across childhood
and adolescence.
Differences in
sensitivity to
internal signals
may be influenced
by genetic
factors as well
as by the
environments in
which we
grow up,
including the
communications we
receive from
caregivers about
how we should
respond to our
bodily prompts.
What we do
know is that
interoceptive
awareness can be
deliberately
cultivated.
A series of
simple exercises
can put us in
touch with the
messages emanating
from within,
giving us access
to knowledge that
we already possess
but that is
ordinarily excluded
from consciousness.
Knowledge about
ourselves, about
other people, and
about the worlds
through which we
move.
Once we establish
contact with this
informative internal
source, we can
make wise use of
what it has to
tell us, to
make sounder
decisions, for
example, to
respond more
resiliently to
challenges and
setbacks, to
savor more fully
the intensity of
our emotions while
also managing them
more skillfully, and
to connect to
others with more
sensitivity and
insight.
The heart and
not the head
leads the way.
To understand how
interoception can
act as such a rich
repository,
it's important to
recognize that the
world is full of
far more information
than our conscious
minds can process.
Fortunately, we're
also able to
collect and store
the volumes of
information we
encounter on a
non-conscious basis.
As we proceed
through each day, we
are continuously
apprehending and
storing regularities
in our experience,
tagging them for
future reference.
Through this
information gathering
and pattern
identifying process,
we come to know
things, but we're
typically not able to
articulate the
content of such
knowledge, or to
ascertain just how we
came to know it.
This trove of data
remains mostly under
the surface of
consciousness, and
that's usually a good
thing.
Its submerged status
preserves our limited
stores of attention and
working memory for
other uses.
A study led by
cognitive scientist
Powell Lewicki
demonstrates this
process in
microcosm.
Participants in
Lewicki's experiment
were directed to
watch a computer
screen on which a
cross-shaped target
would appear, then
disappear, then
reappear in a new
location.
Periodically, they
were asked to
predict where the
target would show
up next.
Over the course of
several hours of
exposure to the
target's movements,
movements, the
participants'
predictions grew
more and more
accurate.
They had figured
out the pattern
behind the target's
peregrinations.
But they could not
put this knowledge
into words, even
when the experimenters
offered them money
to do so.
The subjects were
not able to
describe anything
even close to the
real nature of the
pattern, Lewicki
observes.
The movements of
the target operated
according to a
pattern too
complex for the
conscious mind to
accommodate, but
the capacious realm
that lies below
consciousness was
more than roomy
enough to contain
it.
Non-conscious
information acquisition,
as Lewicki calls
it, along with the
ensuing application of
such information, is
happening in our
lives all the time.
As we navigate a
new situation, we're
scrolling through our
mental archive of
stored patterns from
the past, checking
for ones that apply
to our current
circumstances.
We're not aware
that these searches
are underway.
As Lewicki
observes, the
human cognitive
system is not
equipped to handle
such tasks on the
consciously controlled
level.
He adds, our
conscious thinking
needs to rely on
notes and flow
charts and lists of
if-then statements,
or on computers,
to do the same job
which our
non-consciously
operating processing
algorithms can do
without external
help, and
instantly.
But, if our
knowledge of these
patterns is not
conscious, how
then can we make
use of it?
The answer is that
when a potentially
relevant pattern is
detected, it's our
interoceptive faculty
that tips us off,
with a shiver or a
sigh, a quickening of
the breath or a
tensing of the
muscles.
The body is rung
like a bell to
alert us to this
useful and otherwise
inaccessible
information.
Though we
typically think of
the brain as
telling the body
what to do, just
as much does the
body guide the
brain with an
array of subtle
nudges and
prods.
One psychologist
has called this
guide our
somatic rudder.
Researchers have
even captured the
body in mid-nudge
as it alerts its
inhabitant to the
appearance of a
pattern that she
may not have
known she was
looking for.
Such interoceptive
prodding was
visible during a
gambling game that
formed the basis of
an experiment led
by neuroscientist
Antonio Damasio, a
professor at the
University of
Southern California.
In the game,
presented on a
computer screen,
players were given
a starting purse of
$2,000 and were
shown four decks of
digital cards.
Their task, they
were told, was to
turn the cards in
the decks face-up,
choosing which decks
to draw from such
that they would lose
the least amount of
money and win the
most.
As they started
clicking to turn
over cards, players
began encountering
rewards, bonuses of
$50 here, $100 there,
and also penalties, in
which small or large
amounts of money were
taken away.
What the experimenters
had arranged, but the
players were not told,
was that decks A and
B were bad.
They held lots of
large penalties in
store, and decks C and
D were good, bestowing
more rewards than
penalties over time.
As they played the
game, the participant's
state of physiological
arousal was monitored
via electrodes attached
to their fingers.
These electrodes kept
track of their level of
skin conductance.
When our nervous
systems are stimulated
by an awareness of
potential threat, we
start to perspire in a
barely perceptible way.
This slight sheen of
sweat momentarily turns
our skin into a better
conductor of
electricity.
Researchers can thus
use skin conductance
as a measure of
nervous system arousal.
Looking over the data
collected by the skin
sensors, Damasio and
his colleagues noticed
something interesting.
After the participants
had been playing for a
short while, their skin
conductance began to
spike when they
contemplated clicking on
the bad decks of
cards.
Even more striking, the
players started avoiding
the bad decks, gravitating
increasingly to the good
decks.
As in the Lewicki study,
subjects got better at the
task over time, losing
less and winning more.
Yet interviews with the
participants showed that
they had no awareness of
why they had begun choosing
some decks over others,
until late in the game,
long after their skin
conductance had started
flaring.
By card 10, about 45
seconds into the game,
measures of skin
conductance showed that
their bodies were wise to
the way the game was
rigged.
But even 10 turns later,
on card 20, all
indicated that they did
not have a clue about what
was going on, the
researchers noted.
It took until card 50 was
turned, and several
minutes had elapsed, for
all the participants to
express a conscious hunch
that decks A and B were
riskier.
Their bodies figured it out
long before their brains
did.
Subsequent studies supplied
an additional, and
crucial, finding.
Players who were more
interoceptively aware were
more apt to make smart
choices within the game.
For them, the body's wise
counsel came through loud
and clear.
Demasio's fast-paced game
shows us something
important.
The body not only grants
us access to information
that is more complex than
what our conscious minds
can accommodate.
It also marshals this
information at a pace that
is far quicker than our
conscious minds can handle.
The benefits of the body's
intervention extend well
beyond winning a card game.
The real world, after all,
is full of dynamic and
uncertain situations in which
there is no time to ponder
all the pros and cons.
If we rely on the conscious
mind alone, we lose.
Here, then, is a reason to
hone our interoceptive sense.
People who are more aware of
their bodily sensations are
better able to make use of
their non-conscious knowledge.
Mindfulness meditation is one
way of enhancing such
awareness.
The practice has been found to
increase sensitivity to
internal signals, and even to
alter the size and activity of
that key brain structure, the
insula.
One particular component
appears to be especially
effective.
This is the activity that often
starts off a meditation session
known as the body scan.
Rooted in the Buddhist
traditions of Myanmar, Thailand,
and Sri Lanka, the body scan was
introduced to Western audiences by
mindfulness pioneer John
Kabat-Zinn, now Professor
Emeritus at the University of
Massachusetts Medical School.
People find the body scan
beneficial because it
reconnects their conscious mind
to the feeling states of their
bodies, says Kabat-Zinn.
By practicing regularly, people
usually feel more in touch with
sensations in parts of their body
they had never felt or thought
much about before.
To practice the body scan, he
explains, we should first sit or
lie down in a comfortable place,
allowing our eyes to close
gently.
He recommends taking a few
moments to feel the body as a
whole and to sense the rising and
falling of the abdomen with each
in-breath and out-breath.
We then begin a sweep of the body,
starting with the toes of the left
foot.
Advises Kabat-Zinn,
As you direct your attention to
your toes, see if you can
channel your breathing to them
as well, so that it feels as if
you are breathing into your toes
and out from your toes.
After focusing on the toes for a
few breaths, we shift our
attention to the sole of our foot,
the heel, the ankle, and so on up
to the left hip.
The same procedure is repeated for
the right leg, focusing on each
section for the length of a few
breaths.
The roving spotlight of our
attention now travels up through
the torso, the abdomen, and chest,
the back and shoulders, then down
each arm to the elbows, wrists,
and hands.
Finally, the attentional spotlight
moves up through the neck and face.
If our attention should wander during
the exercise, we can gently guide it
back to the part of the body that is
the object of focus.
Kabat-Zinn recommends doing the body
scan at least once a day.
The aim of this practice is to bring
non-judgmental awareness to any and
all feelings that arise within the
body.
In the rush of everyday life, we may
ignore or dismiss these internal
signals.
If they do come to our notice, we may
react with impatience or self-criticism.
The body scan trains us to observe
such sensations with interest and
equanimity.
But tuning into these feelings is only
a first step.
The next step is to name them.
Attaching a label to our
interoceptive sensations allows us to
begin to regulate them.
Without such attentive self-regulation,
we may find our feelings overwhelming,
or we may misinterpret their source.
Research shows that the simple act of
giving a name to what we're feeling
has a profound effect on the nervous
system, immediately dialing down the
body's stress response.
In an experiment conducted by researchers
at the University of California, Los
Angeles, study subjects were required to
give a series of impromptu speeches in
front of an audience, a reliable way to
induce anxiety.
Half of the participants were then asked
to engage in what the researchers call
affect labeling, filling in responses to
the prompt, I feel blank, while the other
half were asked to complete a neutral
shape-matching task.
The affect labeling group showed steep
declines in heart rate and skin
conductance compared to the control group,
whose levels of physiological arousal
remained high.
Brain scanning studies offer further
evidence of the calming effect of
affect labeling.
Simply naming what is felt reduces
activity in the amygdala, the brain
structure involved in processing fear
and other strong emotions.
Meanwhile, thinking in a more involved
way about feelings and the experiences
that evoked them actually produces greater
activity in the amygdala.
The practice of affect labeling, like the
body scan, is a kind of mental training
intended to get us in the habit of
noting and naming the sensations that
arise in our bodies.
Psychologists recommend keeping two things
in mind as we try it out.
The first is to be as prolific as possible.
The UCLA scientists reported that study
participants who came up with a larger
number of terms for what they were
feeling subsequently experienced a greater
reduction in their physiological arousal.
The second is to be as granular as
possible, that is, to choose words that
are precise and specific when describing
what we feel.
Accurately distinguishing among
interoceptive sensations is associated
with making sounder decisions, acting less
impulsively, and planning ahead more
successfully, perhaps because it gives us a
clearer sense of what we need and what we
want. Sensing and labeling our internal
sensations allows them to function more
efficiently as our somatic rudder, steering a
nimble course through the many decisions of
our days. But does the body really have
anything to contribute to our thinking, to
processes we usually regard as taking place
solely in our heads? It does. In fact, recent
research suggests a rather astonishing
possibility. The body can be more rational
than the brain. Recall that in the study
conducted by John Coates, traders with
keener interoceptive awareness earned more
money. That is, they made more rational
choices about buying and selling, as judged
by the market, than investors who were less
attuned to their bodies. Outcomes like
these may result from the fact that the body is
not subject to the cognitive biases that so
often distort our conscious thought, the
glitches that appear to be hardwired into
the human brain. Take, for example, our
stubborn tendency to insist on notions of
fairness, even at the cost of spiting
ourselves. In the ultimatum game, an
experimental paradigm often employed by
behavioral economists, participants are paired
up with a partner. One of the partner is given a
pot of money to divide as she wishes. The other
partner may then choose to accept or reject
the proposed division. Accepting even a very
low offer is more rational than rejecting the
offer outright, which leaves the receiving
partner with nothing. Yet studies consistently
find that many players decline low offers out of a
sense of being unjustly wronged, a sense that they
should have gotten more. In a study published in
2011, researchers from Virginia Tech scanned the
brains of two groups of people as they played the
ultimatum game, a group who regularly practiced
meditation and a group of control subjects who did
not meditate. The scans revealed that in the
meditators, the insula, the brain's interoceptive
center, was active during gameplay, indicating that
they were relying on their body signals to make their
decisions. The controls exhibited a different pattern. Their
scans showed activity in the prefrontal cortex, the part of
the brain that makes conscious judgments about what's fair and
unfair. The two groups also diverged in their behavior,
researchers reported. The interoceptively aware meditators were
more likely to elect the rational option of accepting a low offer
over no money at all, while the cogitating controls were more apt to
snub a proposed division that was tilted in their partner's favor.
Among social scientists, a character named Homo economicus is often in vote.
The term describes an idealized agent who always makes the perfectly logical and
rational choice. This figure has proved hard to find in the real world, and yet,
the Virginia Tech researchers write, in this study, we identified a population of
human beings who play the ultimatum game more like Homo economicus. In a tone of some
surprise, they continue, experienced meditators were willing to accept even the most asymmetrical
offers on more than half of the trials, whereas control members of Homo sapiens did so in just
over one quarter of the trials. The bias shown by the non-meditators in the
Virginia Tech study is one of many catalogued by behavioral economists. Others include the
anchoring effect, in which we rely too heavily as a point of reference on the first piece of
information we encounter, the availability heuristic, in which we overestimate the likelihood of events
that come more readily to mind, and the self-serving bias, in which our personal preferences incline our
beliefs in an overly optimistic direction. What to do about such biases? The strategy of many
economists and psychologists has been to inform people of their existence, and then recommend
that people monitor their mental activity for signs that their thinking has been swayed.
In the terminology popularized by the psychologist Daniel Kahneman, we're supposed to use rational,
reflective System 2 thinking to override the bias-riddled responses of the faster System 1.
Mark Fenton O'Creevy, a professor of organizational behavior at the Open University in the UK, was once a
believer in this highly brain-bound approach. Then he conducted a series of interviews with expert
traders at six investment banks and found that they almost never proceeded in this fashion.
Instead, the traders told him, they relied heavily on the sensations they felt stirring within their
own bodies. One investor described the process to Fenton O'Creevy in particularly visceral terms.
You have to trust your instincts, and a lot of the decisions are split second, so you need to know
where the edge is and what you're going to do about it, he related. Having a feeling is like having
whiskers, like being a deer. Just hearing something that the human ear can't hear, and all of a sudden
you're on edge. Something somewhere just gave you a slight shiver, but you're not quite sure what,
but it's something to be careful about. Something's around.
Successful financiers are exquisitely sensitive to these subtle physiological cues, Fenton O'Creevy
discovered. What's more, they seem to pick up on such signals early on just as the feelings start
to emerge, and act on them in that moment rather than dismissing them, suppressing them, or holding
them off for later inspection. Because this approach proceeds rapidly and with little mental effort,
it's much better suited to addressing the complex, fast-paced decisions that many of us are called
upon to make, says Fenton O'Creevy. And going around our cognitive biases in this way is more
effective than laboriously trying to correct them. De-biasing approaches, which rely primarily on
shifting cognition from System 1 to System 2, are unlikely to succeed, he maintains. The human capacity
for self-monitoring and effortful System 2 cognition is limited and rapidly depleted. Attempts to reduce
biases by learning about biases and engaging in self-monitoring rapidly come up against human
cognitive limits. Fenton O'Creevy has experimented with techniques intended to increase investors'
interoceptive awareness through the practice of mindfulness and through the provision of frequent
physiological feedback. In his lab, he had participants play a specially designed video game
called Space Investor. As part of the game, they periodically estimated how fast their hearts were
beating. The more accurate their guesses, as gauged by a wireless sensor placed on the chest,
the more game points they accrued. Fenton O'Creevy reports that repeated play appears to produce
lasting improvements in participants' interoceptive awareness. This approach suggests a novel way to
support smart decision-making, not through the application of painstaking deliberation and analysis,
but through the cultivation of what we might call interoceptive learning. This is a process of
learning, first, how to sense, label, and regulate our internal signals, and second, how to draw
connections between the particular sensations we feel within and the pattern of events we encounter
in the world. When we feel a flutter in the stomach as we embark on a certain course of action,
what consequences seem to follow? When we feel our heart leap at the thought of one option before us
and our heart sink at the mention of another, what does that portend for the choice we ultimately make?
We can clarify and codify the body's messages by keeping an interoceptive journal, a record of the
choices we make and how we felt when we made them. Each journal entry has three parts. First, a brief
account of the decision we're facing. Second, a description, as detailed and precise as possible,
of the internal sensations we experience as we contemplate the various options available.
An interoceptive journal asks us to consider the paths that lie before us, one by one, and take
note of how we feel as we imagine choosing one path over another. The third section of the journal
entry is a notation of the choice on which we ultimately settle, and a description of any further
sensations that arise upon our making this final selection. Once you know how a particular decision
turned out, did the investment make money? Did the new hire work out? Was the out-of-town trip a good
idea? You can return to the record of the moment when you made that choice. Over time, you may perceive
that these moments arrange themselves into a pattern. Perhaps you'll see in retrospect that you experienced
a constriction in your chest when you contemplated a course of action that would, in fact, have led to
disappointment, but that you felt something subtly different, a lifting and opening of the ribcage,
when you considered an approach that would prove successful. Such distinctions are delicate and fleeting.
An interoceptive journal can help us fix them in place long enough to see them clearly.
The body, then, can act as a sagacious guide to good decision-making, in the words of John Coates,
as an eminent squeeze, more knowledgeable and judicious than the easily overwhelmed conscious mind.
The body and its interoceptive capacities can also play another role, as the coach who pushes us to
pursue our goals, to persevere in the face of adversity, to return from setbacks with renewed
energy. In a word, an awareness of our interoception can help us become more resilient.
This may seem surprising. If there's any human capacity that calls for mind over matter,
the mental over the corporeal, it would seem to be resilience. We think of ourselves as deciding to
grit it out, as resolving to exert willpower, often over the protests of an unwilling body.
But, in fact, resilience is important.
