This is Audible.
Python Machine Learning.
Machine Learning Algorithms for Beginners.
Data Management and Analytics for Approaching Deep Learning and Neural Networks from Scratch.
Written by Ahmed Abassi.
Narrated by Cole Watterson.
Chapter 1. Introduction.
1.1
What is Machine Learning?
I still remember a story from my first year in primary school named Operation Mastermind.
In that story, a master computer controls all the systems in the island.
Finally, he decides to get rid of human control and to seize all power himself.
He begins to manage the island and its systems depending on his own decisions.
Although the current situation of Machine is still far of this to happen,
people believe that science fiction always comes true.
Human power is normally limited.
The heaviest weight ever lifted by a human being was 6,270 pounds.
That was a great record compared to the average human power.
However, it is nothing when compared to the power of machines,
which had been invented by human himself to lift tens of tons of kilograms.
This is a simple analogy to the realization of machine learning power and its capabilities.
To imagine the situation,
it is known that the data analysis and processing capabilities of a well-trained human
is limited in terms of the amount of data being processed,
time consumption,
and also the probability of making errors.
On the other hand,
the machine computers designed, built, and programmed by humans
can process a massive amount of data in much less time than humans with almost no errors.
Besides, electronic machines never take a break
and never lets its own opinion affect its analyzing process and results.
To grasp the concept of machine learning,
take a corporate or a government-distributed building, for example,
in which seeking the optimal energy consumption is the main goal.
The building consists of walls, floors, doors, windows, furniture, a roof, etc.,
which are the general architecture elements of a building.
These elements consist normally of different kinds of materials
and show different reactions to energy, daylight absorption, and reflection.
Also, the building encounters different amounts of sun radiation,
sun positions, wind, and weather conditions that varies on an hourly basis.
Now consider that the energy and electrical engineers
have considered to construct a photovoltaic system on the building.
The optimal design in this case
would be when they consider the previous aspects,
besides those that are related to choosing the optimal places,
orientation, shadowing, and angles,
considering the directions of the sun on hourly basis for the whole year.
Last but not least,
the building energy requirements for heating, cooling, lighting, etc.
has to be clearly estimated.
This is a complex and a massive amount of data,
considering that this is collected on an hourly basis,
as mentioned above.
What the corporation aspires to achieve
is predicting the optimal model of their building design
that maximizes the renewable power production
and minimizes the energy consumption.
This kind of data changes according to changes in time
and geographic location,
which makes the job very hard for classical ways of programming.
Machine learning, on the other hand,
is the solution when it is related to variable and large amounts of data.
The main goal of machine learning
is to develop a suitable and optimal algorithm
that leads to the best decisions.
1.2. Machine Learning and Classical Programming
It is very common to know a programmer
who implements an algorithm via a programming language.
The programmer gives the chip-slash-machine-specific program commands,
containing the input parameters and the expected kind of outputs.
The program then runs and processes the data,
while being restricted by the code entered by the programmer.
This kind of programming does not contain the realization of learning,
which means the ability to develop solutions
based on background examples, experience, or statics.
A machine equipped with a learning algorithm
is able to take different decisions
that is suitable for every situation.
Practically, in machine learning,
the computer concludes automatically
to an algorithm that can process the data set
to produce the desired output.
Whereas the concept is different in classical machine programming.
Take, for example, the sorting algorithm.
We already have many sorting algorithms
that can deal with our inputs and give us a sorted output.
Our mission here is just to choose the best sorting algorithm
that can do the work efficiently.
On the other hand, in machine learning,
there exists many applications
in which we do not have classical algorithms
that are totally ready to give us the desired output.
Instead, we have what is called example data.
In the machine learning era,
no instructions are given to the computers
telling them what to do.
The computers have to interact with data sets,
develop algorithms,
and make their own decisions,
like when a human analyzes a problem,
but with many more scenarios
and faster processing.
1.3
Machine Learning Categories
The categorization of the machine learning algorithms
depends normally on the purpose
of processing a specific data.
Therefore, it is important to identify the learning categories
to be able to choose the more suitable way.
The machine learning models are divided in general
into supervised and unsupervised learning.
1.3.1
Supervised Learning
When a machine has a set of inputs that lead to an output,
in which the output nature has been determined by a supervisor,
this machine basically follows a supervised learning algorithm.
The supervision term does not necessarily mean human intervention.
It means that the computer has a target,
and he needs to create and tune functions
that pave the way to this target.
This kind of algorithm is also called predictive algorithm.
This is because there is an output that is being predicted
depending on set of inputs.
Being predictive does not only relate to future talk.
It also includes the cases when an algorithm infers a current
or even a previous event.
An obvious example about the present case
is the traffic lights,
which can be optimally controlled
depending on a related data set
following a predictive algorithm.
In the past events prediction scenario, for example,
doctors can predict the specific date of a pregnancy
knowing the mother's current level of hormone.
Common algorithms of supervised machine learning
are classification and regression.
In classification,
the mission is to determine what category
a group of data or observation should belong to.
The use of classification is very wide
in machine learning problems.
For example,
in cellular communication systems,
the classifier technique
can be used to divide a geographical area
into femtocell,
picocell,
or microcell
according to the number of users,
nature of the area,
number of buildings,
signal fading, etc.
Regression, on the other hand,
is employed whenever we want to predict
a numeric data.
This way of supervised learning
can be utilized, for example,
to predict certain test scores,
lab results,
the price of second-hand cars, etc.
Figures 1 and 2
show examples of classification
and regression schemes.
Classification and regression algorithms
will be discussed later in this book.
Figure 1.1
Mobile network classification
based on population
and coverage area.
Figure 1.2
Second-handed car price expectation
following regression.
1.3.2
Unsupervised learning.
It happens
that we have a massive amount of data
and we want to make a kind of regulation
for such an input.
This is the main aim
of the unsupervised learning.
A data set can be processed
following unsupervised algorithm
to tell what patterns
happen more frequently than others,
what occurs,
and what does not.
This kind of learning model
is also sometimes called
descriptive model.
This is because
an unsupervised algorithm
makes a summarizing output
that describes the data
and presents it in a new way.
Compared with supervised learning
where a prediction of a target
is the main aim,
there is not a single feature
that is more important
than another
in the descriptive model.
This type of learning
is applied in data mining application.
Common algorithms
of unsupervised learning are
clustering,
density estimation,
compressing,
and data reduction.
Clustering algorithm
is employed to a data set
to have each collection
of similar data
grouped together.
A famous method of clustering
is the K-mean algorithm,
which will be discussed in detail
later in this book.
The second algorithm,
density estimation,
is applied to a data set
when the goal
is to produce statistical output
of that data set.
Sometimes the aim
is to visualize
and present
a large data set
information
in a summarized
and informative way.
This brings up
the corresponding
and data reduction algorithms
that learn and produce
the required summaries
without the loss
of information.
In this algorithm,
the key feature
is to reduce
the dimensionality
of a data set.
Reducing the dimensionality
of a data set
is also useful
in reducing
the complexity
of other inferring algorithms
that may be applied
later on.
Table 1.1
shows some
common supervised
and unsupervised
machine learning algorithms
used to perform
corresponding tasks.
Supervised learning.
Learning category,
K-nearest neighbor.
Learning method,
classification.
Learning category,
decision trees.
Learning method,
classification.
Learning category,
classification rule learners.
Learning method,
numeric prediction.
Learning category,
linear regression.
Learning method,
numeric prediction.
Learning category,
regression trees.
Learning method,
numeric prediction.
Unsupervised learning.
Learning category,
association rules.
Learning method, Pattern Detection
Learning category, K means Clustering
Learning method, Clustering
1.4. Benefits of applying Python in Machine Learning Programming
For machine learning algorithms, we must have a programming language that is clear and easy to be understood by a large portion of data researchers and scientists.
A language with libraries that are useful for different types of work and in matrix math in specific will be preferable.
Moreover, it is of a very good advantage to use a language with a large number of active developers.
These features make the arrow point to the Python as the best choice.
The main advantages of Python can be summarized in the following points.
Its syntax is clear, easy to manipulate texts, it is used by a high number of people and communities,
possibility of programming in different styles, object-oriented, procedural, functional, etc.
Ideal for processing non-numeric data, ability to extract data from HTML,
Common in the scientific and also the financial communities.
Therefore, there is a seamless connection between the two fields,
especially in machine learning, as the financial field is one of the main sources of the datasets.
Contains useful libraries, such as SciPy and NumPy,
which enables us to perform operations on vectors and matrices.
The installation of Python, and also adding these libraries and others, are shown in Appendix A.
Chapter 2. Data Scrubbing and Data Preparation
2.1. Data Scrubbing
Data scrubbing, also called data cleansing,
is a very important step before applying any machine learning algorithm.
Data sets are normally drawn from real-world sources,
which produce large amounts of messy data sets.
Examples of sources are statistics, or massive amounts of records generated from organizations
that work in data-intensive fields, such as banking, communication systems, insurance, or transportation.
These messy data might have noisy entries, missing data, or have records that contradicts with each other.
There are several reasons for why this may happen.
Noise usually affects the data because of the hardware limitations and problems,
as these can act as noise sources.
For example, if a blood sample is tested by a medical device that encounters a problem,
the device measurements may be affected and vary on each run for the same sample.
If the device is connected to a database via a network,
the unstable readings may be transferred automatically to the database,
regardless of the erroneous that happened.
Noisy data can also be generated by human when he makes faults.
Missing data problem, on the other hand, may occur due to technical issues,
such as a server or a network hang during data transfer,
or because of manual data entry.
These errors combined, noisy data and missing data problems,
may lead also to a third class of messy data, called data contradiction.
Based on that, the data scrubbing step is very important before starting the learning process.
2.1.1. Noisy data
When there is a difference between the model and the measurements,
there exists a noise that causes an error, or variance.
Examples of methods that deal with noisy data sets are as follows.
Binning methods
In this method, the data is stored and then smoothed by considering its neighborhood or surrounding values.
This is called a local smoothing.
Clustering
This method is useful in detecting and removing the outliers.
Clustering each set of similar values means that the values located in one cluster
are different from those in the other clusters.
The remaining outliers can then be easily detected.
Machine learning algorithms
Regression, one of the basic supervised machine learning algorithms,
can be used to smooth data by fitting it into regression functions.
This algorithm will be discussed in detail in Chapter 3.
Human inspection
Humans can interfere to manually detect and eliminate outliers or smooth noise.
2.1.2
The following are some common ways to deal with the missing data.
Ignore the tuple.
This method of healing a data set with missing values is considered efficient
when a record of data contains many attributes with missing values.
However, when the amount of the missing values per variable changes significantly,
this scheme is no more effective.
Efficiency of this method decreases when a regulation of data is produced by an undefined source to the machine.
The machine will deal with this kind of data as missing data.
For example, when a patient is admitted according to unusual condition.
Filling the missing values manually
This scheme might have high accuracy.
However, it is infeasible mainly in terms of time consumption.
Also, it is tiring and tedious.
Making Expectations
Usually, there are ways that one can use to predict a missing attribute.
For instance, the average of the values nearby of the missing values.
Although this way can cause a bias in the data because of mispredictions,
but it can be utilized to check and compare its results
to the results obtained by the first method, ignoring the tuples.
Inferring Based Algorithms
These kind of algorithms are employed when a missing value is filled by the most probable value.
Example algorithms are
Bayesian Formula and Decision Tree
In the next section, 2.2,
More Details About Missing Data Sources
What are the possible solutions for handling missing data
and a practical step-by-step example of a real estate transaction data set
with missing values will be presented and solved using Python.
2.1.3
Inconsistent Data
The real-life massive amounts of data are susceptible to contain discrepancies
or duplications in codes, names, or any other values.
The inconsistency caused by data entry can be fixed manually.
Also, some expert system tools can be adopted to detect
and fixed contradictions in data.
For example, data entered by thousands of individuals,
like in healthcare organizations,
are automatically recorded by cloud-stored databases.
This type of data set is an obvious case of data sets
that are full of inconsistencies.
For instance, the same information data might be provided
by different resources, but in different formats.
2.2
Missing Data
Most of statistical models cannot be processed unless they are complete,
without lost, or missing variables.
Therefore, it is of great importance to solve the missing data problem
by eliminating the incomplete variables
or fill those missing with values resulted from a certain way of estimation,
as indicated in the subsection 2.1.1.
First, it is important to specify if the missing data is really missing data,
i.e., you need to relate the current available data to its attribute
and determine if it makes sense or not.
Another important step is to identify the source that causes missingness,
as it leads to the best choice of the suitable imputation technique.
A value can be missing because it was forgotten,
lost, did not match the instance,
or it was of no interest relevance importance to the instance.
In this section, we will figure out how to deal with missing data using Python,
which is the programming language that will be used throughout this book.
Therefore, it is important to refer first to Appendix A,
where notes are presented on how to install Python
and the packages needed to go further in the coming example.
2.2.1.
Missing Data Healing
Sacramento Real Estate Transactions as a Case Study
The main aim of this example is to illustrate how to
mark the corrupted or deleted values as missing,
delete records with missing data from the data set,
fill the missing values with the average of the nearby values in the data set.
First, downloading and checking the data set.
The data set file used in this example is a real estate data set
that contains a list of 985 real estate transactions
in the Sacramento area that were collected in five days.
This information was reported by the Sacramento Bee.
The data set.csv file can be downloaded from the link provided in the margin below.
The data set contains 985 record observations.
The first step to process any data set is to understand it.
Here, we have 11 variables.
The longitude and latitude are considered as a joint variable.
The variable's declaration is as follows.
0.
Street Name and Number
1.
1.
City
2.
Zip Code
3.
State
4.
Number of Bedrooms
5.
Number of Bathrooms
6.
The Estate Area in Square Feet
7.
The Real Estate Type
8.
The Sale Date
9.
The Price of the Estate
10 and 11.
Location, Latitude and Longitude
The Data Set contains some missing values.
We can print sample data records out of this data set following these steps and Python code.
1.
Open Idle Python
Can be accessed from the Start menu, then typing, quotation, Python quotation, period.
Choose the idle Python 3.6 result.
Create a new file and save it in your work directory.
Example, name here is real underscore estate underscore transactions dot py.
3.
Place the real underscore estate underscore transactions dot csv file in the same working file directory containing the previous Python file.
4.
In the real underscore estate underscore transactions dot py file, type the following code.
Import CSV.
Import Itertools, data dash csv dot reader, open parenthesis, open, open, quotation, real underscore estate underscore transactions dot csv quotation, close parenthesis, close parenthesis.
4.
Record in itertools dot i slice, open parenthesis, data, comma, zero, comma, ten, close parenthesis, colon, print, open parenthesis, record, close parenthesis.
Code 2-1
Reading and Showing a Data Set
The output is as follows.
If we take a look at the variables in this data set, we find that the main variables are 4, 5, 6, and 9.
The number of bedrooms, number of bathrooms, the area in square feet, and the estate price respectively.
These attributes cannot contain zero values, as zero is considered invalid or lost data.
Therefore, in the following step, we are going to check the number of zeros in each of these attributes.
Second, marking the missing values.
The second step is to identify and mark the missing values.
A summary statics can be printed to tell us the number of zeros in each attribute.
As said, there are columns slash attributes in which zero values have no meaning and considered as an invalid or missing data.
First, we mark every missing data, i.e. zero number, as true.
Then, we count the number of trues per column.
The following piece of Python code performs this task.
Print, import CSV, data-csv.reader, open parenthesis, open, open parenthesis, quote, real underscore estate underscore transactions dot csv, quote, close parenthesis, close parenthesis, pound, printing the number of missing data.
Print, open parenthesis, open parenthesis, data, open bracket, open bracket, four, comma, five, comma, six, comma, nine, close bracket, close bracket, close bracket, equals, equals, zero, close parenthesis, period, sum, open parenthesis, close parenthesis, close parenthesis.
The output summary statistics on each attribute, sum of zeros, slash missing data, is given as 4, 108, 5, 108, 6, 171, 9, 0.
It can be noticed from the output above that there are no zero values in the attribute number 9.
However, there are a lot of zero values in 4, 5, and 6 attributes.
This means that our initial assumption about the invalidity of having zero values in these records is true.
Since this dataset contains real values about real estates, the best way to deal with the missing value problems is to delete the empty records.
Although we will first apply the deletion method, other ways will be applied later on for learning purposes.
In general, whenever the deletion method is applied, it is important to pay attention for the amount of records that are sufficient to build a predictive model.
And that is why, in some cases, deleting large amounts of data is considered unpreferable solution.
Import csv, import numpy, data-csv.reader, open parenthesis, open, open parenthesis, quote, real underscore estate underscore transactions dot csv, quote, close parenthesis, close parenthesis, close parenthesis, pound, replacing zeros by, quote,
n-n-a-n, quote, data, open bracket, open bracket, open bracket, 4, 5, 6, 9, close bracket, close bracket, close bracket, close bracket, equals data, open bracket, open bracket, open bracket, 4, 5, 6, 9, close bracket, close bracket, period, replace, open parenthesis, 0, comma, n-u-m-p-y dotits,
five, comma, six, comma, nine, close bracket, close bracket,
equals data, open bracket, open bracket, four, comma, five, comma, six, comma, nine,
close bracket, close bracket, period, replace, open parentheses, zero, comma, n-u-m-p-y, dot, n-a-n, close parentheses,
pound, counting n-a-n-apostrophe-s number in the data set, print, open parentheses, data, period, is null, open parentheses, close parentheses, period, sum, open parentheses, close parentheses, close parentheses.
Code 2-3, marking the missing values with n-a-n.
We now need to place marks instead of the zero values to indicate that there is a missing value in that specific place.
To do that, we can utilize the packages we installed to Python, see Appendix A, to place a n-a-n instead of each zero value.
The benefit of this operation is that the nans are usually ignored by operations on the data set, such as count and sum.
The following code does the process of replacing, using the functions, replace, open parentheses, close parentheses.
Then, we print out the output to check our work.
The function, is null, open parentheses, close parentheses, is used to count the number of nans.
The NumPy package has been imported in the previous example to be able to use is null, open parentheses, close parentheses, and hence be able to count the number of nans.
To check if the zero values have been successfully replaced by the nans, we compare the following output with the previous one, the number of zeros.
It can be easily noticed that they have the same numbers.
The printed out output is as follows.
0, 0, 1, 0, 2, 0, 3, 0, 4, 108, 5, 108, 6, 171, 7, 0, 8, 0, 9, 0, 10, 0, 11, 0.
Note, additional investigation can be done by printing a portion of the new marked data set.
This should show that some records have been replaced by nans as follows.
Third, removing records with missing values.
Having a data set with missing values may limit the ability to apply machine learning algorithms on it and may produce errors.
To clarify this problem, before proceeding to the records removal, we apply the Linear Discriminant Analysis LDA algorithm on the data set.
The algorithm will not work if the data set contains missing values.
The following code is a part of a classification step.
At the end of the code, we check if the LDA algorithm has been applied or not by printing out the values of X underscore test set.
As dictated because of the missing values, the Python platform shows a pop-up message like in the following shot or may show red lines telling you that something was wrong, which is preventing the printout process of any input.
The previous code marks the missing values in the data set, like what had been done before.
Then, it was supposed to calculate the LDA.
However, the algorithm failed to work because of the nans existed in the data set.
Therefore, a method to handle the missing variables has to be applied.
At this stage, we apply a removal scheme, which is the simplest way to solve the missing data problem by eliminating records with missing data.
Utilizing the pandas packages, we use the dropna function to eliminate records, columns, or rows with missing values.
This can be done by the following code.
Import pandas, import csv, import numpy, data equals csv.read, open parenthesis, open, open parenthesis, quote, real, underscore, estate, underscore, transactions, dot csv, quote, close parenthesis, close parenthesis.
Pound, replacing zeros by, quote, nan, quote, data, open bracket, open bracket, open bracket, four, comma, five, comma, six, comma, nine, close bracket, close bracket, period, replace, open parenthesis, zero, comma, numpy, dot nan, close bracket.
Pound, eliminate records that have missing values, data, dot, dot, dropna, open parenthesis, in place, equals, true, close parenthesis.
Pound, summarization of the number of rows and columns in the data, print, open parenthesis, data, period, shape, close parenthesis.
Code 2-5, removing missing data records.
The resulted data set out of this code has all the records containing NAND deleted.
Therefore, a very large cut is done.
The resulted data set is of size 81412, which has been clearly reduced to be less than the 985 records in the original data set.
However, we can now apply the LDA algorithm to the new reduced data set, using the code 3 sequence, but adding the command line,
data, dot, dropna, open parenthesis, in place, equals, true, close parenthesis.
Before the command, x equals data set, period, ILOC, open bracket, colon, comma, open bracket, four, comma, five, comma, six, comma, nine, close bracket, close bracket, close bracket, period, values.
This time, the program will print out the first five records of x underscore test, as follows.
Open bracket, open bracket, dash, 0.25247425, close bracket, open bracket, 1.3618153, close bracket, open bracket, dash, 0.40772602, close bracket.
Open bracket, dash, 0.39940427, close bracket, close bracket, open bracket, dash, 0.29251235, close bracket, close bracket.
Fourth, inputting missing values.
The deletion process of records with missing values reduced significantly the size of the data set.
Therefore, other inputting methods may be used to replace the NAND 0 values with appropriate values.
Some of the common methods to do that are as follows.
1. A value from another randomly selected record.
2. A constant value which makes sense within the domain.
3. A mean value of the record.
4. A value imported from another model in which some of its values can be utilized to solve the missing data problem in the current data set.
In this example, we will replace the missing data with the mean value of each column.
The fill now function, provided by the panda package, is applied for this task as in the following code.
Code 2-6. Filling missing values with mean of the column.
Import pandas, import csv, import numpy, data equals csv.read, open parenthesis, open, open parenthesis, open parenthesis, quote,
real underscore estate underscore transactions, dot csv, quote, close parenthesis, close parenthesis.
Pound, replacing zeros by, quote, nan, quote.
Data, open bracket, open bracket, 4, comma, 5, comma, 6, comma, 9, close bracket, close bracket, close bracket, period.
Replace, open parenthesis, 0, comma, numpy, dot nan, close parenthesis.
Pound, replacing a missing values by a mean value of the column data, period, fill now, open parenthesis, data, period, mean, open parenthesis, close parenthesis, comma,
in place equals, true, close parenthesis.
Pound, counting nan's number in every column, print, open parenthesis, data, period, is null, open parenthesis, close parenthesis, period, sum, open parenthesis, close parenthesis, close parenthesis.
The code is then counts the number of the missing data.
If there is, after updating the data set with the mean values, the following there are no more missing values in the data set.
It is left as exercise for the reader to use the updated data set with mean values in the LDA algorithm code to test for data set validity.
Good to mention that not all algorithms fail to deal with data sets with missing values.
Some algorithms do have the ability to overcome the problem of missing values existence, such as k nearest neighbors,
which ignores distance measurements for column with missing values.
When building a predictive model, classification and regression consider a missing value as a unique value.
In the next chapter, classification using the decision trees and regression algorithms will be discussed in thoroughly.
2.3. Data Preparation
In the two previous sections, we talked about the first step in data preparation, which is data scrubbing.
However, further pre-processing steps must be applied to the data before heading to the learning process.
In this section, we describe briefly the common preparation methods.
2.3.1 Data Integration
In this process, the data derived from multiple resources, such as different data sets or distributed records,
that belongs to a one place is gathered and integrated into one data set.
During the data integration process, some considerations have to be taken into account.
For example, in a data set, we may encounter different attribute values for the same real-world entity.
This happens when data are obtained from different sources.
The reason may be that every source uses different representation or scale, such as metric versus British units.
Moreover, data sources can sometimes refer to different standards.
For instance, some variables can be referred by means of different IDs, in more than one source.
An important problem that may appear during data integration from different sources is the redundant data.
The common reasons are
The existence of a variable or attribute that is already a derived variable in another data set.
Example, when we are talking about annual revenue.
The same attribute has different names in different data sets.
A correlation analysis can be applied to solve the redundant data problem.
The accurate data integration produces a data set with a reduced number of redundancies and inconsistencies.
This is of great benefit, as it enhances the learning speed and quality.
Data integration step, along with the previous data scrubbing steps, produces a data set with reliable entries.
2.3.2
Data transformation
Sometimes, it is more convenient to deal with data set in a specific format, rather than another.
Here, where the data transformation comes into picture.
The goal of the data transformation is to switch the data values to other format or unit that makes the data analysis more meaningful.
Example, transforming a non-linear model into a linear one.
Depending on the nature of the analysis required, it is decided whether a transformation is needed or not.
Common transformation ways are
Aggregation
It is a way of summarization in which a group of values of the same variable can be expressed or aggregated in a single value.
For example, multiple categories can be grouped into one category.
Normalization
This way of data transformation is followed when we want to express a group of data using a specific range.
I.e., the data is scaled to fall within a small, specified range.
This way is used, for example, in the communication systems where the engineers prefer sometimes
to express the base station's power in a normalized power form.
They scale the power values to lie within the range of 0 to 1.
Common normalization ways are
1. Min-max normalization
2. Z-score normalization
3. Normalization by decimal scaling
Choosing whether to normalize the data or not and the best ways of normalization
depends on the nature of data you are dealing with.
Generalization
It can be considered as another way of aggregation, from the summarization concept view.
The idea is that the attributes that fall in low-level are transformed to higher-level attributes.
This process is also called the hierarchy climbing.
Attribute construction
To simplify analysis, new attributes can be executed from the already given attributes.
This is what meant by attribute construction.
2.3.3
Data reduction
The final step of data preparation is the data reduction.
The aim of any data set analysis is to get meaningful outputs that describe the large amount of data set's information,
in simple words, or even in numerical values that can be then employed in the applications of interest.
Large data set's analysis is complex and may take very long time.
Sometimes, infeasible results may appear.
That is why the principle of data reduction arises.
The input data is reduced by introducing an effective representation of the data set
without affecting the accuracy of the original input data.
Data reduction makes data easier to be analyzed by removing noise and the redundant records,
so that the task of the learning algorithms becomes more effective.
The data reduction step has some pros and cons that have to be taken into account.
Pros
Decreases the data complexity
Focuses on the most important attributes
Reduces computational complexity of many algorithms and makes the results easier to be understood.
Cons
Depending on the nature of the data set, the data reduction may not be needed.
Another main disadvantage is that some of the main data may be thrown during the reduction process.
Example
When representing multiple values as an average value
Last words that can be said about the data preparation step
is that there are three basic rules which enable us to judge if a data set is clean
and prepared to be processed by a machine learning algorithm or not.
Clean and prepared data set are more easily to be visualized and processed by statistical analysis.
Cross validation is a statistical step applied on a data sample in order to check the validity of a machine learning model.
In machine learning, the cross-validation technique is mainly utilized to evaluate and expect the performance of a machine learning model.
The benefit of using this method is exploiting a small sample of data to expect how the machine learning algorithm is going to behave
when applied to a new data that has not been used in the model training step.
In the k-fold cross-validation technique, a data set is divided into a testing and a training data.
The data set is partitioned into k-folds of approximately equal size.
The first fold is considered as a testing set, whereas the remaining k1 folds are the training sets.
The k-fold cross-validation method is preferred over other validation method, since it is 1. easy to implement,
2. easy to understand, 3. its results have lower bias than other methods.
The k parameter represents the number of sets that the data set is to be divided into.
That is how this method got the name k-fold.
When a certain value of k is selected, it may replace the k so the name becomes, for example,
10-fold cross-validation, if we choose k to be 10.
Now, the problem in the division process is that each fold, both testing and training folds,
seeks to contain the max, number of data points as seen in figure 2.1.
When the number of data points in the testing set increase, the validation becomes better.
Similarly, for the training set in which we get better learning results,
when the number of its data points increases.
Therefore, a trade-off exists between both of the sets,
so that each data point taken out of the training set into the training set
is a loss for the training set, and vice versa.
This is where the cross-validation comes into the scene.
The basic idea is to partition the data set into k sets of equal sizes.
For example, if we have a data set of the size of 400 and k equals 10 folds,
then the number of data points in each of the 10 folds is 40.
As seen in figure 2.2, we pick one of the folds as a testing fold,
and the other, then as a training fold.
In k-fold cross-validation, we run k separate learning experiment.
In each of those k subsets, we pick one as a testing set.
The remaining k1 sets are put together into the training set.
Then, we train out a machine algorithm,
and then the performance is tested on the testing set.
The key feature in cross-validation is that we run the algorithm multiple times,
10 times in the previous example.
Then, we calculate the average of 10 different testing set performances
for the 10 different holdout sets.
I.e., we calculate the average of the test results from those k experiments.
This obviously takes more computing time
since we have to run k separate learning experiments.
But the evaluation of the learning algorithm will be more accurate.
Moreover, all data have been used for training and also for testing.
This means that each sample of data is given the chance to be used in the testing set one time,
and used to train the model k1 times.
2.4.1. K-Value Selection
The value of k has to be chosen carefully
to avoid having unclear idea of the machine learning model performance.
The followings are common ways to determine the value of k.
1. Choosing the value of k in a way that guarantees that every train or test set
is sufficiently large to represent statistics of the original full data set which has to be processed.
2. It is found, from empirical results,
that choosing k equals tenfolds gives model performance predictions with low bias.
3. Choose the value of k to be equal to the data set size.
This gives each testing set the chance to be used as a training set and vice versa.
This way of cross-validation is known as leave one out.
The k-value choice affects the algorithm bias and variance.
In other word, the larger is the value of k,
the smaller is the size difference between the training group and the resampling subgroups.
The algorithm's bias reduces as that difference becomes smaller.
This means that there exists a tradeoff between choosing the value of k
and the algorithm bias and variance behavior.
2.4.2
How to fold using Python
Suppose that we have the following set of observations.
0, 2, 4, 6, 8, 10, 12, 14, 16, 18
In order to divide this set, we need first to choose a value for k.
For this example, we will choose a value of k equals 5.
This means that we are going to divide the data into five partitions,
in which each group will contain two observations as follows.
1. Fold number 1, colon, open bracket, 0, 2, close bracket.
2. Fold number 2, colon, open bracket, 4, 6, close bracket.
3. Fold number 3, colon, open bracket, 8, 10, close bracket.
4. Fold number 4, colon, open bracket, 12, 14, close bracket.
5. Fold number 5, colon, open bracket, 16, 18, close bracket.
Therefore, with each fold, five models are trained and tested.
Each fold has the chance to be a testing fold for the five separate learning experiments.
That is, if we set fold number 1 as testing set,
then we have folds number 2 through 5 as training set.
In the next run, we have, for example, fold number 2 as the testing set,
so that folds number 1 and folds number 3 through 5 are the training set and so on.
Now, these steps can be simply programmed using Python, utilizing the scikit-learn library.
The code 2-7 performs the previously explained steps.
The kfold command was imported from the scikit-learn library.
It took two arguments, the value of k5 and if a shuffle will be performed to the data.
In our case, we set the shuffle operation to false.
If a shuffle is to be done, means true,
a third argument has to be added to indicate the seed of the pseudo-random number generator.
The split function is called repeatedly to return the groups of testing and training sets on each iteration.
The output of the five-folding process is as follows.
It is advisable to use the kfold partition, a data set, before modeling.
All of the models shall then exploit the same partitions of the data.
This is of main concern if we are processing massive amounts of data.
Moreover, the employment of the same data partitions across the models
is good for future statistical work that may be done later on the data.
Chapter 3
Supervision Learning
Regression Analysis and Decision Tree
3.1
Regression Analysis
Regression is a supervised machine learning algorithm.
In regression, given an input,
when the output of the model is a numeric value,
this means that we are going to learn a numeric algorithm, not a class.
See open parentheses X
E open bracket 0 comma 1 close bracket.
Regression is exploited to derive equations that describe the data best.
The goal is then to utilize this model to generate estimations.
The common and the simplest way of regression is the linear regression.
The linear regression is preferred to find a predictive function
when we have a correlation coefficient indicates that the data can predict upcoming events.
Moreover, if a scatterplot of data seems to be a straight line,
the linear regression is also preferred.
Linear regression is a method to tell how two variables are related.
Recalling from algebra, the famous line equation is Y equals MX plus B.
This is the famous line equation from elementary algebra.
However, we can describe the analogy from the machine learning and linear regression concepts perspective.
We have Y as a variable that is dependent on another variable,
X as an independent variable,
M is the slope,
and B is the Y-intercept.
We need first to find the linear regression equation
to decide how the two variables,
X and Y,
are related.
In a data set,
we normally have a list of values in columns format,
which can be filed as the X and Y values,
or W and Q,
as used in the following series of equations.
Note,
before proceeding to the regression equation,
when we say that there is a relationship between two variables,
this does not necessarily mean that one is the cause of the other.
Therefore,
it is recommended first
to generate a scatter graph
in order to decide if the data somehow makes a line
prior to calculate the equation of the linear regression.
3.1.2
The equation of the linear regression
The formula of the linear regression equation is
Q equals MW plus B,
which is the famous linear equation formula.
The slope,
M,
and the intercept,
B,
are calculated from the following two equations,
respectively.
M equals N
sigma WQ
minus open parentheses
sigma W
times sigma Q
close parentheses
over
N
sigma W
squared
minus
open parentheses
sigma W
close parentheses
squared
B
equals
open parentheses
sigma Q
times sigma W
squared
close parentheses
minus
open parentheses
sigma W
times
sigma W
Q
close parentheses
over n sigma w squared minus open parentheses sigma w close parentheses squared,
where n is the number of records.
Now, to understand how this works, we suppose that we have the following data,
shown in the table 3.1 below.
The data set above relates two attributes.
x is the age, and y is the glucose level.
To make the evaluation easier, we make the following table 3.2, age and glucose relationship.
Our mission now is to use the values in the table above to calculate the slope
and the intercept from equations 2 and 3, respectively.
The values of m and b are found to be m equals 0.2385, b equals 68.428.
Now we insert these values in the linear regression equation, which represent it as follows.
q equals 0.2385x plus 68.43.
Equation 4 is known as the linear regression equation.
The values of 0.2385 and 68.43 are called regression weights.
The calculation process of these weights is known as regression.
Once we calculated the weights, we can utilize the regression learning algorithms to make new predictions,
given an unseen value or data set.
This can be easily achieved by multiplying the inputs by the regression weights,
and then adding them together to get a prediction.
To grasp what have been done, you may export this table into an Excel sheet,
then generate the scatter plot of data set in table 3.1.
Then, the linear equation is plotted.
The resulted plot is as follows in figure 3.1.
The line generated by this equation is called the trend line.
An online linear regression calculator is available on the link in the lower margin.
3.1.2
Testing with R-squared Correlation
Correlation is to estimate how close the trend line to fit the data points that you have in your data set.
In other words, an estimation of the success of the model.
It can be somehow estimated by i from the scatter plot, as we mentioned before.
But, to make an exact decision, we utilize the correlation coefficient, named R-squared.
Normally, the closer R-squared is to 1, the better the line fits the data points.
If R-squared is far from 1, the line definitely will not fit the data.
The correlation factor equation is
R-squared equals N-sigma-w-q minus sigma-w-sigma-q close-parentheses squared over
Utilizing this equation helps engineer or data scientist to make an exact expectation,
of the accuracy of applying the regression algorithm.
3.2
Classification using Decision Tree
A lot of resources discuss the classification and concentrate on K-nearest-neighbor method.
We will talk about it briefly whenever we need it in this book.
However, in this chapter that cares mainly about the supervised learning, we are presenting
the Decision Tree classification as an example of the supervised machine learning algorithms.
3.2.1
Introduction to the Decision Tree
Have you ever accessed the Ask Akinator online game?
For those who do not have a close understanding of the data sets and the data science concept,
this game will appear like a kind of magic or paranormal power.
In this game, you have to think about a specific character.
Then, the Blue Genie will start to ask you a series of questions, like,
is your character a YouTuber, does your character really exist, and so on.
What the game does is a successful splitting of the set of characters it can deduce.
If you repeat the game for different characters,
you will notice that things are working in a branching principle,
i.e. every answer leads to a specific set of questions
that shrink the circle around the possible answer
and cancel a new portion of possibilities.
For example, if you answer the first question with yes,
the game will exclude the non-YouTuber persons from its potential decision.
The Decision Tree technique works just like the Akinator game.
You give it a set of data, and it generates answers to the game.
The Decision Tree is a famous and a widely used classification technique.
The Decision Tree concept is simple,
and the how it works can be illustrated even to people
with almost no knowledge about the machine learning.
You may have already seen a Decision Tree application
without knowing it is a Decision Tree based.
In Figure 3.2, a flowchart example of the Decision Tree.
You can notice that the flow goes in a branching matter,
where finally you reach to a decision.
In this particular example, the algorithm checks the domain of the sent email
and classifies it depending on its domain.
This example is a hypothetical system,
so that, if the domain is equal to MyOffice.com,
the algorithm will classify it as urgent.
If it is not, it makes another check to ensure if the email includes the word University.
If the text contains the word University,
then this email is categorized as important.
If not, the algorithm checks for the word Advertisement.
If the email contains it, the email will be classified as spam.
Otherwise, it will end to the category to be read later.
In this section, our Decision Tree algorithm will have the ability to deal with the input data sets
and draw, implicitly, a tree like shown in Figure 3.2.
The Decision Tree makes a good job of translating general data into a useful knowledge.
The Decision Tree machine learning algorithm creates rules that classify the unfamiliar data sets.
Decision Trees are common in expert systems,
and they give perfect results compared to those produced by human expert
with long periods of experience in a certain field.
3.2.2. Decision Tree Construction
Before applying the Decision Tree algorithm to a data set,
we need first to specify the feature the splitting process will be based on.
If you do not have a clear idea about the most feature that ruled the data set,
you may try different suggested features until you get the best results.
This results in subsets out of the original data set.
Practically, each subset represents a desired feature.
Therefore, the subsets keep splitting depending on the features as they travel down the branches.
When the data on the branches is the same class,
we have classified it and we do not have to continue the splitting process.
Else, the algorithm has to be repeated until we have classified all the data.
The Decision Tree algorithm is summarized in Table 3.3.
From this table, you can notice the recursive nature of the algorithm,
as it calls itself until all data is classified.
A Python code will be written throughout our talk about the Decision Tree section.
Decision Tree algorithm
Each object in the given data set is in the same class or category.
If yes, return the name or label of the category.
Else, try another feature to divide the data set.
Make a new branch node.
For every divide iteration, call the Decision Tree algorithm
and add the results to the new branch node.
Return branch node.
A common way of splitting data in the Decision Tree is the binary split.
But, suppose that we have followed the binary way,
but we still have other possible features for much more splits.
Therefore, in this section, we will utilize the ID3 algorithm.
This algorithm helps in making rules about how to split the data and when to stop.
The Decision Tree algorithm works with both numeric and nominal values.
It is simple in terms of computational complexity
and of understanding the learnt results.
Moreover, it is robust against missing values
and can handle features that are irrelevant.
However, the main disadvantage of the Decision Tree algorithm
is that it is prone to overfitting.
Overfitting is a common defect in the Decision Tree
and other classification algorithms.
It occurs when make multiple hypotheses about the training data,
i.e., assigning meaningful values to noise.
This indeed reduces the errors in the training data set,
but at the experience of the testing data set.
In other words, an overfit model is too specific to its training data,
so that, when it deals with a new data,
its prediction error in this new data is at risk of being very high.
3.2.3
Building and Visualization of a Basic Decision Tree in Python
In this section, we will learn how to use Scikit-Learn tool in Python
to build a basic decision tree.
Moreover, we will show you how to visualize the Decision Tree
using other helping libraries and packages
like MapPlotlib and SciPy.
Chapter 4. Clustering, Bias, and Variance
In the previous chapter, we talked about regression
as an example of the supervised learning algorithms.
In this chapter, we will deal with an unsupervised learning algorithm,
which is clustering.
In unsupervised learning, we don't have a target variable.
Instead, we need to get a description about input data set.
For example, in supervised learning,
the main aim is to get an output Y for a set of data, X.
On the other hand, in unsupervised learning,
we are interested in what the machine can tell us about the data set X.
For instance, we may ask the machine to tell us about the best three clusters
that we can make out of a given data set,
or what are the most repeated four features in the data set.
In this chapter, we will study one type of clustering algorithm,
called K-Means algorithm.
The reason that it is called K-Means is that the algorithm tries to form K,
unique and different clusters,
in which the cluster center is the mean value in that cluster.
Clustering is a type of unsupervised learning
that finds groups of similar observation.
In fact, everything can be clustered.
Clustering is more efficient when the most similar items
are located in the same cluster.
The key difference between the classification, supervised learning,
and the clustering algorithms
is that in classification,
we are looking for ways that distinguish pre-classified groups.
In clustering, on the other hand,
no classes are available,
and we want to make some natural classing of instances.
Clustering is sometimes called unsupervised classification.
This is because it works like the classification in the supervised learning,
but without having pre-defined classes.
In other words, it produces classes.
Some applications of the clustering.
Genomics.
Forming clusters of genes with similar expressions.
Astronomy.
Stars, for example, can be distinguished based on their colors,
sizes, distances, materials, etc.
Clustering can be applied to find groups of similar stars and also galaxies.
Earthquake Investigations.
Detected earthquake epicenters should be clustered along continent cracks.
Marketing.
Identify customer groups and utilize this for targeted marketing and reorganization.
4.1.
The K-Means Clustering Algorithm.
K-Means Clustering is a machine learning algorithm
that tries to form K-groups or clusters for a given data.
The number of clusters K is determined by the user.
Every cluster is described by its centroid,
which is a single point that is located at the center of all the points in the cluster,
and here where the name comes from.
K-Mean.
The K-Means Algorithm works with numeric values,
means symbolic or excluded.
The main advantages and disadvantages of the algorithm are as follows.
Advantages.
Easy to implement.
Fast, simple, and easy to be understood.
The objects are assigned to the clusters automatically.
Disadvantages.
The number of clusters has to be picked in advance.
This can be solved by trying the algorithm, K-Mean, with different Ks,
then choosing the clustering that has the best quality.
Results are affected significantly by the initial choice of labels.
Moreover, the algorithm may converge at local minima.
Suggested solution is to restart the clustering algorithm multiple times
with different random labels.
Slow on very large data sets.
This can be solved using sampling.
High sensitivity to outliers.
When the values are very large, the mean may be skewed.
In this case, the median can be used instead of mean.
The K-Means method approach can be summarized in the following three steps.
1. Choose randomly a K-number of centers for the cluster.
2. Assign every point in the given data to its closest cluster's center.
The assignment continues until finding the nearest centroid using methods such as Euclidean distance.
3. The centroids are all then updated by moving each of them to the average of its assigned points.
4. The steps 2 and 3 are repeated, till a convergence reached, i.e. when the cluster assignment change becomes less than a specific threshold.
4. The flowchart in Figure 4.1 explains how the code of the algorithm looks.
4.2. Bias and Variance
Prediction models' errors can be caused by three main components.
5. Bias, Variance, and Irreducible Errors
In large data sets, there might be a number of undefined variables.
These variables may affect the input-to-output mapping relationship.
Therefore, the irreducible error is resulted.
As can be understood from its name, there is no way to decrease the irreducible error, whatever learning algorithm is employed.
Therefore, we care here mainly about bias and variance errors,
since there is a trade-off between a machine learning model's capability to minimize bias and variance.
Investigating the two errors helps us in expecting model results and avoiding the problems of overfitting or underfitting.
4.2.1 Error Due to Bias
The bias error is defined as the difference between the average prediction of a machine learning model
and the correct value which we are seeking to know.
If we only have one model, speaking about average means that we have to run the model multiple times,
then compare the average of the resulted values out of those runs with the correct value we are trying to predict.
In general, machine learning algorithms which concern themselves with numeric analysis and results,
called parametric algorithms, encounter high bias.
These kinds of algorithms are simple and they learn fast.
On the other hand, in complex problems, the parametric algorithms have poor predictive abilities,
which make them less flexible.
The difference between low and high bias algorithms is that the first makes low number of assumptions
about the required output.
On the other hand, the latter makes more assumptions.
Common examples of the low bias learning algorithms are
the k nearest neighbors and the decision tree.
Linear regression and discriminant analysis are examples of the high bias learning algorithms.
4.2.2.2 Error due to variance
If we can repeat a whole machine learning model running process multiple times,
the variance is how much the prediction of the target function is going to alter
between different runnings of the model at the same given point.
A machine learning algorithm is applied to predict the target function from the training dataset.
Therefore, it is expected that the algorithm will show variance.
If the machine learning algorithm has a good ability to discover the implicit relationship
between the input dataset and the output variables,
the target function should not change too much from one training dataset to the next.
The difference between low and high variances machine learning algorithms can be clarified as follows.
Low variance algorithms
In this algorithm, when changes are made to the training dataset,
small variations occur to the prediction of the target function.
High variance algorithms
In this algorithm, when changes are made to the training dataset,
large variations occur to the prediction of the target function.
In general, non-numeric machine learning algorithms,
also called non-parametric algorithms, show high flexibility.
However, they are high-variance algorithms.
Common examples of the low-variance learning algorithms are
linear regression and discriminant analysis.
The k-nearest neighbors and the decision tree are examples of high-variance learning algorithms.
4.2.3. Graphical Explanation of the Bias and Variance
A friendly and simple graphical explanation of bias and variance
can be created using a bullseye diagram.
Let the center of the eye represent the correct values of a specific model's.
The further we go from the eye center, the worse are the prediction results.
In the bullseyes shown in figure 4.2 below,
every black dot represents an individual run of the model.
In case of low bias, the model realizations, dots,
are very close to the center of the bullseye.
On the other hand, in case of low variance,
the model realizations are very close to each other,
regardless of their distance from the center of the bullseye.
When we have a good distributed training data,
we predict very well, and we are close to the bullseye.
In turn, when the training data is full of outliers or noisy data,
the predictions will be far from the bullseye.
4.2.4. Trade-off between Bias and Variance
The machine learning algorithms aim to attain low bias and variance outputs.
However, as we see in sections 1 and 2 of this chapter,
there is a trade-off between the variance and the bias in the machine learning algorithm.
Figure 4.3 summarized this idea.
In machine learning, there is no escaping the relationship between bias and variance.
That is, as shown in figure 4.3,
increasing the bias will decrease the variance and vice versa.
Therefore, the process of choosing the suitable parameters for a machine learning algorithm
is usually a struggle to balance between bias and variance.
As an example of the bias-variance trade-off for the k-nearest-neighbors algorithm are as follows.
The k-nearest-neighbors, KNN, is a common supervised machine learning algorithm.
It shows low bias and high variance.
However, if the value of the k-neighbors increases,
the trade-off situation becomes different.
In other words, the increase in the number of neighbors increases the contribution to the prediction process
and hence increases the bias of the model.
This indicates that a careful configuration should be done to the selected machine learning algorithm
in order to achieve a balance in this trade-off for the learning problem.
Bias and variance are important methods to have an acceptable guide and impression
about the predictive performance of machine learning algorithms.
Chapter 5. Artificial Neuron Networks
5.1. Introduction
Neural networks are probably one of the very recent fields.
Currently, large corporations and also growth companies are all rush into this state-of-the-art field.
Deep learning is a term that is associated to the neural networks.
Both deep learning and neural networks have great ability to learn from data and the environment.
Therefore, they are the first and preferred choice for machine learning.
Common application in which neural networks employed are
Car's auto-driver
Image processing and recognition
Consult and recommender systems
Artificial neural networks are powerful schemes that show great adaptive ability to wide range of data types.
Being artificial brings to the surface the origin of the naming.
Artificial neural networks mimic the human being nervous system.
Figure 4.1 shows the human neuron.
A neuron consists of the following main parts.
Dendrites
These are the recipients that receive the input electrical impulses from other connected neurons.
The decision is then taken by the cell body, which concludes to the action needed from those inputs.
The axon and the axon endings
These are the neuron transmitters.
They transmit the output decisions as electrical impulses to the other neurons.
The neuron can be divided into three parts or stages.
Inputs, processing, and output.
This is the key to state the analogy between the biological and artificial neural networks.
5.2.
Artificial neural networks
An artificial neural network, ANN, is a computational model benefits the idea of the biological neural networks as depicted previously.
The artificial neural network is a machine learning way that mimics the human intelligence in dealing with data.
Neural networks are a remarkable class of supervised machine learning algorithms.
They are employed in both regression and classification.
In this chapter, we will discuss the basics of neural networks and show a practical example at the last section.
Other known types, uses according to the data to be analyzed, are convolutional and recurrent neural networks.
The first showed very good performance in image processing.
Recurrent networks, on the other hand, achieved a very well level of performance in sequential data-related problems such as dynamic system modeling and natural language analysis.
There are mainly three different layers in neural networks.
1. The input layer
2. The hidden layers
In this layer, the inputs received from the previous layer are processed.
The hidden layer can contain more than one layer.
3. The output layer
After having the data processed, it becomes available in this layer.
4. Figure 4.1 shows a graphical representation of the neural networks' three layers.
It can be noticed that the variables are represented by nodes.
The network layers are distinguished via colors, red, green, blue, for input, hidden, and output layers respectively.
Let us define x0 and h0 as dummy variables.
They always take the value of 1.
The dummy variables are shaded in figure 4.1.
1. Input layer
The input layer rule is to interact with the external environment.
It deals with inputs come from the data sources.
The input is then transferred to the hidden layer.
Each input represents an independent variable which affects the output of the network.
The inputs are some features such as height, age, weight, image data, or hours of sleep.
2. Hidden layer
The hidden layer is the intermediate layer, lies between the input and the output layers.
It consists of the group of neurons that have activation function applied on it.
The job of the hidden layer is to deal with and process the inputs got from the previous layer.
Therefore, its responsibility is to extract features of interest from the input data.
The hidden layer can include more than one hidden layer, so that one of the problems to be considered is the choice of the number of the hidden layers depending on the learning category.
If the input data can be linearly separated, then we can skip the hidden layer.
This is because the activation function can be designed to input layer that is able to solve the problem.
In the problems that have complex decisions to make, 3 to 5 hidden layers can be used.
In the case of multiple hidden layers, the symbol H will have a superscript that indicates the hidden layer number.
A neural network with more than one hidden layer is also referred to as deep learning or a deep neural network.
The choice of the number of hidden layers depends on the problem complexity or on the level of the required accuracy.
Of course, this does not mean that if one keeps increasing the number of hidden layers makes the neural network become more accurate.
The algorithm will reach a level in which adding extra hidden layer will not affect its steady state output,
or sometimes may cause the results to fall below the best reached performance.
This means that the accurate calculation of the number of neurons per network is a very important step,
either to meet the data set's complexity, or to avoid overfitting.
3. Output layer
The output layer is the stage where we got the values that we want to predict.
A good benefit of the neural networks is that the output pattern can be traced back to the input layer.
This means that there should be a logical relation between the number of neurons in output layer
and the type of work performed by the neural network.
The number of neurons in the output layer is related to the intended use of the neural network.
Latent variables
In the neural networks, the latent variable term is common.
However, it is not very new to the reader of this book.
Recalling from Chapter 3, the Regression Analysis section,
when we looked at the relationship between the dependent and independent variables.
Latent variables are functions of the inputs.
This relation is expressed by edges in neural networks.
Each edge flows from left to right,
which means that every variable that receives an edge
is a function of the variable from which the edge comes,
and also the weight associated with this edge.
From Figure 4.1, we can notice that each variable in the hidden and the output layers
is a function of all previous layer variables,
except the bias variable H0, which has the value of 1.
Deep Learning and Deep Neural Networks
As said before, the number of the hidden layers may be more than one layer.
This results in, mainly, two types of neural networks
depending on the number of hidden layers,
deep and shallow neural networks.
Of course, the deep category refers to the neural networks
that have more than one hidden layer.
This distinguish is essential because we cannot use the non-linear transformations
that fit the shallow neural networks in deep networks,
as that may result in poor performance.
In the hidden layer, it is common to use functions
that are known as activation functions.
In Section 4.4, we present famous activation functions.
As will be noticed, they are all almost non-linear function
of the weighted sum of the variables in the layer.
Using these functions to produce a hidden layer
means that a non-linear transformation is needed.
In deep neural networks,
we surely have more than one hidden layer,
which means that we have to perform some feature transformations
on the original input features.
5.3
The Flow in the Neural Network
Every neuron in the ANN represents an activation node.
The activation node is connected to the input node
in order to apply learning algorithms to calculate the weighted sum.
The weighted sum is then passed to an activation function,
which leads to predict the results.
Here, where we have the concept of a perceptron arises.
Perceptron means things that take many inputs that lead to one output.
In Figure 4.3, we have inputs that are direct independent variables
or outputs from other neurons.
Every input X has a weight of W.
The importance of weights is that they are indicators of the significance of inputs.
In other words, as a weight goes higher,
the input has a greater contribution to the output results, and vice versa.
As can be seen from Figure 4.3,
every perceptron has a bias that is an indication of the flexibility of a perceptron.
The bias can be compared to the constant B
in the famous line equation Y equals AX plus B,
as it enables us to adapt the line, either up or down,
to have better prediction results.
As known from simple elementary algebra rules,
if we do not have the constant B,
the line will always pass through the origin point,
and in our case in machine learning,
we will have poor results.
In the bias named stage,
as seen in Figure 4.3,
we can notice that the weights are summed and a bias is added.
This sum goes then through an activation function.
The activation function forms a gate that opens and closes.
The activation function can give ones or zeros,
judge based on a threshold, or make probabilities.
Regarding the probability part,
it may utilize the sigmoid or the rectified linear function.
More about activation function will be discussed later in this chapter.
The activation function output is the weighted,
predicted output from the neural network.
It is indicated as Y in the figure above.
Y may be regression, binary, or classification values,
depending on the nature of the data set and the required output.
5.4, Activation Function
In this section, we will make a quick review
for common activation functions under the ANN.
Threshold or Step Function
If the sum of the weights and inputs is below a specific threshold,
it is zero, and if it is above, it is one.
Recall the gate analogy we talked about.
Sigmoid Function
It is the most widely used activation function.
It looks like the threshold function, but it is smoother.
Sigmoid Activation Function is suitable for making probabilities.
This means that the output range of the function
will be always between zero and one.
For example, if we are investigating a picture,
the output will be the probability whether it is for a cat or a dog.
Hyperbolic Tangent
This function is a stretched version of the sigmoid function,
as it ranges from negative one to one.
This function is chosen when we want deeper gradient and steeper derivative.
Therefore, the choice between the sigmoid or the hyperbolic tangent functions
depends on the gradient strength requirements.
Rectifier Linear Unit
RELU
This function is normally applied in the reinforcement learning application.
From its name, it is a linear function that has been rectified.
This means that it has the value of zero in the negative domain,
and it increments linearly,
i.e., it for a positive input, x,
it gives an output x, and it is zero otherwise.
One major advantage of RLEU
is its simplicity compared to the hyperbolic tangent and the sigmoid functions.
This is a main point to consider in designing the deep neural networks.
According to the characteristics of the function we are trying to approximate,
we can choose the activation function that achieves faster approximation,
which means faster learning process.
The sigmoid function, for example,
shows a better performance in classification problems than the RELU.
This means a faster training process and also a faster convergence.
One may use his own custom function.
However, if the nature of the function we are trying to learn is not very clear,
then it is preferable to start with RELU as a general approximator
and then work backwards.
Table 4.1 summarizes the mentioned activation functions
along with their graphical representations and equations.
5.5. Working Example
In this section, we are giving an example
that translates some of the concepts that explained in the previous sections.
Suppose that we have a trained neural network, figure 4.4.
This means that we have the weights optimized as seen in the figure.
The input layer contains a group of features,
age, distance to the hospital, gender, and income.
The output, on the other hand,
is the variable that is dependent on the input features.
It is the probability that a person will be hospitalized.
If we take the age, for example,
we can state that the older the person,
the more likely he or she will be hospitalized.
In terms of gender, statistically,
men are more likely to be hospitalized than women.
The data collected can be utilized to make several predictions
depending on the input features,
and this is the realization of the neural network's job.
The hidden layer in this example was considered as a black box,
which contains the neurons and weights
that result in the hospitality probability.
The machine can be trained to predict the relation
between the distance to the hospital
and the probability of being hospitalized.
Therefore, the neural network has to try first
to find patterns in the data
and figure out the relations between these different patterns.
This is a kind of understanding data
before making other processing or decisions.
Once trained, we are able to input different features
extracted from the targeted data set.
For example, one record can be something like that.
Age is 65 years old, gender is female,
moderate distance to the hospital, and high income.
The neural network is a magical method
that expect and predict the desired results
after processing the features in a smart way
that mimics the human neuron.
The neural network estimates the different combinations
of the input features to identify patterns in the data set
based on the neural network architecture.
It may be seen as a black box,
but a deeper look allows us to see patterns
amongst the weights, inputs, and hidden layers.
5.6. The Learning Process
How the weights work
We normally need machines to be learnt
since we have large amounts of data
to be processed and analyzed.
Therefore, as we have a large data set,
we feed its inputs into the neural network
to make predictions.
The first step is to determine the neurons' weights.
They can be determined based on initial assumption,
or they can be predefined based on the application features
and the required output.
The normal flow of the neural network
is towards the output Y.
That is, the input features are fed to the hidden layer
where the calculations are performed.
Then, the network flow continues
until we have the desired output.
This kind flow is known as forward propagation.
At this point, the smart error handling appears.
The resulted Y is compared to the actual value of Y.
The target is to make the difference between them
as small as possible, i.e., minimizing the error.
This can be clearer if we imagine the analogy
of a child learning math.
If he answered a specific question with 8
while the accurate answer is 5,
we have an error of 3.
In this case, the calculations have to be re-performed
until we converge as close as possible to 8.
The neural network utilizes the weights
in its task to minimalize the resulted error.
It reduces the weights of the neurons
that make significant contribution to the error.
This process is known as back propagation.
This is because in this turning process,
we travel back from the output to the neurons and input
in order to locate where exactly the error happens.
Here is where the importance of the activation functions
comes to the surface.
The activation functions are differentiable,
and this helps in performing the back propagation process
through the neural network,
by computing the gradients.
Hence, the weights are adjusted.
The simultaneous adjusting process of the weights
continues until we have a neural network output
that is close to the actual output.
The weights are slightly adjusted in each iteration
to have a smaller error at each run.
The process is repeated for all the inputs
and outputs possibilities of the training data set
until the error is significantly small
and acceptable by the application.
5.7. How the back propagation is performed?
The following equation is a representation
of the simplified cost function in a neural network.
E in equation 4.1 is the squared error
between the predicted output Y
and the actual output Y.
Conceptually, we can produce a plot
where the error E and the predicted output Y.
Then, all the possibilities of the weights
in the network can be tried using methods
like brute force technique.
This is supposed to produce a result
like a parabola of data.
However, this easy approach requires
imaginational computing power for a large data set
to examine all the probabilities.
To clarify it, for a moderate-sized data set,
we may need hundreds of years to tune the weights
and get the results.
Here, where weights-optimizing techniques
comes to the surface.
5.7.1. Gradient descent.
In gradient descent, we focus on making accurate predictions
in much less time than the cause of estimating
all of the possibilities as mentioned before.
The first step is to provide the neural network
with initial values of weights.
Then, we can pass in our data
following the forward propagation way.
Our mission now is to estimate the output Y
and compare it to the actual output Y.
In most cases, the predictions resulted from the first run
are not very accurate,
i.e., we have a high error value.
Assume we have a cost function for a specific value of W.
To make it simple, we will assume numerical values.
If the weight W is 1.6
and the resulted cost function value E is 3.2,
we need to adapt the weight in the next run
to reduce the value of the cost function.
The question is, what if we could discover the way
whether to make W larger or smaller
in order to decrease the cost function?
What we are going to do is test the cost function
to the left and to the right at a specific test point.
Then, we check which one of the two ways
produces smaller cost function value.
This method is called numerical gradient estimation.
It is a good approach,
but if we look back at the cost function equation,
we can think in a smarter way
by utilizing derivatives,
i.e., proceeding to the gradient decent concept.
We want to know which way is downhill,
leads the function to the minimum value.
And in other words,
we are going to check the rate of change of E
with respect to W.
What we need to do at this stage
is to derive the of E over W,
which will give us the rate of change of E
with respect to W.
Then, at any value of W,
if we have E over W positive,
then the cost function is directed uphill.
Whereas if E over W is negative,
the cost function is directed downhill.
This means that we now know
which direction decreases the cost function.
So that, we are able to speed up the tuning process.
This is because we saved all the time
needed in searching for values in wrong directions.
Moreover, additional computational time
is saved by taking steps iteratively
at any direction that minimizes the cost function,
and then stopping when the cost function
is not getting smaller anymore.
This is the conceptual and practical realization
of the gradient descent scheme.
We may think of the gradient descent optimization
as a hiker.
The weight in this case
is to climb down the hill towards the valley,
which is the cost minimum.
Following this analogy,
we can determine each step
by the slope steepness,
the gradient,
and the step distance of pay growth,
which is the analogy to the learning rate at this case.
In the gradient descent,
we have high and low learning rates.
In our hiking analogy,
the high learning rate is the case
when we take big steps towards the valley
before checking our position.
This may end to the case
where we never reach the minima of the cost function.
On the other hand,
when small steps are taken,
we boost up our opportunity in reaching the minima.
However, this may take very long time.
So that,
we need to tune the learning rate empirically
until we reach the minima.
A smart solution is to think about
the adaptive learning rate
utilizing the gradient concept.
That is,
the steeper the gradient,
the higher the learning rate,
and the smaller the gradient,
the smaller steps it takes
to reach the final value.
The gradient descent
may not seem very special
in one-dimensional problems.
However,
it significantly decreases the time required
to tune the neural network's weights
in higher-dimensional data sets.
An important issue to take care about
when solving using the gradient descent
is the convexity of the data.
Sometimes the data set we are dealing with
may be a non-convex.
This means that the cost function
doesn't always decrease
while going in the same direction.
Literally,
the cost function in this case
decreases and then increases again.
This behavior is known mathematically
as non-convex function.
Example is shown in figure 4.5.
In non-convex functions,
the gradient descent method
is not able to give accurate predictions
anymore
since it will stick in local minima
instead of spotting the global minima.
Here,
the reason of having a squared cost function
is revealed.
The sum of the squared error values
enables us to utilize
the convexity nature
of the quadratic functions.
Therefore,
as the cost function E
equals the squared values of X,
then the plot of E
is a convex parabola.
A main point to mention
before finalizing our talk
about the gradient descent
is that the practical convexity
of a data set
depends on how we deal with the data.
Sometimes we can follow the principle
one at a time
instead of all at once sometimes.
In this case,
we are not very interested
in the overall convexity issue
of the data set.
This talk leads to our discussion
in the next section
about the stochastic gradient descent.
5.7.2
Stochastic gradient descent
The gradient descent calculated
the gradient of the whole data set.
On the other hand,
the stochastic gradient descent
calculates the gradient
using a single portion
of the data set.
This makes the stochastic gradient descent
shows a faster convergence
than the gradient descent,
since it performs updates
much more frequently.
Considering that the data sets
often contain redundant information,
we are very satisfied
with the stochastic gradient descent,
which does not use the full data set.
Table 4.2
summarizes a comparison
between the gradient
and the stochastic gradient descent.
5.8.
Understanding Machine Learning Categories
in the Context of Neural Networks
First, let us make a quick review
of the data set types
and how it is reacting
with the neural networks.
Mainly, we have three types of data set.
A training data set.
It is a group of data samples
that are utilized
for the learning process.
Therefore, in the neural networks
at this stage.
A validation data set.
A set of samples
which is used
to adjust the network parameters.
For example,
in the neural network,
it can be used
to choose the number
of hidden layers.
A test data set.
This set is a group of examples
that are used
to check the performance
of the neural network
after having it fully built.
Moreover,
it can be used
to predict the output
whose input is known.
It is also used
to ensure
that we do not overfit our data.
We know from the supervised learning
that we have a training data
as the input
to the algorithm or network
and the required output is known.
In neural networks,
the weights are turned
till the output
meets the desired value.
In unsupervised learning,
we have a data
that we are trying to understand
and extract its features.
Therefore,
in neural networks,
the input data is utilized
to train the network
with a known output.
The network then clusters the data
and adjusts the weights
by extracting the features
from the input data.
In the reinforcement learning,
we do not know the output.
However,
the neural network
can give a feedback
telling if the input
is right or wrong.
This type of learning
is also called
a semi-supervised learning.
Offline or batch learning
makes the required tuning
to the weights
and to the threshold
only after employing
the training data set
to the network.
Finally,
in the online learning,
opposite to the offline learning,
the tuning process
of the weight
and the threshold
is made after employing
each string example
to the network.
5.9.
Neural networks applications.
A lot of things
we encounter daily
use the recognition
of patterns
and exploit this results
in making decisions.
Therefore,
neural networks
have the ability
to be adopted
in daily life applications
and missions.
Examples of using
neural networks
are in stock markets,
weather predictions,
radar systems
to detect the enemy's
aircrafts or ships.
It can also be used
to doctors
in diagnosing
complex diseases
on the basis
of their symptoms.
Neural networks
are in our computers
or smartphones.
They can be programmed
to identify images
or handwritings.
Certain neural networks
can monitor
some parameters
to spot the characters
you are typing,
such as
the lines you are making,
your fingers' movements,
and the order
of your movements.
Voice recognition programs
are another example
that significantly utilize
the neural networks technique.
Some email programs
or tools
have the ability
to separate
the genuine emails
out from the spam emails.
These programs
also use neural networks.
Neural networks
have shown
a high efficiency
in text translation
from language
to another.
Google Online Translator
is one of the famous
tools that employs
neural networks
over the last years
to enhance
the machine performance
in converting
or translating words
from language
to another.
Chapter 6
Building a Machine
Learning Model
6.1
How to Develop
a Machine Learning Application
The basic approach
to develop
a machine learning model
can be summarized
in the following steps.
1.
Data Collection
Data or samples
can be collected
from the internet,
RSS feeds,
or API.
Researchers may have
collective devices
that record
test results
such as
wind speed data,
blood glucose level,
salts,
or minerals in water,
or any other
data of interest.
There are almost
endless options.
However,
sometimes,
using available samples
may save
a lot of time
and effort.
Examples of
publicly available
data sets
repositories
are shown
in Appendix B.
2.
Input Data Preparation
Once we got our data,
we need to check
if it is read
and in a usable format.
This step
guarantees
that our data
is ready to be used
in the learning algorithm.
In Chapter 2,
we discussed
the data preparation methods
in detail
and showed
how to clean it
from noise
and how to solve
the missing data problem
and other necessary methods
that make the data set
able to give
meaningful results.
The data preparation
and scrubbing
are usually
non-trivial
compared to
collecting data.
3.
Input Data Analysis
and Human Involvement
This is a manual analysis step.
For example,
we need to check
that the previous two steps
worked fine
and no empty values exist.
An estimation of the data
may be done
in order to recognize
if there are some patterns
that can describe the data
or data values
that are obviously different
from the other points
in the data set.
Sometimes,
creating data plots
in either two
or three dimensions
may be very useful.
However,
there are normally
more than three dimensions
or features
that describe the data
and one cannot generate
the data plots
across all the dimensions
easily at one time.
Therefore,
dimension compression methods
can be employed
to compress the multiple dimensions
to two or three
in which the data
can be visualized.
Considering the human involvement
for automated systems
and trusted data sources,
it may be very limited
or unneeded.
For example,
if the data source
is from a production system
and we have an idea
how the data
is going to behave,
this step can be skipped.
The main benefit
of this step
is that it enables us
to check that there
is no garbage
in the data set.
Four,
the algorithm training.
This is the actual step
where the machine learning
action occurs.
In other words,
this step is the core
and the realization
of the learning process.
First,
we provide the algorithms
a ready,
cleansed,
and healed data
from the steps
one and two.
Then,
information is extracted.
This step creates formats
that are ready
to be applied
and used
by the machine
in the next steps.
Remember that
in the unsupervised learning,
we do not need
the training step.
This is because
we do not have
a specific target value.
Five,
algorithm testing.
In this step,
we have our algorithm
ready to be tested
and applied
to the other data sets
in order to extract information,
utilizing the knowledge
gained in the previous step.
In the of supervised
machine learning,
we usually have
some known variables.
Therefore,
we are able to use them
in evaluating the algorithm.
On the other hand,
in the unsupervised learning,
we may need other ways
or metrics
to make the evaluation.
In the case
we are not satisfied
on the results,
we can go back
to step three
and make some changes
that tune
and affect the flow
of the process,
in which updated
and better results
are gained.
Most of the times,
you may find a problem
in the data collection
or data preparation stages.
In this case,
you need to go back
to steps one and two.
Six,
algorithm programming.
This is simply
the stage
where you choose
the suitable tool
or programming language
to write the algorithm
and make the real program.
Once again,
we need to check
if all of the previous steps
worked very fine.
If new data
has been encountered,
we need to recheck
the steps one to five.
In this example,
we use Python
to build the machine learning model,
which has been used
throughout this book
for the benefits
presented chapter one.
Other tools
or programs
may be used
such as
WCA,
MATLAB,
or any other tool
that you can handle
very well.
6.2
Building a machine learning model
on Python
In this example,
we are going to build
a machine learning model
trying to summarize
and gather
most of the studied topics
in this book.
It is believed
that the best way
to understand a problem
is to put it
into example.
Therefore,
in this example,
we will select a data set
from the UCI
Machine Learning Repository.
After finishing this example,
you will be able to
analyze a data set,
use six machine learning models,
and learn how to make a prediction
based on a trained data set.
Note 1.
Before proceeding to this example,
ensure that you have
Python Platform
installed on your device.
Also check that you have added
the necessary libraries
and packages.
You may refer to Appendix A
for more details.
Note 2.
You can access different data sets
following the resources
shown in Appendix B.
In this model,
we have a balanced scale data set.
In this data set,
we have 625 instances
and four variables.
The information presented
in this data set
contains results
of a psychological experimental.
The classes of the data are
1.
Tip to the right.
2.
Tip to the left.
3.
Balanced.
First,
we need to study,
estimate,
and visualize the data set.
This can be done
as shown
in the following subsection.
6.2.1.
Data Set Estimation.
In this stage,
we need to take
a general look
to the data set
and gain some information
that enables us
to start dealing with it.
First,
let's revise how to
1.
Importing the necessary libraries.
2.
Loading the data set.
The following piece of code
achieves the two previous tasks.
The next step
is to identify
the data set dimension
and classes.
You can also print
a portion of the data set
in order to have
an idea of the nature
of the information
presented in the data set.
The following code
makes this possible.
Note that
every coming code
is added in a series
to the code 5-1.
The following outputs
is generated.
As can be seen,
we have a 625x5 data set.
Then,
the code printed
the first 10 records
of the data set.
After that,
we printed a summary
of the data set description
that showed
some basic statics
of the data set,
such as
mean and std.
Finally,
we counted the number
of values
per each variable class.
The next step
is to visualize
your data set.
In our case,
the data set
has not so much complications,
and it is very simple
to be aware of.
However,
we are going to show you
some kinds of plots
for educational purposes,
and to give you the options
to use them
whenever you want.
The following code
will generate
a univariate plot,
in which a plot
of every variable
is generated individually,
histograms,
and a multivariant plots.
The generated figures
are shown
in series
after the code.
At this stage,
a very useful
data set estimation
has been done.
In the next section,
we are heading
to validate
our data set
using tenfold
cross-validation.
6.2.2
Validate the data set
We have to check
if the model
we have is good.
This is a basic step
for the coming steps,
as we are going
to apply
the learned algorithms
into an unseen data.
Recalling from section 2.4,
we can evaluate
our model
by testing it
over a whole portion
of the data set,
validation data set.
In this example,
we will use 80%
of the data set
for the training process,
and reserve 20%
for testing the model.
This code prepares
for the tenfold
cross-validation process,
which will be utilized
during the test process
of different machine
learning models,
as shown in the next subsection.
6.2.3
Machine Learning Models
As mentioned,
Python is a great
program language
to be utilized
in machine learning field.
In this section,
we are applying
six machine learning algorithms
to the data set
using a very simple code.
We will then test
the score of accuracy
of each algorithm
and decide
which one
is the best choice.
The algorithms
that will be tested are
Logistic Regression,
LR,
Support Vector Machines,
SVM,
K Nearest Neighbors,
KNN,
Linear Discriminant Analysis,
LDA,
Classification and Regression Trees,
CART,
Gaussian Nave Bays,
NB.
The results got
from the previous code
accuracy of each algorithms
are as follows.
It can be easily noticed
that both the Gaussian
Nave Bays
and the support vector machines
have the highest accuracy.
The mean and the spread
of every model
can be shown
through a plot
using the following code.
The previous work
can now be utilized
to make predictions.
We will choose
the NB algorithm
to make prediction
as it was one
of the two best algorithms.
We are going to apply
the NB algorithm
on the validation set
to check its accuracy.
It is advisable
to have a validation set
to check
if an overfitting
or a slip
were occurred
to the training set.
When the NB algorithm
is run
in the following code,
an accuracy score,
a confusion matrix,
and a classification report
are printed out
to indicate
the validation results.
The prediction output
is as follows.
It can be seen
that the accuracy
is 0.928.
There were three errors made
which were indicated
by the confusion matrix,
and finally,
the classification report
shows the breakdown
of every class,
precision,
recall,
F1 score,
and support.
This has been
Python Machine Learning.
Machine Learning
Algorithms for Beginners.
Data Management
and Analytics
for Approaching Deep Learning
and Neural Networks
from Scratch.
Written by Ahmed Abassi.
Narrated by Cole Watterson.
Copyright 2018
by Ahmed Abassi.
Production copyright
by 2018.
Audible hopes you have enjoyed
this program.
Thank you.
Thank you.
