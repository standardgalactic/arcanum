This is Audible.
Bayes' Theorem, The Ultimate Beginner's Guide to Bayes' Theorem.
Written by Arthur Taft.
Narrated by Tim Carver.
Who is this book for?
This book is perfect for those who want to use Bayes' Theorem for weather prediction,
blackjack games, criminal investigation, medical cases, and other real-world applications.
Applications of Bayes' Theorem are not limited to the financial industry.
The theorem determines an event's probability based on information that may be or is isolated to that event.
For instance, it can be used to determine the accuracy of a medical test result
by considering the test's general accuracy and how likely a given person is to suffer from a disease.
If you want to learn how to predict the odds of events
and gain deeper insights on how things work, this book is perfect for you.
Educators who want to brush up or advance their existing knowledge about Bayes' Theorem
and students who want to learn about this method can also benefit from this book.
If you want to see how a piece of information affects the likelihood of an event occurring,
you should read this book today.
Learn Bayes' Theorem in no time at all.
What will this book teach you?
What if you are going on a date with your partner on Saturday?
Today is Tuesday, and you want to make sure that the weather is fine on the day of your date.
What are the odds of raining on that day?
These questions can be answered by applying Bayes' Theorem.
In this book, you will see examples of the theorem and how every conclusion is reached.
This book will help you get an innate feel for the possibility of an outcome
without having to deal with various numbers.
You will learn what Bayes' Theorem is,
the equation used to determine the probability of an outcome,
different applications of Bayes' Theorem,
how to interpret results,
how to solve unclear questions, and a lot more.
After reading this book,
your statistical knowledge of the world will significantly improve.
Introduction
Bayes' Theorem, also known as Bayes' Rule or Bayes' Law,
describes an event's probability based on past knowledge of situations or conditions
that are or might be connected to that event.
For instance, if a certain disease is related to age,
the age of a person can be used to assess the possibility that he has a disease.
The theorem is usually used in Bayesian interference,
which is crucial to Bayesian statistics.
The involved probabilities may have various probability interpretations.
When Bayesian interference is applied to interpret results,
the theorem states how a biased level of belief
must logically change to justify the availability of associated evidence.
Bayes' Theorem was named after Thomas Bayes,
who first formulated a particular equation
that permits new evidence to alter beliefs.
It was shown in
An Essay Toward Solving a Problem in the Doctrine of Chances,
a work regarding the theory of probability work
that was published in 1763.
Pierre Simon Laplace improved the equation
and published the modern version in
Theorie Analytique des Probabilités in 1812.
Sir Harold Jeffries put Laplace's formulation
and Bayes' algorithm were put on a self-evident basis.
Chapter 1. The Bayes Theorem
The Bayes Theorem is a law in statistics and theory
describing the probability that an event will occur.
The prediction comes from the past knowledge of the related conditions.
Also known as Bayes' Rule or Bayes' Law,
this theorem found many applications in the real world.
In medicine, theorem helps in accurately assessing
a person's probability of developing cancer
based on the knowledge that cancer is age-related.
This calculation of the possibility of a person developing cancer
is much better than when making assessment
without the knowledge of ages linked to cancer.
There are many applications to Bayes' theorem
and one of the most popular refers to Bayesian inference,
a certain method used in calculating statistical inference.
Using Bayesian inference,
the probabilities for statistical inference
can have numerous varying interpretations.
The Bayesian analysis of probability
shows how one's subjective belief
can change rationally
after taking into consideration
the available related evidence.
A Short History of Bayes' Theorem
Thomas Bayes, 1701-1761,
an English statistician,
Presbyterian minister, and philosopher,
who studied how to calculate
a binomial distribution's probability parameter,
was the first to provide a formula
which uses likelihood and evidence
in updating beliefs
referred to as the Bayes' theorem.
Richard Price found his manuscript
and edited the unpublished manuscript significantly.
Price then read it at the Royal Society,
which happened well after Bayes' death.
In the introduction of the manuscript,
Price wrote some of the Bayesian statistics'
philosophical basis.
In 1774,
the French mathematician
Pierre Simon Laplace
reproduced and further developed
Bayes' results,
publishing it as a modern formulation
back in 1812.
He mentioned it in his
Theorie Analytique des Probabilités,
his theory in the analysis of probabilities.
Sir Harold Jeffries further developed
the algorithm and modern formulation
created by Bayes and Laplace
by putting them into axiomatic bases.
He referred to the Bayes' theorem
as Probability's own Pythagorean theorem.
On the other hand,
Laplace was the one who mainly developed
the Bayesian interpretations concepts.
In 1982,
Stephen Stigler suggested that
Bayes was not the first
to formulate the Bayesian rule.
According to him,
it was Nicholas Saunderson
who was the first to discover it
earlier before Bayes,
but this interpretation was disputed.
Shannon McGrain
and Martin Hooper
also argued the substantiality
of Richard Price's contribution
to Bayes' theorem.
In truth,
and by modern standards,
people should refer to the Bayesian rule
as the Bayes-Price rule.
If not for Price,
discovering Bayes' manuscript,
seeing its importance,
correcting, contributing,
and finding a use for it,
it never would have been recognized today.
Using only Bayes' name
in the theorem is unfair,
but people are so fascinated with it
they don't really care about anything else.
The Theorem Statement
In mathematics,
you can write the Bayes' theorem statement
in this equation.
The probability of theta given x
equals
the probability of x given theta,
the probability of theta,
over
the probability of x.
In this formula,
theta and x are the events,
whereas the probability of x
is not equal to zero.
There are four parts
to the theorem statement,
which are
The probability of theta given x.
The conditional probability.
It means the likelihood
that event a will happen
if b is true.
The probability of x given theta.
Another conditional probability.
It refers to event b occurring
if a is true.
The probability of theta
and the probability of x.
Known as marginal probability.
The variables probability theta
and probability x
are probabilities
of independently observing a and b.
Use this equation
to calculate the probability
of event a occurring
giving that b is true.
Using past knowledge
of the conditions
and available evidence,
the Bayes' theorem
gives rational prediction
and probabilities.
Chapter 2
Interpretations of the Bayes' Theorem
How to interpret the Bayes' Theorem
depends on the probability interpretations
attributed to the terms.
In probability interpretations,
there are two main categories,
evidential and physical probabilities.
The physical probabilities,
also known as frequency
or objective probabilities,
relates to random physical systems.
Such systems include rolling dice
and roulette wheels, among many,
where a certain type of event
can occur at a relative frequency
after a long time.
Physical probabilities
used to explain
these stable frequencies.
On the other hand,
there is the evidential probability
or Bayesian probability.
This type of probability interpretation
represents subjective plausibility.
You can assign this probability
to virtually any statement
even when it does not involve
any random process.
You can also use this interpretation
to exemplify up to what degree
that the available evidence
supports the statement.
The Bayesian interpretation
The Bayesian interpretation
infers that instead of
propensity of frequency of an event,
probability should be
a reasonable expectation.
In this interpretation,
probability represents
a quantification of personal belief
or a condition of knowledge.
You can view this interpretation
as propositional logic's extension,
one that enables reasoning
with assumptions.
In this reasoning,
the proposition's truth
or falsity
cannot be certain.
Following the Bayesian approach,
you assign hypotheses
to a probability.
A type of probability
belonging to the
evidential probabilities category,
the Bayesian view
assigns prior probability
in evaluating
the likelihood
of a hypothesis.
After examination
of new pertinent data,
the Bayesian probabilist
then updates
the prior probability
to posterior probability.
The stated formulation above
and a set of standard procedures
help in performing
the calculation
of the posterior probability.
Generally,
there are two views
or forms,
objectivist
and subjectivist view,
to Bayesian approach
in interpreting
the concept of probability.
The objectivist view
and subjectivist view.
The objectivist view
interprets probability
as an extension of logic,
a reasonable expectation
representing state of knowledge.
On the other hand,
the subjectivist view
suggests that probability
represents quantification
of personal belief.
This means you can rewrite
the rules so that it justifies
your rationality
and coherence's requirements.
The objectivist view.
The objectivist interpret probability
as a reasonable expectation
after sharing
and analysis of information.
Cox's theorem,
one of the origins
of the laws governing
probability theory,
justifies the objectivist view.
The logical approach
to probability
retains some of the ideas
of the classical interpretation,
determining probabilities
by providing a prior
after examining
the possibilities available.
In logical approach,
however,
they generalize possibilities
in two ways.
According to objectivist approach,
you can assign
unequal weights
to possibilities,
and you can calculate them
on whatever evidence
is available,
symmetrically balanced
or not.
In the objectivist view,
any claim to a probability
means you also claim
a definite certainty
to a knowledge or event.
It does not matter
if they give
a numerical value
to the probability
or not.
If you claim
that a statement's truth
is probable,
then you are also
claiming confidence
about the probability itself.
In the perspective
of logical probability,
you can claim
an estimate
of probability
based on another
prior probability.
When asked
what your prior
probabilities'
foundations are,
you need to provide
at least one certainty
at your basis
for your estimate
of probability.
Failure to provide
even at least
a single certainty
will lead to
infinite relapse
or an unfounded circle,
which are logically
unsustainable.
This means that
for the objective view,
you must base
a claim
of probability
in reason.
It has to have
at least a single
absolute certainty
as its foundation.
Thus,
a claim
to a probability
of a statement's truth
is also a claim
to at least
one certain knowledge.
The subjectivist view
The subjectivist view
of Bayesian approach
explains you can
derive probability
based from a person's
personal judgment
on whether an event
will occur
or if a statement
is true.
This view
of the Bayesian
interpretation
does not include
any formal calculations,
only reflecting
the opinions
and prior experiences
of the subject.
As this type
of probability
comes from
the person's
own opinions,
subjective probabilities
vary from every person,
each containing
a high amount
of personal bias.
The subjective view
of Bayesian approach
applies greatly
in everyday happenings.
Several approaches
justifies
the subjectivist view,
including
De Finetti's theorem,
the Dutch book
argument
and decision theory.
One example
of which
is when fans
of sports games
would give probabilities
of their favorite
team's winning chances.
Even though
they have no
mathematical proof
behind their claims,
they will still
give predictions
in actual percentages
of their team's
winning chances.
Compared to
objectivist view,
the subjectivist
probability
is highly flexible.
It is more flexible
even in terms
of one's beliefs.
Their belief
of an event
occurring
or a statement's
truth can change
based on the existing
belief of the person.
It could also
possibly change
when given
a specific range
to choose from,
even when that range
has no sufficient
hard data in it.
Many events
and factors
can alter
a subjective
probability.
As personal beliefs
shape the probability,
things like
a person's
upbringing
and the events
he witnessed
in life
can have
an impact.
Note,
however,
that even
when you can
rationally
explain the belief,
it does not
make the prediction
actually true.
To better
understand
and differentiate
the subjectivist
view from
the objectivist,
the former
puts emphasis
on prior
probability's
relative lack
of logical
constraints.
To them,
assignments
of prior
probability
are largely
because of
non-rational
factors,
such as
socialization,
evolution,
or the freedom
of choice.
Extreme
of subjective
basions,
such
D. Finetti
believes
that prior
probability's
only logical
constraint
is probabilistic
coherent.
On the other
hand,
some subjective
basions,
like Jeffrey,
call themselves
subjectivist,
although they
allow a small
amount of
rational limitations
on prior
probabilities.
As you can
see,
subjectivists
can disagree
on certain
constraints.
However,
that their
constraints
exclude very
little unites
them as
subjectivists.
Justifications
to Bayesian
Approach
When you
hear the word
probability,
it often
evokes
concepts
related to
uncertainty,
randomness,
the likelihood
of an event,
etc.
This particular
concept is
quite hard
to characterize
it formally.
Typically,
we define the
concept in terms
of occurrence
of event
during repeated
experiments.
However,
this approach
would later
lead in a
baseless,
circular
definition.
In this
section,
you will find
various
justifications
to the Bayesian
approach.
Over the
years,
the use of
Bayesian
probabilities
as Bayesian
inferences
basis has
numerous
arguments
supporting it.
Among these
justifications
that support
Bayesian
approach
includes the
Cox axioms,
De Finetti's
theorem,
the decision
theorem,
and the
Dutch book
argument.
De Finetti's
theorem
De Finetti's
theorem is
one of the
justifications of
the subjectivist
view of
Bayesian
probability,
where an
event's
likelihood
corresponds to
the person's
beliefs.
In this
form of
Bayesian
probability,
coherence
and rationality
allow some
substantial
disparities in
the limitations
they pose.
In addition
to De Finetti's
theorem,
the Dutch
book as well
as decision
theory provides
justification to
the subjectivist
view.
Named after
Bruno De Finetti,
the theorem
states that
probability does
not exist
objectively.
According to
De Finetti,
probability only
exists subjectively
in individuals'
minds.
De Finetti,
along with
Ramsey and
Savage,
all propose
the same
definition of
probability.
They propose
that probability
is the same
as most
people's
behavioristic
characterization
of probability.
In essence,
it refers to
the rate at
which people
would bet on
the chance
of an event
happening.
Axiomatic
Approach
Named after
the physicist
Richard Cox,
the axiomatic
approach justifies
probability's
logical
interpretation,
referring to
probability law's
application to
any proposition.
Known as
Cox's Theorem,
this approach
seeks to
satisfy three
conditions,
divisibility
and comparability,
consistency,
and common
sense.
Cox wanted
his theorem
to placate
the plausibility
of a real
numbers proposition.
This depends
on the available
information related
to the proposition.
In addition,
Cox's Theorem
wants the
plausibilities
to vary sensibly
with the model's
plausibilities
assessment.
In Cox's
Theorem,
common sense
seems that the
condition is
consistent with
that of the
Aristotelian
logic,
where reasonably
equivalent
propositions
have the
same
probability.
Lastly,
the system
seeks that
if there are
numerous ways
to the derivation
of a proposition's
plausibility,
all the results
have to be
equal.
Over the
years,
Cox's Theorem
became one
of the
justifications
used for
Bayesian
probability.
It has
become one
of the
cornerstones
for the
theory,
interpreting
probability
as a
natural
system
extended
from
Aristotelian's
logic,
a perfect
reason approach
towards
uncertainty.
If there
are n
simple outcomes
to an
experiment,
the axiomatic
approach assigns
a probability
of 1
over n
to every
outcome.
Cox's
axiom
assumes that
every outcome
has an
equal
probability
of occurrence.
For example,
if you
roll a
dice,
each
simple
event,
the roll
of dice,
has a
one-sixth
chance
of
occurrence.
Decision
Theory
Approach
Abraham
Wald gave
the decision
theoretic
justification
of Bayesian
inference
to us.
The decision
theory,
also known
as theory
of choice,
refers to
the reasoning
behind
choices.
Closely
related to
game theory,
the concerns
of decision
theory is
more on
individual
agents'
choices,
not their
interactions,
and decisions
that affect
one another.
It advocates
probability
theory,
pointing to
Cox's work
on probability
axioms.
Similarly,
it justifies
De Finetti's
Dutch book
paradoxes,
illustrating the
theoretical
difficulties if
you depart
from probability
axioms.
Decision
theory also
advocates the
complete class
theorems,
showing that
all admissible
decision rules
are equal
to Bayesian
decision rules.
The Dutch
book approach
De Finetti
proposed the
Dutch book
approach,
basing it
on betting.
In this
approach,
you make a
Dutch book
whenever a
gambler places
a bet with
guaranteed profit,
whether you
win or lose
the bet.
A gambler
cannot make
a Dutch
book if
he follows
the Bayesian
calculus's
rule in
constructing
his odds.
The
tradition
Dutch
book
approach
didn't
specify
Bayesian
updating,
but there
is
possibility
left open
that the
rules for
non-Bayesian
updating
can avoid
Dutch
books.
A
Dutch
book
argument
relies on
normative
or descriptive
assumptions.
The
frequentist
interpretation
is one
of the
two
major
interpretations
of
probability.
This
interpretation
defines the
probability
to be the
limit of
the event's
relative
frequency
in a
huge number
of trials.
It supports
pollsters'
and experimental
scientists'
statistical
needs.
One of the
first things
you can learn
about statistics
is there are
two approaches
to it,
the Bayesianism
and the
frequentism.
The difference
between
Bayesians and
frequentists
lies on their
different
definition of
probability.
In
frequentism,
there is only
one meaning
to probability.
To them,
probability is
the limiting
case of
repeated quantities.
That is,
if you
measure a
quantity,
assuming it
does not
vary in
time,
then measure
it again,
then again,
and again,
every time you
do so,
it will get a
slightly different
answer because
of the
measuring device's
statistical error.
Within a
large number of
measurements limits,
any given
value's frequency
indicates the
measuring probability
of that value.
To
frequentists,
probabilities
essentially
relates to
an event's
frequency.
The strict
frequentists
believe it
is pointless
and nonsense
to talk about
the probability
of an event
when it has
a single
fixed value.
This is
different to
Bayesian
approach where
the concept
of probability
relates to
a range of
certainty about
statements.
The Bayesian
approach sees
probabilities to
be one's
knowledge of
what a
quantity
results will
be.
In contrast
to frequentists,
probabilities
essentially relates
to a person's
knowledge about
the particular
event.
When talking
about probability,
the Bayesian
view codifies
a person's
knowledge of
the value
from the
available data
and prior
information.
If you
restrict yourself
to frequent
interpretation,
calculating
probabilities is
different from
Bayesian
approach.
In
frequentist
view, you
interpret p
as an
occurrence
frequency.
If p is
the probability
of an
experiment's
outcome, then
if you repeat
the experiment
in n times,
n is a
large number,
the specific
outcome will
be n times
p times.
You only
discuss
probabilities in
frequentist
interpretation when
you are dealing
with well-defined
random samples.
A random
experiment sample
space refers
to a set of
all possible
outcomes of
the sample,
while they
refer to the
sample space
subset meant
for measurement
as an event.
In
frequentist
view, any
given event may
only hold one
of two
possibilities.
Either an
event occurs,
or it does
not.
An event's
measure of
probability is the
relative frequency
of an event
occurring, which
is observed in
a series of
experiments.
This is the
foundation of
the frequentist
interpretation of
probability theory.
Note, however,
that this
interpretation is
a philosophical
approach to
probabilities
definition and
use.
It is only one
of numerous
approaches to
probability theory
and does not
assert that it
captures the
entire connotations
of the concept
of probability.
Being only an
interpretation of
probability, it
is not in
conflict with the
probability theory's
probability
axiomization.
What the
frequentist
interpretation does
is give guidance
on the real-world
application of the
mathematical
formulation of
probability theory.
The frequentists
provide a unique
guidance in how
to construct and
design practical
experiments by
giving a contrast
contrast to the
guidance given by
Bayesian
interpretation.
If you are
wondering if the
guidance it offers
is useful or that
it is suited to
misinterpretation
remains a subject
for controversy.
Interpretations
based on frequentist
approach are
empirical.
The ratio from
a numerous trials
defines these
interpretations, a
natural interpretation
in terms of
scientific experiments.
While the
frequentist
interpretation does
help in resolving
difficulties regarding
classical interpretation,
however, it does not
tackle issues such as
that of the Dutch
book.
Chapter 3.
The Bayesian
Methodology
There are four main
steps outlined in a
typical Bayesian
analysis.
The following concepts
and procedures
characterize the
methods used under
Bayes'
theorem.
Random Variables
or Data Models
Bayesian approach
utilize random
variables or
unknown quantities
for modeling a
statistical model's
sources of
uncertainty, such as
the lack of
information.
Prior Probability
Distribution
The Bayesian
approach needs to
identify prior
distribution after
considering the
available prior
knowledge.
Bayes'
formula
As more data
becomes available,
this approach
sequentially used
the Bayes'
formula in
calculating the
posterior distribution.
Afterwards, the
posterior distribution
becomes the new
prior when new
data becomes
available again.
Hypothesis
in the range
from 0 to 1
In Bayesian
statistics, a
hypothesis can be
within a range
from 0 to 1,
which indicates
an uncertain
truth's value.
The main goal
for a Bayesian
statistics is
obtaining a
model's
posterior distribution.
The posterior
distribution is
the weighted
average of the
knowledge before
observing the
data, prior
distribution, and
the information
contained with the
observed data,
evidence.
Following Bayesian
approach, you can
answer virtually any
inferential question
through an
applicable analysis
of the posterior
distribution.
When you obtain the
posterior distribution,
you will be able to
calculate the
parameter's point
and interval
estimates.
You can create a
future data's
prediction inference
and even create a
hypothesis
probabilistic
evaluation.
Every posterior
distribution can
update your current
knowledge, showing
that the Bayesian
probability theory is
quite similar to
when you are
learning new
things.
Data models
In Bayesian
analysis, the
first step is
choosing a
probability model
for your data.
As mentioned
above, you need
to use random
variables or
unknown quantities
to model the
sources of
uncertainty of
your statistical
model.
This process is
quite similar to
the classical
approach to
probability.
In this step,
you need to
choose a data
model, which
involves choosing
on the data's
probability
distribution if
you knew the
parameters.
When the n
data values you
need to observe
are x1 through
xn, the
vector for the
unknown parameters
is denoted as
theta.
If these are
independently made
observations, then
the probability
function used for
the data can be
the probability
of y1
over theta.
On the other
hand, if the
situation has
extra covariate
information, such
as if there is
an ith case, x
to the i, the
form's probability
function would be
probability y to
the 1 over x
to the 1 theta.
In the case of
not conditionally
independent data with
given covariates and
parameters, the data
model needs to
stipulate the joint
probability function
in this formula.
P, or
probability,
y1
through
yn
given that
x1
through
xn
theta.
These data
models represent
in idealized
form the
process of
generating data
for posterior
distribution.
Prior
distribution
Every theory
and idea
starts out
with a
baseline,
a probability
of the truth
based on
earlier established
knowledge.
In the
Bayesian
approach, your
baseline is the
prior distribution,
which can be
somewhat subjective.
This is because
most of the time
the person
assigning the
value bases
the random
variable for
the statistic
model on
limited data
and his
biases.
It is
subjective
because in
Bayesian
approach,
the truth
of a
claim can
be a
person's
measure of
belief.
With
Bayes'
probability,
you can
combine
multiple
hypotheses
and rank
them by
probability.
Prior
distributions
are usually
written as
P
parentheses
capital
H.
You can
distinguish
every single
one of a
given
problem's
hypothesis
using a
number
H1,
H2,
H3,
etc.,
with the
null
hypothesis
being
written as
H0.
Before
collecting or
observing new
data,
you can rank
the competing
hypothesis based
on the number
of assumptions
that each
requires.
In assigning
probabilities,
hypotheses
incorporated with
established
knowledge should
have higher
probabilities than
those that
require acceptance
or rejection
of established
knowledge.
When
identifying
prior
probability
for those
with known
background rate,
the base rate
or background
rate as
prior.
For
instance,
when determining
the prior
distribution of
the likelihood
of a disease,
you can use
the community
disease's base
rate as
prior,
where the
patient is a
member of
the community.
In assigning
priors,
many people
will find those
with lower
priors to be
more appealing
because of
their beliefs,
biases,
and world
views.
They are
appealing because
people tend
to find
comfort in
new age
ideas and
unseen forces
when it comes
to explanations.
However,
even appealing
as they are,
one should
assign priors
following
Occam's
Razor.
Informative
Prior
Distribution
There are
a number of
approaches when
choosing prior
distribution.
The first
approach involves
selecting an
informative prior
distribution.
With this
approach,
the statistician
utilizes his
knowledge of the
substantive problem
based on some
other data as
well as help
through an
elicited expert's
opinion.
The data and
expert option can
help construct
prior distribution
properly,
reflecting both
the statistician
and expert's
opinions about
the unknown
limitations.
The nature of
this approach
makes it seem
overly subjective
and not at
all scientific.
However,
one should note
that selection
of model,
which frequentists
often make,
has already been
subjective.
This means that
frequentist analysis
are never devoid
of any subjectivity.
Additionally,
it would be
unscientific if
there were extra
information existing
before observing
the data of
model parameters.
Besides,
to the subjectivity
of this approach,
frequentists also
criticize that
using informative
prior distribution
is that involving
two statisticians
of the Bayesian
approach will
likely utilize
different prior
distributions.
This will lead
to two different
sets of inferences
for one scientific
problem.
In response to
this criticism,
frequentists may
also use different
data models on
the same data
resulting to
different conclusions
just as well.
The prior distribution
is a part of the
complete statistical
model, in the eyes
of the Bayesians.
With that,
two Bayesian statisticians
who select different
distributions is the
same as with two
frequentists selecting
different data models.
Non-informative
prior distribution
The non-informative
prior distribution
is one of the main
approaches when choosing
prior distribution.
This approach
involves creating
priors representing
ignorance over the
model parameters.
Also known as
vague, objective,
or diffuse
prior distribution,
it is an attempt
towards objectivity
by acting as though
there is no prior
information about
the parameters
exists before
observing the data.
In implementing
this approach,
the statistician
assigns equal
probability to all
of the values
within the parameter.
Compared to
informative
prior distribution,
this approach
directly addresses
the criticism
about the former
being subjectively
chosen.
Unfortunately,
this approach
is not without
problems.
For one,
there are several
commonly accepted
criteria in
construction of
non-informative
prior distributions.
This makes it
rare for any
given data model
to have all
these criteria
yield the same
distinct
non-informative
prior distribution.
Another issue
with the
non-informative
prior distribution
is that some
of the common
methods used
for constructing
the priors
often result
in quite a
curious inconsistency.
Lastly,
many of the
methods for
constructing
non-informative
priors result
in probability
functions
integrated
infinity,
which are not
formal probability
distributions.
Fortunately,
even with these
many problems
and improper
prior distribution
by non-informative
approach,
still deliver
rational Bayesian
analysis.
New data
The next step
after choosing
prior distribution
is the
consideration of
new data obtained
through observation
or experimentation.
When considering
new data,
you should ask,
if the hypothesis
is true,
what is the
probability of
finding set of
data?
In essence,
how much
that the new
data support
the given
proposition?
When written
in formula,
this becomes
the probability
of D given
H,
referring to
the data's
probability
given the
hypothesis.
In this step
of the Bayesian
analysis,
you assign a
probability by
which the
hypothesis in
question can
use to
account for
the new
data.
Statisticians
use studies to
test the
hypothesis,
allowing you to
account how
strong the
new data is
through the
study's quality
and the
magnitude of
its results.
There are
different types
of studies that
could support
the new
data.
The larger
ones are
usually more
powerful than
smaller studies.
Similarly,
double-blinded
and randomized
control studies
are much
better than
observational
studies.
If the
results of
the studies
are unmistakably
positive,
then it will
provide more
support to
the hypothesis.
In Bayes'
probability,
updating a
hypothesis'
probability takes
multiplying the
prior distribution
with the
new data.
The result of
this new and
updated probability
is the
posterior probability.
This calculation
comes with a
denominator,
which is the
sum total of
probabilities of
the entire
possible relevant
hypotheses.
This denominator
should be
considered constant,
that if you
sum up all
the truly
accounted
hypotheses,
it should
equal to
1.
After observing
the data and
multiplying it
with the
prior
probability,
it will
result to
posterior.
Once new
data is
available,
the posterior
will become
the new
prior,
and the
whole process
repeats.
Essentially,
an idea's
probability
updates when
new information
comes in light
and you become
closer to the
truth.
The Bayes'
probability
complies well
with the
both Extraordinary
Claims Rule
and OSCAM's
Razor,
helping you put
definition as to
how astonishing
a claim is
by ranking
hypotheses to
a claim of
truth.
Posterior
Distribution
The posterior
probability of
an uncertain
proposition or
a random
event is the
assigned conditional
probability after
taking into
account a
relevant evidence.
Equally,
posterior probability
distribution is an
unknown quantity's
probability distribution
treated as random
variable obtained
from a survey,
experiment,
or study.
In this
situation,
posterior means
new knowledge
after taking
into account
and examining
relevant evidence
related to the
situation of
interest.
The posterior
interval involves
2.5 percentile
and 97.5
percentile of
the posterior
distribution.
You can
evaluate the
competing composite
hypotheses probabilities
by calculating
their posterior,
which refers to
the hypothesis
probability based
on posterior
distribution.
Posterior
probability is
different with
likelihood function
as it is the
probability of
the parameters
theta with the
evidence x with
the probability of
theta given x
where the latter is
the evidence's
probability given
the parameters of
probability of x
given theta.
These two,
however,
relates to each
other with the
following statement.
For the
probability
distribution function,
you have
P of
theta,
then x for
observations,
and the
probability of x
given theta
for likelihood.
Considering these,
you have the
posterior probability
defined as
the P of
theta given
x is equal
to the
probability of
x given
theta times
the probability
of theta
over the
probability of
x.
Posterior
Summaries
After determining
the posterior
distribution,
the next step
of Bayesian
analysis is
creating a
summary of the
inferential
conclusions through
approximate
examination.
For the
mode or
mean of
the
posterior
distribution,
the
parameters
point estimates
are usually
calculated.
By producing
the interval's
endpoints
corresponding to
the posterior
distribution's
specific
percentiles,
you can
calculate the
interval
estimates.
An example
is that of
a 95%
central-posterior
interval.
In terms of
the probabilities
of the opposing
composite
hypotheses,
you can
evaluate them
by calculating
the hypotheses
probability based
on their
posterior
distribution.
Predictive
Distribution
Once you
obtain the
posterior
distribution,
Bayesian
statistics offer
you the
benefits of a
straightforward
computation for
the predictive
distributions.
The posterior
predictive
distribution refers
to possible
unobserved
values that
are conditional
on the observed
values.
The posterior
predictive
distribution takes
into account
the uncertainty
over theta.
On that
note, the
posterior distribution
of the possible
theta values
highly depends
on x.
An example
in the observed
data of x
equals x1
to xn,
you would like
to make a
prediction on
the future of
observation of
x.
Analysis of the
data shows you
obtain the
posterior distribution,
the probability
of theta
given x.
You want to
to make
probabilistic
statements
regarding an
unobserved
x.
With that,
you need to
calculate x's
posterior predictive
distribution using
the formula
p of x
given x.
In this
posterior predictive
distribution,
you want only
the condition
of the observed
previous data.
To calculate
the predictive
distribution,
you will need
to use the
following formula.
The probability
of y
over y
equals
the integral
of the
probability
of y
over theta
times the
probability
of theta
over y
times d
theta.
This shows
an appropriate
assumption of
the fact that
the past data
restricted on
the parameters
and future data
is independent
to each other.
Using the
formula,
you will be
able to obtain
the posterior
predictive distribution
by integrating
the data
model distribution's
product with
the posterior
distribution with
regard to the
model parameters.
You can then
summarize the
posterior predictive
distribution for
predictive inferences.
Likelihood
Function
Simply called
likelihood,
the likelihood
function is a
function of a
statistical model's
parameters given
data.
These functions
play an important
part in statistical
inference,
particularly in
estimating a set
of statistics
parameters.
Many times,
people use the
likelihood synonymous
to probability,
especially in
informal contexts.
In statistics,
however,
there is a
distinction depending
on the parts
played of outcomes
versus parameters.
A key difference
between the two
is that you use
likelihood after
data become
available used
for describing a
parameter value's
plausibility.
On the other
hand,
you use
probability before
data become
available,
using it for
describing a
future outcome's
plausibility.
Distinguishing
likelihood and
probability is
vital in Bayesian
statistics.
A clear
distinction is that
probability attaches
itself to possible
results while
likelihood attaches
to the hypotheses.
In order to
learn distinction
between the two,
distinguishing
possible results
and hypotheses
is necessary.
Possible results
can be exhaustive
or exclusive.
For instance,
you are to
predict the
outcomes of
each of five
rolls of dice,
which has six
possible results
from zero to
five correct
predictions.
Though it has
six possible
results,
the actual result
is always going
to be one
out of these.
This means the
probabilities
attached to the
possible results
sum up to one.
On the other
hand,
hypotheses are
neither exclusive
nor exhaustive.
In the above
example,
suppose out of
the five outcomes,
the test predicted
three correctly.
Then the
hypothesis could be
that you simply
guessed the
correct outcome.
Some people
might hypothesize
you may be
someone psychic
with expectation
that you can
correctly predict
the results
with better
chance rates.
These are
hypotheses,
different to
each other
and not
mutually exclusive
because you
were cautious
and get around
it by saying
maybe.
This means that
the other person
allowed his
hypothesis to
include yours,
meaning your
hypothesis nests
within his.
Another person
makes another
hypothesis,
which may turn
out correct.
Another person
may also
hypothesize
something else
and he or she
might be wrong.
This is simply
no limit to
what hypothesis
someone can
come up
with and
entertain.
The only
limitation to
your hypotheses
is your own
capacity to
conjure them,
which people
are rarely
confident of
saying they
imagined all
possible
hypotheses there
is.
In Bayesian
statistics,
the concern over
hypotheses is the
extent by which
experimental results
can affect the
hypothesis'
relative likelihood.
The hypotheses
you attach to
the likelihoods
do not actually
have meaning in
and of itself
because people
usually don't
entertain the
whole set of
alternative hypotheses
and the fact
that some
hypotheses encloses
in within others.
The only
likelihoods that
have meaning
are relative
likelihoods or
the ratio of
two likelihoods.
The likelihood
function is
fundamental in
Bayesian
statistics,
thus the
importance of
making clear of
the difference
between likelihood
and probability.
Simply remember
that probabilities
assign to
results as
with likelihood
to hypotheses.
Taking note
of the example
above, you can
define likelihood
to be a possible
value, or an
array of possible
values, for the
distribution's
mean or average
range.
Probability results
are usually
exhaustive and
inclusive, while
hypotheses are
neither.
In order to
decide the
likelihood of two
hypotheses given
an experimental
result, you
will need to
consider their
ratios, also
known as
relative likelihood
ratio or
Bayes factor.
Sometimes
written simply
as L
parenthesis
theta, the
mathematical
statement for
solving the
likelihood function
is L of
theta x equals
the product
n i equals
1 times the
function of
xi theta.
In algebraic
expression, the
likelihood, L
of theta x, is
similar to the
distribution f x
of theta.
However, they
have quite a
different meaning
since you observe
the likelihood as
a function of
theta instead of
a function of
x.
Conjugate
prior
In Bayesian
statistics, if the
prior probability
distribution and
posterior distributions
are within the
same family, you
can call the
prior and
posterior as
conjugate
distributions.
On that, the
prior is the
likelihood function's
conjugate prior.
Consider the
problem where you
need to make an
inference of a
distribution or
parameter theta
given data x.
Using Bayes'
theorem, you can
obtain the
posterior distribution
by multiplying the
prior and the
likelihood function,
which then
normalize by the
data's probability.
In mathematical
equation form, this
would be the
probability of
theta given x
equals the
probability of
x given theta
times the
probability of
theta over the
integral of the
probability of
x given theta
times the
probability of
theta times
d theta.
Consider the
likelihood function
fixed, as a
statement from the
data generating
process usually
determines it well.
There are
different choices
for the prior
distribution, and
this can make the
integral somewhat
difficult to
calculate.
Another concern is
the product of
prior and
likelihood may
take different
algebraic
expressions.
The prior can
have the same
algebraic expressions
as the posterior in
for certain prior
choices, which
generally applies to
those with
different parameter
values.
This prior choice is
called conjugate
prior, an algebraic
convenience that
provides the
posterior a closed
form expression.
Conjugate priors give
intuition, showing
more clearly how you
can update prior
distribution with
likelihood function.
The Bayes Factor
The Bayes Factor is a
useful tool in
statistics, a Bayesian
alternative to the
classical hypothesis
testing.
When using Bayes
Factor, the method of
selecting model is the
Bayesian model
comparison.
The Bayes Factor's
purpose is the
quantification of the
support for a model
compared to another,
whether these models are
correct or not.
Defined as the ratio of
two opposing hypotheses,
likelihood probability,
the Bayes Factor is
often null and acts
as an alternative.
In an example for
model M given
data D, the
posterior probability,
probability of M
given D, is given
through the Bayes'
theorem.
It follows the
formula as stated
below.
The probability of
M given D is equal
to the probability
of D given M
times the probability
of M all over the
probability of D.
The term the
probability of D
given M refers to
likelihood.
It represents the
probability that you
can produce data under
the model D's
assumption.
Evaluating this
correctly is the key
for appropriate Bayesian
model comparison.
With Bayes Factor,
you can formally compare
two competing models,
M1 and M2.
Bayes Factor is a lot
similar to the
likelihood radio test
in the classical
statistics.
The only difference
is that in Bayes'
factor, you don't
need to nest one
model within the
other.
If given the
data set Y, you
compare two models
of using Bayes'
factor in this
form.
The M of 1
equals 1
times X
given theta
1.
M2 equals the
function 2
of X
given theta
2.
For the prior
distributions, you
can use the
probability 1 of
theta 1 and the
probability of 2
theta 2.
That will leave the
prior probabilities for
each of the model,
the probability of
M1 and the
probability of M2.
Using Bayes'
law, you can
calculate the
posterior odds for
the calculating of
the ratio of the
two competing
models.
The formula for
calculating the
Bayes'
facto is the
function of 1
times X
given theta
1 over the
function of 2
times X
given theta
2.
Bayes'
factor is vital
as it tells us
about how useful
a data is with
given tested
hypotheses.
If the Bayes'
factor is 1,
it means the
data obtained is
useless and it
is equally
consistent with
the hypotheses and
null hypotheses.
If you obtained
1 to 2 in
the Bayes'
factor, it
means data is
marginally
interesting, but
it is likely to
mean a huge
difference between
the hypotheses and
null hypotheses.
If the result is
between 2 and
5, the data
represents a minor
difference between
the hypotheses and
null hypotheses.
Chapter 4
Bayesian
Inference
The Bayesian
inference is a
powerful mathematical
tool used to
model any random
variable.
This tool offers a
way in which you
can get sharper
predictions inferred
from your data.
You can use in
variety of things
such as modeling
the value of a
business, KPI,
regression parameter
or demographic
statistics.
Bayes'
inference is a
particularly useful
tool if you don't
have much data and
you make the most
predictive strength
you can squeeze out
of it.
Using Bayesian
inference, you provide
your understanding of
a problem along with
some data.
In return, you can
have a quantitative
measure of a certain
fact's certainty.
This is a practical
approach when it comes
to modeling uncertainty.
This is especially so
when you have limited
data or worried about
overfitting.
This tool is also
useful if you
believe that some
of your facts are
more likely than
the other facts, but
the information is not
available in the data
you modeled on.
This particular
approach is useful
still when your
interest is on how
likely certain facts
are then choosing
which ones is the
likeliest.
The Philosophy
Behind Bayes'
Inference
Approach
Bayesian
inference is a
statistical approach
where you express
any form of
uncertainty in
probability.
This is a
statistical inference
method using
Bayes' theorem
used in updating
a hypothesis
probability
whenever new
information or
evidence becomes
available.
It is an
essential technique
in statistics,
particularly in
mathematical statistics.
The use of
Bayesian inference
found application
in a wide array
of activities,
such as medicine,
science, sport,
engineering, law,
and philosophy.
This approach to
solving problems
begins in
formulating a
model in the
hopes that it is
enough in
describing the
particular situation
you are interested.
After formulating
a model, you
then create a
prior distribution
over the model's
unknown parameters.
You use the
prior distribution
in capturing your
theories about the
situation before
seeing the
available data.
Once you have
observed the
data, next is
you apply the
Bayes' rule in
order to obtain
the unknown's
posterior distribution,
taking into
account the prior
distribution and
the data.
From the
posterior distribution,
you can compute
predictive distributions
you can use for
future observations.
A theoretically
simple process,
Bayesian inference
is reasonably the
proper approach in
dealing with
uncertain inference
regarding arguments
consistent with
clear rationality
principles.
Despite this,
however, many
people are still
uncomfortable with
Bayesian inference
because many of
them believe the
selection of prior
distribution to be
subjective and
arbitrary.
That Bayesian
approach is
subjective is true,
but it is not
all arbitrary.
In theory, there
is only one correct
prior that captures
the prior belies,
which are subjective.
On the other
hand, there are
many statistical
methods used in
statistics, which
are truly arbitrary.
While just as good
as Bayesian inference,
these methods do
not have a
principled way of
choosing the prior
between them.
In these pseudo-Baysian
methods, people
usually choose the
prior's base solely
for its convenience.
Sometimes they use
the Bayesian model
when they already
know their models
cannot possibly be
any good or
accurate description
of reality.
These procedures
of obtaining
priors have no
real justification
based on Bayesian's
approach.
Since they do not
have any justification,
you can consider
them highly uncertain.
Theoretically,
the Bayesian method
is simple to use,
but it is in some
cases truly difficult.
When it comes to
translating subjective
prior beliefs into
mathematical model and
priors, most people
find themselves not
sufficiently skilled for
this.
In addition, Bayesian
approach may also have
some computational
difficulties, attributed
mostly to the use of
Markov-chain Monte Carlo
methods.
How Bayes'
inference works
Many people describe
Bayes' inference in some
kind of reverence, but
it isn't magical or
mystical.
The concepts behind
this approach are not
obscure, but completely
accessible, although
math can sometimes get
dense.
The Bayesian
inference approach merely
lets you draw stronger,
more solid conclusions
from your already
existing data by folding
in the things you
already know about the
answer.
In essence, Bayesian
inference is educated
guessing, using
probability and
statistics.
One example of the use
of this approach is when
someone dropped their
wallet in front of you
in the mall.
You want to get that
person's attention, but
what you already know
about them is the way
they look from behind.
Long hair, shirt, and
jeans.
Now, considering male and
female, pretty much wear
jeans and shirt, the hair
is your prior.
From it, you assume the
person is maybe a girl.
However, upon further
observation, you notice
the person is standing in
line to the men's
restroom.
This added information
will infer that the
person might be a man.
This is the kind of
prediction that people
usually do, or often
without thinking, and
uses only background
knowledge and common
sense.
To make a more accurate
prediction, Bayesian
inference captures this
situation in math to
represent the values in the
situation.
The prior distribution is
often subjective, which
mean you will have to
assume certain things
based on your background
knowledge.
On that note, assume
there are 100 people in
that portion of the
mall, 50 women and 50
men.
Of the 50 women, 20
have shorter hair and
30 have longer hair.
Of the men, the men
with shorter hair are
47, the men with longer
hair are 3.
The person you saw who
lost their wallet is
long-haired and in this
number.
There are 30 women and
3 men with longer hair.
Judging from the numbers,
the safest bet is that the
person who left their
wallet is a woman, but
this number is in the men's
line to the restroom.
This time, there are 100
people, but the ratio is
98 men and 2 women together
with their partner.
Based on the number above,
there will be 94 men who
have short hair and 4 men
with long hair.
As for the women, the new
ratio is 2 women with long
hair and 1 with short hair.
In these numbers, the people
with long hair in the men's
line are 4 men and 2 women.
This time, your best bet of
the person who might have
lost their wallet is one of
the 4 long-haired men.
This shows the concrete
principle behind Bayesian
inference.
It means that by knowing
the key information, you
can make better predictions
about a situation.
Chapter 5.
The Importance of Bayes'
Theorem in Probability
Theory
If you have come across
probability before, chances
are you know or at least
you must have heard about
Bayes' theorem.
A mathematical theorem, it
plays a central role in the
theory of probability.
The most common association
of this mathematical theorem
is the use of evidence in
order to update rational
beliefs to a hypothesis.
In this section of the book,
you can learn its connection
to probability theory and
its importance in the real
world.
To some people,
understanding the Bayes'
theorem, even probability
probability theory, is not quite
an easy task.
With all those equations and
overly formal instructions, it
all gets confusing.
This section aims to give you the
general gist of the theorem.
One perfect example of the
theorem's application in the real
world is when you are deciding
whether to bring an umbrella with
you or not.
You have to be somewhere, and you
are thinking whether to bring an
umbrella or not.
You look at the sky, and you see
clear skies, so your first thought
is rain is highly unlikely.
On the other hand, you glance
around at the horizon.
You can see dark clouds pooling.
Seeing that, you decided you should
bring an umbrella after all.
After checking with the day's
weather broadcast, you even improved
your prediction whether there is
going to be rain or not.
This is Bayes' theorem at work.
By using available information
about the current weather
conditions, as well as other
resources, you were able to
estimate a logical probability.
Bayes' theorem is a helpful
mathematical tool that does great
in updating probabilities in the
best way possible using evidence
available.
You will better understand Bayes'
theorem with the knowledge of a few
terms in probability theory.
Events and Probabilities
In Bayes' theorem, there are
events and probabilities.
Events are outcomes, or set of
outcomes, of a process.
Events are the result of something
that happened.
Some examples of the events include
the following.
Raining on a Monday
Getting a king from a deck of
cards
Getting heads after a coin toss
Getting number six from a rolling
die
There are different types of events
independent, dependent, and
mutually exclusive.
An event is independent if no other
event affected it.
It is dependent or conditional when
another event affected it.
Mutually exclusive events are events
that cannot happen at the same time,
such as you can turn left or right at
the same time.
In some of the examples above, like the
coin landing heads, the event is when
it landed heads and the process is the
act of flipping.
The process refers to what happened
leading to the outcome.
With these examples, you get the idea of
what an event is.
The next concept you need to know about
Bayes' theorem is probability.
An event's probability refers to a number
representing the uncertainty of an event
from occurring.
In everyday life, you probably always hear
about people talking probability in
percentage.
One example is the 50% probability that a
coin will land heads.
Besides percentage, mathematicians also use
decimals as more standardized representation
of an event's probability.
In this representation, if an event is
impossible to occur, it will have a zero
probability or 0%.
On the other hand, an event that will
certainly happen will have a probability
of 1 or 100%.
If an event has some degree of uncertainty
that it will occur, the number assigned to
it will be something between 0 and 1,
wherein the closer it is to 1, the event
is likely to happen and vice versa.
A clarification.
There are no negative probabilities,
probabilities, and there are no probabilities
that are greater than 1.
When a probability is 0, it already means
an event will not occur.
If the probability is 1, that means it is
certain to happen.
If the number is in between 0 and 1, it
refers to the event less likely to happen
or not.
This is how you represent probability by
number.
Conditional probabilities.
In the example about whether it will rain
or not, you ask the question, what is the
probability that it will rain when it's
clear skies?
Windy but dark clouds are invisible in the
horizon.
You wanted to know about the general
probability that it will rain, after taking
into account the current weather conditions.
In the theory of probability, they refer to
these probabilities as conditional.
Conditional probabilities help express the
probability of an event, the event 1, to
happen when you know or assume that another
event, event 2, happened already.
When you first looked at the sky, you thought
it is moderately likely to rain, so you
assigned it a probability of 0.6.
However, after seeing it is windy and cloudy
as well as seeing the people bringing umbrellas,
you updated your guess to a probability of 0.80.
Saying it is highly likely it will rain.
After seeing the conditions, you updated your
prediction of the event accordingly.
What happens, however, if you don't know the
probability conditions in an event?
In calculating the probability of an event,
is there any standard way you can use?
How does Bayes' theorem play in predicting
the probability of an event?
Bayes' theorem in connecting probabilities and
conditional probabilities.
Theorem in mathematics refers to true
statements.
It means those statements are the true because
you can use logic to prove them.
In proving it true, it has to start on a few
basic statements referred to as axioms.
When calculating probabilities using Bayes' theorem,
you should know, based on the prior chapters,
that there are four parts.
The probability of prior.
The probability of evidence.
The probability of likelihood.
And the probability of posterior possibility,
wherein theta is event 1 and x is event 2.
Applying the terms to the conditions,
the event 1 indicates your first thought that
it will rain.
Event 2 refers to the current weather conditions,
windy and cloudy, as the evidence.
The likelihood term is the possibility of what
weather condition will occur between windy and
cloudy and rain.
The posterior probability, on the other hand,
is the resulting probability after calculating given
values.
According to Bayes, you can calculate event 1's
posterior probability, if given there is event 2,
by multiplying likelihood and prior probability,
then dividing their product with the evidence.
The formula to calculate the posterior probability say
that it will rain.
Follows the Bayes rule as follows.
The probability of theta given x equals.
The probability of x given theta times the probability
of theta divided by the probability of x.
This means you can only obtain the posterior probability
of an event given another one.
If the prior probabilities are 0.6 chances of raining,
and the evidence shows that there are 0.42 probabilities
it will, the likelihood of it happening is 0.67.
Following the formula of Bayes' theorem and the given values,
you can solve the posterior probability like this.
The probability of theta given x is the probability
of rainy, cloudy, and windy.
Equals the probability of theta given x and possibility of theta.
This is the possibility of cloudy and windy, rainy times
the probability of rainy, divided by the probability of x,
the probability of cloudy and windy or rainy.
The probability of theta given x equals
the probability, which is 0.67 times p.6,
divided by p.42.
The probability of theta given x, then,
is p times 0.402,
divided by p times 0.42,
which gives the probability of theta given x at 0.96.
Following these values,
the result in calculating the posterior probability
should be 0.96.
Going with this equation,
this is a very high possibility of a rain,
and that you should really take an umbrella with you.
Taking in the knowledge that Bayes' theorem
can provide in terms of probability,
it shows this mathematical tool
is one of the best tools to update probabilities
whenever there is a new knowledge at hand.
The calculation is actually quite simple,
that the use of the theorem in real-world situations
may seem straightforward.
The problem, however,
is assigning value to the terms in the formula
is quite a challenge.
Additionally,
using the formula for calculating problems
that are more complicated
is usually difficult.
This is barely the entirety of Bayes' theorem,
but this will give you an idea
about how useful the theorem is.
There are many ways you can use Bayes' theorem
in everyday situations.
In business,
computer science,
engineering,
statistics,
finance,
and more.
The Bayesians
If one has to describe the Bayesians,
it would be accurate to say
they are like snowflakes.
Many people asset the Bayesian approach
to be a unified theory
on decision-making and inference.
The truth, however,
is a great diversity lies
in the middle of all Bayesians.
Every Bayesian is unique to each other,
a distinct accumulation of opinions
regarding the correct approach
for statistics.
Most Bayesian practitioners
believe that every statistical problem
is part of a decision theory,
considering it essential
in properly framing the problem.
Not all Bayesians share this opinion.
The Bayesian approach
divides itself into two categories,
the subjectivist
and the objectivist.
Most Bayesians recognize
the importance of probability's
subjectivist interpretation
in the whole scheme of inference.
There are, however,
strict objectivist Bayesians
rejecting the subjectivist interpretation.
In addition to the division
between subjectivist and objectivist,
Bayesians also differ
from their belief
in the distribution of parameters.
Most Bayesians feel
they should reduce the parameters
to distributions
in obtaining single values.
But many don't have reservations
about including the entire distributions
to describe the parameter's uncertainty.
Most Bayesian practitioners
elicit the use
of precise probability model.
However,
some permits using
classes of probabilities
instead of a single particular model.
These are only some
of the things
separating Bayesians
to each other.
But what surprises most people
is the fact that
not all of its practitioners
uses Bayes' rule.
Using Bayes' rule
is basic
in Bayes' approach
to statistics,
but not all Bayesians
use it regularly
for updating priors.
Instead,
when new data
becomes available,
they simply re-evaluate
the priors
with the Bayes' rule
intended for application
in unspecified time
in the future.
Chapter 6
The Bayesian Approach
Advantages
Disadvantages
and Limitations
in Risk Analysis
Proponents of Bayesian Approach
give high praise
to this probability theory.
The Bayes' theorem
does not have
great advantages
and wide applications
that many fields
use it today.
In this section,
you can find out
all the most important
advantages of Bayesianism
in uncertainty propagation
and risk analysis.
Of course,
there are also
limitations to this approach,
which is also
essential to know.
In this section,
you will find out
the numerous advantages
of Bayesianism
in risk analysis,
one of the fields
in which statistics
are widely used.
We mostly use
Bayesian approach
in statistics,
and it makes sense
to consider its advantages
to the area
where its practitioners
are most familiar.
Frequentist
Statistical Inference
remains dominant
in the field of science.
The last quarter century,
however,
has shown Bayesian approach
grew enormously,
providing many advantages
over the frequentist approach.
On that note,
here are some
of the interesting advantages
of Bayesian probability
in the field
of risk analysis.
Advantages of Bayesian Approach
in Risk Analysis
When using Bayesian approach
in determining the risk
and uncertainty,
here are some
of the advantages
that make it
one of the best
and highly used approaches
in probability theory.
Natural Definition
of Probability
One of the greatest advantages
of Bayesian approach
is the natural way
of defining probability.
It does so by describing
probability
in subjective views.
Instead of a quantity
of limiting frequencies,
it defines an event
or statement's probability
as a subjective quantity.
The Bayesian practitioners
do so
by computing
credibility intervals,
which describes
a parameter
estimates uncertainty.
The methods
in Bayesian approach
are arguably easier
to implement.
Bayesians argue
that compared
to the traditional
confidence interval,
the Bayesian credibility
intervals are easier
to work with
and more natural
with easier
to understand
interpretations.
Compared to frequentists'
methods,
where they do not
allow distributions
as fixed quantity models,
the Bayesians use
distributions for both
the model's data
and parameters.
Since frequentists
do not allow
the use of distributions,
they can only consider
probabilities
for the collected
random data.
With this,
frequentists create
inferences that look
contorted.
This is not the case
with Bayesians,
who uses all
available information,
even those outside
of the experiment.
Following systematic
procedure in acquiring
probability
and updating
new importance,
the Bayesian approach
offers results
with greater flexibility.
A Rational Approach
Rationality is an
important characteristic
of Bayes' probability.
They often tout
the approach's
rationality referring
to technical
mathematical theorems
used to justify
their method
of testing probability.
The method's
rational quality
stems from the idea
of making the most
of the expected utility.
For the Bayesians
to operationalize
this idea,
they use betting
in interpreting probabilities.
When gamblers
act against
their own interests,
it is usually
because their beliefs
and preferences
are irrational.
Gamblers in this
fate are at
a sure loss,
making them
a money pump
for the bookies.
This happens
when their probabilities
are incoherent.
Having blinded
by their irrational
belief,
they have considered
the odds
and were sure
they can win,
but without
considering the
probabilities.
The only way
they can avoid
the loss
is by making sure
they have coherent
probability assignments.
Rationality means
that even though
the estimates
could be wrong,
they should at least
make sense.
That is,
the estimates
should not
contradict each other.
On that note,
the Bayesian approach
proves its rationality
by being the only one
consistent in handling
uncertainty
in inference
and calculation.
It is typical
for different people
to have different
preconceptions,
which causes them
to have different
inferences,
particularly
if there is
limited data.
However,
rational people
can arrive at
more or less
the same probabilities
and inferences
given the same
information by
following Bayes'
rule in observing
prior data
and updating
through accumulation
of knowledge.
Data Mining
and Analysis
The ability
to mine data
is one of the
greatest advantages
of Bayes'
probability.
According to
classical statistics
approach,
data mining
is improper
since you have
no legitimate
way of computing
p-values
once you have
examined the data.
With this,
you have not yet
formed a prior
hypothesis,
then you should
not peek at the data.
Following this
concept,
you should not
peek at the data
because it can
contaminate the
scientist's mind.
However,
in practice,
scientists always
inevitably peek
at the data.
The fact is
that data is
often in short
supply,
and as scientists
collect them,
they end up
peeking at them.
To traditional
scientists,
you can only
make hypotheses
when you begin
to see the world
anew
through new data.
This is different
in Bayesian
approach,
as it is proper
for them to
peek at data.
They do not
have any guilt
or hesitation
in looking at
the data,
estimating the
parameters all
at the same time,
or even
that they have
very few samples,
all because of
Bayes'
rule's rational
method of
observing data
and updating
old information
to current
knowledge.
Use of
subjective
information
In Bayesian
approach,
it is perfectly
normal to use
subjective
information.
This particular
approach does
not only
formalize,
but to some
extent also
legitimize using
subjective
information,
personal judgments,
beliefs,
and preferences,
explicitly to
draw probability.
Bayesian
approach gives
analysis the
freedom to use
their subjective
judgments to
influence their
objective equations.
For risk
analysis,
this feature is
appealing,
one especially
since most
available information
is in some way
subjective.
Of course,
analysts mostly
prefer validated
data,
but to some
it would be
unwise not to
use the
information
available inside
analysts as
well as their
consulting experts
heads.
When available
of data source
is limited,
subjective
knowledge is
the next best
source of
information.
While frequentists
do use subjective
information as
well, they do
so in absurdly
cryptic way that
has Bayesians
complain about it.
To Bayesian
practitioners, the
best way to
utilize subjective
information is by
using it in
explicit accounting.
Decision
making
With Bayesian
approach, decision
makers and
analysts can
take advantage of
decision analysis
allowing them to
construct a set of
decisions in
correct management
of risk assessment
consistently.
The Bayesian
approach provides
sensible decisions
by ensuring they
meet the
decision's
theory's axiom.
Bayesian
practitioners are
able to do so by
utilizing Bayesian
approach in conveying
every uncertainty
with probability.
In frequentist's
approach, the
decision made is
only whether the
set of data is
enough to justify
accepting or
rejecting a null
hypothesis.
When frequentists
make this decision,
they rarely wait
that cost if they
make a wrong
decision.
Instead, they
simply control the
error of rejecting
the null hypothesis
incorrectly.
Making decision in
this way has two
costs, which
frequentists do not
consider.
First, they are
assuming that
something is right
when actually it
is wrong.
Secondly, there is
the cost of
believing something
is false when in
actuality it is
true.
If you want to
make rational
decisions, you
need to consider
both of these
costs weighing them
against each
other, seeing how
they each affect
a decision made.
The Bayesian
decision analysis
does exactly this,
with practitioners
made sure to
integrate at a
fundamental level.
Always yield an
answer.
One of the
pivotal advantages
of Bayesian
approach is how,
even when frequentist
methods do not
apply, it can
still yield an
answer in principle.
Even in
situations where
no data is
available, you
can use Bayes'
rule to produce
answers by using
unmodified prior as
the zeroth posterior.
Since there is no
available data, you
cannot alter the
prior even when you
use Bayes'
rule.
Other advantages
of Bayesian
approach.
In addition to
these advantages,
Bayesian approach
provides inferences
that are exact and
restrictive on the
data without having
to rely on
asymptotic
approximation.
Both small and
large samples
proceed in the same
manner when determining
inferences.
Additionally,
Bayesian
analysis allows
the direct
estimation of
parameters function
without plugging
estimated parameters.
Using Bayesian
approach gives
you the advantage
of a convenient
setting ideal for
wide array of
models.
It means you
can ideally utilize
the Bayesian
analysis for
models such
missing data
issues,
hierarchical
models,
MCMC,
as well as a
number of other
numerical methods.
With using this
method, you can
make calculations
that you can
track at nearly
every parametric
model.
These are among
the biggest
advantages of
Bayes'
approach to
probability.
This approach
ensures that you
arrive at an
answer in a
rational way,
without compromising
the data even if
no data is
available.
On a side note,
while it has
great advantages,
the Bayesian
approach has its
fair share of
disadvantages,
limitations,
and criticisms.
The Bayesian
approach has its
fair share of
disadvantages,
limitations,
and controversy
since statisticians,
analysts, and
scientists employ
this method.
In uncertainty
propagation and
risk analysis,
the most relevant
disadvantages of
the method include
the inability to
distinguish between
equiprobability and
insertude of the
Bayesian model of
ignorance.
In addition to
this, the
Bayesian
approach also
experienced
criticism for
the overconfidence
for its
conclusions, as
well as for the
acceptance of
subjectivist view
in decision-making,
particularly in
public policy.
Another disadvantage
directed to
Bayesian analysis is
how it does not
specify the process
of choosing a
prior.
In Bayesian
approach, there is
no correct way of
choosing a prior.
In order to make
Bayesian inference,
you will need to
skill to translate
the subjective
prior beliefs into
mathematical
formulations of
for the prior.
As such, not
being cautious would
most certainly end
in misleading
results.
At the same time,
the fact that it
produces posterior
distributions with
heavy reliance to
priors can be a
disadvantage as well.
In practical
perspective, a
heavily prior-reliant
posterior distribution
makes it difficult
to convince the
subject matter
experts, especially
if they disagree
with the chosen
prior.
Another salient
disadvantage of
Bayesian approach
is its computational
cost is high.
This is especially
true with models
that have parameters
in large numbers.
Bayesian approach
also provides
simulations with
slightly different
answers unless
the analyst uses
the same random
seed.
While the simulations
are not exact, it
does the effect
the earlier claim
that Bayesian
inferences are.
Given the priors
and likelihood
function, a
parameter's posterior
distribution is exact.
On top of these
disadvantages and
limitations, the
future practitioners
of Bayesian
approach would
inevitably have to
struggle with the
significant
computational and
analytical
difficulty that
creates obstruction
in finding sources
for solutions and
the numerical
calculation of
answers.
Additionally, there
will be most likely
to have to face
other controversies
over the use of
Bayesian methods.
The necessity
for a prior
One of the first
issues novice
practitioners of
Bayesian methods
feel agitated
about is the need
to choose a prior.
You cannot employ
the Bayesian
approach if you do
not select a prior,
a task the
analyst is responsible.
As mentioned
before, the Bayesian
approach does not
specify a correct
way for choosing a
prior.
Supposedly, the
selection of the
prior is subjective.
It means that in
principle, there
should be no one to
complain about the
prior selection.
However, it is the
fact that the choice
the analyst makes
matter extremely to
the results and so
everyone is concerned
about the selection.
With that, even
seasoned analysts are
often anxious regarding
these selections.
According to the
proponents of
Bayesian approach,
data will soon
quickly overwhelm,
which will soon
obviate the prior.
This is true in
some cases, but it
is not always the
case in practice,
which brings about a
serious challenge to
analysts.
They end up with the
conundrum that if
prior distribution has
little or no effect
on the posterior
distribution, then
what should one
bother to factor it?
On the other
hand, if it does
have significant
effect on the
results, how can
the analyst dare to
act on the drawn
conclusions if he
does not know what
he is doing?
What sort of action
should the analyst
take?
The need and
selection of priors
is one of the
foremost limitations a
would-be Bayesian
practitioner will
likely face.
Regarding this,
there are two
advices that Bayesian
analysts can employ in
selecting priors.
First is to validate
their assumptions and
evaluate the
assumptions'
credibility in light
of the known
knowledge.
Second, analysts
should explore the
results of the
investigation of the
prior and the
assumptions.
The question is,
where should you get
the priors and how
do you determine
them?
Up to what extent
can you justify these
priors?
There are different
approaches on selecting
the prior that you can
try.
Some of them
include uninformative
priors, conjugate
prior, and maximum
entropy priors.
Additionally, there are
also personal and
subjective priors,
reference priors,
uniform priors, and
more.
Difficulty in
Determining the
Likelihood Function
Besides the
difficulty in selecting
the prior distribution,
the difficulty in
determining the
likelihood function is
yet another limitation
of Bayesian
statistics.
In fact, selecting
the likelihood
function is perhaps
even more difficult
than with prior
distribution.
Just like the
prior, there is
little to no
guidance in
determining the
likelihood function.
There are plenty
of strategies proposed
by statisticians and
analysts in this
matter, though.
Determining the
likelihood function is
a great difficulty,
not just because of
the likelihood itself,
but also because it
usually involves
complex mathematical
computation.
At best, a
probability model's
relevance and
appropriateness in
justifying a specific
likelihood function is a
matter of the
analyst's opinion.
If data is available,
the analyst will likely
have a difficulty in the
sampling process they
think would be
appropriate in modeling
the distribution.
However, according to
the Bayesian
approach, one can
precisely specify the
likelihood function and
the risk analysis known
precisely with
distribution to get the
sample data.
This is a weak
conjecture because this
supposed knowledge of
the analyst could either
be available or not.
Every risk analysis
must take careful
thought in choosing
what likelihood is
suitable for a risk
analysis distinct case.
In this situation, a
number of classic
cases have become
popular, which are
lucky to have the
prior and likelihood
combined quite
conveniently.
When the likelihood
and prior combine like
this, you can refer to
them as conjugate
pairs.
With this, there is no
need for the risk
analysis to conduct any
calculation of the
normalization factor.
When it comes to the
use of conjugate
pairs and how to
make the calculations,
analysts consult a
standard reference
regarding Bayesian
methods.
With these conjugate
pairs, it becomes
simple to adhere to
the demands of Bayes'
rule application.
For the sake of
convenience, analysts
widely use conjugate
priors for the
likelihood function.
The subjectivity of
the analysis
The subjective
characteristic of
Bayesian approach is
also a limitation.
When inferential
updates occur within
the Bayesian network,
the accumulation of
knowledge happens.
Each link in this
process involves the
application of Bayes'
rule.
Each time new
information becomes
available, the
analyst treats the
earlier obtained
posterior to be the
new prior.
With the new prior,
the whole process
starts again, where
he uses Bayes'
rule to condition
the newly
available information
into new
knowledge.
This is the
inferential process.
The process must
start somewhere, and
that start refers to
the situation of
interest, at the
time when any
specific information
or evidence is yet
to be available.
In this state, the
inferential process
must purely start as
the reflection of
the analyst's
subjective beliefs,
judgments, and
preferences.
It shows how
vital the
subjectivity is as
a component of
Bayesian inference
and probability.
This subjectivity,
however, has its
share of arguments
regarding its part
in the inferential
process.
For instance,
Hamming argued,
even though Bayesian
approach may seem
like it produces
rational answers,
being that it uses
subjective judgments,
make it an
unspecific process.
If you criticize the
Bayesian approach's
employment of
subjectivity, this
claim does hold a
bearing.
It is recognizable
that you cannot escape
subjectivity in
employing Bayesian
approach.
However, that does
not mean that one
should altogether
abandon objectivity.
One should still
strive for objectivity,
at least when using
Bayesian approach in
risk analysis when
applied in public
policy or scientific
uses.
In Bayesian
approach, the
objectivity aspect is
not there because it
allows subjectivity or
that it requires it.
The objectivity aspect
is a vital part of
Bayesian analysis
alongside subjectivity
because it stumbles
in it, meaning that
there are nearly
inseparable part of
each other.
It is the fact that
in this modern world,
it is hard not to see
things without
subjectivity.
However, the
people who bring the
issue regarding
subjectivity are
seemingly scientifically
tenacious.
It is almost the same
with the need for
priors.
If you can never make
perfect measurement,
is there a point in
making measurements at
all?
If even after all the
analysis you make you
are still not getting
results, is there a
benefit to what you are
doing?
It is important to
recognize the importance
of objectivity in any
scientific initiative.
With the objective
aspect, it becomes
possible for the
analysis to have the
reproducibility necessary
that people see it
irrelevant.
At the same, it gives
the results with the
sanction of being
reasonable, if not
correct, due to it
having mutually
subjective validation.
Objectivity is what
holds science together.
It is what gives the
public confidence in
what science has to
offer.
Different analysts will
produce different
analysis, and the same
is with modelers.
However, in science
everyone must strive
that his or her
studies, research, and
investigations remain
objective.
This is necessary for
minimizing volatility,
and determining the
root of all
disparities, which will
serve as the one to
satisfy the need for
further empirical
research.
You see how important
objectivity is, and it
is in connection with
this where the problem
with Bayesian approach
lies.
The Bayesian analysis
starts subjectively, and
it allows virtually any
kind of quirkiness.
With this issue, some
Bayesians malign using
priors analysts, selected
subjectively.
In the many years that
passed, many of Bayesian
statistics have been
disinclined in using
personal priors in
conducting scientific
analysis.
This is why many
Bayesians look for
alternatives, so they
can make their results
more objective, or at
least that they could
give the impression of
being so.
Difficulty in making
computations
Perhaps one of the
biggest disadvantages of
Bayesian approach is its
computation difficulty, of
which it is notorious
about.
Out of the numerous
calculations involved in
Bayesian statistics, the
difficulty often arises when
it comes to computing the
normalizing factor of the
Bayes rule denominator.
This is because, usually,
there is no available
accurate form solution to
compute the integral.
This is unless it is the
case of conjugate pairs,
where the likelihood and
prior conveniently combine.
On the other hand,
conjugate pairs only work
nicely in computations when
it is logical reflection of
the analyst or statistician's
personal prior.
If there is no conjugate
pair, there are some
methods the analyst can
employ for computation of
results.
Some of these non-cojugate
methods are interactive
methods, such as rejection
sampling, importance
sampling, and weighted
bootstrap sampling.
These methods refer to
Monte Carlo methods.
In addition to it, there are
also the Markov chain
Monte Carlo or MCMC
techniques, which include
data augmentation,
substitution sampling, and
Gibbs sampling.
In addition to these
techniques, risk analysis
may also employ the
Metropolis-Hastings
algorithms.
These general methods can
help in optimizing
functions under particular
restrictions.
These are among the
strategies and techniques
that advance the
computation and ease its
accompanying difficulties.
More than the complexity of
the computations in Bayesian
approach, it also has an
intrinsic intellectual
complexity particular to
its practical application.
One of the reasons for this
complexity and the fact there
is no convenient way for
application of Bayes' rule in
real-world problems is because
analysts believe problems are
unique to each other.
With that, they never admit
manufactured solutions or
superficial answers.
Bayesian experts believe there
is no fully defined paradigm
or formula you can apply in
all situations when choosing
the prior or determining the
likelihood function.
Moreover, Bayesians believe this
is to be the responsibility for
every analyst, statistician, or
modeler must shoulder.
The difficulty of Bayesian
approach is not just with the
calculations or the application,
but the whole approach itself.
It has an aura of difficulty quite
similar to ancient mathematics,
one that erects a barrier for
would-be Bayesians.
This is also a notable
disadvantage of the approach.
In some respects, however, you can
also view it in advantageous light
because it pushes outsiders to
become a more respectful in the use
of Bayesian approach.
Bayesian probability can be quite
difficult, but still it plays a vital
role in successful risk analysis
and a number of other applications.
Suppose structural omniscience
of Bayesian analysis.
Many people presume that with
Bayesian methods provide the
analysts with deep understanding of
the structure of the modeled systems.
In some cases, people's requirement
for structural omniscience is using
Bayesian approach looks particularly
severe.
They have false belief that a Bayesian
analyst can look at his or her
subjective presumptions and examine
it in great details, even when he
or she has not thought about the
particular problem ever before.
The structural omniscience, many
people believe, presumes that the
analysts have the ability to compose a
likelihood function even there has not
been any data collected at all.
There is much presumption in Bayesian
approach that the analyst can know a
sample space structure well before they
gathered any kind of samples or study
the problem.
Naked guessing is simply not the way
Bayesians do this.
In the case when no data is available or
when the analyst is yet to study the
problem, using his or her subjective
judgment seems ideal, but would hardly
be any better.
The people who argued for this problem
leveled it to the Bayesians nearly a
century ago, and over those years, the
interest waned greatly.
For all those years, the Bayesians never
found a solution to the problem, and
there seems no solution for it in the
near structure.
Bayesian rationality does extend to the
context of group decisions.
The rationality of the Bayesian method
is one of its touted advantages.
The approach is also the best and the
only coherent way for analysts to
manipulate and represent the uncertainty
in decision-making and inference.
These advantages, however, seem to not
apply in particular group decisions.
When it comes to group decision-making,
a person can encounter countless
paradoxes when you need to take into
account the beliefs, judgments, and
preferences of more than one individual.
In many group decision-making cases, the
people involved find out that the only
way they can ensure there is rationality in
the group decision-making is by reducing
the problem so that it becomes an individual
decision setting.
In cases like this, the group requires a
dictator who makes all of the group's
decisions.
In under particular conditions, it also
shows the Bayesians cannot accumulate the
multiple rational decision-makers' preferences
in such a way that it will nullify the
Bayesian axioms.
On the other hand, some people think that
being dogmatic about coherence at all costs,
such as with Bayesians in being a bad decision
theorist, since all that matters is how well a
model solves a problem.
These were among the numerous limitations and
advantages over the Bayes' approach in
probability and risk analysis.
In over a century since Bayes' theorem first
introduced, it accumulated a fair share of
controversies and many people have studied it.
It has its difficulties and advantages, but no
doubt that it has practical use in risk analysis,
and many other applications.
Chapter 7.
Bayes' Theorem in Engineering Applications
Bayes' theorem is a mathematical tool, which
also found application in the field of
engineering, particularly in safety
engineering.
The traditional frequentist world taught us
that an event's frequency refers to the
number of times observed success or failures
occur in a specific time.
In the field of engineering, conducting a QRA
for advanced systems or emerging technologies
with no few observed failures, the determined
failure rates of the system or technologies are
not a trifling matter.
Bayes' Theorem in Safety Engineering
Quantitative Risk Assessment, QRA, has been
around for about three decades.
First applied in assessing the risk of large
technological systems.
Since then, QRA led to numerous methodological
applications and advances in space systems, nuclear
power reactors, chemical munitions, incinerators, and waste
repositories.
Many are at first skeptical about this technology, but after
seeing the insights provided by QRA, companies began to see how
useful it is to the company.
At first, these companies would pay for the negative
insights, which reveal the system's failure modes they
unsuccessfully identified before.
Upon identification of these insights, next is to take action to
turn these failure modes as well as their consequences to be
less likely.
After a while, they begin focusing on the positive insights of
QRA as it provides them the opportunity of finding failure
modes and fixing them before it results to a serious negative
consequence.
This shows how important QRA is in safety engineering.
It provides insights on failure rates of the different components of
any technology or system, providing quantitative value to risks.
QRA utilizes Bayes' Theorem, a mathematical method in predicting the
probability that an event will occur.
In the case of safety engineering, the Bayes' Rule helps in determining and
predicting failure rates of systems and technologies.
Bayes' Rule Application in Quantitative Risk Assessment
In the previous chapters, you have learned how to use Bayes' Theorem when
calculating conditional probabilities.
Using the formula, the probability of theta given x equals the probability of
x given theta times the probability of theta divided by the probability of x.
You can calculate the probability if an event will occur based on prior data and
the current data.
In the formula, the variable denotes the probability that event A will occur given
that event B happens.
The Bayes' Theorem formula expression is not only a practical mathematical method in
predicting the probability of an event occurring.
At the same time, it imitates the forces at work behind the process of learning and
acquiring new knowledge.
You can best understand it when you write the Bayesian Rule like this.
The probability of posterior given evidence equals the probability of evidence given prior times
the probability of prior divided by the probability of evidence.
Event A is the prior, which express the event's current state of knowledge or uncertainties.
The prior of the event can be generic knowledge.
In this case, the general engineering knowledge of the system or technology in question.
You can also obtain the prior from the same or a similar system's historical performance.
The evidence refers to available data, such as any experience with the same component or
system analyzed, including any failures.
After treating the available data with Bayesian approach, you can update your prediction to
posterior distribution, which represents your newly updated uncertainties or knowledge.
After this, when another set of evidence becomes available for observation, your posterior distribution
becomes prior.
Using the new set of prior and the newly available evidence, you can again update your existing
uncertainties and knowledge.
Risk, Uncertainties, and Precaution
The terms risk, uncertainty, and precaution are widely used in many ways in our society.
We use these terms not just as technical discourse, but also as an everyday language.
In safety engineering, these concepts play an important role, most importantly, in defining
quantitative risks.
Uncertainty
The word uncertainty connotes the three main directions.
The external world, the mind, and knowledge.
You may say the external world's future is not fixed when an election or a rescue operation's
outcome is not certain.
There are various situations in the external world showing uncertainty.
When you feel uncertain about the weather forecast or your molecular model, this shows uncertainty
or imperfection in your knowledge.
When you are not sure about what to do or feel, that is yet another form of uncertainty
in the mind.
These are the different dimensions in everyday notion, where uncertainty can come from, usually
connected but not always.
Leaving behind the uncertainty in the dimension of the mind and into scientific uncertainty,
where the law states that a researcher must always specify and clarify the degree of precision
and certainty research results characterize.
This supports the need for quantitative risk assessment, which wholly within good scientific
practice.
Providing quantitative results such as standard deviations, confidence intervals, standard
errors, etc., with statistics proves a great way to resolve uncertainty in scientific knowledge.
There are different methods and ways in dealing with quantitative uncertainties available, one
of which is through probability distribution and the use of mathematical tools such as Bayes'
rule.
There is a gap in the use of these methods for dealing with uncertainty.
However, Bayes' theorem proves to be one of the most practical methods for predicting risk
and determining uncertainties that will lead to appropriate precautionary measures.
The Bayesian approach can show elegant reflection of the process of updating current safety
knowledge and state of uncertainties in engineering systems.
Risk
An important factor in scientific uncertainty is risk.
Unequivocally difficult to define, risk analysis has two dimensions containing its concept.
The first dimension is in the degree that an event can possibly take place.
The second dimension lies in the consequences of that particular event.
In the first dimension, the degree of possibility or risk is often but not always quantifiable
as one's degree of belief or of probability.
It can be expressed on decimal scale ranging from 0 to 1 or in the common language of percentage
from 0 to 100 percentile.
The 0 or 0 percent signifies the risk's impossibility, and the value 1 or 1 percent signifies necessity
of certainty.
On the other hand, at least some of the consequences of the risk will be mostly taken as undesirable.
In terms of the possibility of risk in an event occurring, the consequences are often some
kind of harm in a quantified magnitude.
An example of this is in the context of investment situations, where risk refers to your chance
of winning or losing.
The term risk is, however, often used synonymous to hazard, indicating possible harm.
In some cases, risk might even take itself to be seen as a source for pleasure, which is
completely different to scientific risk.
In some ways, risk and uncertainty are opposites of each other.
In economics, there is risk when you can quantify an outcome's uncertainty in terms of probabilities.
On the other hand, they speak of strict uncertainty when it is not possible to quantify probabilities
in valid or rigorous manners.
The Bayes' Theorem provides mathematical approach in solving uncertainties.
In safety engineering, you can apply the Bayes' Theorem as a practical mathematical approach
in solving problems and uncertainties, especially in the QRA.
When applied in QRA, the formula becomes like this.
Risk equals uncertainty times damage.
There is no risk when there is no damage, or uncertainty.
In this formula, you will be able to guess the extent of damage through different levels
of uncertainties.
The Sources of Uncertainties
One of the sources of uncertainties in QRA is when it is impossible to enumerate all conditions
of the technology or system explicitly.
In addition, uncertainties may also come from incorrect or inadequate information on conditions.
It can result from inconsistent classification and interpretation of events, when you have
limited size of data sample, or when there is a lack of success data.
Lastly, uncertainty arises when the computer and mathematical modeling of a reality is imperfect.
When these are present, there is risk of damage.
The risk is what QRA aims to determine, predict, and assign probability through the Bayesian rule.
Through QRA, Bayes' Theorem, and other risk assessment methods, you can identify and eliminate
the risk in any system, leading thus to advanced technologies that will benefit the people.
Understanding Uncertainties
Uncertainty is one of the many concepts people use in many ways every day.
You use it in everyday language as well as in technical discourse, such as in risk assessment.
When a system's possible outcomes leave some undesired effect or shows significant loss, there is uncertainty.
It best describes a situation when there is ambiguous and or unknown information involved,
making it impossible to describe a future outcome, a more than one possible outcome, or an existing state.
Generally, there are two major classifications of uncertainties related to risk assessment.
The two classifications are Epistemic Uncertainty and Allatoric Uncertainty.
In the following section, you can learn more of the different types of uncertainties in risk assessment.
Stochastic or Allatory Uncertainty relates to the real variability of the physical property or a population.
You can further classify it as spatial and temporal variability.
Also referred to as statistical uncertainty, it represents the unknowns in an experiment,
which differ from each other every time you run the same experiment.
In this type of uncertainty, the argument is that simply because you cannot measure something sufficiently
due to the limitations of your current measure device does not mean that information's existence qualifies.
In risk assessment, random errors or uncertainties are statistical fluctuations in data.
You can find these errors in the measured data because of the measurement device's limitations in terms of precision.
If you are unable to take the exact same measurement in the same way as he did before in order to get the same number,
then the system must have random error or stochastic uncertainty.
Epistemic Uncertainty
Known as systemic certainty, it refers to things that you can do in principle but cannot in practice.
The reason for such errors is most probably inaccurate measurement or because the model neglected some vital effects.
In some cases, it could also be due to a certain data hidden in purpose.
The reason for this scientific uncertainty may also be limited data and information.
When an experimenter has inadequate understanding of the experiment's underlying processes,
performed imprecise evaluation of the characteristics, or has incomplete knowledge of an occurrence,
epistemic uncertainty is born.
In simple terms, you have uncertainty about something's operation because you do not know how it works.
When there is uncertainty, risk exists.
If you are 100% certain of something, then there is no risk.
What you have is 100% probability and 0% uncertainty.
As such, you need to have uncertainty first before there can be a risk.
If you have uncertainty over a system or technology,
then it has risk and you can use Bayes' approach in order to determine the probability of this risk.
Both of the kinds of uncertainties mentioned above are present in real-life situations.
In engineering, there are many situations where there are two uncertainties let themselves shown.
With uncertainty quantification or risk assessment, you can work towards reducing these uncertainties.
How to handle risks created by uncertainties.
It is probably impossible to eliminate uncertainty altogether.
However, you can work towards reducing epistemic uncertainty or lack of information towards
allotory risk or random errors.
These errors are a naturally occurring process and you can model the resulting risk through a probability distribution function.
With probability distribution function, describe all the possible values in the process and the probability of each.
In allotory uncertainty, you can obtain knowledge through past performance modeling, as well as reference class forecasting.
With the information you obtain through these methods, you can update your system or technology's past performance on the same work.
It will provide you with necessary information needed to ensure an ideal performance.
On the other hand, there is epistemic risk, which results from one's lack of knowledge.
To handle this type of risk, you will need to model it in order to define the probability of risk occurring.
At the same time, your model can also reveal the time frame when the probability becomes active as well as the impact when it occurs.
For epistemic type of risk, you can have either an implicit or an explicit plan for handling risk.
There are different methods such as mitigation, handling like actual work, buying the risk owned or retiring it.
In any case, it will take time and money to reduce the probability of the risk ever occurring.
You can also consume time and spend money instead to reduce the impact of the risk.
In other cases, you can ignore the risk or selectively include it.
Another way of addressing uncertainty is through formal propagation in calculated risk.
Bayes' theorem updates knowledge of uncertainties and determining risks.
In risk analysis, failure data are uncommon, but with Bayes' theorem and its sequential nature,
the analyst can determine methods to handle the uncertainties and risk.
This approach will allow risk analysts to obtain valuable data they can use for updating their knowledge on the parameter's uncertainties.
With Bayes' methods of data updating, assessing events in safety engineering such as human error rates,
hardware failure rates, and frequencies of abnormal environmental events, etc., has become easy.
Bayes' theorem is truly the basis of modern risk analysis, for without it, the QRA's validity will no doubt be questionable.
Conclusion
Bayes' theorem is truly a practical mathematical approach that serves many purposes in wide applications.
In addition to safety engineering, the theorem found use in medicine, computer science, business, marketing, finance, and many others.
Not many people realize that Bayes' theorem is most probably the only math tool they use every day.
From betting on games, finding out the probability that it determines the probability of disease outbreaks or the risks of a system,
you can use Bayesian probability in countless of things.
With it, you have a systematic way of updating knowledge that will result to rational answers.
Of course, your subjective judgments may influence it, but it is extremely useful nonetheless.
There are difficulties and limitations in the use of Bayesian approach.
However, considering what advantages it offers and how practical it is,
it is no doubt to say that it is one of the best mathematical tools for updating knowledge.
With this book, we hope you can learn everything you need in understanding Bayes' theorem.
With the definition, methods, advantages, disadvantages, applications, and examples mentioned,
you could begin using the Bayesian approach in your everyday life.
This has been Bayes' theorem, the ultimate beginner's guide to Bayes' theorem.
Written by Arthur Taff
Narrated by Tim Carper
Copyright 2018 by Arthur Taff
Production Copyright 2018 by Arthur Taff
Audible hopes you have enjoyed this program.
