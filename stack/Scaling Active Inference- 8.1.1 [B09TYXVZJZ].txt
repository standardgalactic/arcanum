welcome to the actin podcast we will present short digestible segments clipped from the active
inference lab weekly live streams if you like what you hear and you want to learn more check
out the entire live stream at the active inference lab youtube channel the link to the live stream is
provided in the episode description my name is blue knight and i will be guiding you through this
podcast episode which is clipped from actinf lab live stream number 8.1 this discussion will be
loosely structured around the paper scaling active inference by alex shans manuel baltieri
enil seth and christopher buckley in this episode daniel friedman is going to briefly outline what
it is to scale active inference and goal directed behavior under the free energy principle
the uh need for this paper was that when you have a short temporal horizon you can just run out every
possible policy like every possible set of five chess moves or every possible set of all the tic-tac-toe
moves it can't be more than nine moves in a game every single step if the branching is small and the
time horizon is short you can evaluate g in full so it becomes rather than sampling over a distribution
literally just calculating a table and then sorting however when you have deep temporal horizons you
can't really evaluate the full branching tree because there's too many options so you have to
sample from that distribution so that's the sampling problem the second problem and that's the deep
counterfactual sampling problem which is even difficult or under uncertainty the second part is the
mario kart paradigm which is that in continuous action spaces there are infinite policies meaning
that some other way is going to have to come into play because not just are we sampling uh deep through
time we're sampling potentially related to counterfactuals which there's infinite counterfactuals
and we're talking about continuous action spaces where there's infinite policies so if my only trading
affordances are should i have zero or one or two bitcoin then i could ask which one of those three
options discreetly is better but if there's infinite and i can do continuous variables uh even smaller
than one you know breakdown it's like then should i do 1.0001 what if that's very different than 1.0002
now you might say well those two should be super similar and you should be able just to sample every
little interval and estimate the distribution right if it's a smooth distribution then it's going to be
easy to optimize maybe you could discretize it maybe you could do some other way to do smooth
optimization but the problem is actions and policies are not smooth so maybe if you're going into that
turn in mario kart you could aim at the corner and just try to take it as fast as you can or you could
slow down and do a slightly different angle or you could do some other skidding maneuver so there's
multiple separate ways that you're going to be able to take this turn potentially with different risks
and reward and they're all very nuanced so that is why the deep through time and counterfactual aspects
respectively which aren't as much dealt with here counterfactuals but through time absolutely is
and then the continuous actions which is really the key introduction of this model that takes it out of
the paradigm of classifying images and puts it into the paradigm of continuous control theory and then it
turns out that there's a lot of trajectories through time given how things can go and so there's some
implementation of a way to sample trajectories that is utilizing what's called a monte carlo sampling
approach or mc and uh there's the markov chain monte carlo sampling mc mc mc squared um and monte carlo
just means that it's uh related to monte carlo being a card playing reference in english and it's
like the way that you can shuffle a deck is so big how are you going to estimate the number of ways
that you can get a royal flush or a pair of fours or something like that instead of just actually doing
well it's one in 24 times one in 52 times three over 52 the way you do it is you just sample randomly
and then you say okay i took a billion samples and one million of them had this feature and then you
just take the ratio as your estimate so it's a sampling and that kind of harkens back to earlier
where we wanted to talk about getting the efficient sampling because that's kind of what it's about
is in the markov um or sorry in the monte carlo sampler which could be markov chain based um we want
to get good sampling we don't want to sample only the hands with a royal flush like if we had a poor
randomizer because then we're going to get an estimate that's wrong all right then they go into a little
bit more detail about the estimated or the expected free energy which is still as it was before about
policy in the future and now it's going to be uh this broken down and this is also where they draw
some of the homologies to reinforcement learning and info gain into the uh the informational gain
kind of like the reward and the info so this is where the explore exploit also comes into play
which is that if one is only optimizing locally they may be exploiting they may be getting reward
for a short period of time but then it doesn't lead to a um uh like we'll see it in the hill climber
example but if you just reward only and only exploit sometimes you don't do very well but if you just
explore you might not ever get to exploit anywhere good and so the whole trade-off and the whole
challenge of modeling is this balance between explore and exploit between searching deeply and
searching broadly and so the holy grail would be the right way to search deeply down the routes that
are interesting and informative but not down the ones that don't tell you that much but we don't know
which paths are going to be informative or not until we go down them however if we have a deep
generative model we can say in terms of me winning this chess game this branch is not informative for
me to search down not because there aren't trillions of options but because i have two castles and they
have no pieces other than their king so i'm going to win so in terms of me winning it doesn't give me
any information to keep on searching down that route i already know i'm going to win and then there
might be another avenue where there's fewer options per se like less branching but there's more
uncertainty that's resolvable more info gain is possible with respect to the policies and winning
we hope you enjoyed this week's podcast stay tuned for next time where we will have a chance to talk
with the first author of the paper and get into some deeper discussion
