This is Audible.
Refresher on .NET and Software Design Fundamentals for C-Sharp Developers
Written by Dr. Alexei Sinyagin
Narrated by Todd Gaddy
Leal Disclaimer
All rights reserved.
No part of this book may be reproduced, stored, or transmitted by any means,
whether auditory, graphic, mechanical, or electronic, without written permission of both publisher and author,
except in the case of brief excerpts used in critical articles and reviews.
Unauthorized reproduction of any part of this work is illegal and is punishable by law.
This book is presented solely for educational and entertainment purposes.
The code samples and descriptions provided in the book are for illustration purposes only,
and as such, the author does not provide technical support for these code examples.
The author and publisher are not offering it as professional services advice.
While best efforts have been used in preparing this book,
the author and publisher make no representations or warranties of any kind
and assume no liabilities of any kind with respect to the accuracy or completeness of the contents,
and specifically disclaim any implied warranties of use for any purpose.
Neither the author nor the publisher shall be held liable or responsible to any person or entity
with respect to any loss or incidental or consequential damages caused
or alleged to have been caused, directly or indirectly,
by the information or programs contained herein.
No warranty may be created or extended by sales representatives or written sales materials.
Preface
My kids love to play video games, so we have multiple consoles at home,
from PS3, Xbox 360, and Xbox One to Nintendo 3D and more.
They are absolute experts on casual gaming.
One day my 5-year-old son came to me with a problem.
One of his consoles didn't power up.
He tried to charge it, he tried different power cords, and even tried to reset it.
Nothing helped.
He told me,
The device is broken.
We need to buy a new one.
From his perspective, he tried all the available options,
and the next reasonable step to solve the problem was to acquire a new device.
I'm not an electronics expert by any means,
but like many of us, I know that there are four screws which can be unscrewed to look inside the device.
I asked him to bring a screwdriver, and we opened it up.
I could see in my son's eyes that something magical had happened.
The black box was no longer a black box.
I did something beyond his ordinary assumptions, and now we have a number of options to consider.
I unstuck the power button, and then reassembled the device back.
It worked.
For my son, it was pure magic.
I didn't need to possess any degree in electrical engineering to make a quick fix,
instead of having to buy a costly new device.
The same principle can be applied to any field or profession, including software development.
I'm often surprised to encounter many smart and qualified developers
who keep forgetting the basics of computer fundamentals and software development.
Knowing how things work can make a big difference.
As Albert Einstein said, any fool can know.
The point is to understand.
New technologies, patterns, and frameworks are being developed every year,
so it's hard to stay up to date.
This audiobook is intended to help bridge that gap.
The book does not provide a secret recipe or uncover new patterns.
My goal was to create an audio guide that provides a conventional overview
of key .NET software development topics and helps formalize definitions of concepts
such as the .NET framework, object-oriented programming, and design patterns.
Part 1.
Concepts of Object-Oriented Programming and Design Patterns
Object-Oriented Programming, or OOP,
takes a different perspective to problem solving than procedural languages such as C or Pascal.
It's a programming language model organized around objects and behaviors.
It's based on the concept of linking real-world things to small, manageable pieces of code called objects,
which contain both code and data that describes its state and behavior.
It's similar to a mini-program.
In a procedural language such as C, a developer defines certain steps to apply to data,
typically necessitating a long list of procedural steps.
Object-Oriented Programming takes the view that what we really care about
are the objects we want to manipulate
and decouples the logic required to manipulate them.
Objects
In the real world, an object is described typically by a noun,
which relates to a real-world entity, such as
the dog, the house, the game.
In an application, the object can represent any logical entity
not necessarily physical or visible, let's say, logger, for example.
Logger object can be a virtual entity, which can provide logging capabilities.
An object has identity, attributes, and behavior.
Let's review it in more detail.
An object has identity, so it's different from other objects, even of the same kind.
For example, if you have two identical bicycles, they are still separate objects.
An object has attributes, so the bicycles may be red or green,
be designed for road or mountain use, and have 12 or 20 gear speeds.
An object might also have behavior, which is specific to that type of object.
In the bicycle's case, they might be moving or stopped.
So, based on the definition above, an object is an entity that can perform a set of related tasks
or activities that define the object's behavior.
Just as objects in the real world have a state and behavior, so too do they in OOP.
Software objects are conceptually similar in that they have fields where it stores its state
or attributes, and exposes its behavior through methods or functions.
Objects control how other objects use them by offering methods that act upon
and can change an object's internal attributes.
Methods similar to subroutines, procedures, or functions.
The activities the object can perform define its behavior,
allowing them to combine into large, complex systems.
Classes
A class serves as a blueprint, defining the structure and behavior of an object.
So, an object will belong to a particular class,
which is the blueprint that defines objects of a specific type.
A class consists of three things, name, attribute, and operations,
or what it is, its state, and what it does.
A class is a template for the methods and variables for a particular object.
An object is considered an instance of a class while still holding real values
and behaving independently of other instances of the same class.
Therefore, objects of the same class may be forged from the same template,
but have different states.
Think of a bank.
It offers a standard checking account,
but the activity and amount of cash in each will be different.
Classes are hierarchical,
meaning that one class can be a subclass of another
and inherit some or all of that class's characteristics.
Subclasses may also be specialized,
having their own methods and attributes that are not part of the base class.
Inheritance
The organization of classes into a hierarchy
is the basis for the principle of inheritance.
At the top of the hierarchical tree is a base class,
which defines all the common characteristics of every subclass.
Each subsequent subclass inherits those base characteristics,
but then also adds or refines the definition.
That in turn becomes the base definition inherited by its subclass.
So the further down the hierarchical tree you go,
the more refined versions of the base class you'll find.
This is termed specialization.
For example,
a dog, subclass,
is a mammal, superclass,
but is also a specialized form of the generalized class, mammals.
So inheritance is the ability to create a new class
from a base class by extending it.
The derived class can add new methods and types
to extend the functionality of the class
while overriding existing ones.
This allows a derived class to inherit functionality
from another class without having to duplicate it.
This is valuable when creating objects that are very similar
and share common functionality with another class
but have extended features.
Inheritance is the basis for code reusability
as base code can be written for a class
that contains core attributes and data methods
that can be used again when creating new classes.
The new class can extend the base class
with new data and methods
to create specialized classes.
Inheritance and specialization
are therefore closely linked
as are generalization and specialization,
which are simply the reverse of one another
and are both reciprocal and hierarchical.
Encapsulation
Encapsulation is a technique where attributes,
data, and methods are encapsulated
into a container called a class.
The goal of this is to contain everything in one place
to protect the information
and to restrict access to the inner workings of the class.
By encapsulating an object's attributes and data,
only essential information is released outside of the class.
This technique, called a black box or information hiding,
ensures access control to the inner methods.
The class facilitates a security via access modifier
and makes the code understandable, modifiable, and portable.
Encapsulation allows classes to hide the complexity
of the internal methods, attributes, and properties
by presenting public methods and properties.
This enables the system to request a class
via its public methods
without needing to know the internal workings.
In practice, this means that another part of the application
cannot access and change data
without going through the object itself.
For example, in a bank account class,
no part of the application is allowed
to update the private property balance
without going through the class's public methods
of withdrawal or deposit.
However, encapsulation is not just about information hiding.
It's also about reducing dependency.
Encapsulation enables internal changes to the class,
such as changing attributes or methods,
without it affecting other parts of the system
so the public methods, name, and properties remain unaltered.
What's more, another class may build upon its own functionality
by using the methods and properties exposed by an existing class.
OOP provides several techniques
association, aggregation, and composition
that classes can use to link with each other
to build modularity through encapsulation.
An association defines a relationship between classes of objects
that allows one object to perform an action
on another object's behalf.
This relationship specifies that objects of one kind
are connected to objects of another
and does not represent behavior.
It can also be viewed as a depends-on relationship.
Aggregation is the relationship between two classes.
A soccer team is an example of aggregation.
The team consists of the players.
The player as an object can exist and function outside of the team
and can also potentially move between teams.
Composition is a specialized, strong type of aggregation
and is a way to combine simple objects or data types
into more complex ones.
Compositions are a critical building block
of many basic data structures
as well as the object used in object-oriented programming.
An example of composition is an automobile
which has or is composed from objects
including the tires, engine, and transmission.
It is a part-of relationship
because the member object is a part-of the containing class.
Imagine a building being a class.
It contains a collection of floor objects.
The floor as an object cannot exist without the context of a building.
Finally, inheritance is known as an is-a relationship
where a new class is created from an existing class.
The containing object can inherit attributes and behavior from its parent.
If animal was the class, then offspring would be a dog or cat.
Abstraction is another key concept in OOP.
The goal of abstraction is to concentrate on the essential
and ignore the irrelevant.
Abstraction uses techniques designed to hide complexity
and suppress detail by using names to reference objects.
It places emphasis on what an object does rather than how it does it.
This is essential in constructing a class
which is the foundation of OOP.
By concentrating on the essentials when constructing a class
and ignoring the unimportant or irrelevant,
a programmer can construct an efficient class
with a focus on its primary concern.
Think about it as the act of representing essential features
without exposing internal details or explanations.
A good example to understand abstraction can be a battery.
Most people don't know how batteries store energy,
what chemical or physical processes are happening inside of it,
or what volotic cells are.
However, most people know what a battery does
and they can use it in many applications.
By utilizing abstraction techniques,
large programs can be constructed simply by reducing the complexity
that the software developers have to deal with.
Generalization is another concept that reduces complexity
by replacing multiple objects or entities
that perform a similar function with a single construct.
It manages complexity by collecting individuals into groups
using variables, parameterization, generics, and polymorphism.
Abstraction and generalization are often used together,
such as with an abstract class,
which is created for the purpose of being inherited.
We cannot create objects from the abstract class,
but it is used by other classes that extend its core functionality.
Abstract classes are conceptual,
and the implementation is completed when realized by a subclass.
A subclass can inherit only from one abstract class.
However, an abstract class can have virtual methods defined,
perhaps as a default implementation,
which the subclass can override when required.
Although a subclass can only inherit from one abstract class,
the abstract class may have many interfaces.
Another method of abstraction is via object interfaces.
Objects interact with each other by sending messages,
and all they know of each other is their respective interfaces.
According to C-sharp reference,
an interface contains only the signatures of methods, properties, events, or indexers.
A class or struct that implements the interface
must implement the members of the interface that are specified in the interface definition.
That means that the interface effectively defines a protocol
on how objects interact with the outside world
through the methods and properties they expose.
However, even though abstract classes and interfaces look similar,
there are key differences.
The biggest is that interfaces cannot have any implementation,
while an abstract class can have a default implementation of methods.
If you don't override them in a derived class,
the abstract base class implementation is used.
In addition, in C-sharp language,
an interface can't contain constants, fields, operators,
instant constructors, destructors, or types.
Interface members are automatically public,
and they can't include any access modifiers.
Finally, interfaces can implement other interfaces.
This is very important in C-sharp,
because the language doesn't support multiple inheritance of classes,
but does support multiple inheritance of interfaces.
Polymorphism
Polymorphism, which comes from the Greek words meaning more forms,
is often considered the third pillar of object-oriented programming,
after encapsulation and inheritance.
It allows developers to deal with generalization,
rather than the specifics.
There are several fundamentally different kinds of polymorphism.
Ad hoc polymorphism,
parametric polymorphism,
and subtyping.
In my experience,
when people talk about polymorphism,
they're referring to subtyping.
Ad hoc polymorphism
In 1967, Christopher Strachey introduced the term ad hoc polymorphism.
Also known as method overloading,
it's the ability to define several methods all with the same name,
but different input parameters.
Method overriding, on the other hand,
relates to inheritance,
where a subclass can override the methods inherited from the superclass,
provided it has the same method name and parameter list.
Parametric polymorphism allows a function or a data type
to be written generically so that it can handle values identically without depending on their type.
These constructs are called generic functions and generic data types.
Parametric polymorphism is implemented in C-sharp in form of generics.
Generics were added to version 2.0 of the C-sharp language and the Common Language Runtime, or CLR.
According to the European Computer Manufacturers Association, or ECMA,
CLR specification generic programming encourages higher-order programming,
where generic functions or methods take function, delegate, type arguments that have generic types.
Use of generic types maximizes code reuse, type safety, and performance.
Subtyping polymorphism
The subtype polymorphism, also known as inclusion polymorphism,
means any object of a derived type can be safely used in a context where a parent type is expected.
It is closely related to the safe substitution principle introduced by Barbara Lisskoff in 1987.
The principle states that functions that use pointers or references to base classes
must be able to use objects of derived classes without knowing it.
For example, a method called draw takes a parameter of type shape.
Subtype polymorphism permits calling such a method by passing object square or circle
as long as both of them inherit from base class shape.
In this example, method draw is agnostic to the concrete type and can operate on the base type.
Type transformations affect subtype relation in four ways.
Assignment compatibility is the ability to assign a value of a more specific type
to a storage of a compatible, less specific type.
The term is coming from the fact that the types are compatible for assignment.
Covariance enables you to use a more specific type than originally specified,
i.e. follow the rule of assignment compatibility,
and preserves the direction of assignment compatibility.
Contravariance enables you to use a more generic, less derived type than originally specified,
i.e. it reverses the rule of assignment compatibility.
Invariance means that you can use only the type originally specified,
i.e. doesn't allow assignment compatibility.
OOP Design Principles
SOLID
SOLID is an acronym commonly used to help remember the first five object-oriented design principles
collected by Robert Cecil Martin in the late 1990s.
The principles of SOLID are guidelines that can be used to design systems
that are easy to maintain and reuse.
According to Martin, these principles expose the dependency management aspects of object-oriented design.
SOLID stands for
S. Single Responsibility
O. Open-Closed
L. Liskov Substitution
I. Interface Segregation
D. Dependency Inversion
As classes encapsulate objects and their methods into narrowly defined definitions of concern,
so there are five principles that must be adhered to when designing a class.
The Single Responsibility Principle
S. R. P.
This design principle is based on the desire for a class to have a very focused and narrow purpose,
a single concern.
A class should have one, and only one, reason to change.
The purpose of this principle is that programmers must focus and segregate code,
maintaining awareness of specific functionality.
In modular coding, the purpose must be to implement a specific function, no more, no less,
hence the programmer's concentration on one single purpose.
This design principle of a single function makes a class more robust and lends it only one reason to change.
The Open-Closed Principle
O. C. P.
This principle originates from the work of Bertrand Meyer.
It states that code should be open for extension, but closed for modification.
It means simply that we should write our modules so that they can be extended to create new modules without requiring modification.
This is the core principle of inheritance and abstraction.
The principle might sound complicated, but the concept is quite simple.
Do not change code that is already working.
Of course, we might want to change what a specific function may do without changing the source code of the module.
This may sound contradictory, but there are several abstraction techniques for achieving the OCP,
such as dynamic and static polymorphism.
The goal of OCP is that it is always better if new code does not make changes in existing code that already works.
If you do not change existing code, then you are unlikely to break it.
The Liskov Substitution Principle
L. S. P.
This principle was posited by Barbara Liskov in her work regarding data abstraction and type theory,
and also is derived from Bertrand Meyer's concept of design by contract, or DBC.
It states that a derived class should be substitutable for its base class.
This is determined by the is-a relationship as a subclass, is a derivative of the parent class,
and as such holds all the attributes and methods of the parent class, and can therefore be substituted.
The LSP supports the inheritance theory that a child class can always substitute for a parent class
because it has inherited all its base functions and attributes, but the reverse is not always the case.
This is where generalization and specialization comes into play.
For example, a dog will always meet the specifications of being a mammal, parent class,
but a mammal is not necessarily a dog, child class.
The Interface Segregation Principle
ISP
While also based on inheritance, this principle looks at it from a different perspective.
ISP considers that not all clients require inheriting all the functions and attributes from the parent class.
Sometimes a child class requires only a single function.
The solution is to present different interfaces offering specific functions that may be inherited by the child class.
This does not mean the programmer constructs individual interfaces,
but instead they organize subclasses, which the programmer categorizes and presents with appropriate interfaces
that offer the client the required functions and attributes.
The Dependency Inversion Principle
DIP
If the open-closed principle defines the purpose of OOP,
then the DIP states the primary mechanisms of abstraction
that allows modules to interact without having concrete interfaces and dependencies.
The whole concept of OOP depends on abstraction,
the ability to interface and communicate with generic classes
that allow code reuse and interfaces without recoding.
The DIP adds a layer of abstraction between high-level and low-level modules.
Review
OOP
OOP
allows you to think about problems in terms of objects and their interactions.
Each object is self-contained and can be perceived as a black box.
The main concepts of OOP
are inheritance, encapsulation, abstraction, and polymorphism.
SOLID is an acronym which is commonly used to help remember the first five object oriented
design principles.
Software Design Patterns OOP has received much praise and criticism
over the years.
The key advantages of OOP are maintainability, reusability, and scalability compared to structured
programming approach.
The major criticism can be summarized by John Oosterhout's well known quote, Implementation
inheritance causes the same intertwining and brittleness that have been observed when go-to
statements are overused.
As a result, OO systems then suffer from complexity and lack of reuse.
In general, the design pattern is a reusable solution to a commonly occurring problem.
Christopher Wolfgang Alexander originated the concepts in his 1977 book, A Pattern Language,
Towns, Buildings, Construction.
The concept is based on observations of ancient architecture.
Alexander noted that while local regulations required specific features, the architect was
still free to adapt them to particular situations.
He concluded the idea of a pattern language appears to apply to any complex engineering
task.
His theory has been especially influential in software engineering where it was popularized
by the so-called Gang of Four, Eric Gama, Richard Helm, Ralph Johnson, and John Vlissides
in their 1995 book, Design Patterns, Elements of Reusable Object-Oriented Software.
They refer to software patterns as a frozen experience and also proposed some fundamental design
principles widely used today as OOP design best practices.
The first principle states, program to an interface, not an implementation.
Eric Gama says this principle is really about dependency relationships that have to be carefully
managed in a large app.
Once you depend on interfaces only, you're decoupled from the implementation, which can vary.
That is a healthy dependency relationship.
For testing purposes, you can replace a heavy database implementation with a lighter weight
mock implementation.
Fortunately, with today's refactoring support, you no longer need an interface up front.
You can distill an interface from a concrete class once you have the full insight into a
problem.
The intended interface is just one extract interface refactoring away.
In other words, this principle is about generalization and reusability by separating what?
the interface from how the implementation.
The second principle is to favor object composition over class inheritance.
The Gang of Four suggests that an inheritance exposes a subclass to details of its parent's
implementation, which means that inheritance breaks encapsulation while composition is more
flexible and less coupled.
A detailed review and analysis of the Gang of Four and other design patterns are outside
of the scope of this book.
However, it is important to at least understand the purpose of the main patterns on the very
high level.
Here is a brief review.
Creational Patterns
Creational patterns are ones that create objects for you, rather than having you instantiate
objects directly.
This gives your program more flexibility in deciding what objects need to be created for
a given case.
Sometimes creational patterns overlap.
There are cases when either one or another pattern would be appropriate.
At other times, they complement each other.
Abstract Factory Pattern
Abstract Factory Pattern describes grouping object factories that have a common theme.
It provides an interface for creating families of related or dependent objects without specifying
their concrete classes.
The intent in employing the pattern is to abstract the creation of objects from their usage.
This approach allows for new derived types to be introduced, with no change to the code
that uses the base class.
The main advantage of using this pattern is the possibility to interchange concrete implementations
without changing the code that uses them, even at runtime.
Builder Pattern
Builder Pattern constructs complex objects by separating construction and representation.
The pattern focuses on constructing a complex object step-by-step.
This allows a client object to construct a complex object by specifying only its type and content.
The Builder Design Pattern is very similar to the Abstract Factory Pattern.
The main difference is the construction process of complex objects.
The Builder is only needed when an object cannot be produced in one step.
Factory Method Pattern
Factory Method creates objects without specifying the exact class to create.
There tends to be a lot of confusion over the difference between Abstract Factory and Factory
Method Pattern.
In essence, Abstract Factory is a creational pattern for the family of objects, whereas the
Factory Method is a way to get one individual member of a family.
In other words, Factory Method Pattern hides the construction of single object, whereas Abstract
Factory Method hides the construction of a family of related objects.
Prototype Pattern
Prototype Pattern creates objects by cloning an existing object.
It is used when the type of object to create is determined by a prototypical instance, which
is cloned to produce new objects, thus it is often referred to as codification of an interface.
This pattern is used when the creation of the object is costly or complex, as prototyping
can eliminate the expensive overhead of initializing an object and improving performance.
Singleton Pattern
Singleton Pattern restricts object creation for a class to only one instance.
It ensures that only one instance of a class is created and provides a global point of access
to the object.
Because the class controls the instantiation process, the class has the flexibility to
change the instantiation process.
Structural Patterns
According to Design Patterns, elements of reusable object-oriented software, the structural design
patterns concern class and object composition.
They use inheritance to compose interfaces and define ways to compose objects to obtain
new functionality.
Adapter
The adapter pattern allows classes with incompatible interfaces to work together by wrapping its own
interface around that of an existing class.
In short, it allows two incompatible interfaces to work together.
This pattern is often used when legacy application components need to work with newly developed
code.
Bridge Pattern
The bridge pattern decouples an abstraction from its implementation so that the two can
vary independently.
My favorite example of this pattern is video cards and desktop computer monitors.
The concrete implementers are the monitors.
They take standard signals from the card and produce concrete representations of the information
on the screen.
You can swap monitors or use LCD, CRT, projectors or any future form factors.
Similarly, you can upgrade your old video card for a new one.
They all work together because the abstraction and implementation layers have been split out
and standardized.
Another great example is .NET itself.
By using IL and JIT compilers as concrete implementers, it decouples the abstraction from its implementation,
allowing .NET code to be platform independent.
Composite Pattern
The composite pattern composes zero or more similar objects so that they can be manipulated
as one object.
It lets clients treat individual objects and compositions of objects uniformly.
Imagine implementing a tree or linked list data structures where you might want to have operations
on the node, and on the tree or the list itself.
The whole point of the composite pattern is that it can be treated atomically, just like
a leaf.
Decorator Pattern
The decorator pattern allows behavior to be added to an individual object, either statically
or dynamically, without affecting the behavior of other objects from the same class.
This makes the decorator transparent to clients.
Facade Pattern
The facade pattern provides a simplified interface to a large body of code.
Decorator known as a wrapper, it is used to give a single point of entry for a subsystem.
It is like your TV remote, which simplifies your interactions with the TV set by exposing
a high-level interface of changing channels or manipulating volume, and abstracting from
you the complexity of such operations.
Publicity is the aim of the facade pattern.
The mediator design pattern may look very similar to facade design pattern.
Mediator abstracts the functionality of the subsystems similar to facade.
The key difference is that in mediator pattern, subsystem components are aware of the mediator,
while with the facade pattern, subsystems are not aware of its existence.
Flyweight Pattern
The flyweight pattern reduces the number of low-level, detailed objects within a system
by sharing objects.
If instances of a class that contain the same information can be used interchangeably, the
flyweight pattern allows a program to avoid the expense of multiple instances that contain
the same information by sharing one instance.
The flyweight pattern reduces the cost of creating and manipulating a large number of similar objects
and saves memory by sharing flyweight objects among clients.
Proxy Pattern
Proxy provides a placeholder for another object to control access, reduce cost, and reduce complexity.
Many people confuse decorator, facade, adapter, and proxy patterns, which all use a proxy to
forward method calls to another object, known as the subject.
Here are the key differences.
Adapter provides a different interface to its subject.
Decorator provides an enhanced interface to the subject.
Facade provides a simplified interface to the subject.
Proxy provides the same interface.
Behavioral Patterns
In software engineering, behavioral design patterns identify common communication patterns
between objects, and increase flexibility in carrying out this communication.
Chain of Responsibility Pattern
The Chain of Responsibility design pattern allows an object to send a command without knowing
what object will receive and handle it.
The pattern allows defining a list of handler objects that have limitations on the nature
of the requests they can work on.
If a handler cannot process a request, it passes it on to the next handler in the chain of command.
For example, when a customer phones a customer service center, they explain the problem, and
if the representative cannot help them, the call is passed to the next level, and so on.
Command Pattern
Command Pattern encapsulates a request as an object and gives it a known public interface.
Command Pattern ensures that every object receives its own commands and provides a decoupling
between sender and receiver.
A sender is an object that invokes an operation, and a receiver is an object that receives the
request and acts on it.
The key advantage of the pattern is materializing commands as objects that provide capabilities
for them to be passed, shared, stored, and otherwise manipulated like any other object.
Interpreter Pattern
Interpreter Pattern implements a specialized language.
The Interpreter Pattern performs activities based upon a hierarchy of expressions.
It is used exclusively in defining grammars, tokenizing input, and storing it.
The implementation of the Interpreter Pattern is just the use of the composite pattern applied
to represent a grammar.
The Interpreter defines the behavior while the composite defines only the structure.
Iterator Pattern
Iterator Pattern accesses the elements of an object sequentially, without exposing its
underlying representation.
The key idea in this pattern is to take the responsibility for access and traversal out
of the list object and put it into an Iterator object.
The Iterator class defines an interface for accessing the list's elements.
In .NET the pattern is implemented by IEnumerableInterface.
Iterers were introduced in C Sharp in Visual Studio 2005 and were introduced in Visual Basic
in Visual Studio 2012.
An Iterator can be used to step through collections such as lists and arrays with for each language
construct.
When you create an Iterator for a class or struct, you do not have to implement the whole IEnumerator
interface.
The .NET compiler detects the Iterator and automatically generates the current, move next, and dispose
methods of the IEnumerator or IEnumerator of T interface.
Mediator Pattern
The Mediator Pattern is intended to have a single object control information flow between
several objects.
It allows loose coupling between classes by being the only class that has detailed knowledge
of their methods.
The idea is to decouple two objects by introducing a third object called Mediator.
In such approach, objects no longer communicate directly with each other, but instead communicate
through the Mediator.
This ensures loose coupling and it lets developers vary their interaction independently.
Memento Pattern
Memento Design Pattern is used for restoring objects to previous state, like Undo.
But we can do Undo and Redo multiple times.
Memento Pattern uses three Actor classes.
Memento contains state of an object to be restored.
Originator creates and stores states in Memento objects and Caretaker object that is responsible
to restore object state from Memento.
Observer Pattern
Observer Pattern is a publish-subscribe pattern that allows a number of Observer objects to
see an event.
The Observer Design Pattern is suitable for distributed push-based notifications because
it supports a clean separation between two different components or application layers.
The pattern can be implemented whenever a provider uses callbacks to supply its clients
with current information.
As mentioned before, Observer Pattern can be implemented in .NET via iObservable interface.
State Pattern
State Pattern allows an object to alter its behavior when its internal state changes.
The pattern allows for encapsulation of an infinite number of states on a context for
easy maintenance and flexibility.
The real-world example can be a customer object with different states, or tiers, such as platinum,
gold, and silver that can be defined in any way.
Usually code dealing with such logic results in a lot of conditional checks.
State Pattern can elegantly solve this problem by decoupling context and allowing a customer object
to change its behavior dynamically.
Strategy Pattern
Strategy Pattern enables an algorithm's behavior to be selected at runtime.
The Strategy Pattern defines a family of algorithms, encapsulates each algorithm, and makes the algorithms
interchangeable within that family.
The most common example is decoupling of the sorting algorithms from your data structure.
Instead of type coupling one algorithm implementation with your class, you can use Strategy Pattern
and create a sorting strategy that can be passed to the object in the runtime.
Usually each strategy needs data from the context to return some processed data to the caller.
In the example above, the strategy will take an unsorted list of data and return sorted list.
The concrete implementations can be Quick Sort Strategy, Heap Sort Strategy, Merge Sort Strategy, etc.
Template Method Pattern
Template Pattern method defines the skeleton of an algorithm as an abstract class allowing
its subclasses to provide concrete behavior.
In other words, template method lets subclasses redefine certain steps of an algorithm without
changing the algorithm's structure.
This provides the ability to fix the order of operations but allows application subclasses
to vary steps as needed.
Visitor Pattern
The Gang of Four's book on Design Patterns says Visitor Patterns allows for one or more
operation to be applied to a set of objects at runtime, decoupling the operations from the
object structure.
The pattern should be used when you have distinct and unrelated operations to perform across a structure
of objects.
By doing so, the visitor pattern also solves a very interesting and commonly misunderstood
problem of double dispatch.
Most of OOP programming languages support single dispatch polymorphism, where a method
call is dynamically dispatched based on the actual derived object type.
In contrast, multiple dispatch allows calls to be dynamically dispatched based on the runtime.
This makes a real-world difference when you have two overloads of a method that differ on
the type of parameter but are polymorphic, and you call with a reference declared as
the higher type that has an object of the lower type.
The Visitor Pattern allows you to emulate multiple dispatches by decoupling the behavior into
a visitor class that is called by individual visitable objects at runtime, passing reference
to itself as an argument.
The biggest drawback of this pattern is that every time a new element is added, every visitor-derived
class should be changed.
The design patterns are both praised and criticized by the development community.
My own perspective has changed on the pattern throughout the years.
I strongly believe that knowledge of the design patterns is required.
Just as we learn algorithms, we should study design patterns to understand how particular
problems can potentially be solved.
But that's where we need to stop.
Blindly pushing design patterns into the code will increase complexity, introduce bugs, and
create a maintainability nightmare.
Instead, we should understand the underlying principles and ideas to apply them or build upon
them as needed in each particular situation.
As Jeff Atwood, the author of the Coding Horror programming blog and the co-founder of the
Q&A website Stack Overflow says, design patterns are a form of complexity.
As with all complexity, I'd rather see developers focus on simpler solutions before going straight
to a complex recipe of design patterns.
If you find yourself frequently writing a bunch of boilerplate design pattern code to deal with
a recurring design problem, that's not good engineering.
It's a sign that your language is fundamentally broken.
Mark Dominus, the author of Higher Order Pearl, Transforming Programs with Programs, and longtime
columnist for the Pearl Journal, also thinks that the patterns obstruct the ideas of Christopher
Alexander and others.
In his article about design patterns, he points out that although the movement was inspired
by the pattern language work of Christopher Alexander, it isn't very much like anything
that Alexander suggested.
And in fact, what Alexander did suggest is more interesting and would probably be more
useful for programmers than what the design patterns movement chose to take.
He makes the same point as Jeff Atwood that the patterns that are used recurrently in one
language may be invisible or trivial in another language and that many patterns aren't really
addressing recurring design problems in object-oriented programs.
They are actually addressing deficiencies in object-oriented programming languages.
He concludes, Patterns are signs of weakness in programming languages.
Instead of seeing the use of design patterns as valuable in itself, it should be widely recognized
that each design pattern is an expression of the failure of the source language.
I personally see the truth somewhere in the middle.
As programming languages evolve, we will see more and more built-in patterns.
At the same time, more and more patterns will emerge as the new problem space develop.
The cycle will continue and for someone to be a well-rounded software developer or architect,
it will be necessary to learn and understand modern and classical patterns.
It is also crucially important to understand patterns even if they are natively built into
languages.
Let me present two examples.
The first is Visitor Pattern, which is a workaround for language limitations.
In multiple dispatch languages, such patterns would be completely useless.
Nevertheless, it is important to recognize the problem and know the potential solution.
The second example is IEnumerable in C-sharp.
Without a clear understanding of Iterator Pattern and its implementation in C-sharp, developers
often make costly assumptions.
In many cases, such code was indeed the performance bottleneck for entire application.
Here are a couple examples of the patterns not covered by the Gang of Four book.
MVC Class of Patterns
The Model View Controller, or MVC, is a standard design pattern that aids developers in separating
the key components of an application into three areas of concern – the model, the view,
and the controller.
MVC facilitates the separation of the logic within an application and maps it to a corresponding
component in the MVC framework.
For example, UI logic would be located in the view, business logic in the model, and input
logic in the controller.
By separating these aspects of an application, a developer can reduce the complexity and duplication
of code, create individual test units, and work on each in isolation or in parallel as
each of the components are only loosely coupled.
The MVC model is responsible for state management and interaction with the application's data domain
through the application's business logic.
The view is responsible for the UI presentation and rendering UI elements.
The controller is responsible for handling all query string user input and user actions.
While the view and the controller rely on the model, the reverse is not true.
The model does not depend on either the view or the controller.
Differences between MVC and MVP and MVVM patterns
The three design patterns have one fundamental purpose.
To separate the view from the model.
They differ in that they use thin layer controllers or presenters in view models to communicate
between the model and the view.
The model view presenter, or MVP, uses a presenter that supports two-way communications, one presenter
per view.
The view communicates with an instance of the presenter via functions calls and the presenter
responds via an interface.
Use MVP in web forms projects or in cases where binding to a data context is not possible.
Mode view view model, or MVVM, uses view model, a one-to-one relationship with view and two-way communication.
The view binds directly with the view model.
Changes in the view are automatically reflected in the view model and vice versa.
The typical application of MVVM pattern is with Windows Presentation Foundation or any other
application where binding to a data context is possible.
MVC uses a controller that communicates with the view.
The controller handles all user input from the view and updates the model or selects a
new view.
A view sends events to the controller via a URL.
Use MVC with web applications, web services, and web-based application programming interfaces,
or APIs, and any web applications where the view, the browser, is separate from the rest
of the application, such as with a web-based API.
MVC.NET Framework.
ASP.NET is a set of web development tools offered by Microsoft.
Forms like Visual Studio.NET and Visual Web Developer allow web developers to create dynamic websites
using a visual interface.
According to Microsoft's ASP.NET website, the ASP.NET MVC Framework is a lightweight, highly
testable presentation framework that integrated with existing ASP.NET features, such as master
pages and membership-based authentication.
The MVC Framework is defined in the SystemWeb MVC namespace.
Being built upon the ASP.NET Framework, MVC.NET can use most of its features such as membership,
roles, windows, and form authentication and URL authorization.
However probably the biggest advantage to using ASP.NET MVC is its inherent design pattern
of model view controller that provides separation of concerns whereby the MVC Framework provides
a complete segregation of the UI and business logic and data.
The MVC.NET Framework does not view state or server-based forms and therefore allows the
developer a larger element of control when designing the application.
The major advantage of separating the functionality of the view and the controller is that development
can proceed on a generic level for the model and controller as these are independent of the
view presentation UI logic.
Because there is a clear separation between the view and the model with no direct dependency,
applications can therefore support multiple views and simultaneous different presentations
are available of the same data.
The separation of the UI logic in the view is an important point when it comes to developing
applications for disparate end-user devices, tablets, smartphones, etc.
The UI is typically more device-dependent than the business logic, so to migrate an application
from a web-based device to a smartphone or tablet may require a complete rewrite of the
UI interface.
Having a clear distinction between the UI logic and business logic speeds and eases migration.
Similarly, in a web-based application, the view pages tend to be redesigned more often
than the business logic components.
Having a clear separation of the classes allows for redevelopment of only the UI logic and the
presentation code, leaving the complex business logic untouched.
By separating the code files into model, view, and controller, the overall application is better
organized, allowing more efficient code execution, code updates, reuse, and importantly, troubleshooting.
In an MVC pattern, designed there is no direct coupling between the data and the resulting presentation,
since the controller is the one handling the request, and then selecting an appropriate view.
Dependency Injection Pattern and Inversion of Control
Another pattern I want to review is Dependency Injection Pattern.
Dependency Injection is the basic concept for implementing the Inversion of Control design
pattern whereby it strives to reduce dependencies and the tight coupling between objects.
First of all, let's define the concept of dependency.
When a class uses a method in another class, for example, if class X uses a method in class
Y, we could say that X has a dependency on Y. Moreover, the dependencies may not be just
between X and Y but stretch or form a chain of dependency over many levels to classes A, B, and C.
The problem comes, of course, if there are changes to any of these classes. It may spawn through
many dependent classes on many levels. The solution would be to reduce the dependencies and loosely
couple the classes from one another.
In Dependency Injection, an object's configuration and its variables are configured by an external
entity called container. This removes the need for an object to configure itself. This
is frequently the case when a class is required to instantiate an object to perform a task on
that it is dependent. An example of this would be if a web object is required to interact with
a database. The class cannot access the database directly. It requires a data source implementation,
and therefore the class has a dependency on the data source implementation. The class will
be required to instantiate an instance of the database access object and pass to it a string,
such as driver, url, user, password, that are the parameters required by the data source object.
Therefore, when a class is dependent on another class, it is also dependent on the dependent class's
dependencies. Moreover, if our WebAccess class instantiates a data source object, it is responsible
for passing to it the required parameters. When this is the case, and a class hardcodes dependencies,
then a class is referred to as satisfying its own dependencies. Satisfying its own dependencies is
not an ideal solution within a complex program and application. So, the ultimate goal of dependency
injection is to configure a class from out with the class, to pass to it configuration variables that
can easily be changed at an external source. There are three most common ways of doing this. One is by
using a constructor that can pass the dependencies into the data source class, also known as constructor
injection. The second, via exposing depending as public properties and setting them from outside,
and also known as field injection. And final, is called method injection that, as the name suggests,
uses methods to instantiate dependencies of an object. Constructor injection is flexible and promotes
loose coupling. However, it has a significant failing in that once the class is instantiated,
the dependencies can no longer be changed. Field and method injection uses properties,
or individual methods, to inject the dependencies that allows resources to be allocated as late as
possible. Furthermore, it allows for expressive names. That makes code more meaningful. The ability of
setter injection to be able to change dependencies without having to create a new instance or change a
constructor class makes its object changeable. It can no longer be considered immutable. Ironically,
this is still not an ideal solution, as all we are actually doing is passing the problem to every
client that uses the class. To mitigate this problem, it is necessary to perform dependency injection
throughout the system by injecting all the way up through the layers of the application. If, however,
we need to have all components in the system having their dependencies injected as early in the life
cycle as possible, then there must be some class that knows what to inject. This entity is called a
dependency injection container, or simply DI container. A container is not just a factory pattern that
instantiates and injects parameters, it actually is involved with life cycle management.
A dependency injection container controls everything about an object during its lifetime, such as how and
when it is created and disposed. A container is configured by listing the components it should
instantiate, therefore mode, and that dependencies are required for each component.
In most cases, we don't need to implement DI containers on our own. There are a number of dependency
injection frameworks that work extremely well and can do all the heavy lifting and provide container
capabilities as well as auto wiring, an automatic detection of dependency injection points.
The two most popular in my opinion are StructureMap and Unity Application Block, also known as Unity.
The Unity Application Block, Unity, is a lightweight extensible dependency injection container with
support for constructor, property, and method call injection. Developed by Microsoft, it is available for
download from CodePlex under Apache 2.0 OSS license. StructureMap is a dependency injection inversion of
control tool for .NET that can be used to improve the architectural qualities of an object-oriented
system by reducing the mechanical costs of good design techniques. StructureMap claimed to be the
oldest IofC DI tool for .NET development and has been used in multiple production systems since June 2004,
and also released under Apache 2.0 OSS license. To summarize, Dependency Injection is a specific
type of IOC using contextualized lookup and it is a prime technique for building loosely coupled
applications. Dependency Injection provides ways to handle the dependencies between objects.
The key benefits of DI are in the possibility to reduce a component's dependencies, making it more robust
and less vulnerable to system changes. Along with other benefits, Dependency Injection simplifies
code testing allowing to mock dependent objects at the runtime, providing flexibility to unit test
specific code functionality. Command Query and Responsibility Segregation Pattern
Command Query and Responsibility Segregation
Command Query and Responsibility Segregation is a design pattern or principle that states that a method
should either be a command that does something or a query that returns a response to the caller, but not both.
CQRS utilizes the Command Query Separation theory by Bertrand Meyer, the creator of Eiffel Programming Language.
In its simplistic form, it states that asking a question should not change the answer, the fundamental
idea being we should always separate queries from commands. The real value of this is that those methods
that change state, commands, and those that don't, queries, can be classified and separated. By separating the
query from the command, you can have confidence that introducing queries will not have an impact or
introduce side effects, as it will not change the state. CQRS embellishes on this theory in that it uses
separate objects for retrieval and for modification . CQRS is therefore an architectural
precept that separates the software architecture into two clearly separated areas of responsibility—execution
of commands that make changes to the software system on one hand, and on the other, side effect-free
processing of queries. At the software architecture level, however, most systems are built around a
multi-tier model, conforming typically to a N-tier application design, that uses the standard
arrangement of database, data access layer, business logic, application layer, and presentation layer.
Unfortunately, this architecture usually entails violating the single responsibility principle
and fails to separate query from command. Applications typically validate and execute user actions
within the data access and business logic layers, while simultaneously provisioning data for display
and reporting in the presentation layer. This model can lead to performance issues if the system has many
simultaneous users. CQRS resolves this potential issue by segregation of the two operations into a read
side and a write side. It provides a model for business logic that is segregated from the model for
provisioning data. The separation of duties allows both models to be optimized to their full potential.
However, that is not the only reason for implementing CQRS principles. There are several motivations for using CQRS.
In general business applications, there is frequently a large imbalance between the numbers of read and write
operations performed during the operation of an application. Write procedures are more complex. The process involves
reading, writing, and verifying, as they have to ensure they are correct and consistent. Write procedures will utilize
commands and events. On the other side, the read operations tend to be far less complex in nature,
with just a simple query required. By separating these two functional areas within the application,
we reduce the complexity to more manageable and specific units. These are known as bonded contexts.
When implementing CQRS, the patterns are applied to these bonded contexts and not to the architecture as a whole.
CQRS works well within specific, carefully analyzed for suitability, bonded context, where there is a clear imbalance
between query and command. By implementing CQRS within a bonded context, or to a component of business logic,
even just in a theoretical sense when considering the business suitability, we are in effect creating two models, or two areas of segregation.
By segregating the two models' responsibilities, they both can be designed to fit their own requirements independently of the other.
The CQRS pattern segregates the command and the query models with a bonded context, or a component of business logic.
However, they cannot be separated completely, something must act between them in order to update the query model, that new fresh data is available.
This is done through commands and events. Commands are imperatives that ask the system to perform an action.
Events are notification messages sent to inform interested parties that something has changed or already happened in the system.
Commands and events are exchanged between software objects. Events provide the basis for a mechanism to synchronize the two models,
so that if the write side makes an update, then the read side will be notified through an event message.
Here are some reasons why you should consider to use a CQRS pattern.
Scalability
Database read and write operations are not balanced. Read queries greatly outnumber write commands.
Therefore, each portion of the infrastructure could be segregated and scaled appropriately.
Reduced complexity
Business logic is usually only applied to transactional operations, or when applying updates, and not to plain queries.
By bundling the commands and the query together in the same model, it becomes more complex to understand
and deal with issues such as multi-users, performance, consistency, and stale data.
Flexibility
This derives from the separation of the read and the write sides of the operation.
It becomes far easier to update and create new read operations with the confidence you will have no impact upon the business logic.
Business focus
A consequence of the separation of the read and write sides is that the business logic can then become the focus of the design as the model has become more adaptable.
Task-based UIs
CQRS uses command to initiate operations, so it makes it easier to build task-oriented UIs.
Collaboration
CQRS is useful within collaboration domains because they are typically the most complex, fluid, and have significant bonded contexts, as they have multi-users all operating on the same shared data.
Stale data
Stale data
Stale data is a significant problem when working in collaborative environments.
The CQRS mitigates this by using the same mechanisms to push the change from the write side to the read side, as it does to control the data on the read side becoming stale.
To summarize
The CQRS pattern is a way to apply a business focus to the application design by applying the concept of bonded contexts to areas of specific business logic.
It brings several benefits, such as flexibility, scalability, and reduced complexity to these individual portions of the system.
Identifying where to use the CQRS pattern requires consideration of the return of investment, ROI, when implementing the pattern, whether that is purely financial or through the future business benefits.
A useful rule of thumb for identifying where you might implement the CQRS pattern are to look for components that are collaborative, use complex business rules, and will deliver a competitive advantage to the business.
CQRS is unlikely to be cost-effective in environments that are static, simple, non-collaborative, with non-core bonded contexts.
Even in collaborative environments, most bonded contexts will not benefit from implementing CQRS.
In short, the CQRS pattern should only be applied when there is a clear business requirement to do so.
Part 2. Algorithms and Data Structures
An algorithm is well-defined, logical, unambiguous, and finite procedural steps, which takes an input value and produces a desired result.
An algorithm is therefore a mini-program that a microprocessor will follow, which transforms the input to the desired output.
It is the technique developers use to solve instances of a problem.
Algorithms are independent of specific programming languages, such as Java, C++, or PHP.
They determine how and why the code will be executed on a computer.
Algorithms are mostly expressed in pseudocode, as they are focused on high-level logic, and can be implemented in any language.
Where an algorithm differs from a data structure is that an algorithm is a set of instruction, whereas a data structure is a way of storing data.
Data structures address the problem regarding how we store and organize data in a computer, so that it can be accessed efficiently.
There are different techniques when storing data, hence the various structures, which are suited to different kinds of applications.
Where data structures provide utility is that they provide a means to manage large amounts of data efficiently, such as in large databases.
However, efficient data structures are key to designing efficient algorithms.
For example, data should be stored in a manner that allows it to be sorted, catalogued, or indexed in some manner.
A structured sort of data will facilitate a more efficient search.
For the readers interested in algorithms and advanced data structures, I would refer to the classical book, Art of Computer Programming, Volume 1, Fundamental Algorithms, by Donald Knuth.
Computational Complexity and Big O Notation
One of the fundamental areas of theoretical computer science is complexity theory, the analysis of the resources needed to solve computational problems.
Two major resources usually considered are the time complexity and space complexity, which represents how many steps it takes to complete a computation, and how much memory is required to accomplish that computation.
This is the key area of computer science that is often overlooked or undervalued by many developers.
Let's look on the problem in more details.
In order to efficiently analyze the time and space complexity of a solution or an algorithm, it is beneficial to express the time or space required to solve the problem as a function of the size of the input.
It is crucial to understand the complexities of algorithms as complexity directly affects performance.
I know it might sound confusing, so let's take a look at an example of a simple game that I like to play with my kids.
It's called Tower Blocks.
The objective of the game is to build a tower made of blocks as tall as possible and as fast as possible by utilizing all of the given blocks.
As you can see, the complexity of this game and the time taken to build the tower varied greatly depending on the number of the blocks given.
While building a tower of 10 blocks can take 15 to 20 seconds, the same simple task with 10,000 blocks can take hours if not days.
The same is true for any algorithm.
Sorting 10 elements array is different than sorting 10 million elements array by using the same algorithm.
As you can see in the example, we are not interested in the exact number of operations performed to build a tower.
Instead, the goal is to find the dependency between the number of operations and the problem size, i.e. the number of blocks.
To formalize the analysis of algorithm function and describe the limiting behavior of a function, the big O notation was adopted.
Big O, also known as Big Omicron notation, is part of larger family notations that is called Bachman-Landau notations, after Edmund Landau and Paul Bachman,
that were introduced in Paul Bachman's 1892 book, The Analytic Number Theory.
In short, it helps to describe how fast a function grows or declines.
The benefit of such approach is that different functions with the same growth rate may be represented using the same O notation.
The letter O is used because the rate of growth of a function is also called its order.
Big O is just one of five standard asymptotic notations, namely, Big O, Little O, Big Omega, Little Omega, and Big Theta.
There is no need to know all those notations as long as you know the Big O represents the asymptotic worst-case performance.
However, if you are interested to learn more about little o, omega, or theta notations, I would recommend to read
Big Omicron and Big Omega and Big Theta article by Donald E. Knuth published in ACM SIGACT News.
Math whizzes may want to look at the original paper that introduced omega notation,
Some Problems of Diophantine Approximation by Hardy and Littlewood in the ACTA Mathematica journal.
The most common big O runtimes are O of 1, called constant complexity.
It means, if the input size changes, the runtime doesn't change.
For example, hash table lookup.
O of logarithm n, logarithmic complexity.
It means, if the input size doubles, the runtime increases by 1.
For example, binary search.
O of n, linear complexity.
It means, if the input size doubles, the runtime doubles as well.
For example, array traversal.
O of n squared, quadratic complexity.
It means, if the input size doubles, the runtime quadruples.
For example, bubble sort is an algorithm of quadratic complexity.
O of 2 to the power of n, exponential complexity.
It means, the input size increases by 1, the runtime doubles.
Exponential time grows incredibly fast.
For example, 2 to the power of 189 will result in about 1 octodecillion, 57 zeros,
which roughly represents the number of atoms in our sun.
Therefore, exponentially complex algorithms are most often impractical.
It's important to say that the big O expressions do not have constants or low order terms.
This is intentional, as when n gets large enough, constants and low order terms don't affect the grow rate of the function.
The calculating big O complexity of a given piece of code can be challenging,
so I usually recommend a couple of simple rules that greatly simplify that process.
First, if you're looking on sequence of commands or statements,
then the total computation time is achieved by simple addition of the times for all commands or statements.
In case of if-else block, remember that big O represents worst-case performance.
That means the time complexity of the statement would be slowest of the two conditions.
For example, in case block if executes in linear time and block else executes in quadratic time,
the time complexity of whole if-then-else statement would be quadratic.
Loops execute in linear time, but it is important to remember that statements inside the loop have their own complexity,
so the complexity of the loop is calculated as n multiplied by the complexity of inner loop statement.
Keep in mind that big O is just a small piece of the story.
There is a lot of information that big O expression of algorithm complexity does not tell you about a given algorithm.
Big O ignores factors that do not contribute in a meaningful way to the growth curve of a function as the input size increases towards infinity.
Thus, it focuses strictly on the scaling nature of an algorithm.
Recursion
Recursion is an algorithmic technique based on the idea that a problem can be solved utilizing solutions to smaller, simpler versions of the same problem.
The theory being that when the smaller versions reduce to easily solvable cases, then one can use a recursive algorithm to solve the larger problem.
Recursion is a method where the solution to a problem depends on finding the solutions to smaller, simpler instances of the same problem.
It achieves this by calling itself with smaller or simpler input values.
It can then determine the output by applying simple operations to the returned values to solve the problem.
A recursive function can have one or more base cases, meaning input or inputs, that are identified and defined within the algorithm,
and it is upon these base inputs that the function produces a result trivially, without recurring.
The next step, the inductive clause, has one or more recursive cases, meaning input or inputs for that the program recurs, or calls itself.
Every recursive solution involves two major parts, or cases, the second part having three components.
The first component is called the base case, or cases, in that the problem is simple enough to be solved directly.
The second component describes the recursive case, or cases.
A recursive case has three components that are, initially, the problem is divided into one or more simpler or smaller parts.
Then, call the function recursively for each part.
Finally, combine the results into a solution for the problem.
The theory behind the tasks of the recursive cases is that if we replace complex inputs with simpler ones,
then in a properly designed recursive function, with each iteration, the input problem is simplified,
so that eventually the base case, and hence a solution, must be reached.
One thing to remember is that any recursive solution can be implemented without the recursion,
and that recursion usually involves significant amount of extra memory to be allocated to keep the recursion tree.
The most common example of recursion is calculation of factorial.
There, the solution can be represented in the following recursive form.
n factorial equals n times n minus 1 factorial, with stop condition of 0 factorial equals 1.
Divide and Conquer Algorithms
Divide and Conquer algorithms are used in applications such as Binary Search, Quick Sort, and Merge Sort.
The idea is to reduce an instance of a problem to one or more smaller instances of the same problem.
By repeatedly reducing the problem, we will have instances that are small enough to solve easily.
A simpler variant of Divide and Conquer is called a Decrease and Conquer algorithm.
The difference being, with Decrease and Conquer, the algorithm's goal is to solve an identical subproblem
and use the solution to solve the bigger problem.
Divide and Conquer reduces the original complex problem into multiple subproblems.
Therefore, the Conquer stage will be more complex than those of the Decrease and Conquer algorithms.
The process behind the Divide and Conquer algorithms works recursively,
in that it sets about reducing the complex problem into two or more subproblems, of similar type,
until these subproblems become trivial.
The solutions to the simple subproblems are combined to give a solution to the original, more complex problem.
A binary search algorithm is an application of the Divide and Conquer technique.
The binary search technique finds the position of a specified value, the input key, within a sorted array.
With each step, the algorithm compares the input key value with the key value of the middle element within the array.
If the key matches, then a matching element has been found.
Otherwise, if the sought key is less than the middle element's key, then the algorithm repeats its action on the element to the left,
or, if the input key is greater, on the element to the right.
If the remaining array to be searched is found to be empty, then the key cannot be found in the array.
With each iteration, or pass-through, a binary search halves the number of items to check,
so locating an item takes logarithmic time.
Dynamic Programming
In computer science, dynamic programming is a method used for solving complex problems by breaking them down into simpler subproblems.
When applicable, the method takes far less time than naive methods that don't take advantage of the subproblem overlap.
In other words, dynamic programming is an algorithm technique that solves an optimization problem by caching subproblem solutions, memorization, rather than re-computing them.
The idea behind dynamic programming is quite simple.
To solve a given problem, we need to solve different parts of the problem, subproblems, and then combine the results to reach an overall solution.
A more naive approach that does not utilize dynamic programming will find that many of the subproblems that are generated are the same and are solved many times.
The dynamic programming approach seeks to solve each subproblem only once, and then store or memorize the result.
The next time the same solution is required, it is simply looked up in memory.
For the dynamic programming algorithm to be applicable, one must ensure that there are two key attributes that a problem must exhibit.
Optimal substructure, as described by means of recursion, and overlapping subproblems.
Overlapping subproblems means that any recursive algorithm solving the problem should solve the same subproblems over and over, rather than generating new subproblems.
Dynamic programming takes account of this fact and solves each subproblem only once.
A caveat to dynamic programming is that each subproblem must be only slightly smaller than the larger problem.
When they are a multiplicative factor smaller, the problem is no longer classified as dynamic programming.
The basic technique of dynamic programming in solving complex problems by reducing the problem into simpler subprograms crops up with many techniques, memorization and recursion, divide and conquer.
So what are the differences?
Well, dynamic programming and memorization work together.
The main difference between dynamic programming and divide and conquer is that subproblems are autonomous in divide and conquer, whereas the subproblems overlap in dynamic programming.
The difference between dynamic programming and straightforward recursion is in caching or memorization of recursive calls.
When subproblems are not repetitive or are independent, memorization does not help.
Therefore, dynamic programming is not a solution for all complex problems.
However, where it is applicable, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
Greedy Algorithms
Greedy algorithms build up a solution, piece by piece, always choosing the next piece that offers the most obvious and immediate benefit, without any consideration of the future.
Although that approach may not seem beneficial in most computer applications, there are many for which it is optimal.
A greedy algorithm is similar to a dynamic programming algorithm in that it again reduces a complex problem to simpler subproblems.
However, where it differs is that solutions to the subproblems do not have to be known at each stage.
The greedy algorithm instead uses its greedy choice of what looks best for the moment in time.
A greedy algorithm always takes the best immediate or local solution.
The greedy method will utilize the solution with the best possible decision based on the current local optimum and the best decision made in a previous stage.
It may not be exhaustive or optimal for all applications.
However, when it is suitable, it will be the fastest method.
There are some other differences between greedy algorithms and dynamic programming.
One is that problems can be solved with greedy algorithms do not have to exhibit qualities of overlapping subproblems, but they will have the greedy choice property.
This property states that a choice that is local and optimal, here and now, is part of the global optimal solution to the problem.
Greedy Choice Property
The greedy algorithm makes whatever choice seems to be best at the time and then solves the subproblems that may arise later.
The choice made by a greedy algorithm may depend on choices already made but not on all the choices or all the solutions to the subproblem.
It is iterative in that it makes one greedy choice after another.
However, a greedy algorithm never reconsiders its choices.
This is another difference from dynamic programming that is exhaustive and is guaranteed to find the solution.
Dynamic programming makes decisions based on all the decisions made in the previous stage.
Data Structures
Arrays
The array is one of the most fundamental data structures that consists of a collection of elements, each identified by at least one array index.
An array is stored sequentially in the memory, so that allows to find the position of each element by calculating the shift from beginning of array via multiplying size of the element on the index.
Due to the nature of an array, it provides constant time access to the elements.
It also requires that all the elements of an array would be of the same type.
Linked List
Data structures come in many forms. Probably the most common is the Linked List.
As its name suggests, it is a data structure consisting of a linked group of nodes that together form a sequence.
Within the Linked List format, each node is composed of data and a reference that is the link to the next node in the sequence.
This structure allows for efficient insertion or removal of elements from any position, front, middle, or rear in the sequence.
Linked List is a linear collection of self-referential class objects, called nodes, connected by reference links.
Many people tend to think of it as of dynamically sized array, but you need to understand the difference between these two data structures.
The most common and the simplest kind of Linked List is a Singly Linked List.
A Singly Linked List contains two values, the value of the current node and a link to the next node.
A little more complex type of Linked List is a Doubly Linked List.
In Doubly Linked List, each node has two links. One points to the previous node, or a null value if it is the first node, and one points to the next, or a null value if it is the final node.
Both Single and Double Linked List can form a circularly linked list.
In a circularly linked list, the first and final nodes are linked together.
To traverse a circular linked list, you begin at any node and follow the list in either direction until you return to the original node.
If instead of pointing to the first node, or to the null final node, points to any other element in the list, the list, called list, with a loop.
Linked lists are among the simplest data structures, and they can be used to implement several other common abstract data types, including stacks, queues, and associative arrays.
However, simple linked lists do not by themselves allow random access to the data or any form of efficient indexing.
Thus, many basic operations, such as obtaining the last node of the list, or locating the place where a new node should be inserted, will possibly require scanning most or all of the list elements.
Queue
A queue is a particular kind of abstract data type whereby the entities are kept in order.
This makes the queue a first-in, first-out, FIFO data structure.
In a FIFO data structure, the first element added to the queue will be the first one to be extracted.
A queue is an example of a linear data structure, and it follows the principle that all entities already present must be served before those that follow in the queue.
The two fundamental operations on the queue are enqueue and dequeue, where enqueue save item into the query, and dequeue takes item from the query.
Stack
A stack is an ordered linear data structure in which all insertions and deletions operations are made at one end, called the top.
A stack is the opposite of a queue in that it is last-in, first-out, LIFO data structure.
A stack data structure provides two fundamental operations, called push and pop.
The push operation adds a new entity to the top of the stack, while the pop operation removes an item from the top of the stack.
Microsoft provides both stack and queue object in its .NET Systems Collections namespace.
In .NET, both stack and queue are implemented with arrays.
Both stack and queue classes implement three interfaces, iCollection, iEnumerable, and iClonable.
The .NET implementation of stack is simple and straightforward.
The cursor, called size, indicates the last element in array that is current element of the stack.
The array reallocation happens inside push method.
Initially stack starts with array size of ten elements, that are defined by private constant called default capacity, that is equal to ten.
Every time stack reaches the size of the array, the array is recreated, and the size is doubled.
The implementation of queue is slightly more complex.
It uses three internal variables called size, head, and tail.
This is done to optimize performance, and doesn't shift all array on one element for each DQ.
Instead, array reallocation happens only when array size changes, and method set capacity is called.
In addition, instead of simply doubling the size of the internal array, .NET queue class gives you the ability to control array size grow, via grow factor variable, that can be passed through the constructor.
Trees
A tree data structure defines a hierarchical structure with a set of linked nodes.
A tree can be defined as having its base called the root node, the tree emanates from the root, and each subsequent leaf node is a data structure consisting of a value, together with a list of child nodes, with the constraints that no node is duplicated.
Within a tree data structure, a node is a unique structure, which may contain a value or condition, or represent a separate data structure, that could well be a tree of its own.
Each node in a tree has either a standalone entity, or has its own child nodes, which are below it in the tree structure.
A node that produces a child is called the child's parent node.
A node must have at least, and at most, one parent.
In order to differentiate between the various nodes within the tree, they are described as an internal node, also known as an inner node, or branch node.
This is any node of a tree that has its own child nodes.
However, there are also external nodes, also known as an outer node, leaf node, or terminal node.
These are any node that does not have child nodes.
The topmost node in a tree is always called the root node.
Being the topmost node, the root node will not have a parent.
It is the node at that algorithms on the tree begin, since, as a data structure, one can only pass from parents to children.
All other nodes can be reached from the root node by following edges or links.
In tree diagrams, the root node is conventionally drawn at the top of the tree.
Every non-root node in a tree can be represented as the root node of the subtree that is rooted at that node.
When we consider tree diagrams, then height of a node is the length of the longest downward path to a subtree leaf from that node.
The height of the root is the height of the tree.
The depth of a node is the length of the path to its root, i.e., its path back to the root.
This is important when we consider calculating the best paths, which are commonly required in the various self-balancing trees.
The root node, therefore, has a depth of zero.
Leaf nodes have height zero.
And a tree with only a single node, hence both a root and leaf, has depth and height zero.
An empty tree, tree with no nodes, has depth and height of minus one.
BINARY TREE
A typical data structure tree is the binary tree, in that each node has, at most, two child nodes, distinguished as left and right.
BINARY SEARCH TREE
Binary trees are used to implement binary search trees and binary heaps.
Its form has a characteristic of being exactly N-1 branches.
As such, a binary search tree can be considered a node-based binary tree data structure that has the following properties.
The left subtree of a node contains only nodes with keys less than the parent's key.
The right subtree of a node contains only nodes with keys greater than the parent's key.
Therefore, both the left and right subtree must also be binary search trees.
There must be no duplicate nodes.
Generally, the information represented by each node is not a key, but a record.
However, for sequencing purposes, nodes are compared, according to their keys, rather than any part of their records.
One of the reasons we tend to use binary search trees over other data structures is that the related sorting algorithms and search algorithms, such as in-order traversal, can be very efficient.
The time it takes to find a desired node depends on the order the nodes are set in.
The tree's height determines how long it takes to find an element.
Even in the worst case, it depends directly on the tree's height, because it may happen that what we are looking for is at the very bottom of the tree.
Searching a binary search tree
The method used when searching a binary search tree for a specific key can be a recursive or iterative process.
We begin by initially examining the root node.
If the tree is null, the key we are searching for does not exist in the tree.
Otherwise, if the key equals that of the root, the search is successful.
However, if the key is less than the root, then we continue the search in the left subtree.
Similarly, if it is greater than the root, we should search the right subtree.
This process will be repeated, throughout the depth of the tree, until the key is found or the last remaining subtree is null.
If the search key is not found before a subtree that returns a null value is reached, then the item must not be present in the tree.
Graphs
In computer science, a graph is an abstract data type, which implements the graph and hypergraph concepts from mathematics.
As in mathematics, an edge is said to point or go from x to y.
A graph data structure consists of a finite set of ordered pairs, called edges, related to certain entities called nodes or vertices .
The nodes may be part of the graph structure, or may be external entities represented by integer indices or references.
A graph data structure may also be associated to each edge or some edge value, such as a symbolic label or a numeric attribute .
The concepts of graph data structures.
Edges connect nodes, and when they do, these nodes become neighbors.
Two nodes are neighbors, if there is an edge in between them.
Neighbor nodes, or vertices, are said to be adjacent.
A graph is described as being undirected if its edges do not have a direction.
In opposite to a directed graph, there the edges have direction, and can only be traversed in that direction.
A graph is also considered to be weighted if the edges have different weight or costs.
A directed graph is a directed graph that does not contain any cycles.
Dijkstra's Algorithm
Dijkstra's Algorithm is one of the most known algorithms to solve the single-source shortest path problem for a graph with non-negative weighted edges.
In a graph theory, the shortest path problem is the problem of finding a path between two nodes in a graph such that the sum of the weights of its constituent edges is minimized.
The shortest path problem usually falls into one of three categories.
First, given nodes S and T find shortest path from S to T.
Second, given node S find shortest path from S to all other nodes.
Third, find shortest paths for all pairs of nodes.
The examples can be finding cheapest flight route to visit multiple cities, or find the shortest path for data packets to take through a switching network.
First published by Edgar Dijkstra in 1956, this is still one of the well-known algorithms to solve any of three problems above, and also often used in routing and as a subroutine in other graph algorithms.
The algorithm utilizes a dynamic programming approach via successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the reaching method.
In other words, starting from the root, the algorithm iteratively updates the distance to every unvisited node that is directly connected to the current node.
This is done by evaluating the sum of the distance between an unvisited neighbor node and the value of current node.
If the new value is less than the neighbor current value, it is updated to reflect new shortest distance.
Algorithm continues this process of updating the neighboring nodes with the shortest distances, then marking the current node as visited and moving on to the lowest unvisited node until it finds the destination.
Once destination is reached, it will be marked with the optimal distance cost.
There are a number of algorithms which are designed to solve shortest path problem.
The most important algorithms for solving this problem are
Dijkstra's algorithm solves the single source shortest path problems described above.
Bellman-Ford algorithm that solves the single source problem if edge weights may be negative.
Floyd-Warshall algorithm solves all pairs shortest paths.
Johnson's algorithm which solves all pairs shortest paths and optimized for sparse graphs.
This book is not intended to go into the details of such algorithms, but if you are interested to learn more,
I would recommend to read Shortest Paths Algorithms, Theory and Experimental Evaluation article by Boris Tchaikovsky et al., published in Mathematical Programming, Volume 73, Issue 2.
Sorting Algorithms
When we consider sorting algorithms, there are two types dependent on the method of sorting.
The types are comparison and linear time.
Bubble Sort
Bubble Sort is the easiest and simplest of all sorting algorithms.
However, its inherent inefficiency makes it only really suitable in academic circles as a teaching aid.
This is because Bubble Sort has a very easy to understand process whereby it works by repeatedly stepping through the list of items to be sorted.
With each step, it compares each pair of adjacent items and swaps them if they are in the wrong order.
It iteratively repeats the process until there are no longer any swaps needed.
Because no further swaps are required, it would indicate that the list is sorted.
The algorithm gets its name due to the way smaller elements bubble to the top of the list as the algorithm steps through each comparison.
It only uses comparisons to operate on elements.
Therefore, it is a comparison sort.
Insertion Sort
Although it is one of the simple sorting algorithms with O of N squared worst-case time,
Insertion Sort is the preferred algorithm when the data set is nearly sorted because it is adaptive or when the list size is small because it has low overhead.
It also has simple implementation, is stable, and requires a constant amount of memory space.
Insertion Sort is also more efficient in practice than other simple algorithms.
For these reasons, and because it is also stable, Insertion Sort is often used as the recursive base case when the data set is too small for the higher overhead, divide-and-conquer sorting algorithms, such as Merge Sort or Quick Sort.
It is more efficient on small data sets than Bubble Sort or Selection Sort.
However, it is much less efficient on large lists than more advanced algorithms, such as Quick Sort, Heap Sort, or Merge Sort.
The Insertion Sort algorithm works by building a new list, or array, which will be the final data structure that it populates by repeatedly taking the next item from the list to be sorted and inserting it into the final data structure in its proper order with respect to the items already inserted.
Runtime is O of N squared because of the need to move other items.
Merge Sort is a divide-and-conquer algorithm that is efficient when working on large data sets.
It works by dividing the items in the data set into two sections.
It then subsequently divides these two sections, or two groups, once more, doing this repeatedly and creating increasingly smaller subsets.
Typically, then, an efficient simple algorithm is used to sort these now small data sets, such as Insertion or Selection Sort.
When the small subsets have all been sorted, they are then merged together, two by two.
It determines how to do this by examining the smallest value in each of the two subsets.
This process merges all of the subsets into a final, sorted sequence.
This algorithm is not efficient when the input data is small, and hence, the common use of a simpler algorithm to sort the small subsets of data.
Quick Sort
Quick Sort is a comparison sort algorithm of the divide-and-conquer type.
On average, this algorithm makes O of N logarithm N comparisons to sort N items.
It is also known due to its partition operation as Partition Exchange Sort.
Quick Sort can often be faster in practice than other O of N logarithm N algorithms.
Additionally, Quick Sort works well with a cache due to its use of sequential and localized memory references.
Quick Sort is a three-part comparison sort and, in efficient implementation, is not a stable sort.
It works by partitioning the data list, then sorts the subsets before finally combining them.
The initial step is to pick a pivot element.
This is any element in the list that you wish to sort.
It then partitions the data list into two.
This is the partition operation, with all elements smaller than the pivot item in one list, and all greater in the other.
It then recursively sorts again using the same Quick Sort process, creating further sublists of lesser elements and lists of greater elements.
One point to note, though, is that when picking the pivot, if the pivot element is chosen so that the list is divided into almost equally large parts,
there will be a runtime of O of N logarithm N.
However, it is not easy to pick a middle element without extensive knowledge of the dataset.
One can, however, simply choose the first value available, for example, the top number in a list.
This may be fine in some scenarios, but if the numbers are already sorted or partially sorted, this method will result in a very inefficient and very long runtime.
Therefore, it is important to randomize the pivot selection.
Quick Sort, randomized, is faster than Merge Sort and Heap Sort, even though the last ones have both average and worst case runtime of O of N logarithm N.
This is because the constant is very small in Quick Sort.
Heap Sort
Heap Sort is a comparison-based, in-place sorting algorithm that is used to create a sorted array or list, and is not a stable sort.
Heap Sort has a guaranteed O of N logarithm N time.
For worst case, that is better than Quick Sort's worst case, O of N squared, that is unacceptable for large datasets.
However, despite this, Heap Sort is generally slower in practice on most machines than a well-implemented Quick Sort, due mainly to Quick Sort's better caching.
Quick Sort, though, has potential security issues.
Thus, because of the O of N logarithm N worst case upper bound on Heap Sort's running time, and the constant upper bound on its auxiliary storage, security-conscious applications with real-time constraints often have a preference for using Heap Sort.
Heap Sort is a two-step comparison algorithm, where the first step is to build a heap out of the entire dataset.
The elements in the list, to be sorted, are placed in a heap, tree, with the biggest elements at the top, so that any element is bigger than any of the elements that are beneath it.
This means, of course, that the element that is the tree root is the biggest element in the tree.
Once this has been achieved, Heap Sort then moves to the second step.
This entails removing the largest element from the heap, and placing it into a sorted array.
We can vary the direction of the sorted elements by choosing a mini-heap or max-heap.
For the first element removed from the heap, this would be inserted in position 0 of the array.
Next, Heap Sort reconstructs the heap, and removes the next largest item, and inserts it into the array.
After we have removed all the items from the heap, and have inserted them into the array, we have a fully populated sorted array.
Heap Sort can be performed in place.
The array can also be split into two parts, the sorted array and the heap.
The heap's invariant is preserved, after each extraction, so the only cost is that of extraction.
The runtime for Heap Sort is O of N logarithm N.
Heap Sort has another competitor called Merge Sort that has the same time bonds.
Merge Sort, though, requires variable amounts of auxiliary space, but Heap Sort requires only a constant amount.
Heap Sort typically runs faster in practice on machines with small or slow data caches than Merge Sort.
However, Merge Sort has several advantages over Heap Sort, such as better data cache performance, due to it referencing contiguous locations in memory,
whereas Heap Sort references are spread throughout the heap.
This also prevents Heap Sort from being used in external sorting, as it does not have good locality of reference.
Heap Sort is also not a stable sort, whereby Merge Sort is.
Selection Sort
There is another simple selection sort that performs in the O of N squared time complexity, which of course makes it unsuitable for large data sets.
However, it is another algorithm that resides in the same class as Insertion Sort, which is a simple sorting algorithm.
Selection Sort is a simple algorithm, and it has performance advantages over other more complicated algorithms in certain scenarios.
This is certainly the case where external memory is limited.
The selection sort algorithm separates the input list into two distinct parts.
The separation, or partitioning, leaves the data set in two distinct sets, one list of elements that has already been sorted, and the other that hasn't.
So, of course, initially the sorted list is empty, and the unsorted contains all the elements.
The algorithm process follows steps whereby it searches for the smallest or largest dependent on the search order.
It then exchanges it with the left-most unsorted element, which puts it in sorted order.
Counting Sort
Within the arena of computer science, Counting Sort is an algorithm for sorting a collection of objects according to keys that are small integers.
That is, it is an integer sorting algorithm.
It works by counting the number of items that have assigned distinct key values, and thereby using an arithmetic calculation on those counts to determine their positions in the output sequence.
The algorithm running time is linear, due to the time taken being determined by the number of items and the difference between the maximum and minimum key values.
Therefore, we can suggest that it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items.
However, the Counting Sort algorithm is often used as a subroutine, supporting other sorting algorithms such as Radix Sort.
This is because Radix Sort can handle larger keys more efficiently.
Counting Sort's runtime depends on the value K.
If K does not increase too much in relation to the size, K equals O of N, then the runtime will be O of K plus N.
If you know that all the integers are in a specific area, i.e. between 1 and 10, then Counting Sort is a very good algorithm.
However, when K is constant, then the running time of Counting Sort will be Theta of N.
In that case, each item is checked for their value once and put in its stall.
If one has no control over the value of K, one cannot use Counting Sort.
If we want to sort the numbers 1, 2, and 1 billion, we would be better off using the very inefficient Bubble Sort.
This is because Counting Sort will have to set out a billion places, or stalls, to put the numbers in, before it even begins to sort them.
Radix Sort
Radix Sort is a non-comparative integer sorting algorithm that is similar to Counting Sort in that it works by sorting data using integer keys.
Radix accomplishes this task by grouping keys by their individual digits that share the same significant position and value.
A positional notation is required, but because integers can represent strings of alphanumeric characters, such as names or dates, and specially formatted floating point numbers, Radix Sort is not limited to integers.
Most computers are digital, and therefore they represent all of their data internally as electronic representations of binary numbers.
Therefore, processing the digits of integer representations by using binary digits is extremely convenient.
Two classifications of Radix Sort are Least Significant Digit, LSD, Radix Sort, and Most Significant Digit, MSD, Radix Sort.
LSD Radix Sort process the integer representations starting from the least significant digit and move towards the most significant digit.
MSD Radix Sort work in reverse.
The integer representations processed by the sorting algorithms, referred to as keys, can exist all by themselves or be associated with other data.
LSD Radix Sort typically use the following sorting order. Short keys come before longer keys, and the keys of the same length are sorted lexicographically.
This follows the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5.
MSD Radix Sort use lexicographic order that is suitable for sorting strings, such as words or fixed length integer representations.
For example, a sequence such as A, B, C, D, AB, BA would be lexicographically sorted as A, AB, B, BA, C, D.
If lexicographic ordering was to be used to sort variable length integer representations,
then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9.
This is as if the shorter keys were left justified and padded on the right with blank characters,
so to make the shorter keys as long as the longest key for the purpose of determining their sorted order.
Bucket Sort
Bucket Sort or Binsort is a sorting algorithm that works by partitioning an array into a number of buckets or groups of elements.
Each bucket is treated as a separate entity and sorted individually, either using a different sorting algorithm
or by recursively applying the bucket sorting algorithm.
Bucket Sorts are considered a distribution sort, and they are a relative of Radix Sort.
Bucket Sorts work well for data sets where the possible key values are known and relatively small,
and there are on average just a few elements per bucket.
Since Bucket Sort is not a comparison sort, the O of N logarithm N lower bound is not applicable as a measure of performance.
The computational complexity estimates involve the number of buckets.
The Microsoft .NET Framework is a software framework developed by Microsoft.
It includes a .NET Framework class library and virtual software execution environment to provide language interoperability i.e. each language can use code written in other .NET languages, and platform independence.
.NET execution environment is known as the Common Language Runtime , an application virtual machine that provides services such as security, memory management, and exception handling.
Another distinctive attribute of the .NET Framework is the ability to emit additional declarative information, also called metadata, into all modules and assemblies.
Metadata is defined as the data providing information about the other set of the data and it is the key to a simpler programming model.
Metadata in .NET servers eliminate the need for interface definition language or IDL files, header files, or any external method of component reference.
Metadata allows .NET languages to describe themselves automatically in a language-neutral manner, unseen by both the developer and the user.
All common language runtime modules and assemblies are self-describing.
The .NET Framework allows developers to declare specific kinds of metadata, called attributes, in the compiled file.
Attributes can be found throughout the .NET Framework and are used to control in more detail how your program behaves at runtime.
Microsoft started development of the .NET Framework in the late 1990s, originally under the name of Next Generation Windows Services .
The first version of .NET 1.0 was released in February 2002.
As stated above, the .NET Framework consists of the Common Language Runtime and the .NET Framework class library.
The Common Language Runtime is the foundation of the .NET Framework.
During my interviews, 99% of candidates will tell me that CLR is needed for memory management.
That is true. However, it is doing way more than managing the memory.
It manages memory, thread execution, manages code execution, performs code safety verification, compilation, and other system functions.
Execution code in CLR environment is called managed execution because it is managed by CLR.
CLR and .NET class library provides an execution in environment and software abstraction layer on top of hardware resources.
In contrast, unmanaged code requires that the programmer write many of these common functions that the runtime handles.
Let's look on this from an extremely simplified angle.
Think about the scenario that you are preparing for two trips.
One is alone, hiking to the top of a mountain.
Living in Seattle, I like to use Mount Rainier as an example.
And the second trip is to an all-inclusive chain resort in, let's say, Mexico.
For the first trip, you need to anticipate and prepare all resources you might need to survive and function in unfamiliar environment with unpredictable weather.
You would need to pack enough food to survive from one side, but not too much so you can still carry it.
You would need all the tools and skills you can gather before the trip.
If something happens, you are on your own.
There is no security, no one will check your equipment before the climb.
If something unexpectedly happens, the recovery can be very costly or even impossible.
From the other side, on your second trip, you will live in familiar environments managed for you by the resort.
They will try to provide a consistent experience no matter what location in Mexico you would choose.
Food, room, and all other resources will be provided to you based on your needs.
The resort will provide a reasonable level of security and safety for you.
You still might run into unexpected situations on your resort vacation, but in the case of the resort, it will provide a variety of options to handle your recovery.
As you certainly understood, I am drawing analogies with managed and unmanaged execution here.
The example is a huge exaggeration, but it does highlight important differences between both of them.
To summarize, the .NET framework provides a runtime environment called the Common Language Runtime.
The Common Language Runtime makes it easy to design components and applications whose objects interact across languages.
Objects written in diverse .NET languages can communicate with each other, and their behaviors can be tightly integrated.
According to Microsoft, CLR provides the ability to easily use components developed in other languages.
Extensible types provided by a class library.
Language features such as inheritance, interfaces, and overloading for object-oriented programming.
Support for explicit free threading that allows creation of multi-threaded, scalable applications.
Support for structured exception handling.
Support for custom attributes.
Garbage collection.
Use of delegates instead of function pointers for increased type safety and security.
Ok, now we know that .NET provides us an execution environment and a class library, but isn't it still a black box?
Let's dig into more details and see how the pieces are working together.
Logically, writing and running any program involves the following steps.
First of all, a developer writes some code in one of .NET languages, let's say C-sharp.
After the code is ready, it is compiled.
The compiler produces executable files, dynamic link libraries, or code modules.
For compiling C-sharp code, you need to invoke csc.exe executable that is provided by Microsoft as part of .NET framework redistributable.
The csc.exe executable file usually is located in the Microsoft.NET backslash framework backslash version folder under the Windows directory.
Compiler translates the source code into common intermediate language and generates the required metadata.
The generated .NET assembly is built on top of the PE file format that is used for all executables and DLLs in Windows.
PE format, in turn, is a derivative of Microsoft common object file format.
PE file historically starts with small MS-DOS stub and split up into sections.
Three main sections of .NET PE file are PE header, common intermediate language instructions, and metadata.
Prior to Windows XP, OS didn't differentiate .NET assemblies, so they had to load as any other executable and the STAT CLR to read and execute rest of the file.
Starting with Windows XP, OS recognizes and loads .NET assemblies natively.
Managed code don't have full access to the entire physical memory.
Instead, it is loaded into virtual memory, split into pages that the OS maps onto physical memory as required.
In the header of each managed executable file is a metadata information telling the loader how to map each section of a file into a page and what access permissions to apply to each page.
When a .NET executable loads, its entry point is a small piece of code.
That code provides an instruction to run an exported function in mscoree.dll called corexmain for exe or coredllmain for DLL assemblies.
That function initializes the common language runtime , locates the managed entry point in the executable assembly's CLR header, and begins execution.
When the code runs, the runtime loads the module into memory and consults the metadata for the code block.
The JIT compiler reads the common intermediate language instruction for the whole method, analyzes it thoroughly, and generates efficient native instructions for the method.
The runtime uses a just-in-time compiler to convert the common intermediate language instructions to native machine code, one method at a time as needed.
After the common intermediate language block is compiled to machine code, the CLR holds the compiled code in a working set.
The next time that the code block needs to execute, the CLR will check its working set and runs the code directly if it has already been compiled.
The importance of this step cannot be overestimated.
Following best programming practices and separate code on logically executable blocks and functions helps to optimize execution by ensuring that the code is not JIT compiled until it is needed.
CLR does not persist compiled code between runs of the application.
It is also worth mentioning that there are two types of JIT compilation in the .NET framework.
The most used type is called normal JIT compilation and works as described above.
In this type, methods are compiled when called at runtime and cached in memory after execution.
The second method is called pre-JIT compiler.
It compiles the entire assembly before it has been used.
Normal JIT compiler is the best option for almost all cases.
CLR is also responsible for code safety verification.
Code running in one application cannot directly access code or resources from another application.
The common language runtime enforces this isolation by utilizing application domains.
Due to the emphasis on code safety and isolation, many people confuse application domains and OS process.
The key difference is that one CLR process might host multiple application domains.
Another common mistake people do in interviews is to confuse .NET Framework class library with Common Language Infrastructure .
The Common Language Infrastructure is an open specification developed by Microsoft.
The Common Intermediate Language describes the executable code and runtime environment that form the core of the Microsoft .NET Framework,
starting from the set of data types and operations to metadata and virtual execution system .
In other words, the .NET Framework is Microsoft's implementation of the Common Intermediate Language standard.
As another example, Mono Development Platform is an open source implementation of Common Intermediate Language sponsored by Exameron.
To recap, .NET uses a two-step implicit compilation process.
This process enables language variability, platform independence, and simplifies development.
The Just-in-Time compiler is part of the Common Language Runtime .
CLR also manages memory, thread execution, code execution, code safety verification, and other systems.
.NET compiles files in portable executable format that consists of three main sections, PE header, MSIL instructions, and metadata.
The JIT compiler processes the Common Intermediate Language instructions into machine code specific for an environment during runtime.
Normal JIT compiler compiles only those methods called at runtime.
JIT stores compiled code in dynamic memory of an application.
The Common Language Infrastructure is an open specification developed by Microsoft.
Application Domain
In short, application domains provide a secure and versatile unit of processing that the Common Language Runtime can use to provide isolation between applications.
Application domains provide an isolation boundary for security, reliability, and versioning, and for unloading assemblies.
The interesting question is relation between process, thread, and an application domain.
In simplistic terms, we can say that a process is what OS uses to facilitate the execution of a program.
It has address space that consists of stack, heap, static, and code region.
Each process has a unique process ID associated with it.
A thread is the operating system construct used by the Common Language Runtime to execute code.
A process has at least one thread, but more threads might run in the context of the process.
If an application fails for some reason, only that process is affected.
Applications in other processes continue to execute.
Application domain is an isolation unit within a process.
There may be multiple application domains in a single process.
The isolation allows app domains to be easily unloaded from a process without affecting code running in other app domains.
According to MSDN, there is not a one-to-one correlation between application domains and threads.
Several threads can belong to a single application domain.
And while a given thread is not confined to a single application domain, at any given time the thread executes in a single application domain.
Every managed application runs in an application domain.
As mentioned above, application domains are usually created and manipulated by runtime host.
However, .NET provides app domain class that represents an application domain can be used to manipulate application domains at runtime.
The class provides capabilities for applications to respond when an assembly is loaded, when an unhandled exception is thrown, etc.
To recap, every managed application runs in an application domain.
Application domain is an isolation unit within a process that provides an isolation boundary for security, reliability, and versioning.
App domains can also facilitate assembly unloading, as unloading an app domain will cause all of the assemblies currently contained in it to be unloaded as well.
Garbage collection
Now, let's look under the hood of garbage collection process in CLR.
Let's start with a little bit of history.
Garbage collection concept is not something new.
It was invented by John McCarthy, the developer of the LISP programming language family.
He outlined the principle of automatic memory management in the CACM paper in 1959.
The two fundamental steps of garbage collection are
Find data objects in a program that are out of scope and will not be used in the future.
Free the resources used by those objects.
There are several GC algorithms in use today.
In the .NET framework, the garbage collector manages the allocation and release of memory for your application.
The .NET application developers work only with virtual address space and never manipulate physical memory directly.
This brings enormous advantage and productivity gain in day-to-day application development.
As Joel Spolsky said in the article titled, How Microsoft Lost the API War,
A lot of us thought in the 1990s that the big battle would be between procedural and object-oriented programming.
And we thought that object-oriented programming would provide a big boost in programmer productivity.
I thought that too.
Some people still think that.
It turns out we were wrong.
Object-oriented programming is handy-dandy, but it's not really the productivity booster that was promised.
The real significant productivity advance we've had in programming has been from languages that manage memory for you automatically.
The garbage collector allocates and frees virtual memory for you on the managed heap.
To maintain type safety and security, C-Sharp does not support pointer arithmetic by default.
However, by using the unsafe keyword, developers can define an unsafe context in that pointers can be used.
In the Common Language Runtime, CLR, unsafe code is referred to as unverifiable code.
The use of pointers is rarely required in C-Sharp, and many developers don't even know about the possibility of using pointers in C-Sharp.
The use of unsafe context in most situations is discouraged as it introduces security and stability risks.
Specifically, an unsafe context should not be used to attempt to write C code in C-Sharp.
Now, let's take a look on the garbage collection execution pipeline.
After the garbage collector is initialized by the CLR, it allocates a segment of memory to store and manage objects.
This memory is called the managed heap, as opposed to a native heap in the operating system.
Each time you create a new object, the Common Language Runtime allocates memory for the object from the managed heap.
The heap also maintains a pointer which indicates where the next object is to be allocated within the heap.
Initially, the next object pointer is set to the base address of the reserved addressed space region.
After an object positioned on the heap, the next object pointer moves by the size of the object to the next available free position.
Before a garbage collection starts, all managed threads are suspended except for the thread that triggered the garbage collection.
Each time garbage collection is executed, it is going through the following phases.
A marking phase that finds and creates a list of all live objects.
A relocating phase that updates the references to the objects that will be compacted.
A compacting phase that reclaims the space occupied by the dead objects and compacts the surviving objects.
The compacting phase moves objects that have survived a garbage collection toward the older end of the segment.
Obviously, the fewer objects allocated on the heap, the less work the garbage collector has to do.
In order to improve execution performance, .NET employs generational GC, also known as ephemeral GC.
The .NET managed heap is organized into four storage bins, namely 0, 1, 2, and the large object heap, so it can handle long-lived and short-lived objects.
Generation 0
This is the youngest generation and contains short-lived objects.
Generation 0
generation 1
This generation contains short-lived objects and serves as a buffer between short-lived objects and long-lived objects.
Generation 2
This generation contains long-lived objects.
Large object heap
Any object created that is bigger than 85 KB will automatically be added to this heap and will only be collected on a full garbage collection.
only be collected on a full garbage collection.
Ordinarily, the large object heap is not compacted, because copying large objects imposes
a performance penalty.
However, starting with the .NET Framework 4.5.1 RC, you can use the GCSettings Large
Object Heap Compaction Mode property to compact the large object heap on demand.
Because objects in generations 0 and 1 are short-lived, these generations are known as
the ephemeral generations.
.NET garbage collection is non-deterministic, that means you cannot predict when it will
happen.
You thus cannot predict exactly when the object will be collected, even if it is out of scope.
According to MSDN, garbage collection occurs when one of the following conditions is true.
The system has low physical memory.
The memory that is used by allocated objects on the managed heap surpasses an acceptable
threshold.
This means that a threshold of acceptable memory usage has been exceeded on the managed
heap.
This threshold is continuously adjusted as the process runs.
The GCCollect method is called.
In almost all cases, you do not have to call this method because the garbage collector runs
continuously.
This method is primarily used for unique situations and testing.
If your managed objects reference unmanaged objects by using their native file handles, you
have to explicitly free the unmanaged objects, because the garbage collector tracks memory
only on the managed heap.
The most common type of unmanaged resource is an object that wraps an operating system resource,
such as a file, window, or network connection .
Finalization allows a resource to gracefully clean up after itself when it is being collected.
When an application creates a new object, the new operator allocates the memory from the
heap.
If the object's type contains a finalize method, then a pointer to the object is placed on the
finalization queue.
The finalization queue is an internal data structure controlled by the garbage collector.
Each entry in the queue points to an object that should have its finalize method called
before the object's memory can be reclaimed.
During GC-run, before finalize method is called, object pointer is removed from finalization
queue and placed on the f-reachable queue.
The f-reachable queue is another internal data structure controlled by the garbage collector.
Each element in the f-reachable queue is a pointer to the object that is ready to have
its finalize method called.
There is a special runtime thread dedicated to calling finalize methods.
First time, while the f-reachable queue is empty, this thread sleeps.
However, it wakes as the new entries appear on the queue, removes each entry from the queue,
and calls each object's finalize method.
As you can see above, finalize methods are executed on different threads.
Thus, a finalize method should not throw exceptions.
Due to the fact that they will be thrown on the different thread, exceptions thrown in
inside finalize method cannot be handled by the application and can cause the application
to terminate.
Note that implementing finalize methods, or destructors, can potentially have a negative
performance impact and you should avoid using them unnecessarily.
The behavior of the garbage collector can be controlled, to some degree, via static methods
on the class system GC.
This class can be used to request a collection to occur, destructors to be run, or not run,
and so forth.
iDisposable Interface
Another potentially confusing topic is use of iDisposable Interface.
The lack of deterministic finalization is a common complaint about garbage collected
languages.
.NET CLR does provide a way to implement deterministic finalization of unmanaged resources through
the iDisposable Interface and dispose method.
By MSDN definition, iDisposable is a standard interface in the .NET framework that facilitates
the deterministic release of unmanaged resources.
iDisposable Interface contains single method called dispose.
The consumer of an object can call this method when the object is no longer needed.
The key to understand is the consumer of an object is responsible to call dispose method.
Storage collection doesn't care about iDisposable Interface and will not call dispose method
on your object during finalization.
To further facilitate use of iDisposable, C-Sharp provides keyword USING to help consumers
of iDisposable objects explicitly call dispose method.
The USING statement ensures that dispose is called even if an exception occurs while you
are calling methods on the object.
You can achieve the same result by putting the object inside a try block and then calling
dispose in a finally block.
In fact, this is how the USING statement is translated by the compiler.
Another source of confusion with iDisposable Interface is so-called dispose pattern.
The dispose pattern is intended to standardize the usage and implementation of finalizers
and the iDisposable Interface.
The basic implementation of the pattern involves implementing the system iDisposable Interface
and declaring the dispose method that implements all resource cleanup logic to be shared between
the dispose method and the optional finalizer.
According to the pattern, a base class with subclass that should be disposable must implement
iDisposable as follows.
First, it should provide one public non-virtual dispose method and a protected virtual dispose
method.
Second, the dispose method must call dispose true and should call the GC suppress finalize
method to suppress finalization for better performance.
A dispose method should call the suppress finalize method for the object it is disposing.
If the object is currently on the finalization queue, GC suppress finalize prevents its finalize
method from being called.
Finally, the base type should not include any finalizers.
The main motivation for the pattern is to reduce the complexity of the implementation of
the finalize and the dispose methods.
There is no performance benefit in implementing the dispose method on types that use only managed
resources because they are automatically reclaimed by the garbage collector.
Another important point is to remember to call object-based dispose method when you overrode
dispose with your implementation by calling base dispose.
To review.
The .NET CLR uses a non-deterministic, mark-compact-type garbage collector that utilizes generations and
a separate large object heap.
The generation 2 heap and the large object heap are only collected when the GC performs
a full garbage collection.
Finalize methods can be used to free unmanaged resources.
At the very high level, when the garbage collector detects that an object needs to be collected,
the garbage collector looks for the object's finalize method, calls it if present, and then
the object's memory is released.
In reality, more than two collection passes may be necessary for memory to be reclaimed.
The dispose and finalize methods serve very different purposes and have different language
syntax for declaring them.
Dispose pattern is intended to standardize the usage and implementation of finalizers and
the iDisposable interface.
The types of the C-sharp language are divided into three main categories.
Value types, reference types, and pointers.
Both value types and reference types may be generic types that take one or more type parameters.
A third category of types, pointers, is available only in unsafe code.
As we talk about reference vs value types in .NET, it's important to review a couple of fundamental
principles of memory allocation.
Managed heap, unmanaged heap, and the stack.
Both stack and the heap are the memory blocks allocated by the application.
In the contrast, the unmanaged heap is used for runtime data structures, method tables, common
intermediate language, JIT code, etc.
There is a way for an application to allocate memory on unmanaged heap by calling Win32 APIs
or creating COM objects.
As mentioned above, the managed heap is where managed objects are allocated, and it is the
domain of the garbage collector.
The stack from the other side is memory block for local variables and method parameters.
A stack is allocated on a per-thread basis and serves as a working memory area for the
thread to perform its work.
When a variable is declared using one of the basic, built-in data types or a user-defined
structure, it is a value type.
The only exceptions are string and object types that are special cases discussed later.
All value types store its contents in memory allocated on the stack.
Examples of value types are int, byte, bool, short, etc.
Using the stack is efficient.
The stack space used during a method call is reclaimed once that method returns or variable
goes out scope.
In contrast, reference variable memory isn't returned to the heap when a method finishes.
It is only reclaimed when C-Sharp's garbage collection system determines it is no longer
needed.
GC is not responsible for cleaning up memory of the stack that give clear performance benefits
for storing variables on stack vs. managed heap.
However, the limited lifetime of value types makes them less suited for sharing data between
different classes.
All value types implicitly inherit from the class SystemValueType.
It is not possible for any type to derive from a value type as value types are implicitly
sealed.
In a contrast, a reference type value is a reference to an instance of an object stored
on the heap.
The reference type can be a class type, an interface type, an array type, or a delegate
type.
As an example, classes and structs are two of the basic constructs of the common type system
in the .NET framework.
Both are very similar with one significant difference.
A class is a reference type, and struct is a value type.
Because of that structs are best suited for small data structures, while classes should be
used to model more complex behavior.
Sometimes you need to store value type on the heap, or in other words, convert value types
to reference type.
Such process is called boxing and unboxing.
According to C-sharp language specification, the concept of boxing and unboxing is central
to C-sharp's type system.
It provides a bridge between value types and reference types, by permitting any value of a
value type to be converted to and from type object.
Boxing and unboxing enables a unified view of the type system, wherein a value of any
type can ultimately be treated as an object.
A boxing conversion permits a value type to be implicitly converted to a reference type.
A boxing conversion implies making a copy of the value being boxed.
An unboxing conversion permits a reference type to be explicitly converted to a value type.
There is a common misconception that managed code does not have memory leaks.
That's wrong.
It can, and it often does.
Both stack and heap memory can leak as a result of large object heap fragmentation, unneeded
rooted references, and an overuse of managed heap memory with excessive amounts of processor
time spent in the GC and number of other cases.
To recap, all value type stores its contents in memory allocated on the stack.
All reference types stored on managed heap.
Managed heap is the area of memory managed by CLR garbage collector.
Garbage collector is not responsible for cleaning up memory of the stack.
Boxing and unboxing is a process of converting value type into a reference type and vice versa.
Accessibility levels In order to control whether members can be accessible
from another code in the same assembly or from other assemblies, .NET and C-Sharp specifically
provides concept of access modifiers.
There are five accessibility types available in C-Sharp.
Public that is selected by including a public modifier in the declaration.
The generic meaning of public is access not limited.
Private selected by including a private modifier in the declaration.
The meaning of private is access limited to the containing type.
Protected selected by including a protected modifier in the declaration.
The meaning of protected is access limited to the containing class or types derived from
the containing class.
Internal selected by including an internal modifier in the declaration.
The meaning of internal is access is limited to the current assembly.
The last accessibility level is a combination of protected and internal levels selected by
including protected internal modifier in the declarations.
The meaning of this accessibility level is protected or internal.
This is the only example of combining two modifiers together.
If no access modifier is specified in a member declaration, a default accessibility is used.
Access and struct members can have any of the five kinds of declared accessibility and default
to private.
In contrast, interface and enumeration members implicitly have public declared accessibility.
Please note that there are restrictions on how access modifiers can be used with different
types or members.
For example, derived classes cannot have greater accessibility than their base types.
Operators cannot have accessibility modifiers and user-defined operators must always be declared
as public, etc.
Exception Management
Another fundamental concept of application development, and particularly .NET development, is exception
management.
.NET best practice is to use exceptions as the standard mechanism for reporting errors.
Applications and libraries should not use return codes to communicate errors.
In the .NET framework, an exception is an object that inherits from the exception class.
This exception object contains methods such as stack trace of exception, source of exception,
exception message, inner exception of the object.
An exception is thrown from an area of code where a problem has occurred.
The exception is passed up the stack until the application handles it or the program terminates.
According to MSDN, the runtime creates an exception information table for each executable.
Each method of the executable has an associated array of exception handling information that
can be empty in the exception information table.
Each entry in the array describes a protected block of code, any exception filters associated
with that code, and any exception handlers .
This exception table is efficient, and there is no performance penalty in processor time
or in memory penalty when an exception does not occur.
Applications uses the resources only when an exception occurs.
C-sharp language provides try, catch, finally construct to handle the exceptions.
The code inside try block is the code that can potentially fail and raise the exception.
The purpose of a catch clause is to allow application to gracefully handle exception.
The purpose of a finally clause is to allow you to execute cleanup code regardless of whether
an exception was thrown.
You also can explicitly throw an exception using the throw statement.
You can also throw a caught exception again using the throw statement.
When exception occurs, the runtime scans the array for the first protected block of code that
contains the current executing instruction and that also has an associated exception handler.
If the handler is not found, the exception is propagated to the parent object.
If present, a finally block is always executed, regardless of whether an exception is thrown.
Finally block is always executed after catch block.
Here are a couple best practices with regards to exception handling in .NET recommended by Microsoft.
First, it is not recommended to throw or catch a system exception or system system exception.
These are the exception thrown by CLR automatically.
Second, use try finally and avoid using try catch for cleanup code.
In well written exception code, try finally is far more common than try catch.
Finally, prefer using an empty throw when catching and re-throwing an exception.
This is the best way to preserve the exception call stack.
This concludes this book, The Refresher on .NET and Software Design Fundamentals for C-Sharp Developers.
Thank you for listening.
If you enjoyed this book, you might enjoy Job Interview Patterns, 100 Behavioral Interview
Questions and Answers by Alexei Sinyagin.
Job Interview Patterns is a must-have tool for anyone trying to find employment in today's
competitive job market.
This audiobook takes the guesswork out of intimidating job interviews by sharing with
the listener the 100 most common interview questions, why they're asked, and the answers
recruiters want to hear.
Questions such as, why should we hire you?
What is your greatest weakness?
And what is your communication style?
Beyond sharing the 100 most common questions with the listener, Interview Patterns explains
the meaning behind tricky questions and offers sample answers.
The answers offered in this book serve as a guideline from which any job seeker can use his or her
own unique information and qualifications and craft his or her own personalized answer.
Armed with this all-important knowledge, job seekers are more confident and better prepared
for the interview process, and may stand a better chance of impressing the employment recruiter.
Job Interview Patterns provides valuable information for anyone seeking work, from college students
who are trying to land their first professional job, to those who are changing careers later
in life.
References CommonInterview.com Collection of Common Interview Questions and Answers
C Sharp Language Specification Version 5.0 MSDN.com The Microsoft Developer Network
Wikipedia.org The Free Internet Encyclopedia
.NET Framework Development Guide Scripting IEEE Computer March 1998
This has been Refresher on .NET and Software Design Fundamentals for C Sharp Developers
Written by Alexey Sinyagin Narrated by Todd Gaddy
Copyright 2014 by Alexey Sinyagin Production Copyright 2014 by Alexey Sinyagin
Audible hopes you have enjoyed this program.
