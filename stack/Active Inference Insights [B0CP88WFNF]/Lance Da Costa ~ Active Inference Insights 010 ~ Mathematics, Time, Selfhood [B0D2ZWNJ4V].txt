Hello everyone and welcome back to Active Inference Insights. As always I'm your host
Darius Parvizzi-Wayne and today I have the great pleasure of speaking to Mr, although
very soon it will be Dr, Lance D'Acosta. Lance is a PhD student with Greg Pavliotis
and Carl Friston jointly at Imperial College London and UCL and his work lies at the intersection
of mathematics, cognitive science and machine learning. Indeed he is at the very cutting
edge of the technical formulations underlying Active Inference and so I'm personally looking
forward to learning from him today and I hope you are too. Ladies and gentlemen, I bring you Lance
D'Acosta. Lance, thank you so much. I genuinely meant what I don't really like doing the introduction
because it feels sort of a bit stagnant and rigid but I genuinely meant what I wrote there which is
that I'm really looking forward to learning. Thanks for having me. I appreciate the introduction.
Yeah, and when do you expect to become a doctor? When's that title happening?
My defense is at the end of February. Oh my gosh.
It's in a month, yeah. Wow, fingers crossed. I'm sure you'll smash it.
Thanks. So before we start getting into the maths, I think this is very exciting for me because you're
the first, I would say, pretty much pure mathematician we've had on. We've had computer scientists,
we've had cognitive scientists, but no one really delving into the nether regions of the maths.
You know, when I was researching what you're interested in, I came across the fact that
you're interested in world models. I'm also very interested in world models. So I wanted to start
there as like a teaser before we get into the maths. And I wanted to ask whether you could explain what
you mean by a world model and how you distinguish that from a self model. And the reason why I asked
that question is kind of twofold. One is that I think the word world models might be slightly
misleading because it kind of implies that we're modeling the world. And in some sense we are,
but we're definitely not modeling the entire world. So it might be called something like an eco niche
model. Although that's less catchy. And then the other reason is I think there's still an open
question, at least to my eyes in active inference about whether we are modeling the world and
ourselves, or whether we're just modeling the world and we fit into that in some way.
Right.
It would be great to unpick that if possible.
Yeah, that's actually a cutting edge question, but I really like it.
Great.
So let me start by world model. What I mean, what I personally mean by world model is that you,
as an individual, you have always some sensory data coming in,
olfactive, visual, auditive, and so forth. And you're going to build a model for what's going
on outside that causes your data. Now, the model that you build is a generative model,
which means that you can generate counterfactual data about what might happen in the future,
what might happen if I do this and that. It's a probabilistic model because you're always
uncertain about what's going on around in the world. And it's also a causal model.
This means that you're not exploiting the, well, you're not exploiting just the correlations in
the data that come in, but you're actually explaining the causes of the data. Something
like more powerful than a statistical model. Now, I think you're right in calling it an
eco-niche model because, I mean, the data that we have coming in at any point in time is just from
around us. And so that's ultimately what we want to model. When we talk about the free energy
principle, we say that the brain optimizes its model to maximize accuracy and minimize complexity
in that. And that entails having a model that's as simple as possible. And therefore,
you don't want to model things that are like too far away from you, that are not important
from you. And so this is why having an eco-niche model as a terminology makes more sense.
Sure.
So when you talked about world model and self-model, and this is where it really gets
cutting edge. So in the like simple applications of active inference and AI, you just care about
modeling the world. But now as humans, we don't only do that. So for instance, if I take a
particular, if I make a particular decision or take a particular course of action, and you
ask me, why did you do that? I'm not going to be able to answer in saying, oh, this neuron
did this, and this neuron did that, and this happened. But I'm going to give you a high level
overview using natural language of why I did this. Now, this is the idea of metacognition,
which means of having a model of the self, in addition of having a model of the world. So you
have a model of yourself, and you have a model of your own cognition. And so this is kind
of like a higher form of cognition and active inference, which has not so far been really
implemented out there. I believe at least in machine learning, it's kind of come out
in papers on modeling meditation with active inference.
Lars's work.
Yeah, very nice. Cool. So this is a question I've been entertaining. When I wrote this first
paper on flow that I've now mentioned far too many times, so I shan't mention it again.
I was thinking exactly what are we talking about when we're talking about something like
an epistemic agent model. And I remember one commentator said, oh, is it just metacognition?
So that's an interesting alignment there. Because in a sense, yes, you're right, we have to reflect
on our own capacities, and model that as an inference. My only question there is, you know,
in Lars's paper, you have these really beautiful hierarchical Bayes graphs. Do you think there's a
there has to be an upper bound on the computational complexity of that metacognitive? And what? And
is that just a limitation of the computation in and of itself? Or does it mean that there might be
something ontologically, sort of actually there?
Right? What do you mean by ontological? Exactly?
Uh, let's, well, let's, let's use the word. Well, I'm not going to invoke something like a
Cardesian soul. But is there any, I guess, is there anything unique beyond the algorithm beyond
the probabilistic model?
Right? So what I would say to that, I mean, what we're personally investigating with Lars,
is what does metacognition actually allow us to do allow us to that's, you know, important for
survival, that an agent that does not have metacognition is not able to do. So the idea
is, if metacognition arose at some point is because it increases our chances of survival,
our fitness, it minimizes our free energy. How exactly? I mean, how is metacognition useful
beyond what, you know, what we just talked about? It's currently unknown, at least for me.
Sure. Yeah. I think it's, I think it's a complex issue, because it, it's entangled with common
folk psychology notions of what it is to be a self. Um, so most people will think of themselves,
uh, probably if you ask them, they would say, yes, I am this substantial thing, somewhat behind
my eyes, uh, that persists through time. So somewhat of some kind of Cartesian reified ego.
And I actually found, I find the active inference is a lot easier to pass once we take that out of
the equation. When you just have the physics, or when you just have the statistical mechanics,
it becomes a lot easier to say, okay, maybe this is, maybe the self is just a further inference.
That's kind of where I was pushing you in terms of the ontological thing, because I think people
listening, if they haven't really, you know, thought about the, the Buddhism and the metacognition
and the Derek Parfit related philosophy might think, well, there's just something there that's
doing it, right? There's something there that's a homunculus behind the eyes. It's actually doing
the calculations. There's doing the variational Bayesian inference, but I assume you can't,
you know, there's no notion of that in your models. Um, so you mean like, kind of like a,
an agent inside the agent. Indeed. And obviously this leads to a homuncular infinite regress,
because who's watching the screen and so, you know, and so on. But I think people have an issue
with just the raw notion that there's just algorithmic computation going on because they're
going to ask, well, who, wait, what, who is doing it? And that leads us back to some kind of, uh,
active agent that's actually there, not just an emergent phenomenon. Right. Uh, so, so I have a few
things to say about that. I think, uh, so when we, when we engage in meditation, we are computationally
looks like we're building a higher level in our giant model, which is kind of like another agent
inside inferences about our cognition. Yes. And so when you're a trained meditator, you not only
think about the world or experience the world, but you experience your own cognitive processes
and you train yourself in doing that. Now, how that looks like computationally is that you have
a hierarchical giant model with many levels of abstraction. But when you translate that into
math and physics, it's kind of this idea of the Russian doll, like an outside agent and some more,
like more agents inside hierarchically stacked that just infer each other. Yes. Um, so then the
Markov blanket in some sense. Yeah. And so, but that, but you know, that analogy implies that there
is some outer shell that might be right only that might be, you know, yeah, this makes me think of
Maxwell Ramsey, the mouth paper on consciousness, that there's this right only layer that is
contextualizing, but not contextualized. Um, but that's kind of the other way around, right? That's
like, that's the, that's kind of like the prime mover unmoved. Whereas your, I feel like your
argument is saying, well, everything is contextualizing everything else. And actually what we might call
the outer shell, the autobiographical self is just the aggregation of all of these other selves.
Does that sound about right? Yeah. I feel it's, uh, kind of hard for me to, to describe it. Um,
I'm also thinking along these lines, but I would say, I mean, the 300 principle, everything is
dependent on the blanket that you select. Right. And it's the idea that we have multiple nested
blankets. Now, the story that you tell depends on which blanket you decide to talk about.
Um, do you, I think people have an intuitive folk psychology notion that of course, there's a point
at which the modeling of me as a discrete identity needs to stop. So we could keep more. So, so most
people would probably think about the surface of the skin, the epithelium and say, well, that that's
me. Right. And then we can get into conversations about extended mind hypothesis, but I actually
don't think it's that relevant here. I guess the point is, um, it seems like that the point at which
we stopped the Markov blanket is actually, it's not arbitrary, but it's an open question. It's
philosophically vague, so to speak, um, which, which, which negates the kind of folk psychology
notion that there is something like me, which has these strict bounds. Yeah. So, so I completely
agree with, uh, kind of like the dissolution of those strict bounds. Once you start looking at it
very carefully, intuitively, I think that my, you know, my hands are mine because I can control them
in a relatively precise way. And I think this is where that comes from. Okay. It is the preciseness
at which you can act on something or infer. Okay. That's nice. Yeah. When you, when you do a lot
of meditation or when you, when you look at it like very carefully, you start realizing that there's so
many things in your body that you cannot control. It's kind of obvious when, when you think about it
and you start realizing that you have very little agency over many things that are happening
in your body. So I think every one of us has things that they're satisfied and dissatisfied
about themselves. When you think about it, where maybe I'm dissatisfied with something about myself,
uh, but then this is not really my fault of me being that way. So here's an example of, um,
of something that I cannot control. Once you reflect that on that more and more, you start realizing that,
uh, you know, uh, we are self-organizing systems just like a tornado is a self-organizing system.
And we have a lot of things that are going on inside our bodies, inside our minds that you cannot,
just cannot control. So, yeah, that's interesting. I guess, I guess my rebuttal to that would be
arguably the literature says that, or suggests that there are processes that underlie something like
a, uh, minimal self like interoceptive inference. So that can be done reflexively or through kind of
allostatic control. So let's say my glucose level drops beyond a certain level. I either,
you know, secrete the necessary hormones or go out there and eat, but let's just say I'm secreting
those hormones. I'm not, you know, my autobiographical self doesn't have the, doesn't have the sense it's
doing anything. Yeah. That mechanism of prediction error suppression is supposedly at the heart of
the sense of mindless and presence. So how do we resolve that if we're casting selfhood as that
which is within my control? Um, well, I don't know, to be honest, but, um, it's a, it's a difficult
question, but, uh, yeah, I mean, the, the more, the more I think about all of this and the more I read,
the more I realized that we have control over very little. Yeah. And, um, so in, in the meditation
books, so I keep coming to that because they're a source of inspiration for me for describing
high level cognition. There is this, um, like fundamental truth quote unquote, which there is
no self. The more you start like investigating the mind through a self inquiry, the more you realize
that there is no self. Yeah. Yeah. Um, so maybe it speaks kind of to the things that we're just
discussing. Sure. Yes. This is, yeah, this is an interesting point. Um, I guess you, one would
have to ask very dedicated meditators, uh, whether when they're doing, let's say Vipassana and they
break that kind of duality, um, because I, you know, the, the subject object, object duality
yielding to oneness is kind of a seminal phenomenon that occurs, whether that sense of mindness
and presence is also lost. So there's an, you know, uh, John Vavakey, who's one of my favorite
cognitive scientists and who was on the podcast speaks about adverbial qualia. And what he means
by that is you, there are these so-called pure consciousness events. And even in those, the sense
of presence and perhaps mindness, although we kind of weren't sure, um, might still be
present, which I, which to my ears sounds like some computational mechanism like interceptive
inference still being at play. So I guess this is calling for calling for a denial of the strict
distinction between the body and the mind that the interceptive inference can still underlie
or bodily inference can still underlie very important phenomenology. Yeah. Well, uh, active
inference is definitely taking this, uh, philosophical position that, uh, cognition is
embodied. So, so yeah, it speaks exactly to what you said. Now I just want to add something
else. So we talked about the, the model of the self and then I would say there's actually
two models of the self. So there's the model of the body and, and this has a like
clear, um, evolutionary, like reason for being there. So for example, if something in my
body hurts, I need to figure out what to do about it for it not to hurt. So it's about
preserving your homeostasis and fulfilling preferences. Now there's this other level that
we've also been discussing, which is like the model of our own cognition. These are two
different things. Yeah. So that's, that's interesting. Yeah. Cool. There's lots, there's lots
to chew on there. I'm glad that I'm glad that this is cutting edge questions. These are really
things, uh, that I know a lot of people are thinking about. Yeah. I think, I think I'm
gonna, I'm going to have Lars on at some point. Yeah. He owes me that to be honest. Uh, he keeps
promising. Okay. Um, let's to the best, you know, this is this self modeling philosophical
conversation is more my bag, but let's take that deep, uh, plunge into the mathematics.
So as a, as a kind of, uh, forewarning or a precursory warning to the audience, I'm not
a mathematician. Um, so I, everything I'm doing when I'm reading the math, I'm trying
to ground in intuitive, philosophically robust concepts. So just, just for the audience and
yourself, please do excuse me if I cock up. Um, you won't, I mean, this is a, I think it's
very important to be able to talk about these things without the math. Hmm. It's
really, literally impossible to talk in math. So yes. Yeah. Yeah. Yeah. There's
this, uh, about this in a, in a natural language way. That's hopefully as simple
as possible, please. Well, yeah, I guess that's how it has to be taught is through
metaphor. Um, sort of, yeah, life through metaphor for George Lakoff kind of stuff.
So, um, let's start starting right at the basis in active inference. The world
is described as a random dynamical system defined or described by a stochastic
differential equation, like a Langevin equation. Yep. Why, why is it worthwhile
starting there? Why is that the starting point?
Right. Um, should I, should I explain briefly what a Langevin equation is?
Sure. It would be better you doing it than me. Okay. Uh, sure. Well, um, so here's the
thing. We want a fundamental theory of the brain and cognition, and we want it to be
grounded in math and physics. Um, now, if you look at the fundamental theories that
exist in physics, these would be, uh, general relativity, classical mechanics, statistical
mechanics, quantum physics, they are all based on stochastic differential equations or
Langevin equations at some level. Now, um, a stochastic differential equation is a very
fundamental and also basic object. And I'll explain why it's basic. It's basic in the sense
that when you're modeling a system, the, there are the known knowns and there are the known
unknowns. Yep. Stochastic differential equation, uh, brings the two together. So there may be
the known knowns about the system, which could be forces that act on different components of
the system. This is going to drive the system deterministically along a predetermined path.
And then there are known unknowns. These could be random fluctuations. For example, if you have
a drone, this could be random fluctuations due to wind. If you have a boat, random fluctuations
due to waves. And these are things that you possibly cannot accurately capture.
Hmm. Uh, so you're going to summarize them in a random term and then together. So you have this
deterministic, uh, term that gives you a predetermined trajectory and you have this random
term that sort of perturbs the trajectory in a random way. And when you bring the two together,
you get, uh, kind of like a random evolution of your system with some structure. And this is
basically what the Langevin equation or stochastic differential equation, um, encodes.
So this is to say, uh, it's a very general starting point and it's also the starting point that's
consistent with the rest of physics. And that's why we started there when we, uh, when we think
about the free energy principle. Excellent. Yeah. So there's a couple of things that I wanted to pick
up. Um, am I right to say that it's about the rate of change of, so, uh, of some X, let's say of some
states. Is that, is that, so that's, that's kind of what we're looking at here is the rate. Okay. And
then the second thing is, um, this notion of random noise, if you speak to certain philosophers,
for example, they might say, well, it depends on who you ask, I guess. Um, because for a universal,
you know, for, um, for some demon that knows the entire unfoldings of the world in a single
glance, there's no randomness. Sure. And yet to us, the modelers, well, it's just computationally
impossible to know every single element of the wind, let's say, or the waves. So what, what are
we talking about? Are we talking about randomness in, in and of itself in the actual system that's
being modeled or for the modeler? This is a great question. Uh, so here it's, it's agnostic
in the sense that, uh, you could imagine you can model the, the, the system that you're
trying to model, you know, in exquisite detail. So, you know, absolutely everything that's going
on in that case, you would have no randomness in that case. You still have a stochastic differential
equation with no stochastic term, which is an ordinary differential equation, but it's still
a stochastic differential equation. So the same math applied. Sure. You just have a random
term that's like zero. Yeah. Yeah. Yeah. Maybe there are things that you cannot model accurately
in a system. So you put them in some noise and the same math applies. So I would say it's noise
from the perspective of the modeler or not. And, and we don't actually care. The point is that
once you start from the assumption that you can model the system in that way, then, then things
carry through. And I make the presumption, I I'm presuming here that that noise is Gout is,
is considered Gaussian. It's considered a normal distribution generally. Yeah. So generally for,
uh, for, for you to be able to do mathematics, you take Gaussian noise. Uh, and is that an issue?
Well, it depends on who you ask, like, uh, most math and physics is based on Gaussian noise because
it represents the most, the world most accurately. You have something called the central limit theorem
that if you have a lot of like fluctuations, they kind of average away in a pile of sand.
Yeah. Yeah. So your noise turns out to be oftentimes very well, uh, approximated by, by Gaussian process.
Uh, but then there, there are instances where Gaussian noise is not accurate. So for example,
if you're talking to people who model the financial market, they do not use Gaussian noise. They use
noise with like jumps to model like a very sharp, um, yeah, jumps in the market. So Gaussian noise is
a good place to start. I think ultimately, as we work on the free engine principle, mathematically,
we want to be much more inclusive. Sure. Isn't that, I mean, just as a side, isn't that a remarkable
fact that like stochastic processes tend towards being Gaussian? It's just like, yeah, I think it's
awesome. It's amazing. Yeah. Yeah. I mean, you have all these remarkable properties of the Gaussian
distribution. Um, and it's also like the only one distribution that we actually know how to work
with very well. So it's a good thing. Yeah, it's cool. Um, and then I guess my other question about
the Langevin equation is you have sort of F of X as the deterministic force. So it's a function of
the states of X, the flow of X. Is that right? Yeah. Right. Yeah. But that doesn't have a kind of
comma T next to it. Um, why is that not, uh, a function of time as well? So it could be in,
in its most general form, F the flow would be also a function of time. Now, if you think of it,
let's say you have a state space and you have time, you can just incorporate time as another element,
as another state. So you consider your state space to be space time. And then you're back at F of X.
So I think that's why, that's why oftentimes, uh, people just go with F of X. Cool. Okay. Excellent.
Now let's jump to the next step. We have density dynamics and the Fokker Planck equation
and the path integral formulation, right? Very fundamental question. What is the difference
between those two? There's no difference. There's just different ways of articulating the same thing.
Okay. So the launch of my equation gives you like the rate of change as a function of the flow and a
random component, random noise. Yep. Uh, then when you sort of solve that, you, you sort of get, um,
this is the probability that the system takes, takes this trajectory. And this is a probability that gets
this, this other trajectory and so forth. Uh, this information is what's carried in the path integral
formulation. The path integral formulation gives you the negative log probability of a path.
Yep. And then the Fokker Planck equation, uh, which says at any point in time, this is the distribution
that encodes the system. So you might in a predefined place that, you know, I mean, the system might be,
uh, might start in a particular location, safe space, maybe because of the flow and the random
fluctuations, uh, at time T plus one system will be, it could be here. It could be there. It could
be there with a different probability. And then there's a probability distribution that summarizes
this. And this is what's encoding the Fokker Planck equation. They both come to the same conclusion.
You run the maths both ways and they'll converge the answer.
Yeah. It's, it's exactly the same thing. Yeah. It's different ways of looking at the same thing.
And depending on the question that you want to answer, uh, taking one formalism or another is
easier. But the point is that you can always go from one to the other and back. Nice. Okay. That's
cool. Um, okay. So let's jump into the Fokker Planck equation because I've been, I've been doing
some reading and I might be wrong, but I'm excited about it. It's excited me. So yeah. So for what I've
learned, it's a partial differential equation. Yeah. That rate that relates the rate of change of the
probability density function. So that's that P with the little dot and X comma T with respect to time,
drift and diffusion. Right. How's that sounding? Yeah. It's a great summary.
I can elaborate on that a little bit too. Oh, please. Yeah. Yeah. Well, people have no idea
what I'm talking about. So, okay. Well, you'll see that you got it perfectly right. Oh, thank God
for that. All right. Please explain. In a large amount equation, you have the rate of change of the
state. Yep. The flow and the random term. Yes. Um, now this is what physicists call it. But if you talk to a
mathematician, he would say you have the rate of change of states, the drift and the random term,
which they call the diffusion. Yes. Nice. Oh, which is characterized by the diffusion. So then
the Fokker Planck equation is very similar. You have, instead of having the rate of change of the
state, you have the rate of change of the probability density that describes the state. And then this is
given as a function of the drift. So the flow and the diffusion. So the random term. Okay, cool. So,
so, um, if we were going to cast this in like, in terms of, uh, just Cartesian coordinates, just X and Y
axes, would it be right to say that we could have X as like the position, let's say of, cause I read
there's a lot here in sort of Brownian particles. So let's say I had my particles in some, you know,
doing Brownian motion. The X would be the position of some particle and the Y would be the probability
of finding it there. When you talk about the Fokker Planck equation. Yes. Yeah. So the Fokker
Planck equation would, would be that. So at each point in time, the, the solution to the Fokker
Planck equation is the density of the system that evolves over time. And so you could plot this on the
XY axis, as you said, on the X axis, you would have the position of the system and the Y axis,
you would have the probability distribution. And so imagine things are simple. So if we imagine the
system is always Gaussian distributed according to Gaussian distribution, then if you saw the Fokker
Planck equation, you would have a Gaussian that evolves over time. So the mean would be shifting and
the variance would be shifting also potentially. Nice. Okay, cool. That's, that's really useful.
So how would we then integrate time? Is it, is it this like, is, is the, is that X and Y
axis considered like a snapshot in time? Yeah. Okay. Excellent. Yes. Okay, cool. That's actually
really useful because I think people might have got confused by that. Yeah. Now let's get to,
so this gives us density dynamics, but when we talk about active influence, we go about, we talk about
steady state densities. Right. So people, I think have this intuitive notion from whether they're coming from
dynamical systems theory or something else of a kind of basin, right? And attract to say, I mean,
I think of it kind of like a magnet just metaphorically. What is, how do we get from
density dynamics to a steady state density? So I should caveat this by saying that the latest
formulations of the free energy principle are not based on having a steady state density, but it's
kind of like a new thing. And I think for like intuition, it's best to think about the steady
state case. So, um, kind of like the intuition is that if you're modeling an agent, uh, it has a
preferred set of states that it likes to come back to, or they like does everything it can to like
be back at those states. You could think of like homeostatic states, for instance. Um, you can also
think about the fact that if you're modeling a system that does not dissipate in time. So let's say
like not a gas, for instance, uh, then the system should also have like a steady state, at least over a
short period of time, because it doesn't dissipate. So now we're going to try to model. Um, I mean,
the free energy principle is about modeling or analyzing those systems that have a steady state
that have these sort of preferred set of states. And so you ask if a system has a preferred set of
states and it's also decomposed into an organism, its boundary on what's not the organism. And this
could be like a living organism. It could be a stone. It could be anything that has a boundary. Uh,
then you basically look at what the internal states do. What do you, what does the boundary
states, uh, do, how do they evolve and what do the external states do? And then the conclusion is that
in general, the internal states look as if, I mean, they, they're, you can describe them as inferring the
external states based on the blanket states. Yeah. Yeah. Yeah. Yes. That's an interesting formulation. And one we could
come back to, which is this kind of, uh, yeah, tracking and some variational Bayesian inference
way. Um, we've spoken about this sort of looking like very important two words in the free energy
principle, um, a number of times in this, in this podcast, but I was curious more about in terms of
the Fokker-Planck equation. I don't know why I've turned into a master. Um, so, so this is the way that
I would think about it. And please feel free to, from what I've learned very, you know, in a very shallow
way is that we had this, um, this probability density function. So let's say, as you say,
we have our X, we have our position, Y, we have our probability distribution. Let's just say it's
Gaussian and per attractor state, uh, per organism, per attractor state, it's going to be different,
right? So mine might be really close to this side, yours, whatever. Um, what, what I kind of
understood as a steady state is over time. So let's say now we like put all of those images of the X
and Y axis on top of each other. Right. Right. And let's say they were somewhat translucent.
You wouldn't, if it's steady state, you expect that probability distribution to like stay on top
of itself. If it's not, you'll expect it to shift from side to side. Yeah. Or, or like become
completely flat or come. Okay. Or become completely flat. I guess reincarnation would be kind of
Gaussian flat Gaussian. What do you mean by reincarnation? Well, let's say I, it was,
it was a tongue in cheek comment, but like I have a Gaussian distribution of being in a certain state.
Right. Right. Right. Right. I die. I enter into the ether. I
dissolve into the heat bar. I flatten out and then I reappear as a frog and I enter into a new
Gaussian distribution. Yeah. That's a good way to put it. All right. Nice. Okay. So is that, is that,
how does that sound mathematically? What does that look like mathematically? Okay. So, um,
I think one, yeah, I think there's kind of like two scenarios. Uh, one, one is that there's no steady
state and the system dissipates. Um, and therefore your, your like snapshots of, uh, of the, like the
density at each point in time will converge to a flat distribution. Okay. Um, uh, and the other case
is that there is a steady state and, uh, and the snapshots converge onto like, uh, a clear well-defined
distribution. Yeah. Which would be like a Gaussian or something else. The point, the point of a steady
state is, uh, that if you start the system in that distribution, it will remain in that distribution.
Sure. Although, okay. Okay. Fine. And that's strict as in you will stay within that distribution or is
a permission, let's say in sentient creatures like our own for that distribution to change somewhat?
Um, so that's a good question. Uh, that there's kind of like two answers to that.
You being at a pre-specified distribution does not mean that you're not doing interesting things.
Yeah. So if we just think about a convection cell, uh, you basically like, or a tornado,
but you have like this circling around, uh, the probability distribution of where the gas is
or where the tornado is just stays constant. It's kind of like this range. Uh, but you still have
some interesting motion inside. Hmm. Um, right. So I might not always.
Right. Because I might just, you know, over time it will look like I'm converging on that state,
but I might not always be there. Yeah. You're probably, it's kind of like hard to wrap your
head. Yeah. Uh, if you think about like homeostasis, right. If you models, if you like,
let's say temperature, if you model temperature, you can imagine it will have a Gaussian distribution.
So with the peak at 37.9 degrees centigrade, but you are not going to spend your entire life at 37.9
degrees centigrade. Exactly. You're going to move around and we'll be according to this, uh,
Gaussian distribution. Right. Right. Okay, cool. Um, okay, good. Um, I, I want to jump to that notion
of the internal states tracking the external states, because I think people struggle with that. Um,
and I definitely did for a while. So, so the way that I read internal and external is tracking and
tracked. Um, that for me has been a really parsimonious and useful way. I think I got that
from Maxwell that that's a really nice way of thinking about it because the internal external
is such a philosophically troublesome distinction. So tracking and tracked. And then I think of
something like the equation for variational free energy. And I think about the KL divergence
between the recognition density and the true posterior and the maximization of model evidence.
Now in that, I guess I'm not, I don't have to necessarily make this strict distinction between
the internal states, my states and the worldly states, because all I'm doing here is I'm saying,
I have, you know, give, uh, given some observation. I have a probability distribution of some state,
whether they're mine or whether they're the world's because for me, I'm not going to,
for me, there, that distinction is not that strong because for me, I think you could pose
actually all that we are adjust is aggregation of worldly states, right? I am just, I am one
temperature distribution. I am one glucose distribution. That's not mine. That's the world's.
Yeah. Um, so, so that's the first thing, right? So we're doing this X, uh, X given Y,
this recognition density. And then you have the maximization of the model evidence,
which is the probability of Y given your model. But again, there's nothing like
internal that's going on there. It's just statistics. So do you think that it's somewhat
troublesome to cast, like put this little homunculus or this little ego in the internal
states and being like, he's doing sufficient statistics. Is that, do you see that as slightly
troublesome? Um, I think it's a nice picture for intuition, but it's certainly, uh, it can be
misleading for the reasons that you pointed out. Uh, internal and external states can be
interchanged. Um, and again, it's all about the blanket, uh, the external, the external states could
be part of your own body and the internal states could be part of your brain and the blanket somewhere
in between or the internal states could be like a school of fish and the external states could be
like a predator. Right. Um, yeah, I just never thought I, this is no attack on anyone. I don't
know who came up with it. Probably Carl. Sorry, Carl. I never thought that was a particularly good
formulation of the free energy principle. I always thought that, um, you know, things that seem to
persist look like they're maximizing model evidence for themselves is way better.
Um, but this is something to take out with him. Yeah. But, but it's kind of like two things we're
touching on here because tracking and the model evidence, uh, they're kind of like two things are
going on because when you think about it, the free energy is this KL divergence plus the surprise.
Uh, and we use this tracking to like minimize the free energy. Right. So what I mean by that is the
maximization of, uh, model evidence, or I think it's the minimization of log model evidence, but
it's the same thing. Um, but that entails that that in and of itself, why the way I'm trying to frame
it is that that entails some form of tracking. Yeah. Cause you've got that, you've got that
probability of why, but it's sort of given your model. Um, so yeah, I, I just think for people,
this is again, there's more of an education point about active inference, not about the
maths or anything. Um, I just sense that for me, it's always been more intuitive to think about
that. Although again, it's hardly intuitive, is it? I mean, it takes a bit of time to really wrap
your head around. Yeah. There's, there's another important point that came out in the research.
Uh, when we are talking about the fact that you can, I mean, a lot of people say that you can always
interchange, uh, internal and external states and that one is tracking and the other is tracked.
But it turns out that this is not always the case because for the internal states to track the
external states, they need to have enough like representational capacity to make inferences about
all the internal states that are out there. Hmm. They don't have enough representational
capacity. Maybe they might just infer some or infer linear combinations of external states.
In any case, they're just going to have like part of the picture.
Okay. That's now that's, that's interesting.
Which means that for external states to track internal states, they also need to have like enough
representational capacity and a sufficiently high quality coupling with their own sensory states
with the boundary to make inferences about the internal states. So you always have this
synchronization and this inference going on, but you can't necessarily say that the internal states will
be tracking all the internal states all the time or, or vice versa. Right.
It depends on the nitty gritty of the coupling and how much representational capacity these states have.
Well, again, that's why for me, this maximization of model evidence just seems a little bit simpler
because whether you're a drop of oil or a rock or a human, we can sort of, you know, not arbitrarily,
but we can generously give out models for each one of them and say that they look like they're maximizing
model evidence. Like how long something lasts is a different question. So my, you know, the, the,
the salt that I put in water maximizes model evidence for itself being salt for a very short
amount of time. So, you know, you could cast that as its surprise or being very high, but at least for
while it's salt, it's maximizing model evidence. And then I guess it's maximizing model evidence within
a Markov blanket. That's the water salt solution. Anyway, it's an interesting foray, but it's no,
that's a good point. Yeah. So we're now going to jump to the next part of this. So we will want to
follow along by the way. A lot of this is from the free energy principle made simple, but not too
simple. Or I think that's the name of the title. That's right. Yeah. That's a great, this is a, well,
I wouldn't say it's necessarily a perfect starting point for someone who hasn't done maths or physics.
Don't we, I would, I would recommend, um, the textbook. So Paul Petsulo and Friston recommendations
actually. Yeah. There's also a Maxwell's paper on a Bayesian mechanics. Yeah. That's very good.
High and off beliefs. And this is really cutting edge in describing the math, but it's also meant to
be a much more approachable. Yes. Okay. That's yep. So Maxwell's 2023 paper, which is on the, uh,
preprint server, um, the textbook. And I've also been reading and I will come to this. I've been
reading Sanjeev Namjyoshi's draft of his, uh, book. It's, it's, it's being sort of distributed.
There's a group in the active inference Institute where you can read it. And that's been really
useful. So there's, there's stuff out there and, and this podcast. And so, yeah, but if you want to
follow along with this, this is that, uh, base, uh, the free energy principle made simpler,
but not too simple. So the next thing that you come to there is solenoidal and gradient flows.
Right. So maybe we can talk about, maybe we should start by just explaining what those two
flows mean and how they're distinct from one another. Okay. Um, so this is a decomposition
of the flow in the Langevin equation. Yep. And so, uh, to put it simple, to put it very simply, the,
um, um, um, well, just to give an analogy in vector calculus, you have this fundamental theorem
about, uh, that's known as the Hemhold's decomposition. Yep. It says that if you have
a vector field, so think about a bunch of vectors sitting on a table, uh, you can decompose them onto,
uh, one that's kind of like circling around an origin to put it simply, and one that's going inside or
outside. Okay. So mathematically, this is what we're doing here to decompose, uh, the flow
in the Langevin equation. Now, what this really means in practice is that you have, uh, the solenoidal
flow, which is the interesting part of the flow and the, how did you call the other one? Uh, what do
I call it? Gradient flow. Right. And there's the gradient flow. That's the in an uninteresting one.
If you are, um, if you are a living system, you have by definition solenoidal flow. And why is that?
Solenoidal flow is, um, related to time irreversibility and entropy production.
Um, okay. If, if you don't have solenoidal flow, it means that the system, if you, if you played in time,
uh, as time goes forward and then you, you reverse the movie, the system will look the same.
So it's essentially a dead system. It could be like a stone for instance.
Yeah.
On the other hand, if I, you know, you played a podcast for 20 seconds as time goes forward,
and then you played backward for 20 seconds, it will look super weird and very different.
Right.
This is the proof that there is time irreversibility. Time has a lot. There's a direction of time.
Um, and this is, yeah, characteristic of the fact that we are alive.
Interesting. Okay. So let me just get some, some more, you know, some simple things outlined here.
So you said the Helmholtz scene, the Helmholtz decomposition is of the flow. Is that if I'm
looking at the, uh, Langevin equation, that's not the, that doesn't include the random noise.
That's just F of X F.
Yeah. It's a decomposition of F of X, but also done in relation to the noise term.
So is the noise term. So would that, what I'm asking here is when I'm splitting it into solenoidal
and gradient, is that splitting just the F of X or the F of X and the, okay, cool. So,
okay. So there's a, there's a dissipative aspect in all of this. So the analogy that I thought of
was, um, if you create a whirlpool, let's say in your sink or in a cup of coffee, you get this
solenoidal flow, right? It's conservative, but due to, I presume just friction, you get this
dissipative, uh, uh, this, this aspect where the swirling motion fades away.
Yep. Totally. Right.
Is that, is that the gradient flow then sort of infiltrating a purely solenoidal flow?
Yeah. Yeah. That's absolutely right. Another example is if you put milk in your coffee,
there will be like the standard mixing, uh, of things, which will be the gradient flow.
And if you start stirring the cup of coffee with a spoon, then you're adding solenoidal flow.
And is it where they meet that things become like, we can say things are dissipative because for me,
philosophically, dissipative means that you had order and now you've got less order, but if you,
if everything is, if there's no solenoidal flows, then arguably there's no, is there,
is there any order to begin with?
It's hard to say, like I have, yeah, I have trouble relating these notions to order,
but I think there's like, there's like an indirect way of relating them, which is,
solenoidal flow is always something that circles around something.
Yeah.
You think about the circadian rhythm, us waking up and going to sleep, all these are cycles.
They are examples of solenoidal flow.
Solenoidal flow has, um, produces entropy.
So I think that's in some sense related to order and disorder.
If you want to maintain as a living being, if you want to maintain solenoidal flow,
or just as a physical system, you need to be using energy and dissipating entropy.
So that's kind of the idea.
And that's just a consequence of the mathematics and the physics.
Yeah.
Yeah.
Basically.
Okay.
The fact that we have solenoidal flow within us, like rhythms and cycles means that we need
to eat and, uh, and secrete.
Right.
Right.
Okay.
Okay.
Because in some sense, the, that's a form of order, which is negentropic.
Yeah.
Yeah.
Okay.
Okay.
Now, uh, okay.
So what is this?
Um, the other thing that comes up is the term divergence.
Right.
So people say that solenoidal flows are zero divergent.
Yeah.
What does that, what does that mean?
Right.
Uh, it's kind of, that's just kind of a mathematical term.
Uh, now, uh, uh, because if you take the divergence of the vector field, it's zero.
Yeah.
Let's get into that.
I'll give you a picture.
If we, if we are at our steady state distribution, so let's say, um, our system is two dimensional
and it's distributed according to Gaussian that's sitting on my table here.
Um, the solenoidal flow is a motion around the contours of this Gaussian distribution.
Hmm.
And the gradient flow is motion towards the peak or away from the peak.
Interesting.
So we have our overall motion, which is given by the flow in the Langevin equation, which
could be in any direction.
And, um, the fact is you can decompose it on motion towards the peak.
So towards states that are more expected or preferred and motion away from the peak,
strictly away from the peak.
These two are divergence free flow.
You can also, the other part of the decomposition is motion that just goes around the contours
of the distribution, which means that it does not approach the peak or go away from the peak.
Is it, if I'm thinking about it diagrammatically, does it matter where that circulation is going?
Does it?
So, so it could be at the base of the Gaussian, which means it would in some, like, again,
just diagrammatically means it would be wider or it could be at the top of the Gaussian,
which means it's occupying these high probability states.
Does it matter?
Um, I think it matters as a signature of the system, probably.
I mean, this idea of solenodal flow is one of the most important like signatures of, uh, of being alive
and what your states are doing, what kind of cycles you have in your system or in yourself.
Uh, so it's a very important, um, descriptor of what's going on.
Okay, cool.
And the other thing was, um, and again, you can tell me when I'm, when I make a terrible
faux pas is that solenoidal flows break what's called detailed balance.
Yeah.
And what I've learned about this is that detailed balance is when the rate of transitions
of any two states is equal in both directions.
That's right.
So like, you can think of it in terms of, I, I, I thought of it in terms of like a crowd
going in and out of a, like a stadium or in and out of a room.
That rate is going to be the same in and out.
Um, and that I believe, and again, you can tell me if I'm wrong, there's something to
do with Boltzmann, Boltzmann's H theorem, which says that, that, that detailed balance implies
zero net entropy.
Right.
Um, so, so two parts, um, it's absolutely right.
That detailed balance is means that there is flow, um, in and out, say, uh, of a stadium
at the same rate means that if you play the flow in and out of the stadium, forward or
backward, it will look the same.
Yeah.
There's no production of entropy and there's no solenoidal flow and the system is time
reversible.
Yeah.
Yeah.
Yeah.
Um, okay.
So that kind of explains why it's sort of net, net zero entropy, right?
If entropy is foundational to the passage of time.
Yeah.
Yeah.
Yeah.
Basically.
Yeah.
As soon as you have solenoid flow, time, uh, starts taking place.
You start having a narrow time.
Okay.
So if, if we took, so yeah.
Okay.
So this is interesting.
So let's, let's kind of, uh, this is an odd exercise, but we can kind of build a human
from scratch, right?
So let's try.
Right.
Um, so the, the gradient flows are going up or down the probability distribution.
So we take a human as just a Gaussian distribution.
It's kind of going up or down, but let's say it without any solenoidal flows, we have this
detailed balance.
Yeah.
So it's going up and down.
Now, does that not mean that because it's doing it, uh, equally, you're going to end up
at that peak.
Uh, no, because you have equal flow towards the peak or away from the peak.
So how does that average out?
What does that look like in terms of where I, where the crowd might be, or they're just
everywhere.
Are we talking about the human or, or the crowd going in and out?
Let's say the crowd.
Let's say, cause we've got detailed balance, so let's say the crowd.
Okay.
If I'm going to say, okay, uh, I have a random friend X and there, they belong to this crowd.
That's, uh, behaving in terms of detailed balance and I want to find them.
Is there a probability distribution that could help me to find your friend?
Yes.
Who's going in and, you know, is part of that tide.
Um, yeah, I'm not sure about that.
Okay.
Well, what I will just say is that like, if you have one exit in your stadium with people
going in and out at the same rate, and they're kind of like mixing with each other.
Yeah.
Who is who you have detailed balance.
You just start having one entrance and one exit.
Then you do not have detailed balance because you have people coming in one way and going
out the other way.
So you start having a cycle.
Ah, okay.
Now that's, that's okay.
That's really useful.
Nice.
So let's now talk about the human in a sense, because the human is a combination of, we're
kind of what Carl would say we're in this Goldilocks zone.
So we're not orbital planets with just solenoidal flows.
So we, uh, so we're not just this circular, uh, circular thing, but we're also not just
not neg entropy, non entropy, one exit.
Right.
So we have both.
So we have both.
Yeah.
Um, yeah, it's a very interesting sweet spot.
If you just consider systems that have just the gradient flow.
Yeah.
They are dead.
And you know, they might look like gases and stones and stuff like that.
Yeah.
Yeah.
Yeah.
Yeah.
You should just look at things that has solenoidal flow.
Well, I would say they're probably dead as well.
It could be like, uh, like planets orbiting around the sun.
Like they're going one direction and not the other.
And we are kind of like in between where we have both going on.
So I guess this is where my question was going, which is, okay, let's say, let's take a rock.
The rock has, uh, just gradient flows.
So it's dead, but it seems to me that the rock is doing a pretty good job at self-evidencing.
Right.
Um, so does in some ways the solenoidal flow take us away from it?
Yes.
It gives us animacy and whatever, but does it take us away from like just being able
to rigidly self evidence rigidly maximize model evidence?
No, um, no, no, it does not.
And so the beauty of the mathematics here is that we're, we're taking the most general
sort of equation that can describe the world.
So the Fahrenheit principle applies if there is solenoidal flow and no dissipative flow.
There is dissipative flow and no solenoidal flow.
And it applies if there's both.
Yes.
But what I mean by this is it's, it's probably worth mentioning that I'm kind of thinking
about this over time.
So my, uh, my plastic bottle is going to outlive me.
And, and because things over time have to, you know, have to abide by the free energy
principle, they have to look as if they're maximizing model evidence or doing some variational
Bayesian inference.
You could say if we had some normative sense, which again, I'm quite skeptical, but you could
say like the water bottle is doing better self evidence over a period of time.
Uh, yeah.
Over, over a long period of time, certainly.
And that's because, well, that seems to me to map on quite nicely to this solenoidal
gradient flow distinction, because like if rocks and, uh,
uh, I don't know, particles are not solenoidal, where am I?
I feel like I feel like there's something I'm missing.
Yeah, that's, uh, I mean, I think it's very interesting.
You point out that things that do not have solenoidal flow sort of endure over a longer
period of time.
Um, I never thought about that.
Okay.
Okay.
Fine, fine, fine, fine.
I won't push it then.
Okay.
That's really, okay.
But yeah, it's interesting.
I need to think about it.
Yeah, yeah, yeah.
It is.
Well, God, don't know what kind of worms I've just opened up.
I really don't want to get shouted out on the comments.
Um, okay.
So something I do understand a little bit better is just, uh, KL divergence.
Uh, that's more up my street, because I feel like it's a little bit more cognitive
sciency.
So there's callback Liebler divergence.
And as I said, this, um, this can be cast as a distinction.
If we're doing Bayesian inference between a true posterior and what's called the recognition
density, um, which is like your version of your, of, of a posterior.
Now there was something you brought up in that, uh, or Carl brought up or whoever wrote
it in that, uh, simplifying the free energy principle paper, which is called a Fischer
information metric tensor, which measures changes in the KL divergence.
It might be.
So people, I think, uh, view the variational free energy as a kind of snapshot in time.
So given like this perceptual inference, how big is my KL divergence, given my action,
how much surprise is there, but this seems to me to integrate change over time.
So what is the Fischer information metric tensor and how can we integrate that into a
more accurate or more faithful model of the world or model of ourselves?
Right.
Uh, so once, once you accept that you have a recognition density about the world and that
this is minimizing free energy, you can ask how much does your recognition density or your
beliefs about the world have to change in order to minimize free energy.
And so if you, if you want to quantify how much your beliefs move in a certain amount of time,
you need what's called, uh, an information metric, information metric in the sense that it, uh,
quantifies how much information changes over time, the information.
And the information, the Fischer information metric is what allows you to do that.
Nice.
Okay, cool.
Uh, are you, are you familiar with this notion that's been coming out of maybe like the more
philosophical or cognitive science side of active inference?
So people like Mark Miller and Julian Kiverstein of prediction error dynamics.
Um, I mean, uh, intuitively I can see what that would mean, but yeah, the idea is that we have
like a higher order prior that we will track how good we're doing at minimizing free energy over
time. And this constitutes, and this in terms of psychology maps onto something like positive
or negative affect, because you know, we've got a great, we're doing well, right? We're doing well.
We're finding these slopes of, um, variation, variational free energy that we can descend down.
And it's kind of an explanation for why we don't seek out dark rooms.
So there's one other solution to the dark room problem. It sounds quite similar to what you're saying,
which is except what you're saying seems strictly, I don't know in terms of the KL divergence tied
to perceptual inference, but we can say that a system is becoming better at perceptual inference
because over time, those KL divergences are going down. Is that reasonable to say?
Well, yes and no, because if, if the data stream sort of like stayed constant or if you stop the
data stream, then your KL divergence would go down because you would kind of like get to the free
energy minimum, but you guys have new data coming in. So you have two forces at play. You have you,
uh, who are like predicting the data and minimizing free energy, but then you have data that's
potentially increasing the free energy. Right. But it's the world, but going back to our
Longerian equation, the world is, the world is modelable in some sense. It's predictable in certain
senses. So I, I may be wrong, but it seems to me that the more data you get over time,
the better your model is going to be, right. It's, it's just going to become more powered,
right. To use a statistical term, what, what's the kind of, so that seems to me that like your
information metric should be pointing that, you know, your KL divergences are going down,
even if you're getting more data, because that data is just, it's got a similar distribution
to the data that came before. Right. Okay. So, so I see, I see where, where the confusion lies.
Um, I agree with you that as we get more data, our model gets better and our free energy, um,
in general tends to be lower. Uh, so that's one truth. The other truth is that even though,
even, even though, I mean, that's a fact you're the data that's coming in keeps changing.
Uh, which means that the posterior density, the true, the true keeps changing. So your
recognition dynamics always have to keep tracking this posterior distribution. Right, right, right.
Now the Fisher information looks at how much these, um, recognition dynamics have to travel
in a certain period of time to match the two posterior. Is there an equation that governs that?
Because obviously I've become obsessed with equations. Um, is there, how much your beliefs
change? No, not just how much your beliefs change, but kind of how the data changes.
How the data changes. That's the Langevin equation.
That's the Langevin equation, but it's, so that's the function of the X being the worldly states.
Got it. Got it. Okay. Okay. Yes. Okay. I see where my confusion was. I guess I was viewing the world as
kind of static. I mean, you can even like take away the, the, the random noise, right? If your f of
x is changing your agent, who's modeling that is going to have to keep updating. Yeah.
Keep changing the model. Nice. Nice. Okay, cool. That's, that's, that's very useful. Okay. Excellent. Um,
okay. Have I got any more maths? Um, I got a bit more maths, I guess.
Something I wanted to ask about is that this is a broader question.
So in, so in active inference, we have discrete state spaces and continuous state spaces.
So, and it's kind of about the, how we carve up time in some sense. So continuous state spaces is
where it doesn't really make sense to put time into separate chunks or sequential chunks. So I'm,
you know, if you're thinking about attention or perception or perception, let's say, um,
um, I know perception includes some action, but let's just take perception. It's better to kind
of model that in terms of a continuous state space. If you're thinking about decision-making,
you would think about that in terms of discrete state spaces, because an action policy is a discrete
thing. In discrete state space, we've seen this kind of proliferation of so-called POMDP schema.
Um, so these are these partially observable Markov decision blanket, but processes are close.
I always think about Markov blankets. Um, is there an equivalent of that in continuous state spaces?
Uh, so you can definitely have continuous, uh, partially observed Markov decision processes.
Um, so, so let, let me take a step back. There's two things at play. There's, uh, whether the state
space is discrete or continuous and whether time is discrete or continuous. Okay. Now in active
inference implementation so far, you basically have either continuous space, continuous time
or discrete space, discrete time or both, but people haven't really been playing with
continuous space, discrete time, uh, continuous time, discrete space.
But there's no reason why in principle you would not be able to do that.
Okay. That's cool. Well, uh, this is again, my very much layman knowledge of physics.
Um, do we not take time and space to be somewhat interwoven?
Um, yeah, this is a bit beyond me.
Sure. I guess, I guess, I guess a way around this is to say that we're modeling,
right? So in our modeling, we can take time as continuous and space is discrete and vice versa.
Yeah. This is, this is only about modeling about how you represent the world.
Yeah. There is a deep, uh, a deeper question, which is how do we ultimately like represent the
world in physics. Right.
And it's true that some theories, you know, consider time and space as jointly, but for us humans,
uh, yeah, we don't need to get into that for, uh, active or some funding.
Okay. So we, we've, we've kind of clarified that. So when, so where would the POMDP schemes
be able to be used if it's not just discrete time, discrete space, can they applicable to
continuous space, continuous time?
So, so where would you actually use them?
So I've seen, for example, just to give some background, I've kind of seen POMDP schemes used
in mainly just aspect like context of decision-making. So, um, for example, yeah, going back to the flow
paper flow involves selecting action policies, meditation involves mental action, um, attention,
right? Like, um, intentional, uh, attention and, uh, endogenous attention is a decision.
These are, can all be modeled quite nicely, but to my eyes, everyone who talks about perception
will use something like a predictive coding scheme. Yeah. Um, is that, is that, I'm just curious about
what is, if there is an alternative to that in the literature?
In the literature? No. Okay.
I was talking to Ryan Smith recently, and he was interested in continuous state POMDP with discrete
time. And so the idea here is that you can model a continuous representation of the world
and the sequential decision-making at the same time of that continuous representation.
So yeah, it opens up a lot of possibilities.
Okay, cool. I'm speaking to Ryan fairly soon-ish, so that, that will be useful. That'll be cool.
That's a good thing to bring up. Yeah, that's really interesting. I don't know if this brings
to mind, there's this incredible paper by a guy called Juan Diego Bogota. And I think his name
is Zacharia Jebara. And it's sort of synthesizing subjective time and objective time through a
Bayesian model. And I'm going to tell you, I stared at that graph for about six hours.
Um, and it still didn't make sense. So that's a, you know, if anyone wants to read that and try
and think about how maybe not space and time, but like two different types of time can be coalesced,
that's a good place to start. So I'll give you another, uh, theory of mine. Uh, I did this
wonderful workshop on computational neurophenomenology, uh, last year. And I had a wonderful conversation
with, uh, annual Seth and we've kind of converged onto the hypothesis that subjective time is, uh,
the amount of distance that your beliefs travel over some period of objective time.
Okay, cool.
So here's the thing, like when you, when your beliefs move a lot,
you can quantify that with the Fisher information metric, you would have a perception that a lot of
time has elapsed. Okay. So that you've now got me on this new pet hobby of mine because, uh,
I'm sure you haven't seen it, but literally yesterday I published a preprint
called distrusting the policy, how inference over action shapes our perception of time.
That's cool. So please, uh, I would love some feedback. Basically I wrote it. So this, this
starts off as being trying to explain temporality and flow states. And I thought, no, actually this
works. I think this works more broadly. So I'm not going to spoil it. Well, I will spoil it, but
basically my idea, so I'm building off what Jakob Howie called distrusting the present. So this is very
similar to what you're saying. So Jakob Howie has this idea that this flow of time is contingent
on, um, the rate at which we update our perceptual hypotheses about the world.
So if I'm in a volatile environment, I, the, uh, the state estimation I have at time T is not going
to work for T plus one. And so what happens is the moment I find T, I immediately start discounting it,
which you can consider this or precision waiting. And immediately I have to update that. And he calls
that. Now I, I don't think that that totally maps onto the phenomenology because I'm really
curious about what you think about this. And again, I will try and do it quickly because I don't want
to bore you, but we have, um, so I say, so he was kind of, it seemed to me that he was rooting,
uh, flow of time in predictability. But the problem is if you're going to cast it only in terms of
perception or only in terms of the A matrix, then you have, you've got a slight problem
because there are two different types of predictability. You've got one, which I call
static predictability, which is basically when you're staring at a wall. Now, when you're staring
at a wall, your perceptual hypothesis is going to be pretty good. Let's say over a prolonged period
of time. And so Jakob's theory says time will slow down and it does, right? You stare at a wall for
an hour. It's going to feel like a day. Yeah. But then the other problem is something called dynamic,
what I call dynamic predictability. So let's say you play the video game level once. It's very
exciting. Time seems to flow quickly. Now let's say I make you play that same video game a hundred times.
Now, perceptually the world is changing. You got to do all that state, like that state estimation,
that combination of the D matrix and the A matrix needs to keep happening at the same rate.
And I want to flesh this out experimentally, but I have a hunch that time is going to move way slower.
So what I've, what I've now said is it's about the frequency with which you update your action
policies. And the reason why I said that is because with each action policy updating,
you also have to update your beliefs in the C matrix. And the C matrix is your expected,
your preferred sensory observations. And that I think accounts for the perceptual aspect
of the flow of time. That's really, that's a really interesting hypothesis. Yeah.
Yeah. So I, so it's Friday the 19th of January on Tuesday, people are not going to be able to,
you know, this is not, people can't go back in time, but I'm doing a talk about it at the,
at the Institute on Tuesday. Um, so anyway, that that's, so I think we're converging on similar
points here. I think it's kind of, where is the source of your information change?
So for me, it's the C matrix really, because it can't be the B matrix. Cause in dynamic,
predictable, so B matrix of state transitions in dynamically predictable situations, like
playing the same video game, a hundred times, I'm going to have the same number of B matrices
compared to when I played it the first time. So I didn't think it's the B matrix,
but cause I, you have to keep doing this expected free energy calculation for each action policy.
And the C matrix feeds into that. My hunch is that it might be rooted in that.
Right. You could also think about the fact that if you replay the video game, uh, like many times,
your beliefs still have to move to adapt to the current, uh, situation in the video game,
but there will be less and less novelty because you adapted your like big world model to like predict
the video game. Hmm. So how that translates to in practice is that you have low level beliefs
that, that move a lot to adapt to a situation, but high level beliefs that don't move.
Okay. Nice.
Yeah. Again, I think I, I'm trying to get this to be done experimentally to see
if it's, if, if this theory maps onto anything, it would be really nice to test.
But that's a, that's, that's a very interesting. Yeah. I'm always in favor of all of the empirical
experiments. Uh, I feel as a community where we're lacking that. Right. And I think it shouldn't
be that hard to do. I think you just have to give people statically predictable context,
dynamically predictable context and flow context. Yeah. Um, as soon as you can map a pump DP onto the
data that's coming in, it'd be very easy to model everything that's going on.
Well, I may reach out to you or Lars on that one. Um, okay, cool. So that, that was a fun little
foray. Um, okay, actually let's, let's, um, we, we spoke, we very briefly touched upon attention.
Um, and I have a question about attention, if I may, I've, I've been, I've been, I've been grappling
with attention for quite a long time. Um, so attention is kind of construed as the optimization
of the likelihood distribution and active inference. So the A matrix in these POMDP schema.
Now there are these really lovely papers by his name is Mietzer. So, uh, Mietzer et al 2016, 2018,
and 2019, I think. And he shows, he gives this example of like searching for a red pen.
And he says that when you're searching for a red pen, what you do is you down weight the
sensory precision of stimuli that are relevant, irrelevant to that context. And so basically,
you end up doing salient behavior, which is, we can view as epistemically salient because it yields
information gain, but the information that you want, um, now given the, um, yeah, so that makes
sense because task relevant stimuli are down weighted. So only task relevant stimuli involved
in this maximal information gain. Okay, fine. But pragmatically, so, so that epistemically,
that information gain is like, I didn't know where the red pen was. Now I know where the red pen is.
Yeah. But in terms of pragmatic, in terms of pragmatics, we know that action policies in, uh,
active inference are driven by a kind of almost a false belief, a false preference. So I might say,
I have a red pen and I'm going to go out there to confirm that belief because without it, there's
lots of prediction error or variational free energy. This seems to be a kind of issue because
on one hand, I'm gaining information about the world. So I kind of knew that I didn't have a pen
and now I know where the pen is. And so my, I have this KL divergence, but then I also have this oddly
pragmatic belief that I do have a red pen because without that red, without that belief, I wouldn't
be motivated to go and get the red pen. So I'm, I'm trying to grapple with how to resolve that clash.
And I'm curious if you can help. Yeah, not sure. Uh, but what I'm thinking is, uh, I always think
about attention as control of the A matrix, like in Lars, Lars's paper on meditation, uh, you make some,
some, uh, sensory maladies more precise and others less precise and feeds into, into your computations.
Yes. So yeah, see, I've always, I've always found it quite useful to distinguish precision
from precision weighting. So precision, I just view as the kind of inverse entropy
of a distribution, but precision weighting is actually what I do, right? I weight precision
and that's not all going to be down. Weirdly, precision weighting is not all down to precision.
Well, I think it often is, it often is in terms of
they talk about how attention is basically one definition of it is the capacity to kind of sift
through the data that you currently have for the most, uh, informationally reliable or precise,
um, data, right? So the one that's going to say, okay, given this, I know exactly what caused it,
but clearly like, for example, in my searching of the red pen, there's going to be loads of things in
the environment that are, that have a very, uh, you know, very strict, uh, relationship mapping
between the observation and the state. So while I'm scanning, I might see something and go, oh,
obviously that's my phone, right? It's not ambiguous whether it's my phone or not, but because I'm,
but the whole point of this Mirza paper is I don't, I don't care. So I'm down weighting that stimuli,
but that has nothing to do with how well it maps onto some state. Yeah. Yeah.
Does that make sense?
It isn't. Okay. So if I understand correctly, this is, um, because you, you want to conform to your prior
belief and therefore if your prior belief is very precise, uh, you will discount whatever sensory
information comes in. Right. So my prior belief is, so my prior belief is something like I have a red
pen and, and, and so that's very precise. And yeah, so that leads you to discount things that
are, because you're, you're, you're, you're, yeah, you're only going to pay attention to things that
are task relevant. Yeah. So, but that's, but that's kind of the point is that's kind of a belief
saying I have a pen, but then there still is this epistemic game, which is at the heart of salience,
which is, I didn't have a pen or I didn't know where the pen was. And now I do know where the pen
is. And so you're starting with, I don't know where the pen is, but for your act to trigger action,
you have the belief. I know where the pen is to, to trigger action, to look for the pen.
I'm getting, I'm getting a little bit lost. Okay. So, so, um, preferences underlie action,
right? So I can see matrix. So I have the preference about the sensory information that I expect to receive.
Mm hmm. So let's say, and preferences can be cast as just beliefs. So I believe, so for example,
uh, in some very, it's, it's, it's a very sub-personal way. I believe that, uh, I'm going
to be at 37.9 degrees. And so I act, so I remain around that. Now I want to find my red pen. We can
cast that volition as a preference, as a belief. Yeah. I know where my red pen is. And the reason why you
would do that is because, well, the world is not giving you that information. And so you've got
prediction error and you want to minimize that prediction error. So that triggers action to
confirm the belief that you have a red pen. Okay. That's on one side, but on the other side,
what captures attention is that, which is most salient and what salience can be cast as is
epistemic gain information gain. And that for me in this red pen example looks like, well,
I didn't know where the red pen was. And now I do know where the red pen is.
But what you've got on one hand is on the epistemic side, you've start with the belief.
I don't know where the pen is because if I didn't know where the pen is, finding the pen would not
be very useful. Right? So that's, um, inhibition of return. That kind of explains inhibition of return.
That's on the epistemic side on the pragmatic side. I have, I have a pen.
So I'm just asking how those two things come together.
Uh, I'm, I'm so sorry, Darius. I think my brain is fried.
That's all right. That's all right. I'm sure someone in the comments will have an answer.
Yeah. I kind of feel that Carl will just have an answer. Um,
I mean, I'm sure, I'm sure.
He hasn't answered everything. Oh, maybe not. Maybe not. Okay.
Give me a research paper.
Yeah, maybe. I know that one's going to take me down some wormholes.
Um, so, okay. Last thing I guess I want to talk about is you went on the machine learning street
talk podcast and you spoke a lot about core knowledge, which I thought was really interesting.
I really didn't expect that to come up. Um, but I know it's one of your interests.
Yeah. What are the kind of priors that need that are inbuilt that facilitate intelligence,
sentient behavior? So I guess what are your current thoughts on how, so it's interesting
because I actually come from a linguistics background, as I told you before we started,
and at least, you know, at Oxford where I was there, they love Chomsky. So Chomsky has this notion
of universal grammar. There's something built into the genetic code that facilitates language learning.
So I'm very familiar with these arguments, poverty of stimulus and these arguments in terms of
nativism, uh, nativism. What are your thoughts on how fleshed out those priors are, um, in terms of being innate?
Well, I think there's definitely innate priors when we're born. This is all studied. Um, I think one of
the best people is Elizabeth Chepelke. Yep. Yep. Her whole career, uh, describing and characterizing those
priors. Uh, these seem to be incredibly precise, uh, much like we cannot change them. And they seem
to be super general in the sense that they would apply to any kind of a naturalistic or physical
environment that you could ever be born in. So these are things like object permanence. Two objects
cannot, uh, interact at a distance, but two people can interact at a distance. So, so two agents can,
uh, I think there's a core knowledge system for, um, numbers, learning numbers. There might also be a
core knowledge system for language. Uh, they might also be one for vision. Uh, so there's six of them.
And she wrote a book about it, uh, which is what babies know, uh, that I absolutely recommend with,
with all of that research. So it's a big research program to actually being able to understand those
and reveal those core knowledge systems. It's another one to be able to reverse engineer them.
Uh, interesting. Yeah. I think one of the issues I guess is at least, um, in the history of
linguistics, one of the great problems was how rich are these priors. So this is what later got known
as Darwin's problem. Uh, because Chomsky has this idea that, well, in the eighties, he had this
principles and parameters idea, which is that what UG is, is a bunch of parameters that can be switched on,
or, uh, you know, given sensory data, those parameters get switched on or off. And then
the problem is you had this massive proliferation of parameters. So there were 500 parameters and
and finished people, you know, there was one only for finish, but obviously we all had it,
but only unless you were exposed to finish, did you ever turn it on? And it just became completely
inexplicable given evolution, because how on earth did one mutation or a couple of mutations trigger that?
So he now thinks that there's something a lot more pared down, something like recursion.
And I think why that's interesting is because recursion is not,
it's not what we call propositional knowledge. It's not knowledge that something. So Chomsky might
say, oh, okay. You know, in the sixties might say, okay, I know that the baby knows that after this
comes this or something, but now this is just actually a mechanism. So what we might call procedural
knowledge, when you think about, uh, core knowledge for maths or core knowledge for vision, or I guess
vision, it really won't be propositional or core knowledge for language. Are you construing these
as propositional or procedural? Um, so, well, these would be priors in the giant model,
which could be expressed, um, in a number of ways. It could be, uh, sort of like, um, like a grammar
of, uh, like core components that you already have in your giant model. Well, well, let me put it in
a different way. Like when we're born, we have a very simple giant model and gradually we learn
about the world around us and we make it more complex. Now what we have at the beginning
is this core knowledge. It's kind of like a starting point giant model that allows you to
structure learning, learn about everything else. Now the structure of that is, uh, very debated
among computational people. So for example, uh, Josh Tenenbaum is pursuing this idea of, uh,
probabilistic programs, which are, uh, based on symbolic grammars and, uh, probabilistic rules
between, uh, different entities. Uh, then there's other people who might, uh, model these things with
neural networks. Right. So in those cases, you don't have symbols. You don't have that grammar going on.
Yeah. Yeah. It's very much an open area of research. Cool. Yeah. I mean,
I guess all these neural networks still have some priors, um, in, in a lot of cases. Yeah.
Awesome. Yeah. Yeah. I used to love these debates between Chomsky and people like, um,
Jeffrey Ellman, sort of mid 1990s, proper hardcore stuff. Cool. And yeah, I, I, it's interesting
you mentioned vision because I always thought that for me, the one glaring one where there needs
to be prized is vision. Vision perception is well, visual perception is such an amazing capacity.
Yeah. To be able to do right. Yeah. There's a startup called a common sense machines
has pushed the core knowledge of vision really to a state of the art level where they have a world
model for vision and then they do inference on that world model. And they, they're able to reconstruct
3d scenes from 2d images. Wow. Uh, in, in a pretty amazing way. So yeah.
Have you seen these things that, uh, the, this is slightly different, but you have these neural nets
that are trained on like, uh, brain imaging data and they can take an MRI of the brain and say that
this person was thinking about a dolphin or something. They can have an image of what that
that person is thinking about. Obviously we're given the training that, you know, there is this
I'm not going to say causation correlation. I won't go into it, but that, you know, when this part,
when this brain data is like this, this person is thinking this. And so you have the testimonies of
the people that just shows the power of those and the, and the kind of nuance of those large language
models that they can be able to do that. Um, but actually I think it's arguably less exciting than
people think it is because they forget about the training process that underlies it.
Yeah. And it doesn't allow you to understand. It just allows you to predict.
Exactly. Exactly. And that's where, that's where our, our favorite topic of active inference might,
might come in maybe and yield some artificial understanding.
Hopefully. Hopefully. Well, people are getting very excited.
Yeah. They're getting very excited. Lance, that was, that was awesome. I feel like I've learned,
I feel like I've gone to university again.
Yeah, me too.
No. No, this was, this was so much fun. I knew it would be fun. Thank you for being
gentle with me and kind with the maths.
Well, thanks for having me.
No, it was an absolute pleasure. I really want to see what comes out of this stuff,
especially with Anil about the Fisher information metric and time.
Yeah, it's going to come out.
Really cool. Yeah. Cool. It sounds like we're converging potentially on a similar idea.
So it would be really nice to talk. I need to flesh out these ideas and then, yeah, we can talk more.
Yeah. Splendid. Okay. Well, thank you from Langevin and Fokker Plank and all of this stuff.
It's been an absolute pleasure. So thank you for myself and the Institute.
It's been a great pleasure for you too. Thanks.
