Welcome to the ActInf Podcast, where we will present short, digestible segments clipped
from the Active Inference Lab weekly livestreams.
If you like what you hear and you want to learn more, check out the entire livestream
at the Active Inference Lab YouTube channel.
The link to the livestream is provided in the episode description.
My name is Blue Knight and I will be guiding you through this podcast episode, which is
clipped from ActInf Lab livestream number 6.2.
This discussion will be loosely structured around the paper, A Tale of Two Densities,
Active Inference is Inactive Inference, by Maxwell Ramstad, Michael Kirchhoff, and Carl Friston.
Daniel Friedman will facilitate this discussion about structural representationalism, and he
starts us off with a quote.
And just a short little quote by Buckminster Fuller, because sometimes I just have to.
He wrote,
Rope may not be much like water, but not is like the wave.
And this is from the synergetics framework.
And it's getting at this notion that there's a difference between structural integrity,
which is like the rope and the water, which are quite different.
They're both static and stable.
To pattern integrity.
And knots and waves can travel through medium.
And so by virtue of traveling through medium, kind of like that domino wave, there's something
similar about these traveling waves that goes beyond the mere structural integrity of
their media.
And so I just, a thought question for later is really, is Active Inference a structural
integrity or a pattern integrity?
I would say that it's a pattern integrity.
And just bringing up this notion that many far from equilibrium systems, though their mechanisms
are extremely different, for example, the bacteria and the person, there's going to be similarities
because of the way that they're needing to persist from a cybernetic perspective.
Enough there.
Let's get to that question about structural aspects of attractors.
So here, a participant provided feedback and wrote, why can't the recognition and generative models be
considered as structural representations?
So they're coming from a structural representationalist perspective.
They wrote, in which case the dynamics of the structure are taken into account as part
of the representation.
Even if we're appealing to dynamical systems, the states of any particular dynamical system
over time make up the structural representation.
Take the Lorenz attractor.
The Lorenz attractor has a specific approximate structure around two attractor states, but the
states and the trajectory of any point are sensitive to initial conditions.
So any two initial points will lead to different precise physical states and dynamics, but will
still travel through a very similar trajectory, which hovers around these attractor basins.
The states and the dynamics of a set of points over time can be taken as a whole and be identified
as embodying structural representations of the Lorenz system.
So this is something from the realm of chaos and complexity theory.
And the Lorenz attractor is an attractor that describes a simplified system of equations
describing the flow of fluid, and it's a fluid of uniform depth, and there are also parameters
for the imposed temperature difference, the gravity parameters, the buoyancy, the thermal
diffusity, and the kinematic viscosity.
And the simple physical analogy here is like heating up a pot of water.
There's heat coming from the bottom, and there's cooling coming from the top, and there's this
churning happening, and it goes around these two different attractor states.
So in this case, maybe Alex or anyone else who raises their hand, what is structure and
what is representation?
And if we agree on what this system is, then what is this representation debate that's happening?
Mel?
Cool.
I'm Mel Andrews.
I'm in Ohio.
I'm a doctoral student in philosophy of biology.
I don't know what a representation is.
And so I, for that reason, I guess I think that the question of what a representation is,
and what in the world is a representation is, is maybe too ambitious.
Only because I haven't solved it.
But there's a related question that's less ambitious, which is what is the difference
between a biological function and a representation?
And that, I think, is more manageable in some respects.
In Phil Biomind CogSci, we speak of the disjunction problem.
So we speak of, and I know Maxwell, like, hates this framing and thinks it's updated and we
don't need it anymore.
But the idea is that in order for representation to be a representation, it needs to have the
capacity to represent.
And we have a similar idea in biological function, which is that in order for a function to be
a function, it needs to have the capacity to malfunction.
Right?
So you need malfunction to have function, you need misrepresentation to have representation.
But how do we differentiate a function from a representation?
Good question.
Maxwell, then Alex.
Yeah, so I'm Maxwell Ramstead.
I'm based in Montreal.
So regarding the question, what is a representation, we targeted a very specific account of representation.
I mean, in the philosophy of cognitive science, a representation is basically an internal thing
within an organism that stands for some feature of the environment or the organism's own body
in a way that the organism can leverage to, you know, do something interesting.
We focused more specifically on the claim that generative models are structural representations.
So, I mean, this has been an account that's been, you know, Opie and O'Brien first worked
on this in the mid-2000s, and then Vladzievsky and Mielkowski in the context of predictive coding,
and then Alex Kiefer and Jacob Howie, you know, built on that, I think, in a very interesting way.
And, you know, from that point of view, structural representation is an internal structure that's
internal to an organism that gets its representational content because it stands in a relation of
structural similarity to some target domain in the sense that second-order structural features
of that target domain, like the statistical properties of the domain, are recapitulated
or, like, mirrored in the actual properties of the representations themselves.
That's the first.
There's a structural similarity first property.
Second property, it's not just that these structures are in the organism.
They have to be exploitable in some sense.
So the organism needs to be able to use the content encoded in the representation to guide
intelligent, adaptive behavior.
And then there are these two other points that I think are pretty minor.
It has to be detachable, meaning that the system can use it offline, like, as in, you know,
not, like, actually engaging with the environment.
And it has to be able to afford representational error detection in a manner similar to maps.
So those are the main features of structural representations according to the philosophical account on offer.
And in this paper, we propose that generative models are not structural representations
precisely because they don't really meet these, these properties don't seem to be true of
the generative models.
I mean, yeah, I think Alex and I are going to still disagree about this, even after talking about it
for, like, dozens of hours at this point.
But, yeah, the main reason why in this paper we argue that they're not is that the generative
model isn't encoded in anything.
And this is, like, one of the main differences between more traditional Bayesian brain architectures
and active inference.
As Dan was pointing out really early on, in traditional architectures, well, the recognition
and generative models are just inverse, the ones of the others.
The recognition model is a mapping from the data to states, and the generative model is a
mapping from states to data.
So it's really just the same kind of set of connections and everything, but it's just,
like, which direction are you considering it?
Like, from the top-down flow or the bottom-up flow of information.
In the active inference scheme, it doesn't work like that.
The generative model is nowhere in the dynamics.
Sorry, it's nowhere in the physical organism.
It's only in the dynamics.
So it's very much like these dominoes falling over.
The wave of falling dominoes isn't present in any of the kind of single dominoes falling
over.
The wave is part of the dynamic, you know, phenomenon.
And it's the same with the variational free energy and the generative model.
The generative model just is not there at any time slice.
It only exists insofar as we consider a lapse of time as the point of reference for the
free energy gradients.
So from that point of view, it can't possibly be a structural representation because it's
not encoded by anything.
That's the point.
A lot there.
Let's go to Alex Alejandro.
Yeah, please do.
Hi, I'm Alex Kiefer.
I'm in the philosophy department at Monash University, currently in New York City.
I mean, there was a really nice explanation of what structural representationalism is.
So I don't have much to argue about there.
And in general, I don't want to argue more than is necessary.
You know that about me.
But I think the reason I want to insist on some of this stuff is just for my own basic
sanity to just feel like I have an understanding of what's going on with this stuff.
Right.
So the reasons, despite I understand, I think I understand exactly why from an active inference
perspective, you wouldn't want to say that the generative model is encoded.
Let me just say it first.
I think the question of whether or how it's encoded is maybe a slightly distinct question
from whether it's a representation.
And the reason I think that it has to be a representation, well, there are many reasons.
One argument, the recognition model is an approximation to the generative model, right?
And so that's one example of like a rational or probabilistic relationship between these
models.
And I don't think that that makes any sense if the generative model is not a representation
also.
I'm not sure that that's how that works under active inference.
I agree that in a more traditional Bayesian brain sense, the recognition model is trying
to approximate the generative model.
I mean, that's essentially how it's working.
It's an approximate posterior.
Oh, yeah.
It's an approximate posterior, but it isn't approximating the generative model.
Well, it's approximating the posterior under the generative model.
Right.
So, so that means, right.
And both, both the generative and the recognition models are concerned, their distributions in
part over, I have to be careful here, not the actual external states necessarily, but
over hidden, hidden states.
I don't know how on earth you get those into the picture if it's not a representation that
you're talking about, because the hidden states are environmental states.
They're not necessary, right?
We have to be, as you point out in the paper, we have to be careful to distinguish the generative
model from the generative process.
We can't just identify what actually happens with the generative model.
I think we need to have, this is one of the fundamental questions that I think in activism
often is faced with, is how do we deal with, yeah, I mean, Mel brought up misrepresentation.
So the fact that, the fact, the point is the way that you see the world, the organism or
creature or system sees the world is not necessarily the way it is, but we still have that distribution
over external states as essentially part of the generative model.
But let me just say one more thing here about encoding.
So the reason, I think if you look under the hood a little bit, so the generative model
in underactive inference, as I understand it, can roughly be identified with the non-equilibrium
steady state density, right?
Something like that.
So, I mean, I would just say, right, the dynamics, the dynamics sort of instantiate the generative
model.
I think that's a really cool point.
And I actually think that structural representationalists didn't do quite enough to emphasize the fact
that the generative model is used as a control system.
But I think if you look under the hood, there'll be some features of the system, like stable
structural features in virtue of which it has that nest density.
So, in fact, you will find some stable thing that you could think of as encoding the generative
model, maybe.
At least that's why I still have, you know, I'm still attached to my business.
Cool.
Let's do Alejandra, then Mel.
Well, hello, everybody.
I'm Ale, or Alejandra.
I'm a professor in the Department of Psychology.
I think I agree with Alex, where I'm kind of confused how the generative model can be conceived
just like this inactive process.
Really, I was reading the paper all over again, and I actually don't get it.
For me, it was the recognition density is the inversion of the generative model.
So, if it is not encoded, like, anywhere, it's, I feel kind of lost there.
For me, yeah, these top-down connections, this is the generative model, talking about the
brain specifically, and the recognition density is related with this button-up connection.
So, you can recognize your best guess, right?
So, if it is not encoded, what can be said about these top-down connections?
Yeah, maybe I can jump in here and just provide some points of clarification about the generative
model, if that's okay.
Yep.
Yeah, so, the generative model is defined as a joint probability density over all of the
variables of interest in the system, right?
So, in this case, the variables of interest are the states that we're trying to infer,
the policies that I'm trying to pursue, and the data that I'm observing at any given time step.
And so, look, the generative model itself, like I said, is just a joint probability density.
It's like the probability of all these variables connected with ands, effectively.
And when we say that the generative model is not present in the system, I mean that this
density here, this joint probability density, is never encoded anywhere in the system.
Because the generative model itself is factorized.
So, basically, you take this joint probability density and then using some, you know, Bayes
rule and other, you know, chain rule, et cetera, manipulations, you can write this joint density
as the product of a bunch of likelihoods and priors.
And it's these likelihoods and priors that are updated constantly as part of the recognition
density.
So, this prior, you know, about your data given your states and this other prior about
your states given the next state and the policy and all this, these are updated dynamically
and their current value, like all of them together, collectively comprise the recognition
model.
The generative model is how these quantities are all connected, the ones to the others.
So, like, if you want to think about it heuristically, you know, this, all of these specific parameters,
like the S, the pi, the O, all that, like, they're, all of those values together as they're
being updated, comprise the recognition density.
And the generative density or the generative model is really just these relations between
the different quantities as they change.
So, it's literally that the inference process itself is what holds all these quantities together.
And the generative model is just a description of how those quantities flow together.
So, again, it's like the dominoes falling over thing.
You know, the wave of dominoes only exists in that motion.
And similarly, the generative model only exists in that kind of coordinated inference or update
dynamics of the quantities that are part of the recognition density.
And the reason it's called the generative model at all is just because it's borrowed,
it's a terminology borrowed from machine learning.
In machine learning, the joint probability density over all of your variables is known
as a generative model.
So, it's called that way just because that's the technical term.
It's also called that way because if you're starting from a joint density over all of your
variables, you can actually generate fictive data that you would expect under this configuration
of different parameters.
Thanks for that clarification.
Go for it, Mel.
Yeah, just to follow up to what Maxwell was saying about structural representation,
the sort of detachability of the representation, right?
That's what should get us the distinction between a biological function
and at least a structural representation, right?
Because my feet and legs and knees are a representation of ground in some very minimal sense in the same
way that fish fins are representations of fish fins and bird wings are representations of fluid dynamics,
right?
But this isn't detachable from immediate environmental circumstance for offline use,
right?
So, that's what should get us the difference between function and representation.
But I'm curious to see what role that plays in the FUP and active inference, if any.
Do we want to, do we want to, is there merit in retaining a distinction between function
and representation under active inference in the FUP, or are they just sort of continuous
with one another?
Nice.
Very good question.
Kate, did you have something there?
I think I'm still, with the, in the case of the, this paper was really useful for me
in, like, helping me understand what the generative model was in the FUP almost all the way.
Like, when it's described as, like, a statistical description of the generative process, the way
you've explained it here, all of that makes sense to me.
But then there's also parts in the paper where it says, where you think it's like the generative
model sort of, is what the organism expects and it guides what the organism does, which makes
it sound like the generative model is playing this kind of causal role.
Like, not the generative process, but the generative model itself.
Um, and I find that kind of confusing if the, given that the generative model isn't something
encoded by the organism, um, or by the system, um, I'm still, I get the idea of it as a statistical
description of the dynamics.
That sounds right.
I just don't understand how in that, how that's all it is, that it's something that the organism
uses.
It seems like it's something we use to describe the organism rather than something the organism
itself is in any way using, um, that's a good point.
I, I, I've proposed in a recent paper, uh, that came out in Entropy not too long ago that,
uh, both are correct.
So the, you basically, the, the, the correct way to interpret the FEP is as instrumentalism
nested within instrumentalism, meaning that we can have a model theoretic, uh, philosophy
of science reading of the FEP, uh, and, you know, for that matter, any other theory of
cognition as this is a model that we as scientists are using to explain the behavior of organisms,
uh, it, what I've been trying to argue is that the FEP itself at the theory level is also
saying the organism is exploiting the statistical structure of its body and movement, aka the
generative model to, to guide adaptive behavior.
So the model really is, uh, the, these harnessed relations between the variables that have to
obtain for survival to persist, uh, and, you know, so it's, it's sort of this idea that once
the system kind of gets moving, then there's kind of a, a, an inertial dynamic thing kind
of keeping it moving.
So, so like, you know, my core body temperature is 36 and a half degrees and the fact that it
is keeps a bunch of other processes in play that in turn keep me preserving my temperature,
uh, you know, by initiating, uh, adaptive behavior like putting on a parko when it gets cold and
so on.
Uh, so, uh, yeah, I mean, both, both kinds of generative models are used, uh, separately
in dynamical causal modeling, uh, of the kind that the Friston group used to model the spread
of COVID, we're not assuming that the process that we're modeling is itself an active inference
agent, um, and it, so you can do that, uh, but you can also make the additional assumption
that it is, and then you're in the game of active inference proper.
I think, I, the thing, the kind of worry I get there then is it sort of seems like you're
losing the distinction between the vehicle of the model and the target of the model, such
that I worry it sounds a bit like just saying everything is a model of itself, essentially.
Um, I mean, effectively that is what we're saying.
Um, and that's where it gets a little hazy for me, like, you know, that's probably the
point of disagreement between Alex and I. Also, I want to say the generative model just is the
organism in the sense that, like, this, this specific, you know, statistical structure that
harnesses all of these different, you know, existential variables, as Mel was saying, like,
this really just is the organism, like, this kind of statistical structure that keeps kind of
reiterating itself and coming into existence. That just is the organism. And that's one of the
reasons why I don't want to say that this model is a representation of anything. You could just call
it, you know, the phenotype. We call it a generative model because formally speaking, the same construct
is borrowed from a field where, in which it's called a generative model. But you can think of it
as the organism's phenotype. And precisely because it's a bit weird to think of the model as modeling
itself, I kind of resist that interpretation and say, well, you know, if anything is a model, really,
in the more traditional sense, it's this recognition density that's performing posterior state inference.
We hope you enjoyed this podcast episode. Stay tuned for next time, where we move on to discuss the
paper, Variational Ecology and the Physics of Sentient Systems.
