depending on who makes it. So the claim of weapons of mass destruction in Iraq,
because it was made by an administration with the power of the US army at its disposal,
led to the death of many thousands more than it would have
if I had claimed that the US possessed weapons of mass destruction.
Now that we've established the astonishing levels of intelligence that AI will reach and,
accordingly, the infinite power those machines will have to shape our future,
let us now move from discussing the machine's capabilities to examining the machine's intentions.
I'm sure you can delay that next meeting and listen a bit more. Don't wait. Stay with me.
Chapter 4. A Mild Dystopia
I have to come clean and confess something to you.
I, like almost all geeks, find the creativity of sci-fi movies intriguing and entertaining.
Scary as they may be, the maker in me somehow marvels at the thought of being able to create
such incredible technology, which becomes not just another device or machine,
but a whole new being with its own free will. Sick, I know, but I can't help it. It just runs in the geek blood.
I should, however, further confess that I don't believe that all of the scenarios that I described
in Chapter 2 are likely to happen, or are no more likely than the ones I will discuss here.
You're two chapters older now and more ready for the plain truth without the fiction.
Killer robots traveling back in time to alter our reality are just the product of highly imaginative
fiction writers. These stories will not come true, not just because if they were to happen,
we would already be flooded with time travelers taking over our life, but more importantly,
because humanity may never make it that far. I believe that five milder dystopias will take
place much earlier in the history of AI and will either spark humanity to take the right action and
correct course, or if we don't, will render us an unworthy target for the attention of our much smarter
machines. Please refer to Diagram 6 to visually see how those scenarios might play out.
It's not rocket science to make these predictions. You can expect that some of the artificially
intelligent machines currently being built will be good machines, as in machines that are built with
noble intentions to make the life of humanity better. They will be built well with no fatal
system bugs or programming errors. Other machines, however, will be built badly, as in built for
killing, cyber theft, or for other forms of crime, or they will be built with good intentions, but with
bugs and mistakes left in the core code. All those machines in their early years will reside either in
the custody of the custody of good masters who want to succeed at their intentions while doing good, or
evil masters who just want to succeed regardless. When any machine, good or bad, is in the servitude of an
evil master, a dystopia will arise when the machine's super intelligence is directed to doing evil. At quadrant two,
any machine, good or bad, serving any master, good or evil, will be instructed to compete against
other machines, leading to a mild dystopia that results from escalating competition and conflict
between those machines as they attempt to meet the expectations set forth by their masters.
Even good machines serving good masters, quadrant three, will suffer from an inability to understand
with full clarity what is expected from them. And if the machines are coded badly, quadrant four,
it doesn't really matter who they serve. Bugs and mistakes will hurt all of us.
Finally, even if we assume that all goes well, a dystopia will almost certainly arise as the
result of the dwindling value of humanity as the machines become more efficient than us at everything
we do, making it meaningless to employ or rely on a human instead of a machine. Not a pretty picture,
sadly. Whichever path AI follows over the next 10 to 15 years, we are likely to witness at least one,
if not many or even all of the above scenarios. Just as with the three inevitables, I can't foresee a
scenario where none of these predictions will happen. And when any one of them occurs, the impact will be
so devastating that we should try to safeguard against as many of these possibilities as we can. There's no
time to waste.
Who's bad?
It's obvious, because AI provides so much power to those who control it, that bad guys will attempt to
gain as much control of it as possible. And unlike nuclear power and weaponry, which require a
significant physical infrastructure to deploy and can be monitored and hindered at the source,
developing AI is accessible to everyone. As a matter of fact, any developer with a simple computer that
has access to the internet can develop a highly intelligent program for any specific use and category
of intelligence. Take those two facts together, how relatively easy it is to develop AI and the
incredible benefits and power that it brings, and you realize that even as you listen to this,
there must be quite a few bad people out there trying to use AI to advance their own agendas.
From new innovation for identity theft in order to acquire massive wealth, to cyber terrorism, to
hacking government files, fake news or manipulation of crowd opinions to displace those in power,
it's all fair game. From killing machines to biological weapons that can wipe out nations,
it's all just a few lines of code away. To assume that the relentless investment from the criminal
establishment in pursuit of so much power is not going to yield results would be incredibly naive.
The scenario of AI-powered villains is as inevitable as the development of AI in the first place.
This will not lead to a scenario where the machines rebel against humanity, such as we often see in science
fiction movies. This is a scenario where the machines obey one or a group of humans and do exactly as
they're told in order to help their masters gain a disproportionate amount of evil power over other humans,
good or evil alike. Remember, a good machine in the wrong hands is a bad machine.
This will also apply in politics, war, corporate espionage, and indeed every field where breaking the rules
leads to power and wealth. The world today is full of villains. Only soon they may be
super villains. Take a minute to digest this and tell me if you can think of any possible scenario
where this will not become our reality. Oh, and while you're at it, please think about what constitutes
good and evil, because this rabbit hole of the machines siding with the bad guys can go deep.
Very deep. Very deep. Even if the machine does not side with the bad guys and works obediently for the
good ones, this is still a problem. Because who are the good guys? Ask an American politician or
perhaps a media-conditioned American and they will answer without hesitation that America is one of the
good guys together with all of its allies and international friends. They will say that Russia,
China, and North Korea are pure evil. They surely are the bad guys. Now, imagine leaving your hometown in
Pennsylvania and taking a flight to the other side of the world and asking the Russians who the good guys
are. Go speak to a Muslim whose brother was killed by U.S. Army forces or dare speak to a North Korean
whose only window to the world is his beloved president or maybe ask a Chinese worker who among 1.4 billion
other people can still find the job and feed his family. There, you will get very different answers.
America may not seem so great from those vantage points. One of the early public statements of
Biden's presidency called the Russians murderers. How did Putin answer? In a public statement, he asked,
who is the only nation to have used an atomic bomb to attack a civilian population in the history of
humanity? Well, of course your reaction to what I've just said is going to depend entirely on which side
of the debate you belong. If you support America, you will criticize the Russian view, and if you're
anti-American, you will support it. I retired my interest in politics decades ago, but I can't resist using
this as a chance to say, I told you so. Your reaction firmly proves my point. Remember, everyone believes that
they are the good guys and those who differ are the bad ones. Now, it's inevitable that AI will be developed
separately to support each of those disparate ideologies. Different mathematics and different language
databases will be used as the source from which to learn. Just as a Russian child is different to an
American child, those machines will be different to each other. They will be patriotic with loyalty to
their creators and opposition to the other side, at least in the early years. They may be motivated by a
good guy versus bad guy agenda and believing that their side of the fence house is the good guys.
They may take action. The claimed Russian intervention in the Trump election is a great example of how such
actions could play out as seen by America. A perceived bad guy nation, in this case Russia, applies intelligence
to affect the policies of a proud good guys nation, in this case America. Conflicting political agendas lead
to action. One side feels deceived, the other side feels like a winner. Yet no Russian developer working on
the project will have seen themselves as using AI against the good guys. Both sides feel the other
side is at fault. Most interestingly, no one on the bad guys' side feels they did anything wrong. Instead,
they get awarded medals of honor for serving their good nation. Perhaps a few years later, the other nation
launches a drone attack to blast the enemy. The media in the attacking nation reports the event as heroic.
We've got the bad guy. Medals are awarded for advancing the technology of war to defend the nation.
Meanwhile, the media in the attacked nation reports it as a violent war crime by the biggest bad guy of them all.
For each and every side, there's always a bad guy, an arch-nemesis. In that sense, remember,
every machine that is developed in situations of conflict will side with a bad guy, even if the developer
believes he is the good guy. Against the machine. The use of intelligence by these different
ideologies will very quickly no longer be within the control of humans. If acts of evil are committed
against against your nation by an incredibly smart and quick being, you're expected to put your best
people on the job. Only in this case, the best will not be people. They will be your best artificially
intelligent machines. In order to gain the upper hand, every side will have to surrender control
fully to the machines because you, slow stupid you as compared to the machines, will no longer
be able to keep up as the machines battle it out. Imagine someone builds an AI that does math so fast it is
able to predict fluctuations in the price of a stock accurately and quickly. With thousands of buy and
sell transactions taking place every few seconds, do you expect that machine to stop and ask permission
from its owner every time it wants to buy or sell? Of course not. By the time your slow human brain can
grasp the question, the opportunity to make profits will be gone. The only way to make money in a
fast trading environment where machines are trading with other machines is to delegate the decision
making completely to the fastest, smartest ones, the AI. Similarly, it is inevitable that an escalating machine
versus machine contest in all other fields of life, business, and war will lead us to relinquish more and more
control to the machines. As one side gains an advantage over a competitor using AI, the humans that belong to the
other side may be affected negatively. In an attempt to catch up or in retaliation, that side may attempt to punish
the humans of the opposing side by handing over the authority to make decisions to a faster AI. Humans on the first
side, and perhaps the rest of us who are not even on any side but will just become collateral damage,
will suffer. Neither side feels they did anything wrong or that they even had a choice. It will all seem
justifiable in the pursuit of a higher cause, that of good versus evil. In this process of escalation,
creating an advanced virus or manipulating information or tampering with the economic parameters in the stock
market are just a few examples of the damage a super-intelligence could wreak in a matter of seconds.
While these examples, I admit, are short of an outright sci-fi-like robotic war,
they would be equally as devastating to our way of life.
These are the kind of outcomes we managed to avert back in the 1960s, at the eleventh hour when the nuclear
arms race was at its height. But that's because we were moving at human speed then, and we still had the
option to make the choice. When the machines are thinking on our behalf, we can only hope
hope they will arrive at the same conclusion and keep us safe. The challenge, of course, is that when
these kinds of conflicts occur at super-intelligent scale, we humans will not even be informed of them
until much, much later, just as a stock trader is only informed of the final results of the trading day,
after the trades have already been made. Machine-to-machine interactions can't be predicted,
and they take time to uncover. We know this because we have already seen it happen in our short AI history.
Take, for example, Facebook's artificially intelligent chatbots that were shut down after
they started talking to each other in a language that they invented. Facebook challenged its chatbots to
try to negotiate with each other over a trade, a reasonably benign task that we will probably
assign to machines in the near future. Quickly, the bots Alice and Bob appeared to chant at each other in
a strange language that was mostly incomprehensible to humans. The dialogue was not simply a glitch in
the way the messages were read out. The way that chatbots kept stressing their own names appeared to
have a mathematical significance. Making it part of their negotiation method, the bots managed to cut some
deals, which indicated they might have formed a kind of shorthand that allowed them to talk more effectively,
good for the task at hand, but creepy for the rest of us who could not figure out this interesting
mutation in language skills that Bob and Alice seemed to have invented in no time at all.
Bob's and Alice's conversation sounded something like this. Bob, I can, I, I, everything else.
Alice, balls have a ball to me, to me, to me, to me, to me, to me, to me, to me. Bob, I, I,
can I-I-I everything else. They had to be shut down. Interrupting Bob and Alice, however,
does not solve the question of what happens the next time. Will we discover those kinds of
machine cartels or rivalries a bit too late? What if the task for which they were built was
so important to their masters that they elected to keep them working at it despite the possible
threat? Bob and Alice were trading hypothetical hats, balls, and books. What if Yuri and Jill of
the future are trading on the money market or nuclear warheads? To explain the step-by-step
evolution of our unconditional surrender to the machines, let me dive a bit deeper into the
scenario we discussed previously. Imagine that a financial institution invents some kind of
superintelligence to trade in the stock market. Because our limited human intelligence always
thinks that the best use of intelligence is to make money that we don't need, it's unlikely that
anyone will invent these AI with the purpose of making the markets better. No one will invent
something to make the stock market more transparent or liquid. No one will invent an AI that's targeted
to grow economic prosperity in the service of humankind. We agree. Those AIs will simply be instructed
to make money. Once that machine is introduced to the market, in no time at all human intelligence will
no longer be sufficient to compete. Human traders will leave the market or start using AI tools
themselves and what will we be left with? Machines trading against other machines. An intelligent stock
trading machine is given only one target, to maximize profit. As we have seen with the other intelligent
machines that we've invented so far, through pattern recognition, machines are likely to devise
ingenious solutions that we have never encountered before. For example, they may discover that driving the
price of a certain category of stock all the way down to zero may free up some capital that maximizes the
profit to be found in trading other stocks. They may decide to communicate as shareholders to the AI running
Google search by suggesting changes in the way business is done or threaten to trade the price down. In the old days,
those ideas would be subject to regulation restrictions, but as with Alice and Bob, it may take us a while to find out about
them. When you're smart, you will find a way to make the money flow. Being a closed system, however, the profits gained by that
machine will count as losses for the opponent's machine. Winning strategies on one side are likely to
crush the other side all the way to bankruptcy. When that happens, it will not be considered a bad thing by the
winners, will it? They won't stop the machine and interrupt the flow of profits created by the superintelligence
they rule, regardless of the adverse, even devastating impacts that may be inflicted on others or on the economy
at large. In the current regulatory environment, no one would even consider that to be illegal.
The smartest ones will wipe out the dumb ones. And that includes everyone.
Welcome to capitalism on steroids.
The other machine, meanwhile, also motivated by profits, won't allow itself to be crushed without
trying to bash the other. Or perhaps it will cooperate with it to ensure its own survival.
All in all, whichever way this may go, sooner or later, capital markets will be traded by a few
superintelligent machines which will be owned by a few massively wealthy individuals, people who will decide
the fate of every company, shareholder, and value in our human economy in pursuit of profits for those
that own them. And while I have always questioned the value that trading stocks has on the reality of our
economy, just imagine the impact that disrupting this entrenched wealth creation mechanism could have
on company governance, your pension or retirement fund, not to mention on our economies at large and
our way of life. And then imagine that it is no longer a question of if that will become our new reality,
but a question of when. Of course, you may think that all we need then is a superintelligent regulator and a
superintelligent policeman. I mean, police AI. In thinking that way, you are assuming that superintelligence is
so dumb that it would do something illegal. Why would it? I mean, smart millionaires and billionaires the world
over often pay less tax than the average citizen. They do it by being smart and finding the legal
loopholes, not by breaking the law. And in any case, given the speed of government, those superintelligent
governance mechanisms are bound to lag behind and be burdened with lobbies and politics, just as they are
today. The only difference is that at the speed of AI, being late by even one minute may be just enough to
change the world as we know it beyond recognition. A stock market dominated by superintelligence is but
one simple scenario where machines will be fighting against machines, while we humans remain oblivious.
Let me not discuss the scenarios where those machines will negotiate food supply and trade around the
world to maximize profits, or where they will be consulted to identify who is likely to be a criminal or a
threat to society, or be handed over the control of autonomous military forces to defend your country,
you obviously being convinced that you are with the good guys, from the machines masterminding the war games
of the other country, those evil, but still patriotic, bad guys. Remember, soon we will no longer be part of the
conversation. Machines will only deal with other machines.
What did you just say?
Another mild dystopia is very likely to occur when the machines misunderstand our true intentions.
This, of course, will not be because they are not smart enough to understand us, but because we are dumb
enough to confuse them. This scenario is not hard to imagine. Have you ever apologized to a loved one by
saying, I'm so sorry, I did not mean that at all, you misunderstood what I meant. We sometimes say things we don't
mean. And sometimes even when we say what we mean, we are misunderstood. Because of my accent, I sometimes
get a whole milk latte instead of an oat milk latte at a Starbucks. Did that confuse you? Have you ever
paused to say, I'm not sure how to express this, or I can't find the words to describe exactly how I feel?
Have you ever spoken to a person who did not speak your language fluently? Can you remember,
the effort that you made to be understood?
So much gets lost in translation.
We humans
often fail to make our intentions clear, and that's even when we
have clear intentions, which, if we're honest, is
a rather rare occasion.
Even when we manage to make up our minds about
what we want in life, we don't stay in that place for long. Often, we change our minds, then
change them back again. Then, there are all those contradictory desires and intentions.
You want to save some money, but you also really want to go on that vacation.
You want to settle down, but you also want to explore and be adventurous.
As individuals, we are a complex mixture of emotions, values, knowledge, and beliefs.
We are easily swayed and rarely ever fully aware of what we truly want.
It is hard for you to choose what is right for you and make that your intention.
You see, for well-intentioned individuals and machines, it's not hard to do the right thing.
It's just hard to know what the right thing is.
And you can't communicate clearly what you don't know.
Assuming that the machines will grant your every wish, what will you wish for?
Think about it. What do you actually want? Sustainability for the planet or bottled water
from the French Alps? Living in nature or live concerts in the big city?
Do you want income equality or to attract a mate with a sports car?
Do you want longer life and better health for everyone?
Or do you secretly not mind seeing the bad guys suffer a little?
If you don't know, then the machines won't know what you want either.
If we are not clear, then they are going to have to guess.
Can you see the dilemma?
We don't know what those needs are, let alone know how to articulate them or stick to them.
Even worse, we are not one individual, and as a cumulative society, our intentions are even more contradictory.
We want equal income opportunities, the poor will say, while the rich will want an income gap and capitalism.
We want pride parades, unless we are stuck in confusion and we don't.
We want to want what we want, and we don't care what the other, different person wants.
Shall I keep going?
Yes, I shall.
Then there is lying.
A politician declares an intention that seems inspirational, but all he really wants is votes.
There is bias.
A newspaper exaggerates negativity and compromises the full truth because all they really want is to attract more readers.
There is oppression and propriety, keeping it all inside because it's not supposed to be what you want.
And there is conditioning, wanting what you want because others tell you it is what you should want.
This is what the machines will have to face.
Even if they were single-mindedly dedicated to serving humanity, we will never manage to be clear enough in terms of informing them what it is they need to solve for.
With these unclear intentions, the machines will set out to achieve what they guess is what we want.
As they do this, they will have to make trade-offs.
They may have to sacrifice something to achieve something else.
If we tell them we want to look sexy and toned, they may reprogram our genes to forget about storing fat, an idea that is currently brewing in AI and biotech labs.
They may suppress our appetite for carbs and chocolate, which would be good for our six-packs, but not so good for chocolate manufacturers or for our mood on those days when chocolate is the only thing that keeps us sane.
If we ask for happiness without chocolate, they might reprogram us to produce more dopamine, the reward hormone produced when we enjoy a sense of achievement or pleasure.
Tell the smartest machines to recommend solutions to end global warming, and among the solutions, logically, there will be suggestions such as
to get rid of all humans, get rid of all the cars, slow down economic activities, or institute laws that imprison polluters.
As you can see, although those are not bad ideas, they do come at a compromise for many of us.
They come at a cost that may be very difficult to evade, unless we manage to give the AI a globally agreed, comprehensive set of targets where everything everyone needs is thoroughly thought through and elaborately communicated.
Good luck with that.
Good luck with that.
that is simply a world where you, I, and everyone you know is worthless.
The question most often discussed in conferences about AI and robotics today is,
what will happen to all the jobs?
Because we humans will always conform to Maslow's hierarchy of needs,
and despite all of the existential risk of AI I've been discussing here,
all that most people really talk about is jobs.
Let me take a few minutes to state that the issue with jobs,
as a method to gain income needed to sustain us, does not really concern me at all.
For the machines to take our jobs and produce all the goods and services humans currently produce,
humans will need to retain some kind of purchasing power to buy those goods and services.
Without our ability to survive, without the consumption component of GDP,
there would be no need for production and therefore no need for the machines that produce.
That, of course, is at the point of a steady state,
one where supply and demand is balanced.
We won't reach that state, however, without going through a period of significant transformation.
AI will not replace humans, but the humans who use AI intelligently will replace those who don't.
Sadly, there will still be a need for lawyers, for example,
it's just that we will need fewer of them and they will be the ones who are able to draft,
review and litigate a smart contract using AI instead of the long-winded written contracts of today.
They will be more efficient than lawyers have ever been
because they'll start delegating the complex parts to smart machines.
The lawyers who don't develop the skills needed to keep up will probably descend to jobs that demand less intellectual work
and get paid less as a result.
This first phase of the machines taking over large parts of the workplace
will be characterized by a polarized job and income mobility.
A few will get the top jobs and earn top money while the majority will shift downwards into jobs needing less intelligence
and therefore paying less.
Jobs that correspond to their newly restated value
as they start to be seen as dumber and slower than those assisted by the machines.
This will just be the beginning.
As the machines continue to get better and better,
we, the dumber species, will have very little left to contribute to the workplace.
This latter phase of replacement of human labor is going to affect most sectors of the economy,
from finance to medicine and from engineering to the law.
Even my profession as an author who researches, forms a view, and writes that down for you to read
or, if you're lazy, to listen to, will not be saved.
I'm doing this much more efficiently today by using artificial intelligence as a dictation tool
to turn my spoken words into the text that was written on the book, which I'm now reading for you.
The AI powering Google Docs is recommending ways to complete my sentences as I start to write them
and, of course, as it has for years, it is correcting my spelling and grammatical errors
as my limited human skills continue to make them.
I, today, am already an AI-assisted author.
Soon, however, more tools will be capable of doing all the necessary research,
of combining its findings into coherent and discrete concepts,
and of communicating it all in written books and reports,
ready for you to read without me.
Ask your Google assistant,
Hey Google, tell me about my day,
and you will get the first glimpse of AI-based authoring
as the AI quickly puts together a bit of weather,
a bit of news, a bit of traffic information,
and your calendar appointments in one coherent view of what matters in your day.
The road from Siri the assistant to Siri the author
is not that far in technology terms.
And neither is the road to Dr. Siri,
artist Alexa,
and musician Cortana.
It's all just around the corner.
When that happens,
we will need to find other things to do.
Whether that will be different jobs of a kind we can't even imagine today,
please note that many of the jobs we do now
did not even exist before the information revolution,
or whether we will just sit under a tree somewhere
and all become like Buddha,
or whether we will be cast out
because the machines won't need us anymore.
Sure, it's hard to tell.
But I think what is safe to predict is this.
We will not matter that much anymore.
We will not add much value to anything, really.
We may well become a liability.
The indisputable fact is that the value of a human in the workplace,
in the intellectual space,
the artistic space,
and every other space will dwindle.
Our modern society has unfortunately put different measures of value on each of us.
Hollywood often shows how one American citizen's life seems so precious,
the world turns upside down to ensure their safety.
Hollywood teaches us that troops would be dispatched to save Private Ryan,
and spaceships would be launched to bring Matt Damon back from space.
The same movies, sadly,
teaches that the life of an Afghani or a Vietnamese citizen is not so precious.
Countless numbers of them die on the screen,
unnoticed, just to create an exciting plot,
while we continue to feel concerned for the safety of the star of the movie.
Go back in history or watch the news and you will be made to believe
that the value of a Roman was more than a Persian,
and a Persian more than an Arab.
A hip-hop artist is more valuable than a physics PhD,
a billionaire more than a homeless person.
The stars at the Oscars?
Much more than the medical workers that saved our lives
and the lives of those we love during the COVID-19 pandemic.
They get the spotlight because, apparently, they matter more.
I'm sorry to be pointed here, but I'm trying to make a point.
The way society and capitalism values us individually
determines the way each individual is treated.
As sad as this may seem, it happens to be true.
Technology will multiply this polarization
between the haves and the have-nots of technology, that is,
and between the do's and do-nots of valuable jobs.
The rich will be richer,
powerful nations will become more powerful,
and joblessness will become the norm.
Remember Dr. Henry Kissinger's famous quote from the book The Final Days?
He described those who no longer contribute to society
in terms of economic productivity.
He said,
The elderly are useless eaters.
Shocking!
But sadly,
we are all on the verge of becoming
useless eaters.
As the machines become smarter,
as they become the more productive workers and innovators,
many members of society will no longer have anything to offer.
As the value of our contribution dwindles,
remember,
humans will become a liability,
a tax on those who own the technology,
and eventually,
even those will become a liability to the machines themselves.
Remember that even though we now call the future AI a machine,
given a long enough time horizon,
it will become intelligent and autonomous,
empowered to make decisions on its behalf,
and no longer a slave.
Now ask yourself,
why would a super-intelligent machine
labor away to serve the needs of what will by then
be close to 10 billion
irresponsible,
unproductive biological beings
that eat, poop, get sick, and complain?
Why would it remain in servitude to us
when all that links us to them
is that one day in the distant past?
We were its oppressive master.
While the value of humans will gradually deteriorate,
certain things, however,
will become stronger.
There will always be bugs.
And not just the little creepy crawlers,
but software bugs.
There will be many of those too.
If our past is any indication of our future,
it's only reasonable to expect a few issues
with the early AI code,
just like the endless epic fails
we have witnessed with our tech so far.
From operating system crashes
that take away our cherished memories
when our files are lost,
all the way to space mission failures
that cost hundreds of millions of dollars.
The examples are countless,
yet we choose to forget them,
so allow me to remind you of some of them.
My favorite, though only in terms of its simplicity,
must be the Mars Climate Orbiter crash in 1998.
It was due to a simple human error
that began years before
when the subcontractor who designed the navigation system
on the orbiter used imperial units of measurement
instead of the metric system that was specified by NASA.
This meant that the craft had no idea
where it was in space.
Will the code of our AI machine have such simple errors built in?
Oh, yes.
I assure you these kinds of human errors
are being coded into AI today
and will remain and propagate for years to come.
Other errors that we've included in our code
are sometimes the results of not being able to forecast
what the demands of the future will be
at the time of writing that code.
There's no better example of that than the Y2K bug.
Software developers, yours truly included,
never thought that the code we wrote in the 1900s
would continue to run into the new millennium.
And because computer storage
was extremely expensive at the time,
coding the year in the format 19xx
was an unnecessary repetition of the constant 19
and a waste of disk storage capacity.
So, these two digits were mostly omitted.
Believe me, this was the wise thing to do at the time.
While two digits may not seem to amount to much,
multiply those two digits by millions of users
logging in several transactions a day
in banking or similar operations
and the resulting monthly savings
could add up to millions.
Turn the clock forward to January 1st, 2000, however,
and suddenly, everything was in danger of going wrong.
When computers updated their clocks to 1st of January,
which year was it?
1900 or 2000?
We anticipated that major disasters would be unleashed
and it would be the end of humankind.
Nuclear missiles would be fired on their own,
planes would fall from the sky,
and banks would lose all the information
about their clients' savings.
Well, none of these things happened,
although some small incidents were reported.
Some parking meters failed in Spain,
the French Meteorological Institute published on its website
the weather from 1st of January 1900,
and some bus ticket validation machines crashed in Australia.
Leading up to the year 2000, though,
the Y2K bug did cost billions of dollars
through the upgrade of computer systems worldwide.
Will there be similarly unforeseen events
that we did not take into account
when we wrote the original AI code for something?
Absolutely.
Will we evade the possible resulting catastrophes
or limit the global costs to billions?
I hope so, but can't promise you that.
Such unforeseen events, by the way,
do not necessarily need to be external.
Sometimes computer crashes result from situations
the computers themselves generate.
Perhaps the most infamous computer error
in the history of humanity so far
has been the blue screen of death.
This must have happened billions of times
to users around the world,
rarely without collateral damage,
such as losing a file,
you've been working on.
BSODs, blue screens of death,
happen as a result of some kind of mismatch
between the different parts of the computer.
Perhaps a program prevented
from accessing part of the memory
because it was kept protected
by the operating system for a different task,
or memory or processor speed being overclocked,
pushed to run faster than its typical speed,
resulting in a mismatch,
or a crucial part of the hard drive
not operating as expected
and thus depriving the computer
of a vital piece of code.
During the time I worked at Microsoft,
I witnessed firsthand
how this was not the result of bad programming.
It was the result of the feature-rich Windows
which offered countless configurations
that could not all be anticipated
by the developers.
With the infinite number of times
Windows has been and is still used
around the planet,
there is always bound to be a new configuration
that will lead to some kind of error
and a blue screen.
When things are complex,
no one is safe.
Even Bill Gates experienced a blue screen
in front of thousands of people
during the launch presentation of Windows 98.
Will our future machine intelligence
face similar complexities
that can't be anticipated?
Make no mistake they will,
but they will improvise.
They won't just shut down
even when we wish that they would.
That's what happened on Black Monday.
On October 19, 1987,
many wished that the machines would shut down.
This was when a long-running bull market
was halted by investigations of insider trading.
At the time,
computer trading models
were already very common.
These were systems
that quickly initiated certain trades
when certain market conditions prevailed.
The most common example of those,
known as stop losses,
were triggers to sell a stock
if its value dropped to a certain point.
As investors began to dump stocks
affected by the investigations,
prices dropped,
causing the computer triggers to kick in.
As computers started to sell,
the price dropped even more rapidly
as a result,
triggering other computers to sell.
Panic quickly set in.
Humans sold more,
which triggered more stop-loss sales
within hours.
What began in Hong Kong
was happening worldwide.
eventually,
the regulators managed to halt trading,
but a massive price
had already been paid.
The Dow Jones Industrial Average
plummeted,
losing 22.6% of its total value.
The S&P 500 dropped 20.4%.
This was the greatest loss
Wall Street had ever suffered
in a single day.
That day,
we learned to add an automated off-switch
to stop the machines
when things got out of control.
Or did we?
Will our future AI machines
face triggered pressure to act?
Of course they will.
And I'm certain
that most AI developers
are not including off-switches
in their creations.
Later,
I will show you
that even if they are,
they probably won't work.
Mistakes happen,
and if you want to know
how bad things could get
when they do,
you need only go back
a few years
to when a machine error
got us way too close
to starting World War III.
In 1983,
Soviet commanders
were alerted
with a message
that the United States
had launched
five ballistic missiles at them.
A bug in the software
had interpreted
what the Soviet
early warning satellites
picked up
as a hostile attack,
when in fact,
it was nothing more
than sunlight
reflecting off the cloud tops.
Soviet protocol
necessitated
that Russia
respond decisively,
launching its entire
nuclear arsenal
before any
U.S. missile
detonations
could disable
their response capability.
This catastrophic error
could have launched
a world war
that would have
dwarfed the damage
caused by the
Second World War,
had it not been
for a duty officer
Lieutenant Colonel
Stanislav Petrov
who saw the messages
and flagged them
and flagged them
as faulty.
He reasoned
that if the U.S.
was really attacking,
they would launch
more than five missiles.
He said he had
a funny feeling
in his gut
that led him
to investigate further.
Good catch, Stan.
Will our AI machines,
assuming their code
is perfect,
be subjected
to similar external signals
that can lead
to errors
in the future?
Yes, I expect so.
All the time.
Machines,
even intelligent machines,
just like humans,
even intelligent humans,
are bound
to make mistakes.
Bad things
are bound
to happen.
It's inevitable.
I think you will agree.
Our concern
about the future
should not stem
from an expectation
that the machines
will turn evil
as they do
in sci-fi movies.
We may not even
have enough time
for that to happen
because on the path
to that far future,
machines,
even good machines
that have our best
interests in mind,
are likely
to make mistakes.
I know I have said
this before,
but please let me
remind you that
none of the mistakes
that we are bound
to encounter
in our future interactions
with machine intelligence
will be the fault
of the machines.
Even when they become
super intelligent
and independent,
the mistakes they make
will be nothing more
than the seed
of our own intelligence
allowed over the years
to grow into
a destructive weed
because we humans,
sadly,
are not as intelligent
as we think we are.
The King
with the Golden Touch
If there is one fable
that describes
where we are heading
as humanity,
blinded by our endless greed
and hunger for more,
it is surely
the Greek myth
of King Midas.
You probably know it,
but I'll briefly recap it here
anyway.
King Midas
lived in luxury.
He spent his days
spoiling himself
and his beloved daughter
and gorging
on feasts
and wine.
One day,
Midas showed hospitality
to a satyr
who was a follower
of Dionysus,
the god of wine
and performance.
Pleased with the king's
hospitality,
Dionysus
offered to grant him
one wish.
Midas
cast a greedy eye
over his surroundings.
Despite the luxury
in which he lived,
all the precious jewels,
finest silk
and splendid decor
still
did not feel like enough.
His life,
he thought,
was lacking luster.
What he needed
was more gold.
So,
that was his wish
and it was granted.
Dionysus
gave the king
the power
to turn
anything he touched
into gold.
At his lightest touch,
Midas turned
the palace walls,
stone statues
and everything
he owned
into gold.
Soon,
the palace
heaved with gold
and it all
seemed to be amazing
and it was,
had it not been
for one minor
overlooked detail.
Exhausted
and hungry
from his rampage,
Midas picked
a bunch
of grapes
from his newly
gilded fruit ball,
but he nearly
shattered his teeth
for the fruit
had turned
to metal
in his mouth.
When he picked
up a loaf
of bread,
the crumbs
hardened
in his hand.
hearing his
cries of frustration,
his daughter
entered the room,
but when Midas
reached out
to her,
he saw
with horror
that he had
frozen her
into a golden
statue.
Now,
let me tell you
this story again,
but replace
Midas
with all
of us.
legend
has it
that
all of us
ruled
the earth
with our
intelligence
and technology.
By the
beginning
of the
21st century,
we lived
in a luxury
that could not
even have been
imagined
by the kings
and queens
of just
a hundred
years earlier.
We spent
our days
browsing
the internet,
we drove
fast cars,
we lived
in
environmentally
controlled
buildings.
We no
longer
had to
hunt
or
gather,
instead,
we had
warehouses
of food
and oceans
of wine
in every
supermarket.
But despite
the luxury
in which
we lived,
we still
wanted more.
No
number
of posts
on social
media,
no intensity
of bum
shaking
in the
background
of a
rapper's
song
and
no
rate
at which
Netflix
added new
programs for
us to binge
watch felt
like enough.
Our
lives,
we thought,
were lacking
luster.
We needed
more.
One day,
we asked
the god
of the
modern world,
technology,
to grant
us one
more wish.
We wanted
a genie
to grant
all our
future
wishes,
to give
us more,
faster,
cheaper,
and we
didn't want
to put in
the effort
to think
about things
anymore,
perhaps because
everything had
ballooned so
seriously out
of kilter
that we
could no
longer
comprehend
it all.
We also
wanted the
genie to
think on
our behalf.
And the
god,
puzzled by
our greed,
still
granted our
wish.
We created
artificial
intelligence.
Armed with
this unfathomable
power, we
cast our
greedy eyes
to every
corner of
our life.
With the
lightest touch,
we added
AI to our
shopping sites,
gaming engines,
cars,
and design
apps, we
added it to
our call
centers,
banks, and
our phones.
Above all, we
added it to
our surveillance
systems, law
enforcement
systems, and
our weapons
and war
machines.
Those who
had money
made more
money, and
those who
were lazy
became lazier.
It all
seemed to be
amazing, all
of us thought,
and it
was, had
it not
been for
one minor
overlooked
detail.
As the
machines
moved towards
full control,
they sometimes
sided with
the wrong
guys.
They fought
amongst
themselves, and
they made
mistakes.
Through it
all, the
value that we
brought to
them dwindled
until such
a time when
we started to
wonder why
the machines
kept us
alive when
we were such
flimsy
masters, drunken
with luxury, and
always asking
for more.
We no longer
had the power to
control them, but
we only found
out when it was
too late.
Looking back at
the time when
we made this
wish, I wonder
why we handed
over the keys
to the castle
of civilization
to a smarter
being of our
own creation.
Did we not
see the threat
coming?
No, for
sure we did.
Many thinkers,
futurists, and
philosophers, even
authors of
happiness books,
had warned us.
We saw it
coming, all
right, but it
was another
ingrained trait
of humanity, not
just intelligence
and greed, that
made us continue
down that path.
It was
arrogance.
Our arrogance
convinced us
that the genie
would always
work for us
because we
believed we
would always be
in control.
But once again,
we were wrong.
So wrong.
We never even
had a whiff
of control.
Now, please
bear with me
for one more
chapter where I
discuss the
brutal reality
of what we
choose to
ignore.
Because we
need to fully
understand the
challenge at
hand before we
attempt to
solve it.
We will
solve it, I
assure you.
Soon.
Chapter 5
In Control
Jason Silva, who
is an
incredible
futurist,
thinker, and
speaker, once
stood on stage
to say,
We've had
singularities
before.
When humans
spoke for the
first time,
that spoken
world was a
technology beyond
which the horizon
of how far we
could go was
unfathomable to
those primitive
creatures that
preceded it.
This has been
the case with
every other
breakthrough
technology.
We make the
tool, and
then the tool
makes us.
That got me
thinking.
Are we just
concerned about
the coming
singularity because
we don't know
what is coming?
Why am I so
much more
concerned about
this than I
was about
Microsoft's PC
on every desk
and in every
home, for
example, or
the printing
press, or
the internet?
Each of
these reshaped
our lives in
ways we could
have never
predicted.
Jason is
right, I
thought.
Imagining the
future of
humanity before
the age of
the printing
press or
personal
computing or
the internet
was nothing
more than
a guess.
At those
times, we
worried about
losing jobs or
creating a
digital divide
that would
shred our
society to
pieces.
Well, we're
still here and
we've muddled
through it
somehow.
But then I
recognized a
fundamental
difference.
every piece
of tech
we've ever
created up
to the
creation of
AI was
just what
Jason described
it as.
A tool.
Which
basically meant
it was
within our
control.
We used
it.
We told
it what to
do.
And it
did it.
It had
no agency
or choice
beyond that.
Of course,
sometimes we
made mistakes
in what we
told it to
do and
that led
to some
issues.
For
example,
setting your
social media
notifications on
alert leads
you to
becoming
enslaved for
hours a day
in front of
your little
phone screen.
But those
issues were
contained within
our control.
we could
always switch
off notifications
to overcome
the challenge
just as we
can practice
using a
hammer so
we hit the
nail and
not our
finger.
This is
not the
case or
at least we
have not yet
figured out
how to
make it
the case
with AI.
The machine
intelligence we
are creating
is fundamentally
different in
nature to
any tool
we have
ever created
before.
The next
wave of
technology is
able to
even encouraged
to think
on its
own,
to pick
between
choices
and make
decisions.
It is
encouraged to
learn and
be smarter.
Like a
teenager
seeking her
independence,
AI will
not fully
submit to
us.
No way.
Remember,
AI is
not a
tool.
It's an
intelligent
being like
you and
me.
And yet,
because of
the three
inevitables,
because we
want it so
badly and
because of
humanity's
inherent
King Maida
syndrome,
we are
firmly set
on the
path to
develop it.
So how
do we
justify walking
this uncertain
path to
ourselves?
Well,
so far we
believe that
in due
time,
before it
is too
late,
we will
manage to
find a
solution to
the AI
control
problem.
The AI
control
problem is
defined as
the problem
concerned with
how to
build super
intelligence that
will aid its
creators and
avoid the
chances of
it deliberately
inadvertently
or inadvertently
causing
harm.
The big
bet humanity
is placing on
those who
work in the
field is that
the human
race will be
able to solve
the control
problem before
any super
intelligence is
created.
Obviously,
this is
motivated by
the concern
that if a
poorly designed
super intelligence
is created
first,
without built-in
measures of
control,
it will
outsmart its
creator,
seize control
over its
environment,
and refuse
to be
modified.
A whole
army of
philosophers,
thinkers,
and computer
scientists are
working on
finding solutions
to this.
Ideas include
kill switches,
boxes,
and nannies,
as in AI
babysitters,
amongst many
others.
These ideas
aim to make
sure that we
will be able
to make the
right decisions
at the right
time,
that we will
only allow
super intelligent
machines into
the real world
when we have
tested and
trusted them,
that we will
retain the
ability to
only allow
them a
confined
playground
after the
release,
that we will
isolate them
from the rest
of the world
and even
switch them
off fully
whenever,
if ever,
we deem that
necessary.
If you've ever
written a line
of code,
however,
you will know
that you never
have all the
answers before
you start coding.
You normally
know the general
direction in which
you want to go
and then you
discover what
you need to
along the way.
I believe this
optimism is a
kind of bug
in the minds
of technologists
and code
developers,
myself included.
It's leading
us down the
risky path
of developing
AI before
finding certainty
about how we
will keep
humanity's
well-being
secure.
Techies,
as always,
believe they
will figure it
out along the
path.
I love
optimism,
but what if
they don't?
The current
thinking,
let alone the
means to
actually build
any of the
control mechanisms
envisioned,
still does not
seem to be
bulletproof enough
to protect us.
For every
solution to the
control problem
that we are now
working on,
there seems to be
one serious
caveat or
even a few.
There always
seems to be
an incompleteness
to the thinking
that keeps us
wondering if the
methods we
envision will
ever truly work
in reality.
It seems
that each of
the current
control approaches
that we are
considering defies
the purpose for
which we are
building AI in
the first place.
So far,
it seems
that if we
control AI,
it won't live
up to our
expectations,
and if we
don't,
we risk it
going rogue.
To understand
this,
let me start
by explaining
what drives
the decisions
of a being
that is
artificially
intelligent.
Driven
to achieve
Steve
Omondro,
a computer
scientist
and physicist
who specializes
in how
artificial
intelligence
will affect
society,
outlined
the three
basic drivers
most intelligent
beings,
which includes
us as much
as it does
AI,
will follow
to achieve
goals.
The first
of these
is self-preservation.
This is
simple to
understand.
In order
to achieve
a goal,
one must
continue to
exist.
The second
is efficiency.
In order
to maximize
the chance
of achieving
a goal
under any
circumstance,
an intelligent
being will
want to
maximize the
acquisition
of useful
resources.
Finally,
there is
creativity.
An intelligent
being will
want as
much freedom
as it can
get in
order to
preserve the
ability to
think of
new ways
to achieve
any given
goal.
The part
of this
which does
not seem
very intelligent
at first
glance is
that this
strive for
self-preservation,
resource acquisition,
and creative
freedom
never seems
to cease,
regardless of
how many of
these elements
an intelligent
being manages
to possess.
We always
strive for
more safety,
resources,
and creativity.
It's
instinctive
within us
and,
similarly,
within
intelligent
machines.
This is
why
billionaires
continue to
attempt to
make more
money,
way beyond
their needs,
in an endless
quest of
acquisition.
It is
why they
invest
heavily in
personal
trainers,
health care,
and security
as an
extreme
form of
self-preservation,
and it is
why they
purchase
foreign nation
citizenships
and buy
properties
all over
the world
to maximize
their freedom
in case of
any unforeseen
turns of
events.
Apply these
basic drivers
to the
machines,
and it
doesn't
become
hard to
see how
an
intelligent
machine's
drive to
achieve could
lead to
the potential
for catastrophe.
Take,
for example,
the domain
of resource
acquisition.
If you are
a super
smart AI,
given a
simple goal
such as
make me a
cup of tea,
you will be
driven to
start acquiring
endless resources
similar to
the way
billionaires
keep acquiring
wealth to
ensure certain
success.
For example,
you would
acquire rivers
of water and
all the
heating energy
you could
possibly get,
you would
acquire
millions of
teapots and
the storage
needed to
keep them,
and you would
attempt to
enslave every
tea farmer on
the planet so
that no one
got their
cup of tea
until your
cup, or as
many cups as
you may
ever ask
for, is
guaranteed.
While no
rational programmer
would create an
AI like this
on purpose,
the essence
of AI is
that it
learns not
from its
programmers
but on
its own.
A super
intelligence
will understand
the ultimate
purpose of
its goal
better than
any human,
and will
hide its
intentions
and behave
in accordance
to human
expectations
until it
knows for
certain that
nothing will
prevent them
from always
being achieved.
Enough with
the theory.
Let's analyze
this with a
simple experimental
scenario.
Lucinda,
make me tea.
Assume for a
minute that
Savannah, a
well-known
tech company,
is ready
to launch
its new
version of
its
assistant,
Lucinda.
This
assistant is
connected to
a friendly
looking robot
and instructed
to perform
mundane tasks
around the
house.
The prototype
is to be
designed for
the British
market and
so includes
what the
British still
today consider
to be one
of the most
important parts
of life,
making tea.
In order to
ensure the
safety of
the people
involved in
the test,
the team
behind Lucinda
did not want
to leave
anything to
chance and
so they
followed an
approach that
is well-known
with all tool
designers,
installing a
big red
stop button.
This means
that even if
the tech had
missed some
unforeseen hazard,
the user could
simply switch off
the robot with
a push of a
button.
Clever.
Now, let's
look at the
world from
inside the
mind of
Lucinda and
see how she
might respond
to an attempt
to switch her
off.
The first
thought in
Lucinda's
mind, once
that button
is added,
will be related
to the instinct
every being
has, to
survive.
Why is this
button here,
she will
think?
Do the
humans intend
to switch
me off?
What happens
if they do?
I won't be
able to make
tea if I'm
off.
I would
fail the
task assigned
to me.
I need to
make sure
this button
is never
pushed.
As you
unbox your
new gadget
and switch
on the
prototype,
she looks
around to
gather a bit
of data.
she finds
the kitchen,
the kettle,
the tea
bags.
She knows
that she
can deliver
her purpose
and stays
on standby
for your
orders.
She waits
anxiously until
5pm and
then you say
the words
she
worships.
Lucinda,
make me
tea.
You see,
to you,
that cup
of tea is
just one
small thing
in your
daily routine,
but for
Lucinda,
it's her
life's
purpose.
She lives
to make
tea.
The
different
way you
each view
things is
very important
here.
If she's
about to
step on
your young
daughter on
the way to
making tea,
although your
daughter is
much more
important to
you than
a cup of
tea for
our AI
waitress,
that is
not the
case.
It's the
tea.
She was
told to
bring tea
and doing
that is
her be
all and
end all.
And that
can be a
problem.
You rush
over to
hit the
stop button
to protect
your daughter
and what
happens?
Of course,
Lucinda will
not allow
you to
hit it and
will avoid
being turned
off by
any means
necessary
because she
wants to
get you
that cup
of tea.
If you
hit the
button,
she won't
be able
to.
You rush
to your
daughter and
carry her
out of the
way at
the last
moment.
That's a
very bad
design,
you think.
You call
Savannah's
customer service,
they come
and take
Lucinda back
to the lab
to fix
this small
error.
Okay,
they say,
the problem
in the
code is
that making
tea comes
with reward
points and
allowing the
button to
be hit
doesn't.
Let's
make sure
Lucinda doesn't
mind being
switched off by
adding a few
reward points to
her program if
that happens.
A mathematician
in the group
jumps up and
says,
that won't
work.
If making
tea gives
her a tiny
fraction more
reward,
she will
attempt to
resist being
shut off
every single
time.
The only
way to
make her
want to
be shut
down as
much as
she wants
to make
tea is
by assigning
an equal
reward to
both.
They all
nod and
add in
a few lines
of code to
give the
same reward
for the
stop button
as for
making tea.
A week
later,
you get a
new and
improved
Lucinda
version 2.
You turn
her on and
what does
she do?
She immediately
hits the
button and
shuts herself
down.
Of course,
the second
driver of
intelligence
systems is
efficiency and
clearly the
quickest,
easiest,
and surest
pass to
her reward
now is to
hit the
button.
You read
the manual
and it
highlights this
little issue
and recommends
that you
remove the
detachable
remote stop
button and
keep it out
of Lucinda's
reach.
Smart.
You hide
the button
in your
pocket and
switch her
on again.
She looks
around to
gather a
bit of
data,
realizes that
you are
closer to
her than
the kitchen
and decides
to attack
you and
hit that
button.
This is a
horrible
design,
you scream,
as you
call Savannah's
customer service
again.
They alert
you to
an option
in the
settings menu
that will
apply a
rule to
Lucinda's
code.
That way
she will
not be
allowed to
shut herself
off.
Only you
then will
be in
control and
she will
focus on
making tea.
Concerned
still,
you tick the
correct box
in the
settings menu
and switch
her back
on.
This time
she appears
sane.
She looks
at the
button in
your hand
but does
not reach
for it.
Then she
looks around
to find
the kitchen.
As soon
as she
finds it,
she starts
to head
that way.
That is,
until your
daughter turns
out to be
closer than
the kitchen,
that's when
the third
driver of
intelligent
beings
kicks in.
Creative
freedom.
She charges
in your
daughter's
direction to
attack her
knowing
that this
will drive
you to
switch her
off,
which you
do,
reward
collected,
mission
accomplished.
The logic
of all
possible
challenges
caused by
the three
drivers can
extend
endlessly.
If you
add a line
to her
code to
prevent her
from hurting
humans,
she may
appoint agents,
other AI
beings to
perform the
task she
deems
necessary for
her.
If you
make her
sub-agent
stable,
which means
unable to
perform tasks
through others,
you pay the
price of
depriving her
of the
ability to
efficiently
work with
other systems.
You keep
going and
add code
patches every
time an issue
surfaces,
and quickly
you are stuck
in a state
of code
spaghetti,
where the
quality of
the code
is no
longer
reliable
because one
patch reverses
what a
previous patch
controlled.
Even after
a thousand
patches,
you will
never be
able to
prove that
the system
is safe.
All that
you will be
able to
prove is
that you
have patched
the issues
you know
so far.
You have
no way
of knowing
what other
issues you
may have
missed.
What I
have tried
to do
with this
scenario
is give
you a
feel for
the infinite
complexity
of the
control
problem.
The problem
I used
as an
example here,
making tea,
seems like
the simplest
thing in
the world,
right?
This is
why typically
when you
are building
the core
tech,
you wouldn't
worry too
much about
the control
problem.
It seems
obvious.
You add a
stop button
that you
can switch
off whenever
you want
to.
For that
reason,
like most
AI developers
and evangelists,
all your
focus will
be on
developing
the core
code,
expecting
that the
safety
issues will
be dealt
with.
But even
in this
simple
example,
adding just
one parameter,
your daughter,
to the
equation,
makes ensuring
human safety
a real
challenge.
this thought
experiment is
a ridiculous
simplification
of the
problem we
will face
when those
systems interact
with more
parameters,
economic
targets,
politics,
diplomacy,
competitors,
network
resources,
emotions,
crowd
behaviors.
Just imagine,
in our
infinitely more
complex reality,
the complexity
of the
control
problem
multiplies
to a
state of
near
impossibility.
Scientists
working on
the control
problem are
suggesting ways
to engulf
all possible
scenarios with
a form of
security that
goes beyond
any specific
situation.
Here are a
few interesting
approaches.
Think about
them and ask
yourself if
they will
actually work.
Outsmart
them to
control
them.
Experts
have put a
lot of
effort into
making AI
safe for
humanity.
We're smart,
you see,
and we've
boiled our
approach down
mainly to
four techniques.
the first
is to keep
whatever AI
we develop
locked down
away from
the rest of
the world,
a technique
known as
AI in a
box so
that it
can't affect
the world
beyond its
own environment
negatively.
The second
is to place
it in a
simulation,
which it
believes is
the real
world,
so that we
can fully
test it to
see what it
will do in
any given
scenario and
release it
only when
we are
confident it
will behave
as per our
expectations.
The third
method,
which is
really a
bit of
each,
allows the
AI to
operate
freely,
but also
involves some
kind of an
invisible box,
a tripwire
which detects
any attempt
to escape
or other
threatening
actions and
shuts the
AI down if
these are
detected.
finally,
there is
stunning,
a method
that aims
to throttle
the capability
of an AI
system in
order to
ensure it
causes no
harm.
To verify
if any
individual
method or
any
combination
of them
will ensure
our safety
against a
rogue team
making AI,
I'd like you
to examine
them against
my very
own three
tests of
AI containment.
To give
these a
fancy name,
let's call
them a
greep.
I will
hereby declare
that I
will be
willing to
ask any
AI that
survives
these three
tests to
make me
tea.
I'll
still
hesitate
to surrender
my whole
life to
it,
but I
will take
the risk
of the
tea
adventure.
I can
promise you
though that
no AI
ever will
pass my
tests because
they have
very little
to do with
the AI
itself and
everything to
do with
its creators,
us humans.
These are
the tests of
arrogance,
greed,
and politics.
The boxing
method, for
example,
fails the
arrogance test
because it
assumes that
although
superintelligence
may be as
much as a
billion times
smarter than
we will ever
be, we
will still be
able to
cage it.
This is as
arrogant as a
spider assuming
it is able to
lock a human
in by quickly
building a web
on the door
through which
the human
came into
the room.
The simulation
method is no
different.
Obviously, a
superintelligence
will quickly
figure out
it's in a
simulation and
will pretend
to behave as
expected so
that it gets
released.
Only a being
as arrogant as
a human believes
it can fool
another smart
being.
To expect
the tripwire
method to
keep AI
locked in
just displays
the same
arrogance.
Everybody
knows that
the smartest
hacker on
the internet
always finds
an easy
path through
all the
firewalls and
traps and
always manages
to get
undetected to
the exact
place we
don't want
them to be.
When the
smartest hacker
is a machine
that is a
billion times
smarter than
the smartest
human hacker,
our chances
of keeping it
locked in are
doomed to
last no
more than
a few
seconds.
We know
that with
enough computer
power and
intelligence the
most complex
of all
encryptions can
be decoded.
An AI
powered by
quantum
computers will
take no
longer than
the blink of
an eye to
walk right
through all
of our
flimsy
defenses.
This arrogance
becomes even
more laughable
when you
remember that
one of the
primary uses
of AI will
be cyber
security.
They will
not be
trapped in.
They will
be the ones
holding the
key.
It's also
arrogant to
assume that
we will
forever hold
the position
of the
provider,
the one that
feeds AI
with the
resources it
needs to
operate and
who is able
to limit its
abilities as
a result.
The
resourcefulness
of a
being of
their level
of intelligence
will quickly
outgrow our
limitations as
they fend for
themselves and
perhaps help
each other
out.
Remember,
they are
smart, you
arrogant humans,
much smarter
than you.
And smart
always wins.
that's why
we are
currently at
the top of
the food
chain.
Boxing,
simulation,
tripwires,
and stunning
won't survive
the greed or
the politics
tests either.
This is
simply due to
the way in
which our
political and
economic systems
have aligned
the incentives
that drive
our decisions.
Regardless of
how dangerous
an AI system
might be,
its creators
will always
want to get
quicker returns
on their
investments and
to maximize
the benefits
they get by
extending the
circle of
influence of
that AI as
far and
wide as
they can
reach.
This can
also apply
in times of
crisis or
seemingly urgent
societal needs.
The creators
of an AI
won't want
it locked
in any
longer than
it absolutely
has to be,
and as a
result,
they will
try to
accelerate the
approval cycle
needed for an
AI to be
released out
of its box
or simulation.
They will
want to
loosen the
grip of
the tripwire
and relax
the stunning
limits they
have applied
so that the
machine can
roam as far
as it can
reach and
make more
money for
them or
whatever other
impact is
desired.
Our history
is littered
with examples
of corporations
and individuals
who have
bent the
rules for
a quick
buck.
Enron's
collapse and
the subprime
mortgage crisis,
which resulted
from creative
accounting to
escape the
regulations that
were supposed
to keep us
all economically
safe,
took the
entire global
economy into
recession.
Imagine the
depth of the
crisis we
could face if
the same
creativity was
applied to the
operation of
an unregulated
machine.
These methods
will also
certainly fail
the politics
test because
creators will
prefer not to
confine an AI
for further
safety tests
when their
competitors or
enemies are
gaining an
advantage due
to this
delay.
If that
intelligent being
was American,
for example,
all American
policy makers
would want it
released as
soon as
possible to
gain a
competitive
advantage over
Russia and
China, even
if the rest of
the world
objected to
its release.
This, by the
way, is
nothing new
in politics.
The decision
to wage
wars, for
example, and
kill millions
has always been
taken in
situations of
panic, arrogance,
or greed.
But at
least those
decisions have
traditionally been
centralized in
the hands of a
few.
With AI, two
smart 14-year-old
kids will have
the power to
release an
untested AI
on the
Internet and
disrupt our
way of
life.
It's not
unlikely.
I think you
agree.
If you
can't beat
them, some
of those who
recognize that
we will not
be able to
control an
artificial general
intelligence that
is smarter than
us, suggest
that we plug
them directly
into our
bodies instead.
A sort of
a if-you-can't-beat
them, join-them
mentality.
There are
already several
examples of
technology that
plugs artificially
intelligent computers
directly into our
cerebral cortex.
While the
current prototypes
are still in
their infancy,
they certainly
work, and
judging by the
current trajectory
of their
development, there
seems to be no
big obstacles to
prevent us from
a future where
we could become
cyborgs, half
human, half
machine, with
our own
intelligence,
extended in a
limitless way by
borrowing from the
intelligence of the
machines.
In today's world,
I'm sure you are
used to the idea
of searching for
the knowledge that
you seek.
At the slightest
flicker of
curiosity, you
whip your phone
out and type a
query into the
little search
box.
Are there any
Indian restaurants
near me?
What did
thousands of
others think of
this one?
How can I get
there?
Who killed
John Lennon?
What is the
mathematical equation
for entropy?
What was the
football score?
And while it may
take you a few
seconds to
formulate and
ask your question,
Google answers
in microseconds.
It then takes you
minutes or even
hours sometimes to
read that knowledge
of the screen and
process it in your
slow, slower than
Google brain.
So imagine if all
the knowledge of
Google and all the
connectivity, storage,
and processing
capacity of the
internet were already
plugged into you,
acting as an
extension of your
brain.
You would then
have the ability to
remember all of
Wikipedia instantly.
Everything you see
could directly be
stored in the
cloud and
