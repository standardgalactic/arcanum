This is Audible.
Python Machine Learning.
Beginner's Guide to Get You Started with Machine Learning and Deep Learning with Python.
Written by Scott Harvey.
Narrated by Russell Newton.
I probably don't need to tell you that machine learning has become one of the most exciting technologies of our time and age.
Big companies such as Google, Facebook, Apple, Amazon, IBM, and many more heavily invest in machine learning research and applications for good reasons.
Although it may seem that machine learning has become the buzzword of our time and age, it is certainly not a hype.
This exciting field opens the way to new possibilities and has become indispensable to our daily lives.
Talking to the voice assistant on our smartphones, recommending the right product for our customers, stopping credit card fraud, filtering out spam from our email inboxes, detecting and diagnosing medical diseases, the list goes on and on.
If you want to become a machine learning practitioner, a better problem solver, or maybe even consider a career in machine learning research, then this book is for you.
However, for a novice, the theoretical concepts behind machine learning can be quite overwhelming.
Yet, many practical books that have been published in recent years will help you get started in machine learning by implementing powerful learning algorithms.
In my opinion, the use of practical code examples serve an important purpose.
They illustrate the concepts by putting the learned material directly into action.
However, remember that with great power comes great responsibility.
The concepts behind machine learning are too beautiful and important to be hidden in a black box.
Thus, my personal mission is to provide you with a different book.
A book that discusses the necessary details regarding machine learning concepts offers intuitive yet informative explanations on how machine learning algorithms work,
how to use them, and most importantly, how to avoid the most common pitfalls.
If you type machine learning as a search term in Google Scholar, it returns an overwhelmingly large number, 1,800,000 publications.
Of course, we cannot discuss all the nitty-gritty details about all the different algorithms and applications that have emerged in the last 60 years.
However, in this book, we will embark on an exciting journey that covers all the essential topics and concepts and gives you a head start in this field.
If you find that your thirst for knowledge is not satisfied, there are many useful resources that can be used to follow up on the essential breakthroughs in this field.
If you have already studied machine learning theory in detail, this book will show you how to put your knowledge into practice.
If you have used machine learning techniques before and want to gain more insight into how machine learning really works, this book is for you.
Don't worry if you are completely new to the machine learning field.
You have even more reason to be excited.
I promise you that machine learning will change the way you think about the problems you want to solve
and will show you how to tackle them by unlocking the power of data.
Before we dive deeper into the machine learning field, let me answer your most important question.
Why Python?
The answer is simple.
It is powerful, yet very accessible.
Python has become the most popular programming language for data science because it allows us to forget about the tedious parts of programming
and offers us an environment where we can quickly jot down our ideas and put concepts directly into action.
Reflecting on my personal journey, I can truly say that the study of machine learning has made me a better scientist, thinker, and problem solver.
In this book, I want to share this knowledge with you.
Knowledge is gained by learning.
The key is our enthusiasm, and the true mastery of skills can only be achieved by practice.
The road ahead may be bumpy on occasions, and some topics may be more challenging than others,
but I hope that you will embrace this opportunity and focus on the reward.
Remember that we are on this journey together, and throughout this book,
we will add many powerful techniques to your arsenal that will help us solve even the toughest problems the data-driven way.
Data is the new oil, and machine learning is a powerful concept and framework for making the best out of it.
In this age of automation and intelligent systems, it is hardly surprising that machine learning and data science are some of the top buzzwords.
The tremendous interest and renewed investments in the field of data science across industries, enterprises, and domains are clear indicators of its enormous potential.
Intelligent systems and data-driven organizations are becoming a reality, and the advancements in tools and techniques is only helping it expand further.
With data being of paramount importance, there has never been a higher demand for machine learning and data science practitioners than there is now.
Indeed, the world is facing a shortage of data scientists.
It's been coined the sexiest job in the 21st century, which makes it all the more worthwhile to try to build some valuable expertise in this domain.
Practical Machine Learning with Python is a problem solver's guide to building real-world intelligent systems.
It follows a comprehensive, three-tiered approach packed with concepts, methodologies, hands-on examples, and code.
This book helps its readers master the essential skills needed to recognize and solve complex problems with machine learning and deep learning by following a data-driven mindset.
Using real-world case studies that leverage the popular Python machine learning ecosystem, this book is your perfect companion for learning the art and science of machine learning to become a successful practitioner.
The concepts, techniques, tools, frameworks, and methodologies used in this book will teach you how to think, design, build, and execute machine learning systems and projects successfully.
This book will get you started on the ways to leverage the Python machine learning ecosystem with its diverse set of frameworks and libraries.
The three-tiered approach of this book starts by focusing on building a strong foundation around the basics of machine learning and relevant tools and frameworks.
The next part emphasizes the core processes around building machine learning pipelines, and the final part leverages this knowledge on solving some real-world case studies from diverse domains, including retail, transportation, movies, music, computer vision, art, and finance.
We also cover a wide range of machine learning models, including regression, classification, forecasting, rule mining, and clustering.
This book also touches on cutting-edge methodologies and research from the field of deep learning, including image classification and neural style transfer.
The main intent of this book is to give a wide range of readers, including IT professionals, analysts, developers, data scientists, engineers, and graduate students,
a structured approach to gaining essential skills pertaining to machine learning and enough knowledge about leveraging state-of-the-art machine learning techniques and frameworks so that they can start solving their own real-world problems.
This book is application-focused, so it's not a replacement for gaining a deep conceptual and theoretical knowledge about machine learning algorithms, methods, and their internal implementations.
We strongly recommend you supplement the practical knowledge gained through this book with some of the standard books on data mining, statistical analysis, and theoretical aspects of machine learning algorithms and methods to gain deeper insights into the world of machine learning.
Machine Learning Basics
The idea of making intelligent, sentient, and self-aware machines is not something that suddenly came into existence in the last few years.
In fact, a lot of lore from Greek mythology talks about intelligent machines and inventions having self-awareness and intelligence of their own.
The origins and evolution of the computer have been really revolutionary over a period of several centuries, starting from the basic abacus and its descendant, the slide rule in the 17th century, to the first general-purpose computer designed by Charles Babbage in the 1800s.
In fact, once computers started evolving with the invention of the analytical engine by Babbage and the first computer program, which was written by Ada Lovelace in 1842, people started wondering and contemplating that there could be a time when computers or machines truly become intelligent and start thinking for themselves.
In fact, the renowned computer scientist Alan Turing was highly influential in the development of theoretical computer science, algorithms, and formal language, and addressed concepts like artificial intelligence and machine learning as early as the 1950s.
This brief insight into the evolution of making machines learn is just to give you an idea of something that has been out there since centuries, but has recently started gaining a lot of attention and focus.
With faster computers, better processing, better computation power, and more storage, we have been living in what I like to call the age of information, or the age of data.
Day in and day out, we deal with managing big data and building intelligent systems by using concepts and methodologies from data science, artificial intelligence, data mining, and machine learning.
Of course, most of you must have heard many of the terms I just mentioned and come across sayings like data is the new oil.
The main challenge that businesses and organizations have embarked on in the last decade is to use approaches to try to make sense of all the data that they have and use valuable information and insights from it in order to make better decisions.
Indeed, with great achievements in technology, including availability of cheap and massive computing hardware, including GPUs and storage, we have seen a thriving ecosystem built around domains like artificial intelligence, machine learning, and most recently, deep learning.
Researchers, developers, data scientists, and engineers are working continuously around the clock to research and build tools, frameworks, algorithms, techniques, and methodologies to build intelligent models and systems
that can predict events, automate tasks, perform complex analyses, detect anomalies, self-heal failures, and even understand and respond to human inputs.
This chapter follows a structured approach to cover various concepts, methodologies, and ideas associated with machine learning.
The core idea is to give you enough background on why we need machine learning, the fundamental building blocks of machine learning,
and what machine learning offers us presently.
This will enable you to learn about how best you can leverage machine learning to get the maximum from your data.
Since this is a book on practical machine learning, while we will be focused on specific use cases, problems, and real-world case studies in subsequent chapters,
it is extremely important to understand formal definitions, concepts, and foundations
with regard to learning algorithms, data management, model building, evaluation, and deployment.
Hence, we will cover all these aspects, including industry standards related to data mining and hardware learning workflows,
so that it gives you a foundational framework that can be applied to approach and tackle any of the real-world problems we solve in subsequent chapters.
Besides this, we also cover the different interdisciplinary fields associated with machine learning,
which are in fact related fields all under the umbrella of artificial intelligence.
The Need for Machine Learning
Human beings are perhaps the most advanced and intelligent life form on this planet at the moment.
We can think, reason, build, evaluate, and solve complex problems.
The human brain is still something we ourselves haven't figured out completely,
and hence, artificial intelligence is something that's not surpassed human intelligence in several aspects.
Thus, you might get a pressing question in your mind as to why do we really need machine learning?
What is the need to go out of our way to spend time and effort to make machines learn and be intelligent?
The answer can be summed up in a simple sentence.
To make data-driven decisions at scale.
We will dive into details to explain this sentence in the following sections.
Making Data-Driven Decisions
Key information or insights from data is the key reason businesses and organizations invest heavily in a good workforce
as well as newer paradigms in domains like machine learning and artificial intelligence.
The idea of data-driven decisions is not new.
Fields like operations research, statistics, and management information systems have existed for decades
and attempt to bring efficiency to any business or organization by using data and analytics to make data-driven decisions.
The art and science of leveraging your data to get actionable insights and make better decisions is known as making data-driven decisions.
Of course, this is easier said than done because rarely can we directly use raw data to make any insightful decisions.
Another important aspect of this problem is that often we use the power of reasoning or intuition to try to make decisions based on what we have learned over a period of time and on the job.
Our brain is an extremely powerful device that helps us do so.
Consider problems like understanding what your fellow colleagues or friends are speaking, recognizing people in images, deciding whether to approve or reject a business transaction, and so on.
While we can solve these problems almost involuntarily, can you explain to someone the process of how you solved each of these problems?
Maybe to some extent, but after a while it would be like, hey, my brain did most of the thinking for me.
This is exactly why it is difficult to make machines learn to solve these problems like regular computational programs, like computing loan interest or tax rebates.
Solutions to problems that cannot be programmed inherently need a different approach, where we use the data itself to drive decisions instead of using programmable logic, rules, or code to make these decisions.
We discuss this further in future sections.
Efficiency and Scale
While getting insights and making decisions driven by data are of paramount importance, it also needs to be done with efficiency and at scale.
The key idea of using techniques from machine learning or artificial intelligence is to automate processes or tasks by learning specific patterns from the data.
We all want computers or machines to tell us when a stock might rise or fall, whether an image is of a computer or a television, whether our product placement and offers are the best, determine shopping price trends, detect failures or outages before they occur, and the list just goes on.
While human intelligence and expertise is something that we definitely can't do without, we need to solve real-world problems at huge scale with efficiency.
Why Machine Learning?
We will now address the question that started this discussion of why we need machine learning.
Considering what you have learned so far, while the traditional programming paradigm is quite good, and human intelligence and domain expertise is definitely an important factor in making data-driven decisions, we need machine learning to make faster and better decisions.
The machine learning paradigm tries to take into account data and expected outputs or results, if any, and uses the computer to build the program, which is also known as a model.
This program or model can then be used in the future to make necessary decisions and give expected outputs from new inputs.
The machine learning paradigm is similar yet different from traditional programming paradigms.
The machine learning paradigm, the machine, in this context the computer, tries to use the input data and expected outputs to try to learn inherent patterns in the data that would ultimately help in building a model analogous to a computer program,
which would help in making data-driven decisions in the future, predict or tell us the output, for new input data points by using the learned knowledge from previous data points, its knowledge or experience.
You might start to see the benefit in this.
We would not need hand-coded rules, complex flowcharts, case and if-then conditions, if other criteria that are typically used to build any decision-making system or a decision-support system.
The basic idea is to use machine learning to make insightful decisions.
In the traditional programming approach, we talked about hiring new staff, setting up rule-based monitoring systems, and so on.
If we were to use a machine learning paradigm shift here, we could go about solving the problem using the following steps.
Leverage device data and logs to make sure we have enough historical data in some data store, database, logs, or flat files.
Decide key data attributes that could be useful for building a model.
This could be device usage, logs, memory, processor, connections, line strength, links, and so on.
Observe and capture device attributes and their behavior over various time periods that would include normal device behavior and anomalous device behavior or outages.
These outcomes would be your outputs, and device data would be your inputs.
Feed these input and output pairs to any specific machine learning algorithm in your computer,
and build a model that learns inherent device patterns and observes the corresponding output or outcome.
Deploy this model such that for newer values of device attributes, it can predict if a specific device is behaving normally or it might cause a potential outage.
Thus, once you are able to build a machine learning model, you can easily deploy it and build an intelligent system around it
such that you can not only monitor devices reactively, but you would be able to proactively identify potential problems
and even fix them before any issues crop up.
Imagine building self-heal or auto-heal systems coupled with round-the-clock device monitoring.
The possibilities are indeed endless, and you will not have to keep on hiring new staff every time you expand your office or buy new infrastructure.
Of course, the workflow discussed earlier with the series of steps needed for building a machine learning model
is much more complex than how it has been portrayed.
But again, this is just to emphasize and make you think more conceptually rather than technically
of how the paradigm has shifted in case of machine learning processes,
and you need to change your thinking too from the traditional-based approaches toward being more data-driven.
The beauty of machine learning is that it is never domain-constrained,
and you can use techniques to solve problems spanning multiple domains, businesses, and industries.
You always do not need output data points to build a model,
and sometimes input data is sufficient, or rather output data might not be present,
for techniques more suited toward unsupervised learning,
which we will discuss in depth later on in this chapter.
A simple example is trying to determine customer shopping patterns
by looking at the grocery items they typically buy together in a store based on past transactional data.
In the next section, we take a deeper dive toward understanding machine learning.
Building Machine Intelligence
The objective of machine learning, data mining, or artificial intelligence
is to make our lives easier, automate tasks, and make better decisions.
Building machine intelligence involves everything we have learned until now,
starting from machine learning concepts to actually implementing and building models
and using them in the real world.
Machine intelligence can be built using non-traditional computing approaches like machine learning.
In this case, we establish a full-fledged, end-to-end machine learning pipeline
based on the CRISP-DM model,
which will help us solve real-world problems by building machine intelligence
using a structured process.
Machine Learning Pipelines
The best way to solve a real-world machine learning or analytics problem
is to use a machine learning pipeline,
starting from getting your data to transforming it into information and insights
using machine learning algorithms and techniques.
This is more of a technical or solution-based pipeline,
and it assumes that several aspects of the CRISP-DM model are already covered,
including the following points.
Business and data understanding
ML-DM technique selection
Risk, assumptions, and constraints assessment
A machine learning pipeline will mainly consist of elements
related to data retrieval and extraction,
preparation, modeling, evaluation, and deployment.
The major steps in the pipeline are briefly mentioned here.
Data retrieval
This is mainly data collection, extraction, and acquisition
from various data sources and data stores.
Data preparation
In this step, we pre-process the data,
clean it, wrangle it, and manipulate it as needed.
Initial exploratory data analysis is also carried out.
Next steps involve extracting, engineering, and selecting features
or attributes from the data.
Data process and wrangling
Mainly concerned with data processing, cleaning,
munging, wrangling, and performing initial descriptive
and exploratory data analysis.
Feature extraction and engineering
Here we extract important features or attributes from the raw data
and even create or engineer new features from existing features.
Feature scaling and selection
Data features often need to be normalized and scaled
to prevent machine learning algorithms from getting biased.
Besides this, often we need to select a subset of all available features
based on feature importance and quality.
Modeling
In the process of modeling, we usually feed the data features
to a machine learning method or algorithm and train the model,
typically to optimize a specific cost function in most cases
with the objective of reducing errors
and generalizing the representations learned from the data.
Model Evaluation and Tuning
Built models are evaluated and tested on validation data sets
and, based on metrics like accuracy, F1 score, and others,
the model performance is evaluated.
Models have various parameters that are tuned in a process
called hyperparameter optimization
to get models with the best and optimal results.
Deployment and Monitoring
Selected models are deployed in production
and are constantly monitored based on their predictions and results.
Supervised Machine Learning Pipeline
By now, we know that supervised machine learning methods
are all about working with supervised labeled data
to train models and then predict outcomes for the new data samples.
Some processes like feature engineering, scaling, and selection
should always remain constant
so that the same features are used for training the model
and the same features are extracted from the new data samples
to feed the model in the prediction phase.
Unsupervised Machine Learning Pipeline
Unsupervised Machine Learning is all about extracting patterns,
relationships, association, and clusters from data.
The processes related to feature engineering, scaling, and selection
are similar to supervised learning.
However, there is no concept of pre-labeled data here.
Hence, the unsupervised Machine Learning Pipeline
would be slightly different in contrast to the supervised pipeline.
Understanding Machine Learning
By now, you have seen how a typical real-world problem
suitable to solve using machine learning might look like.
Besides this, you have also got a good grasp
over the basics of traditional programming
and machine learning paradigms.
In this section, we discuss machine learning in more detail.
To be more specific, we will look at machine learning
from a conceptual as well as a domain-specific standpoint.
Machine learning came into prominence perhaps in the 1990s
when researchers and scientists started giving it more prominence
as a subfield of artificial intelligence, AI,
such that techniques borrow concepts from AI, probability, and statistics,
which perform far better compared to using fixed rule-based models
requiring a lot of manual time and effort.
Of course, as we have pointed out earlier,
machine learning didn't just come out of nowhere in the 1990s.
It is a multidisciplinary field that has gradually evolved over time
and is still evolving as we speak.
A brief mention of history of evolution would be really helpful
to get an idea of the various concepts and techniques
that have been involved in the development of machine learning and AI.
You could say that it started off in the late 1700s and the early 1800s
when the first works of research were published,
which basically talked about the Bayes' theorem.
In fact, Thomas Bayes' major work,
An Essay Towards Solving a Problem in the Doctrine of Chances,
was published in 1763.
Besides this, a lot of research and discovery was done
during this time in the field of probability and mathematics.
This paved the way for more groundbreaking research and inventions
in the 20th century, which included Markov chains by Andrei Markov
in the early 1900s, proposition of a learning system by Alan Turing,
and the invention of the very famous Perceptron by Frank Rosenblatt in the 1950s.
Many of you might know that neural networks had several highs and lows
since the 1950s, and they finally came back to prominence in the 1980s
with the discovery of backpropagation, thanks to Rumelhart, Hinton, and Williams,
and several other inventions, including Hopfield networks,
neocognition, convolutional and recurrent neural networks, and Q-learning.
Of course, rapid strides of evolution started taking place in machine learning, too,
since the 1990s, with the discovery of random forests,
support vector machines, long short-term memory networks, LSTMs,
and development and release of frameworks in both machine and deep learning,
including Torch, Theano, TensorFlow, Scikit-Learn, and so on.
We also saw the rise of intelligent systems, including IBM Watson, DeepFace, and AlphaGo.
Indeed, the journey has been quite a rollercoaster ride,
and there are still miles to go in this journey.
Take a moment and reflect on this evolutional journey,
and let's talk about the purpose of this journey.
Why and when should we really make machines learn?
Why make machines learn?
We have discussed a fair bit about why we need machine learning in a previous section
when we addressed the issue of trying to leverage data to make data-driven decisions at scale
using learning algorithms without focusing too much on manual efforts in fixed, rule-based systems.
In this section, we discuss in more detail why and when should we make machines learn?
There are several real-world tasks and problems that humans, businesses, and organizations
try to solve day in and day out for our benefit.
There are several scenarios when it might be beneficial to make machines learn,
and some of them are mentioned as follows.
Lack of sufficient human expertise in a domain,
e.g. simulating navigations in unknown territories or even spatial planets.
Scenarios and behavior can keep changing over time,
e.g. availability of infrastructure in an organization, network connectivity, and so on.
Humans have sufficient expertise in the domain,
but it is extremely difficult to formally explain or translate this expertise into computational tasks,
e.g. speech recognition, translation, scene recognition, cognitive tasks, and so on.
Addressing domain-specific problems at scale with huge volumes of data
with too many complex conditions and constraints.
The previously mentioned scenarios are just several examples
where making machines learn would be more effective than investing time, effort, and money
in trying to build subpar intelligence systems
that might be limited in scope, coverage, performance, and intelligence.
We as humans and domain experts already have enough knowledge about the world
and our respective domains, which can be objective, subjective, and sometimes even intuitive.
With the availability of large volumes of historical data,
we can leverage the machine learning paradigm to make machines perform specific tasks
by gaining enough experience by observing patterns in data over a period of time,
and then use this experience in solving tasks in the future with minimal manual intervention.
The core idea remains to make machines solve tasks that can be easily defined intuitively
and almost involuntarily, but extremely hard to define formally.
Data Science
The field of data science is a very diverse, interdisciplinary field,
which encompasses multiple fields.
Data science basically deals with principles, methodologies, processes, tools, and techniques
to gather knowledge or information from data, structured as well as unstructured.
Data science is more of a compilation of processes, techniques, and methodologies
to foster a data-driven, decision-based culture.
Basically, there are three major components, and data science sits at the intersection of them.
Math and statistics knowledge is all about applying various computational and quantitative math
and statistical-based techniques to extract insights from data.
Hacking skills basically indicate the capability of handling, processing, manipulating, and wrangling data
into easy-to-understand and analyzable formats.
Substantive expertise is basically the actual real-world domain expertise,
which is extremely important when you are solving a problem,
because you need to know about various factors, attributes, constraints, and knowledge
related to the domain besides your expertise in data and algorithms.
Thus, Drew rightly points out that machine learning is a combination of expertise
on data hacking skills, math, and statistical learning methods.
And for data science, you need some level of domain expertise and knowledge,
along with machine learning.
You can check out Drew's personal insights in his article at
where it talks all about the data science Venn diagram.
Besides this, we also have Brendan Tierney, who talks about the true nature of data science
being a multidisciplinary field with his own depiction.
If you observe his depiction closely, you'll see a lot of the domains mentioned here.
You can clearly see data science being the center of attention
and drawing parts from all the other fields and machine learning as a subfield.
Statistics
The field of statistics can be defined as a specialized branch of mathematics
that consists of frameworks and methodologies to collect, organize, analyze, interpret, and present data.
Generally, this falls more under applied mathematics
and borrows concepts from linear algebra, distributions, probability theory, and inferential methodologies.
There are two major areas under statistics that are mentioned as follows.
The core component of any statistical process is data.
Hence, typically, data collection is done first,
which could be, in global terms, often called a population,
or a more restricted subset due to various constraints, often known as a sample.
Samples are usually collected manually from surveys, experiments, data stores, and observational studies.
From this data, various analyses are carried out using statistical methods.
Descriptive statistics is used to understand basic characteristics of the data
using various aggregation and summarization measures to describe and understand the data better.
These could be standard measures like mean, median, mode, skewness, kurtosis, standard deviation, variance, and so on.
You can refer to any standard book on statistics to dive deep into these measures if you're interested.
The following snippet depicts how to compute some essential descriptive statistical measures.
First, the code to produce an array of 15 numbers, integers, between 1 and 20.
Nums equals np.random.randint open parens 1,20
comma size equals open parens 1,15 close parens close parens
open bracket 0 close bracket
We can get descriptive statistics about this array of numbers through the following print command
For the mean, print open parens
quote mean close quote comma
sp.mean open parens
close parens close parens close parens
This command would yield a median of 12.33333
and so on with median, mode, standard deviation, variance, skew, and kurtosis
Libraries and frameworks like pandas, scipy, and numpy
in general help us compute descriptive statistics and summarize data easily in Python
Inferential statistics are used when we want to test hypothesis, draw inferences, and conclusions about various characteristics
about our data sample or population
Frameworks and techniques like hypothesis testing, correlation and regression analysis, forecasting, and predictions
are typically used for any form of inferential statistics
We look at this in much detail in subsequent chapters when we cover predictive analytics as well as time series-based forecasting
Data Mining
The field of data mining involves processes, methodologies, tools, and techniques to discover and extract patterns, knowledge, insights, and valuable information from non-trivial data sets
Data sets are defined as non-trivial when they are substantially huge, usually available from databases and data warehouses
Once again, data mining itself is a multidisciplinary field incorporating concepts and techniques from mathematics, statistics, computer science, databases, machine learning, and data science
The term is a misnomer in general since the mining refers to the mining of actual insights or information from the data and not the data itself
In the whole process of KDD or knowledge, discovery, and databases, data mining is the step where all the analysis takes place
In general, both KDD as well as data mining are closely linked with machine learning since they are all concerned with analyzing data to extract useful patterns and insights
Hence, methodologies, concepts, techniques, and processes are shared among them
The standard process for data mining followed in the industry is known as the CRISP-DM model
Artificial Intelligence
The field of artificial intelligence encompasses multiple subfields including machine learning, natural language processing, data mining, and so on
It can be defined as the art, science, and engineering of making intelligent agents, machines, and programs
The field aims to provide solutions for one simple yet extremely tough objective
Can machines think, reason, and act like human beings?
AI, in fact, existed as early as the 1300s when people started asking such questions
And conducting research and development on building tools that could work on concepts instead of numbers, like a calculator does
Progress in AI took place in a steady pace with discoveries and inventions by Alan Turing, McCulloch, and Pitts artificial neurons
AI was revived once again after a slowdown till the 1980s with success of expert systems
The resurgent of neural networks thanks to Hopfield, Rumelhart, McClellan, Hinton, and many more
Faster and better computation thanks to Moore's Law led to fields like data mining, machine learning, and even deep learning
Come into prominence to solve complex problems that would otherwise have been impossible to solve using traditional approaches
Some of the main objectives of AI include emulation of cognitive functions, also known as cognitive learning, semantics, and knowledge representation
Learning, reasoning, problem solving, planning, and natural language processing
AI borrows tools, concepts, and techniques from statistical learning, applied mathematics, optimization methods, logic, probability theory, machine learning, data mining, pattern recognition, and linguistics
AI is still evolving over time, and a lot of innovation is being done in this field
Including some of the latest discoveries and inventions like self-driving cars, chatbots, drones, and intelligent robots
Natural language processing
The field of natural language processing is a multidisciplinary field combining concepts from computational linguistics, computer science, and artificial intelligence
NLP involves the ability to make machines process, understand, and interact with natural human languages
The major objective of applications or systems built using NLP is to enable interactions between machines and natural languages that have evolved over time
Major challenges in this aspect include knowledge and semantics representation, natural language understanding, generation, and processing
Some of the major applications of NLP are mentioned as follows
Machine translation
Speech recognition
Question answering systems
Context recognition and resolution
Text summarization
Text categorization
Information extraction
Sentiment and emotion analysis
Topic segmentation
Using techniques from NLP and text analytics, you can work on text data to process, annotate, classify, cluster, summarize, extract semantics, determine sentiment, and much more
The following example snippet depicts some basic NLP operations on textual data, where we annotate a document, text sentence, with various components like parts of speech, phrase level tags, and so on, based on its constituent grammar
Comment
Print the constituency parse tree
Print
Open parens tree
Open parens root
Open parens np
Open parens np
Open parens dt
space the
Close parens
Open parens
JJ
space quick
Close parens
Open parens
JJ
space brown
Close parens
Open parens
nn
space fox
Close parens
Close parens
Close parens
Open parens
np
open parens
np
space
open parens
nns
space
jump
Close parens
Close parens
Close parens
Open parens
PP
open parens
in
space
over
close parens
Open parens
np
space
open parens
dt
space
the
Close parens
space
open parens
jj space lazy close parens open parens nn space dog close parens close parens close parens close
parens close parens close parens close parens comment visualize constituency parse tree tree
dot draw open parens close parens the constituency grammar-based parse tree for our sample sentence
which consists of multiple noun phrases np each phrase has several words that are also annotated
with their own parts of speech pos tags we cover more on processing and analyzing textual data for
various steps in the machine learning pipeline as well as practical use cases in subsequent chapters
deep learning the field of deep learning as depicted earlier is a subfield of machine learning that has
recently come into much prominence its main objective is to get machine learning research
closer to its true goal of making machines intelligent deep learning is often termed as a
rebranded fancy term for neural networks this is true to some extent but there is definitely more
to deep learning than just basic neural networks deep learning based algorithms involves the use
of concepts from representation learning where various representations of the data are learned
in different layers that also aid in automated feature extraction from the data in simple terms a deep
learning based approach tries to build machine intelligence by representing data as a layered
hierarchy of concepts where each layer of concepts is built from other simpler layers this layered
architecture itself is one of the core components of any deep learning algorithm in any basic supervised
machine learning technique we basically try to learn a mapping between our data samples and our output
and then try to predict output for newer data samples representational learning tries to understand the
representations in the data itself besides learning mapping from inputs to outputs this makes deep
learning algorithms extremely powerful as compared to regular techniques which require significant
expertise in areas like feature extraction and engineering deep learning is also extremely effective
with regard to its performance as well as scalability with more and more data as compared to older
machine learning algorithms indeed as rightly pointed out by andrew ng there have been several
noticeable trends and characteristics related to deep learning that we have noticed over the past decade
they are summarized as follows deep learning algorithms are based on distributed representational
learning and they start performing better with more data over time deep learning could be said to be a
a rebranding of neural networks but there is a lot into it compared to traditional neural networks
better software frameworks like tensorflow theano cafe mxnet and keras coupled with superior hardware
have made it possible to build extremely complex multi-layered deep learning models with huge sizes
deep learning has multiple advantages related to automated feature extraction as well as performing supervised learning
operations which have helped data scientists and engineers solve increasingly complex problems over time
the following points describe the salient features of most deep learning algorithms some of
which we will be using in this book hierarchical layered representation of concepts these concepts are also
called features in machine learning terminology data attributes distributed representational learning of the data happens
through a multi-layered architecture unsupervised learning more complex and high-level features and concepts
are derived from simpler low-level features a deep neural network usually
a deep neural network usually is considered to have at least more than one hidden layer besides the input and output layers
usually it consists of a minimum of three to four hidden layers deep architectures have a multi-layered architecture
where each layer consists of multiple non-linear processing units each layer's input is the previous layer in the
architecture the first layer is usually the input and the last layer is the output can perform automated feature
extraction classification anomaly detection and many other machine learning tasks this should give you a good
foundational grasp of the concepts pertaining to deep learning suppose we had a real world problem of object
recognition recognition from images you can clearly see how deep learning methods involve a hierarchical layer
representation of features and concept from the raw data as compared to other machine learning methods we conclude this
section with a brief coverage of some essential concepts pertaining to deep learning
important concepts in this section we discuss some key terms and concepts from deep learning algorithms and architecture
this should be useful in the future when you're building your own deep learning models
artificial neural networks
an artificial neural network
ann is a computational model and architecture that simulates biological neurons and the way they function in our brain
typically an ann has layers of interconnected nodes the nodes and their interconnections are analogous to the network of neurons in our brain
any basic ann will always have multiple layers of nodes specific connection patterns and links between the layers
connection weights and activation functions for the nodes or neurons that convert weighted inputs to outputs
the process of learning for the network typically involves a cost function and the objective is to optimize the cost function
typically minimize the cost the weights keep getting updated in the process of learning
backpropagation the backpropagation algorithm is a popular technique to train annns and it led to a resurgence in the popularity of neural networks in the 1980s
the algorithm typically has two main stages propagation and weight updates they are described briefly as follows
1. propagation
a. the input data sample vectors are propagated forward through the neural network to generate the output values from the output layer
b. compare the generated output vector with the actual or desired output vector for that input data vector
c. compute difference in error at the output units
d. backpropagate error values to generate deltas at each node or neuron
2. weight update
a. compute weight gradients by multiplying the output delta error and input activation
b. use learning rate to determine percentage of the gradient to be subtracted from original weight
and update the weight of the nodes
these two stages are repeated multiple times with multiple iterations or epochs until we get satisfactory results
typically backpropagation is used along with optimization algorithms or functions like stochastic gradient descent
multi-layer perceptrons a multi-layer perceptron also known as mlp is a fully connected feed forward
artificial neural network with at least three layers input output and at least one hidden layer where each
layer is fully connected to the adjacent layer each neuron usually is a non-linear functional processing unit
backpropagation is typically used to train mlps and even deep neural nets are mlps when they have multiple
hidden layers typically used for supervised machine learning tasks like classification and input layer and output layer
machine learning methods machine learning has multiple algorithms techniques and methodologies that can
be used to build models to solve real world problems using data this section tries to classify these machine
methods under some broad categories to give some sense to the overall landscape of machine learning methods
that are ultimately used to perform specific machine learning tasks we discussed in a previous section
typically the same machine learning methods can be classified in multiple ways under multiple umbrellas
following are some of the major broad areas of machine learning methods
one methods based on the amount of human supervision in the learning process
a supervised learning b unsupervised learning c semi-supervised learning d reinforcement learning
two methods based on the ability to learn from incremental data samples a batch learning b online learning
three methods based on their approach to generalization from data samples a instance-based learning b model-based
we briefly cover the various types of learning methods in the following sections to build a good
foundation with regard to machine learning methods and the types of tasks they usually solve
this should give you enough knowledge to start understanding which methods should be applied in what scenarios
when we tackle various real world use cases and problems in the subsequent chapters of the book
supervised learning supervised learning methods or algorithms include learning algorithms that take in data samples
known as training data and associated outputs known as labels or responses with each data sample during the model training process
the main objective is to learn a mapping or association between input data samples x and their corresponding outputs y
based on multiple training data instances this learned knowledge can then be used in the future to predict
an output for y prime for any new input data sample x prime which was previously unknown or unseen during the model training process
these methods are termed as supervised
these methods are termed as supervised because the model learns on data samples where the desired output responses or labels are already known beforehand in the training phase
supervised learning
supervised learning basically tries to model the relationship between the inputs and their corresponding outputs
from the training data so that we would be able to predict output responses for new data inputs
based on the knowledge it gained earlier with regard to relationships and mappings between the
inputs and their target outputs
this is precisely why supervised learning methods are extensively used
in predictive analytics where the main objective is to predict some response for some input data
that's typically fed into a trained supervised ml model supervised learning methods are of two major
classes based on the type of ml tasks they aim to solve classification regression let's look at these two
machine learning tasks and observe the subset of supervised learning methods that are best suited for tackling these tasks
classification
the classification based tasks are a subfield under supervised machine learning where the key objective is to predict output labels or responses
that are categorical in nature for input data based on what the model has learned in the training phase
output labels here are also known as classes or class labels and these are categorical in nature meaning they are unordered and discrete values
thus each output response belongs to a specific discrete class or category
suppose we take a real world example of predicting the weather let's keep it simple and say we are trying to
predict if the weather is sunny or rainy based on multiple input data samples consisting of attributes
or features like humidity temperature pressure and precipitation since the prediction can be either sunny or rainy there are a total of two distinct
classes in total hence this problem can also be termed as a binary classification problem
a task where the total number of distinct classes is more than two becomes a multi-class classification
problem where each prediction response can be any one of the probable classes from this set
a simple example would be trying to predict numeric digits from scanned handwritten images
in this case it becomes a 10 class classification problem because the output class label for any image can be any digit from 0 to 9
in both the cases the output class is a scalar value pointing to one specific class
multi-label classification tasks are such that based on any input data sample
the output response is usually a vector having one or more than one output class label
a simple real world problem would be trying to predict the category of a news article that could have multiple
output classes like news finance politics and so on popular classification algorithms include logistic regression
support vector machines neural networks ensembles like random forests and gradient boosting k nearest neighbors
decision trees and many more regression machine learning tasks where the main objective is value
estimation can be termed as regression tasks regression based methods are trained on input data samples
having output responses that are continuous numeric values unlike classification where we have discrete
categories or classes regression models make use of input data attributes or features also called
explanatory or independent variables and their corresponding continuous numeric output values
also called as response dependent or outcome variable to learn specific relationships and
associations between the inputs and their corresponding outputs with this knowledge it can predict output
responses for new unseen data instances similar to classification but with continuous numeric outputs
one of the most common real world examples of regression is prediction of house prices
you can build a simple regression model to predict house prices based on data pertaining to land plot areas in square feet
the basic idea here is that we try to determine if there is any relationship or association between
the data feature plot area and the outcome variable which is the house price and is what we want to predict
simple linear regression models try to model relationships on data with one feature
or explanatory variable x and a single response variable y where the objective is to predict y
methods like ordinary least squares ols are typically used to get the best linear fit during model training
multiple regression is also known as multi variable regression these methods try to model data where we have one
response output variable y in each observation but multiple explanatory variables in the form of a vector x
instead of a single explanatory variable the idea is to predict y based on the different features present in x
a real world example would be extending our house prediction model to build a more sophisticated model
where we predict the house price based on multiple features instead of just plot area in each data sample
the features could be represented in a vector as plot area number of bedrooms number of bathrooms
total floors furnished or unfurnished based on all these attributes the model tries to learn the
relationship between each feature vector and its corresponding house price so that it can predict
them in the future polynomial regression is a special case of multiple regression where the response variable
y is modeled as an nth degree polynomial of the input feature x basically it is multiple regression
where each feature in the input feature vector is a multiple of x
non-linear regression methods try to model relationships between input features and outputs
based on a combination of non-linear functions applied on the input features and necessary model parameters
lasso regression is a special form of regression that performs normal regression and generalizes the model
well by performing regularization as well as feature or variable selection lasso stands for least absolute
shrinkage and selection operator the l1 norm is typically used as the regularization term in lasso regression
ridge regression is another special form of regression that performs normal regression
and generalizes the model by performing regularization to prevent overfitting the model typically the l2
norm is used as the regularization term in ridge regression generalized linear models are generic frameworks
that can be used to model data predicting different types of output responses including continuous discrete
and ordinal data algorithms like logistic regression are used for categorical data and ordered probit regression for
ordinal data unsupervised learning supervised learning methods usually require some training data where the outcomes
which we are trying to predict are already available in the form of discrete labels or continuous values
however often we do not have the liberty or advantage of having pre-labeled training data and we still want to extract
useful insights or patterns from our data in this scenario unsupervised learning methods are extremely powerful
these methods are called unsupervised because the model or algorithm tries to learn inherent latent
structures patterns and relationships from given data without any help or supervision like providing
annotations in the form of labeled outputs or outcomes unsupervised learning is more concerned with trying to extract
meaningful insights or information from data rather than trying to predict some outcome based on previously available
supervised training data there is more uncertainty in the results of unsupervised learning but you can
also gain a lot of information from these models that was previously unavailable to view just by looking at
the raw data often unsupervised learning could be one of the tasks involved in building a huge intelligence
system for example we could use unsupervised learning to get possible outcome labels for tweet sentiments by using the
knowledge of the english vocabulary and then train a supervised model on similar data points and their
outcomes which we obtained previously through unsupervised learning there is no hard and fast rule with
regard to using just one specific technique you can always combine multiple methods as long as they are
relevant in solving the problem unsupervised learning methods can be categorized under the following broad
areas of ml tasks relevant to unsupervised learning clustering dimensionality reduction anomaly detection
association rule mining we explore these tasks briefly in the following sections to get a good feel of how
unsupervised learning methods are used in the real world clustering clustering methods are machine learning
methods that try to find patterns of similarity and relationships among data samples in our data set and then cluster
these samples into various groups such that each group or cluster of data samples has some similarity
based on the inherent attributes or features these methods are completely unsupervised because they
try to cluster data by looking at the data features without any prior training supervision or knowledge about
data attributes associations and relationships consider a real world problem of running multiple servers in a data
data center and trying to analyze logs for typical issues or errors our main task is to determine the various
kinds of log messages that usually occur frequently each week in simple words we want to group log messages into
various clusters based on some inherent characteristics a simple approach would be to extract features from the log
messages which would be in textual format and apply clustering on the same and group similar log messages together
based on similarity and content basically we have raw log messages to start with our clustering system
would employ feature extraction to extract features from text like word occurrences phrase occurrences and so
on finally a clustering algorithm like k-means or hierarchical clustering would be employed to group or cluster
messages based on similarity of their inherent features dimensionality reduction once we start extracting
attributes or features from raw data samples sometimes our feature space gets bloated up with a humongous
number of features this poses multiple challenges including analyzing and visualizing data with thousands or
millions of features which makes the feature space extremely complex posing problems with regard to training
models memory and space constraints in fact this is referred to as the curse of dimensionality
unsupervised methods can also be used in these scenarios where we reduce the number of features or attributes for
each data sample these methods reduce the number of feature variables by extracting or selecting a set of
principle or representative features there are multiple popular algorithms available for dimensionality reduction
techniques can be classified in two major approaches as follows feature selection methods specific features
are selected for each data sample from the original list of features and other features are discarded no new
features are generated in this process feature extraction methods we engineer or extract new features from the
original list of features in the data thus the reduced subset of features will contain newly generated
features that were not part of the original feature set pca falls under this category anomaly detection
the process of anomaly detection is also termed as outlier detection where we are interested in finding out
occurrences of rare events or observations that typically do not occur normally based on historical data samples
sometimes anomalies occur infrequently and are thus rare events and in other instances anomalies might not
be rare but might occur in very short bursts over time thus have specific patterns unsupervised learning methods
can be used for anomaly detection such that we train the algorithm on the training data set having normal
non-anomalous data samples once it learns the necessary data representations patterns and relations among
attributes in normal samples for any new data samples for any new data sample it would be able to identify
it as anomalous or a normal data point by using its learned knowledge anomaly detection based methods are
extremely popular in real world scenarios like detection of security attacks or breaches credit card fraud
manufacturing anomalies network issues and many more semi-supervised learning the semi-supervised learning
methods typically fall between supervised and unsupervised learning methods these methods usually use a lot of training
data that's unlabeled forming the unsupervised learning component and a small amount of pre-labeled and annotated data
forming the supervised learning component multiple techniques are available in the form of generative methods
graph-based methods and heuristic based methods a simple approach would be building a supervised model based on
labeled data which is limited and then applying the same to large amounts of unlabeled data to get more
labeled samples train the model on them and repeat the process another approach would be to use unsupervised
algorithms to cluster similar data samples use human in the loop efforts to manually annotate or label these
groups and then use a combination of this information in the future this approach is used in many image tagging systems
reinforcement learning the reinforcement learning methods are a bit different from conventional
supervised or unsupervised methods in this context we have an agent that we want to train over a period of time to
interact with a specific environment and improve its performance over a period of time with regard to the type of
actions it performs on the environment typically the agent starts with a set of strategies or policies for interacting with the environment
environment on observing the environment it takes a particular action based on a rule or policy
and by observing the current state of the environment based on the action the agent gets a reward which
could be beneficial or detrimental in the form of a penalty it updates its current policies and strategies if needed
and this iterative process continues till it learns enough about its environment to get the desired rewards
the main steps of a reinforcement learning method are mentioned as follows one prepare agent with set of
initial policies and strategy to observe environment and current state three select optimal policy and perform action
four get corresponding reward or penalty five update policies if needed six repeat steps two through five iteratively
until agent learns the most optimal policies consider a real world problem of trying to make a robot or a machine
learn to play chess in this case the agent would be the robot and the environment and states would be the chess board
and the positions of the chess pieces batch learning batch learning methods are also popularly known as offline
learning methods these are machine learning methods that are used in end-to-end machine learning systems
where the model is trained using all the available training data in one go once training is done and the model
completes the process of learning on getting a satisfactory performance it is deployed into production where it
predicts outputs for new data samples however the model doesn't keep learning over a period of time
continuously with the new data once the training is complete the model stops learning thus since the
model trains with data in one single batch and it is usually a one-time procedure this is known as batch or
offline learning online learning online learning methods work in a different way as compared to batch learning
methods the training data is usually fed in multiple incremental batches to the algorithm
these data batches are also known as mini-batches in ml terminology however the training process does
not end there unlike batch learning methods it keeps on learning over a period of time based on new data
samples which are sent to it for prediction basically it predicts and learns in the process with new data
on the fly without having to rerun the whole model on previous data samples there are several advantages
to online learning it is suitable in real world scenarios where the model might need to keep
learning and retraining on new data samples as they arrive problems like device failure or anomaly
prediction and stock market forecasting are two relevant scenarios besides this since the data is fed to the
model in incremental mini batches you can build these models on commodity hardware without worrying about
memory or disk constraints since unlike batch learning methods you do not need to load the full data set in
memory before training the model besides this once the model trains on data sets you can remove them
since we do not need the same data again as the model learns incrementally and remembers what it has
learned in the past one of the major caveats in online learning methods is the fact that bad data
samples can affect the model performance adversely all ml methods work on the principle of garbage in
garbage out hence if you supply bad data samples to a well-trained model it can start learning
relationships and patterns that have no real significance and this ends up affecting the
overall model performance since online learning methods keep learning based on new data samples you should
ensure proper checks are in place to notify you in case suddenly the model performance drops also suitable
model parameters like learning rate should be selected with care to ensure the model doesn't overfit
or get bias based on specific data samples instance based learning there are various ways to build machine
learning models using methods that try to generalize based on input data instance based learning involves ml systems
and methods that use the raw data points themselves to figure out outcomes for newer previously unseen data samples
instead of building an explicit model on training data and then testing it out
a simple example would be a k nearest neighbor algorithm assuming k equals three we have our initial training data
the ml method knows the representation of the data from the features including its dimensions position of each data
point and so on for any new data point it will use a similarity measure like cosine or euclidean distance and find the
three nearest input data points to this new data point once that is decided we simply take a majority
of the outcomes for those three training points and predict or assign it as the outcome label or response
for this new data point thus instance-based learning works by looking at the input data points and using
a similarity metric to generalize and predict for new data points model-based learning the model-based learning
methods are a more traditional ml approach toward generalizing based on training data typically an
iterative process takes place where the input data is used to extract features and models are built based
on various model parameters known as hyper parameters these hyper parameters are optimized based on various model
validation techniques to select the model that generalizes best on the training data and some amount of
validation and test data split from the initial data set finally the best model is used to make
predictions or decisions as and when needed the crisp dm process model the crisp dm model stands for cross
industry standard process for data mining more popularly known by the acronym itself crisp dm is a tried
tested and robust industry standard process model followed for data mining and analytics projects
crisp dm clearly depicts necessary steps processes and workflows for executing any project right from
formalizing business requirements to testing and deploying a solution to transform data into insights
data science data mining and machine learning are all about trying to run multiple iterative processes
to extract insights and information from data hence we can say that analyzing data is truly both an art as
well as a science because it is not always about running algorithms without reason a lot of the major effort
involves in understanding the business the actual value of the efforts being invested and proper methods to
articulate end results and insights the crisp dm model tells us that for building an end-to-end solution for
any analytics projects or system there are a total of six major steps or phases some of them being iterative
just like we have a software development life cycle with several major phases or steps for a software
development project we have a data mining or analysis life cycle in this scenario
the python machine learning ecosystem machine learning is a very popular and relevant topic in the world
of technology today hence we have a very diverse and varied support for machine learning in terms of
programming languages and frameworks there are machine learning libraries for almost all popular languages
including c plus plus r julia scala python etc in this chapter we try to justify why python is an apt language for machine learning
once we have argued our selection logically we give you a brief introduction to the python machine learning ml
ecosystem this python ml ecosystem is a collection of libraries that enable the developers to extract and transform data
transform data perform data wrangling operations apply existing robust machine learning algorithms
and also develop custom algorithms easily these libraries include numpy scipy pandas scikit-learn stats models tensorflow
karas and so on we cover several of these libraries in a nutshell so that the user will have some
familiarity with the basics of each of these libraries these will be used extensively in the later chapters of the book
an important thing to keep in mind here is that the purpose of this chapter is to acquaint you
with the diverse set of frameworks and libraries in the python ml ecosystem to get an idea of what can
be leveraged to solve machine learning problems we enrich the content with useful links that you can refer to
for extensive documentation and tutorials we assume some basic proficiency with python and programming in
general all the code snippets and examples used in this chapter is available in the github repository for this book
at https colon slash slash github dot com slash dipanjan cap s slash practical machine dash learning dash with dash python
under the directory named python underscore ml underscore ecosystem dot pi for all the examples used in this chapter
and try the examples as you read this chapter or you can even refer to the jupiter notebook named
the python machine learning ecosystem dot ipynb for a more interactive experience python an introduction
python was created by guido van rosam at sticktingmathmetisch centrum cwi c https colon slash slash www.cwi.nl
in the netherlands the first version of python was released in 1991 guido wrote python as a successor of
the language called abc in the following years python has developed into an extensively used high-level
language and a general programming language python is an interpreted language which means that the source
code of a python program is converted into bytecode which is then executed by the python virtual machine
python is different from major compiled languages like c and c plus plus as python code is not required
to be built and linked like code for these languages this distinction makes for two important points
python code is fast to develop as the code is not required to be compiled and built python code can be much
readily changed and executed this makes for a fast development cycle python code is not as fast in
execution since the code is not directly compiled and executed and an additional layer of the python
virtual machine is now responsible for execution python code runs a little slow as compared to conventional
languages like c c plus plus etc strengths python has steadily risen in the charts of widely used
programming languages and according to several surveys and research it is the fifth most important
language in the world recently several surveys depicted python to be the most popular language for machine
learning and data science we will compile a brief list of advantages that python offers that probably
explains its popularity one easy to learn python is relatively easy to learn language its syntax is simple for a beginner
to learn and understand when compared with languages like c or java there is minimal boilerplate code required
in executing a python program two supports multiple programming paradigms python is a multi-paradigm
multi-purpose programming language it supports object-oriented programming structured programming functional programming
and even aspect-oriented programming this versatility allows it to be used by a multitude of programmers
three extensible extensibility of python is one of its most important characteristics python has a huge number
of modules easily available which can be readily installed and used these models cover every aspect of programming
from data access to implementation of popular algorithms this easy to extend feature ensures that a python
developer is more productive as a large array of problems can be solved by available libraries
four active open source community python is open source and supported by a large developer community
this makes it robust and adaptive the bugs encountered are easily fixed by the python community
being open source developers can tinker with the python source code if their requirements call forth
pitfalls although python is a very popular programming language it comes with its own share of pitfalls
one of the most important limitations it suffers is in terms of execution speed
being an interpreted language it is slow when compared to compiled languages
this limitation can be a bit restrictive in scenarios where extremely high performance code is required
this is a major area of improvement for future implementations of python and every subsequent python
version addresses it although we have to admit it can never be as fast as a compiled language we are convinced
that it makes up for this deficiency by being super efficient and effective in other departments
the starting step for our journey into the world of data science is the setup of our python environment we usually have two options for setting up our environment
install python and the necessary libraries individually
use a pre-packaged python distribution that comes with necessary libraries
i.e anaconda anaconda is a packaged compilation of python along with a whole suite of a variety of libraries
including core libraries which are widely used in data science developed by anaconda
formerly known as continuum analytics it is often the go-to setup for data scientists
travis oliphant primary contributor to both the numpy and skippy libraries is anaconda's president and one of the co-founders
the anaconda distribution is bsd licensed and hence it allows us to use it for commercial and redistribution
purposes a major advantage of this distribution is that we don't require an elaborate setup and it works
well on all flavors of operating systems and platforms especially windows which can often cause problems with
installing specific python packages thus we can get started with our data science journey with just one
download and install the anaconda distribution is widely used across industry data science environments
and it also comes with a wonderful ide spider scientific python development environment besides other useful
utilities like jupiter notebooks the ipython console and the excellent package management tool conda recently they have
also talked extensively about jupiter lab the next generation ui for project jupiter we recommend using the anaconda
distribution and also checking out https colon slash slash www.anaconda.com slash what dash is dash
anaconda to learn more about anaconda to learn more about anaconda set up anaconda python environment
the first step in setting up your environment with the required anaconda distribution is downloading the
required installation package from https colon slash slash www.anaconda.com download which is the
provider of the anaconda distribution the important point to note here is that we will be using python
3.5 and the corresponding anaconda distribution python 3.5.2 was released on june 2016 compared to 3.6 which
was released on december 2016. we have opted for 3.5 as we want to ensure that none of the libraries
that we will be using in this book have any compatibility issues hence as python 3.5 has been
around for a long time we avoid any such compatibility issues by opting for it however you are free to use
python 3.6 and the code used in this book is expected to work without major issues we chose to leave
out python 2.7 since support for python 2.2 will be ending in 2020 and from the python community vision
it is clear that python 3 is the future and we recommend you use it download the anaconda 3 dash 4.2.0
dash windows dash x 86 underscore 64 package the one with python 3.5 from https colon slash slash repo dot
continuum dot io slash archive a screenshot of the target page is shown in figure 2.1 we have chosen
the windows os specifically because sometimes few python packages or libraries cause issues with
installing or running and hence we wanted to make sure we cover those details if you are using any
other os like linux or mac os x download the correct version for your os and install it
installing the downloaded file is as simple as double clicking the file and letting the installer
take care of the entire process to check if the installation was successful just open a command
prompt or terminal and start up python we also recommend that you use the ipython shell the command
is ipython instead of the regular python shell because you get a lot of features including inline plots
autocomplete and so on this should complete the process of setting up your python environment for data
science and machine learning installing libraries we will not be covering the basics of python as we
assume you are already acquainted with basic python syntax feel free to check out any standard course
or book on python programming to pick up on the basics we will cover one very basic but very important
aspect of installing additional libraries in python the preferred way to install additional libraries is
using the pip installer the basic syntax to install a package from the python package index pipi using pip is as follows pip install required package
this will install the required package if it is present in pipi we can use other sources other than pipi to install packages but that generally would not be required
the anaconda distribution is already supplemented with a plethora of additional libraries hence it is
very unlikely that we will need additional packages from other sources another way to install packages
limited to anaconda is to use the conda install command this will install the packages from the anaconda package
channels and usually we recommend this especially on windows why python for data science
according to a 2017 survey by stack overflow https colon slash slash insights dot stack overflow dot com slash survey
slash two zero one seven python is the world's fifth most used language it is one of the top three languages used by data scientists and one of the most
wanted languages among stack overflow users in fact in a recent poll by katie nuggets in 2017 python got the
maximum number of votes for being the leading platform for analytics data science and machine learning based on the
based on the choice of users http colon slash slash www dot katie nuggets dot com slash two oh
two zero one seven slash zero eight slash python dash
python dash overtakes are dash leader dash analytics dash data dash science dot html
python has a lot of advantages that makes it a language of choice when it comes to the practices of data science
we will now try to illustrate these advantages and argue our case for why python is a language of choice for data scientists
powerful set of packages python is known for its extensive and powerful set of packages in fact one of the
philosophies shared by python is batteries included which means that python has a rich and powerful set of
packages ready to be used in a wide variety of domains and use cases this philosophy is extended into the
packages required for data science and machine learning packages like numpy skippy pandas scikit-learn
etc which are tailor-made for solving a variety of real-world data science problems and are immensely powerful
this makes python a go-to language for solving data science related problems easy and rapid prototyping
python's simplicity is another important aspect when we want to discuss its suitability for data science
python syntax is easy to understand as well as idiomatic which makes comprehending existing code a relatively simple task
this allows the developer to easily modify existing implementations and develop his own ones
this feature is especially useful for developing new algorithms which may be experimental or yet to be supported by any external library
based on what we discussed earlier python development is independent of time-consuming build and link processes
using the repl shell idees and notebooks you can rapidly build and iterate over multiple research and development cycles
and all the changes can be readily made and tested easy to collaborate data science solutions are rarely a one-man job
often a lot of collaboration is required in a data science team to develop a great analytical solution
luckily python provides tools that make it extremely easy to collaborate for a diverse team one of the most
liked features which empowers this collaboration are jupyter notebooks notebooks are a novel concept that
allow data scientists to share the code data and insightful results in a single place this makes for an easily
reproducible research tool we consider this to be a very important feature and will devote an entire
section to cover the advantages offered by the use of notebooks one-stop solution in the first chapter we
explored how data science as a field is interconnected to various domains a typical project will have an
iterative life cycle that will involve data extraction data manipulation data analysis feature engineering
modeling evaluation solution development deployment and continued updating of the solution python as a
multi-purpose programming language is extremely diverse and it allows developers to address all these
assorted operations from a common platform using python libraries you can consume data from a multitude of
sources apply different data wrangling operations to that data apply machine learning algorithms on the
process data and deploy the developed solution this makes python extremely useful as no interface is required
i.e you don't need to port any part of the whole pipeline to some different programming language also
enterprise level data science projects often require interfacing with different programming languages
which is also achievable by using python for example suppose some enterprise uses a custom-made java
library for some esoteric data manipulation then you can use jithon implementation of python to use
that java library without writing custom code for the interfacing layer large and active community support
the python developer community is very active and humongous in number this large community ensures
that the core python language and packages remain efficient and bug free a developer can seek support
about a python issue using a variety of platforms like the python mailing list stack overflow blogs and usenet
groups this large support ecosystem is also one of the reasons for making python a favored language for data science
introducing the python machine learning ecosystem
in this section we address the important components of the python machine learning ecosystem
and give a small introduction to each of them
these components are few of the reasons why python is an important language for data science
this section is structured to give you a gentle introduction and acquaint you with these core
data science libraries covering all of them in depth would be impractical and beyond the current
scope since we would be using them in detail in subsequent chapters another advantage of having a great
community of python developers is the rich content that can be found about each one of these libraries
with a simple search the list of components that we cover is by no means exhaustive but we have shortlisted
them on the basis of their importance in the whole ecosystem jupiter notebooks jupiter notebooks
formerly known as ipython notebooks are an interactive computational environment that can be used to
develop python-based data science analyses which emphasize on reproducible research the interactive
environment is great for development and enables us to easily share the notebook and hence the code among
peers who can replicate our research and analyses by themselves these jupiter notebooks can contain code
text images output etc and can be arranged in a step-by-step manner to give a complete step-by-step
illustration of the whole analysis process this capability makes notebooks a valuable tool for
reproducible analyses and research especially when you want to share your work with a peer while developing
your analyses you can document your thought process and capture the results as part of the notebook
this seamless intertwining of documentation code and results makes jupiter notebooks a valuable tool for
every data scientist we will be using jupiter notebooks which are installed by default with our anaconda
distribution this is similar to the ipython shell with the difference that it can be used for different
programming backends i.e not just python but the functionality is similar for both of these with
the added advantage of displaying interactive visualizations and much more on jupiter notebooks
installation and execution we don't require any additional installation for jupiter notebooks as it is
already installed by the anaconda distribution we can invoke the jupiter notebook by executing the following
command at the command prompt or terminal jupiter space notebook this will start a notebook server at
the address localhost colon 8888 of your machine an important point to note here is that you access the
notebook using a browser so you can even initiate it on a remote server and use it locally using techniques
like ssh tunneling this feature is extremely useful in case you have a powerful computing resource that
you can only access remotely but lack a gui for it jupiter notebook allows you to access those resources in a
visually interactive shell once you invoke this command you can navigate to the address localhost colon 8888 in
your browser on the landing page we can initiate a new notebook by clicking on the new button on the top
right by default it will use the default kernel i.e the python 3.5 kernel but we can also associate the
notebook with a different kernel for example a python 2.7 kernel if installed in your system
a notebook is just a collection of cells there are three major types of cells in a notebook
one code cells just like the name suggests these are the cells that you can use to write your code and
associated comments the contents of these cells are sent to the kernel associated with the notebook
and the computed outputs are displayed as the cells outputs two markdown cells markdown can be used to
intelligently notate the computation process these can contain simple text commands html tags images and
even latex equations these will come in very handy when we are dealing with a new and non-standard
algorithm and we also want to capture the stepwise math and logic related to the algorithm
three raw cells these are the simplest of the cells and they display the text written in them
as is these can be used to add text that you don't want to be converted by the conversion mechanisms of the notebooks
neural networks and deep learning
deep learning has become one of the most well-known representations of machine learning in the recent
years deep learning applications have achieved remarkable accuracy and popularity in various fields
especially in image and audio related domains python is the language of choice when it comes to learning deep networks
and complex representations of data in this section we briefly discuss
anns artificial neural networks and deep learning networks then we will move on to the popular deep
learning framework for python since the mathematics involved behind anns is quite advanced we'll keep
our introduction minimal and focused on the practical aspects of learning a neural network we recommend
you check out some standard literature on the theoretical aspects of deep learning and neural networks like
deep learning by goodfellow and bengio if you're more interested in its internal implementations
the following section gives a brief refresher on neural networks and deep learning
artificial neural networks deep learning can be considered as an extension of artificial neural networks
anns neural networks were first introduced as a method of learning by frank rosenblatt in 1958 although the
learning model called perceptron was different from modern day neural networks we can still regard the
perceptron as the first artificial neural network artificial neural networks loosely work on the principle of
learning a distributed distribution of data the underlying assumption is that the generated data is a result of
non-linear combination of a set of latent factors and if we are able to learn this distributed representation
then we can make accurate predictions about a new set of unknown data the simplest neural network will
have an input layer a hidden layer a result of applying a non-linear transformation to the input data
and an output layer the parameters of the ann model are the weights of each connection that exist in the
network and sometimes a bias parameter this network is having an input vector of size three a hidden layer of size four
and a binary output layer the process of learning an ann will involve the following steps one define the
structure or architecture of the network we want to use this is critical as if we choose a very extensive
network containing a lot of neurons or units then we can overfit our training data and our model won't
generalize well two choose the non-linear transformation to be applied to each connection
function this transformation controls the activeness of each neuron in the network three decide on a loss
function we will use for the output layer this is applicable in the case when we have a supervised learning
problem ie we have an output label associated with each of the input data points four learning the parameters of
the neural network ie determine the values of each connection weight we will learn these weights by
optimizing our loss function using some optimization algorithm and a method called back propagation
we will extend these topics when we actually use neural networks deep neural networks deep neural networks
are an extension of normal artificial neural networks there are two major differences that deep neural
networks have as compared to normal neural networks number of layers normal neural networks are shallow which
means that they will have at max one or two hidden layers whereas the major difference in deep neural
networks is that they have a lot more hidden layers and this number is usually very large for example the
google brain project used a neural network that had millions of neurons diverse architectures we have a wide
variety of deep neural network architectures ranging from dnn's cnn's rnn's and lstms recent research have even given
us attention-based networks to place special emphasis on specific parts of a deep neural network hence with deep
learning we have definitely gone past the traditional ann architecture computation power the larger the network and the
more layers it has the more complex the network becomes and training it takes a lot of time and resources
deep neural networks work best on gpu-based architectures and take far less time to train than on traditional cpus
although recent improvements have vastly decreased training times python libraries for deep learning
python is a language of choice across both academia and enterprises to develop and use normal or deep neural
networks we will learn about two packages theano and tensorflow which will allow us to build neural network
based models on data sets in addition to these we will learn to use keras which is a high level interface to
building neural networks easily and has a concise api capable of running on top of both tensorflow and
theano besides these there are some more excellent frameworks for deep learning we also recommend
you to check out pi torch mxnet cafe recently cafe 2 was released and lasagna theano the first library
popularly used for learning neural networks is theano although by itself theano is not a traditional
machine learning or a neural network learning framework what it provides is a powerful set of
constructs that can be used to train both normal machine learning models and neural networks theano
allows us to symbolically define mathematical functions and automatically derive their gradient
expression this is one of the frequently used steps in learning any machine learning model
using theano we can express our learning process with normal symbolic expressions and then theano can
generate optimized functions that carry out those steps training of machine learning models is a
computationally intensive process especially neural networks have steep computational requirements
due to both the number of learning steps involved and the non-linearity involved in them this problem
is increased manifold when we decide to learn a deep neural network one of the important reasons of theano
being important for neural network learning is due to its capability to generate code which executes
seamlessly on both cpus and gpus thus if we specify our machine learning models using theano
we are also able to get the speed advantage offered by modern day gpus in the rest of this section we see
how we can install theano and learn a very simple neural network using the expressions provided by theano
this has been python machine learning beginner's guide to get you started with machine learning and deep
learning with python written by scott harvey narrated by russell newton copyright 2018 by scott harvey
production copyright 2018 by scott harvey
audible hopes you have enjoyed this program
