Welcome to the ActInf Podcast, where we will present short, digestible segments clipped
from the Active Inference Lab weekly live streams.
If you like what you hear and you want to learn more, check out the entire live stream
at the Active Inference Lab YouTube channel.
The link to the live stream is provided in the episode description.
My name is Blue Knight and I will be guiding you through this podcast episode, which is
clipped from ActInf Lab live stream number 6.1.
Daniel Friedman will introduce the paper and facilitate this discussion about the relationship
between the recognition model and the generative model.
Today we're going to be talking about a tale of two densities.
Active Inference is inactive inference, which is an article in Adaptive Behavior in 2020 by
Maxwell Ramstad, Kirchhoff, and Friston.
And in this paper, they lay out their goal really clearly, which I always love to see
in a paper.
They write, at the very beginning, the aim of this article is to clarify how best to interpret
some of the central constructs that underwrite the free energy principle, or FEP, and its
corollary active inference in theoretical neuroscience and biology, namely the role that generative
models and recognition densities play in this theory, aiming to unify life and mind.
So what are the two densities?
They're going to be the generative models and the recognition densities.
And we're going to learn more about them and hear about how they're related and discuss
different perspectives on how the densities are linked.
And specifically, the question is, what is the tale of these two densities?
It's alluded to in the title, and it's awesome that we have Maxwell and Alex and so many other
voices here to make that synthesis and that tale that we're all telling together realized.
For now, we're going to just jump into the abstract.
And at any point, people can just raise their hand and I'll just pause right there and we'll
take a comment or a thought.
In the abstract, they begin by rehearsing what I had just read, that they're looking to clarify
how to best interpret some of the constructs underwriting the free energy principle.
And those two constructs are the generative models and the variational densities.
So those are the two densities and the tale is going to link them.
We argue that these constructs, generative models and variational densities, have been
systematically misrepresented in the literature because of the conflation between the FEP and
active inference on one hand and distinct, albeit closely related, Bayesian formulations centered on
the brain, variously known as predictive processing, predictive coding, or the prediction error minimization
framework. More specifically, we examine two contrasting interpretations of these active
inference type models, a structural representationalist interpretation and an inactive interpretation.
So we're setting up the two sort of sub stories. These are the tension between these two perspectives
that we're going to be looking to resolve under the FEP through active inference.
We argue that the structural representationalist interpretation of generative and recognition
models does not do justice to the role that these constructs play in active inference under the FEP.
We propose an inactive interpretation of active inference, what might be called an active inference.
In active inference under the FEP, the generative and recognition models are best cast as realizing
inference and control. The self-organizing, belief-guided selection of action policies,
and do not have the properties ascribed by structural representationalists.
On the left side here, we have the Bayesian structural representationalist perspective that
we're just trying to highlight the features that are going to be most simple to carry forward.
And the Bayesian structural representationalist story is about how data and another type of data,
which are often called hyperparameters, are linked through a recognition model
that takes data, like sensory data, and recognizes it. And then going the other direction, you have
the hyperparameters that are generating sensory data. And we can also talk about why it's important
to have this generative step. And the outcome of this Bayesian computationalist scheme is that there's
a statistical convergence of a multi-level model that represents structures of the world through
something like expectation maximization or EM models. And we can contrast that with the inactive
paradigm. And the inactive paradigm is about how agents and the world are related through
perception and action. And the outcome of the inactivist perspective is an embodied ecological
action sequence, really, from an embedded agent who is enacting behavior. And so this is always the
school of thought where we see all the ease in cultured and embedded in all these things.
And they're seemingly, at least at the first pass, up to two quite different explanatory outcomes.
They seem to be talking about somewhat different aspects about the world, and they definitely link
them through different ways. So I'm just curious, Maxwell or anyone else, like what led to these two
models being the two cities, the two densities that were chosen? How does one come down to just two?
Why is there not one or three cities? And then how did the Bayesian structural representationalist
and the inactive viewpoint rise up as like the two kind of tier one theories that we wanted to find a
synthesis between? Yeah, so I'm Maxwell Ramstad. I'm based in Montreal, where I'm talking to you from
at McGill University. And I'm also the first author on the paper that we'll be discussing today.
Well, so when I got into this literature, especially from the vantage point of philosophy,
what I noticed was that very little of it was technically rigorous in the sense that, you know,
a lot of it was telling like a story about how, you know, the brain roughly performs Bayesian inference.
And then, you know, kind of saying, well, there's like a family of different theories that do this
in various ways and grouping the free energy principle under that. I say, you know, there was
like a lack of technical rigor. I want to emphasize that Alex's papers with Yakub are probably the exception to
that. When I consulted Alex's papers in 2018, I thought, well, here's, you know, some wonderful work
that is really taking the time to drill down on the formalisms as they're used to study the brain.
I thought that was great. But I spent a lot of time really drilling into the formalism of the free
energy principle per se. And one of the things that I was sort of surprised to find out as I was
learning the formalism was that although everyone is talking about the generative model, there are
really two models at play. So those are the generative and the recognition models or densities
equivalently. I mean, so first of all, we say model by model, we just mean a probability density,
right? So a probability distribution over a bunch of variables that are of interest to us.
And so, yeah, the two models in question under the free energy principle function slightly
differently than they do in more traditional brain based. So step back a bit in machine learning
and in statistics, a recognition model basically tells you the probability of some state given a
bunch of other things. It's not a joint probability distribution. And it's used essentially to recognize
what's causing your data. So you're using it basically to invert your mapping. I mean, Alex,
also, if I'm saying anything inaccurate here, just please jump in and let me know. But yeah,
so there's basically in traditional kind of, you know, Bayesian brain machine learning architectures,
the generative and the recognition models are basically just the inverse of one another. So
the kind of top, the kind of bottom up pass is a recognition pass. It's a recognition model in the
sense that it's starting from the data, and then kind of passing through the network, you're able to
infer what must have caused your data. And the generative model is the inverse path where you're
starting from your beliefs about states, and you can generate fictive data, you know, based on on this
model. And at least in Alex's papers, the way that this will be, this is how it was described as applying to
the free energy formulation as well. And my point in this paper was that, well, this is a very
technically rigorous and accurate description of what's going on in the Bayesian brain. But these
constructs have a slightly different meaning under active inference. To bring it back to these schemas,
basically, the so the recognition density is sort of like your best guess right now, you can think
about it sort of like as your posterior and your prior. So your recognition density is a density
defined over all of your states and your parameters. And it basically tells you what do I believe is the
most probable value of these states and parameters now, you know, given my prior beliefs and my evidence.
Your generative model, to the contrary, is the point of reference, you know, for the generation of free
energy gradients. So it's not your posterior, it's your prior, it's sort of like it harnesses all of
the priors, especially the priors about your preferred data distributions, relative to which
the free energy and therefore the dynamics are defined. So you know, you write inference and control,
I think you discussed in 6.0, Dan, yeah, the so the recognition model is responsible for inference,
and the generative model is responsible for control, you might say. So I'll stop there.
Cool. Alex Kiefer, and then we'll go to anyone else with a raised hand.
Hi, everyone, I'm Alex. I'm coming from a philosophy and cognitive science background.
I stopped myself from jumping in. I mean, that was that was a good summary. The only thing,
the point at which I wanted to jump in was to say, well, so the recognition model isn't as
construed in these Bayesian brain theories is an approximate inversion of the generative model.
So my my if I have any complaint about this paper, it's that I think that there's a closer sort of
conceptual connection between the generative and recognition densities than then maybe the paper
suggests. And that I don't think you can cleanly separate these things. So well, anyway, I don't
want to launch into this yet. What I wanted to do first was just address the question was sort of how
did these two visions sort of arise? And my sense is that what Carl Fristen did, he did a lot of great
stuff. But the main thing he did that distinguished his approach from the existing stuff in machine
learning, a lot of which was based on free energy minimization, which I think he came to around
the same time as people like Jeff Hinton. But anyway, you know, he added action into the picture. And he
pointed out that you can act so as to reduce the surprise or the free energy cause by your sensory states
instead of just revising your generative generative distribution.
Anyway, so I think I'll hold off on on arguing for the moment.
Awesome. I just want to say I agree with you now.
These two things do do not really come together, come apart. Sorry.
I mean, it's really just an implementation of variational inference from that point of view.
And variational inference, and this is why it's a tale of two densities, is that to do this variational
inference thing, you need both. You need a kind of point of reference that's going to give you the
free energy gradients. And then you need a sort of what is my best guess as I am performing gradient
descent on my free energy. And yeah, I don't want to say that the two come apart.
Indeed, I mean, I think, you know, in a newer paper that I've written on this, I basically just
straight up say that you were correct initially with respect to the recognition density. So the
recognition density, it's fair to say that it's a representation in the structural representationalist
sense that you've been articulating in the series of really awesome papers, which you should all read,
by the way, I think. Yeah, whereas the one point of disagreement that I think we've clarified now
is the status of these generative models. And yeah, yeah, I'll stop there too.
Nice. We'll go to Steven. And then anyone else with a raised hand before the roadmap?
Okay. Hi, I'm Steven. I'm in Toronto. And I do community development projects using theater
and immersive participatory approaches. And one question I've got is in terms of if these kind of
hyper parameters that they use in machine learning are using a free energy, is it that they're just
minimising the kind of energy expenditure and the entropy internally? And they're not using Shannon
entropy. They're not looking at how entropy is inferred or transmitted sort of in a second order process
from interacting in the world. They're just minimising it within the kind of data that's being
kind of accrued, which kind of in some ways seems to happen easily when you look at like vision data,
but may not be so easy to pass when you sort of look at the whole body. But that question of entropy,
is it that they use entropy in a different way and they're not really using entropy in the kind of
external entropy of interaction, but kind of just the entropy within the calculations in terms of
There are two kinds of entropy at play in general, right? So entropy in the thermodynamic sense is a
measure of how many macro states are compatible with a given value of a macro state, right? So I don't know,
your temperature is 36 degrees Celsius, how many different macro state configurations are compatible
with that. So that form of entropy is one special case of the broader kind of entropy, which is more
or less a measure of how flat your probability distribution is over your states. So if you have
a perfectly flat distribution, your entropy is optimal. And so the the entropy that we're concerned with
is really the second type, it's the information theoretic entropy. But it's transpired over the
last few years that variational free energy is also a thermodynamic free energy, you just have to
multiply by Boltzmann's constant. So essentially, yeah, it's always information theoretic measures.
But when this is realized in actual physical systems, it is also a thermodynamic free energy. But you know,
the the variational free energy, it sounds kind of spooky and esoteric. But it's really simple,
really, is that you have a preferred data distribution. And you have an actual data
distribution. And the free energy is just a way to quantify the difference between the two.
Yeah, awesome. We'll go to Alex, and then back to Steven.
Yeah, I was just going to briefly say in the, you know, the earlier machine learning literature,
the free energy definitely wasn't supposed to be anything thermodynamic, or maybe that was,
maybe that was an open possibility. But it was just a, it was just a measure, as you were saying,
I think, Steven, between two internal, internally determined distributions, it was the top down
generative posterior versus the approximate posterior. And any connections to thermodynamics are really
cool. But I think that's a additional, like very substantive question slash thesis.
Steven, and then anyone else who raised their hand?
Yeah, I think this is this is quite a useful distinction, because I think that's where
a lot of the mixing gets caught up at the moment, because a lot of stuff has been referred to in like
the last 15 years around Bayesian optimization and Bayesian stuff. And then where it's all pretty much
as if it's in contained in the data in the brain. And then now it's like, it's like a reconceptualizing
of that. So I think it is a big, big challenge for people to like, forget about what they've learned
before.
I really agree with that, Steven. And that's really what this conversation and synthesis is about,
is about bringing those qualitative, often insights from inactivism and saying, well, wait a minute,
perception isn't simply the reverse of action. You know, there's not photons coming out of your eye.
So what is different between perception and action, but also recognizing that the world and the agent
have this sort of symmetry and that they are linked?
Thank you for tuning into this episode. Stay tuned for next week when we'll take a deeper dive
into the relationship between the recognition model and the generative model.
