This is the ActInf Podcast, where we provide short clips from the Active Inference Institute
weekly live streams. If you're interested in hearing more of this material, check out
the entire live stream at the Active Inference Institute YouTube channel. The link to the
live stream is provided in the episode description. I am Blue Knight, the host and curator of this
episode, which is clipped from live stream number 11.1. This discussion will be facilitated
by Daniel Friedman and is based on the 2020 paper entitled, Sophisticated Active Inference,
Simulating Anticipatory Effective Dynamics of Imagining Future Events by Casper Hesp, Alex
Shantz, Baron Milledge, Maxwell Ramstead, Carl Friston, and Ryan Smith. Enjoy!
Three areas we could imagine this area of research heading towards would be more advanced simulations,
so agents that are simulating in continuous time or with higher dimensional action policies
just like we've seen from ActInf 8 with Scaling Active Inference, or towards the computational
psychiatry angle, something that Ryan Smith and others have written review papers about and a lot
of empirical work as well. So we'll hear more about that next time. And then there's this angle of the
models for robotics, which is like what if the drone had a top-level summary statistic that's just like,
is this mission going well or not? And if not, then that might engage some policies like head back home or
pause or land or get to safety. But that's a top-level summary statistic that even robots might be able to
utilize. And I know that some people are doing that kind of stuff. So in Sophisticated Inference,
and this is to bridge the gap between the formalisms and the actual sentences that are driving us,
we're going to kind of write it in a little bit of a hybrid script, like a rebus. So the big question that
agents are trying to answer in the world and in Sophisticated Inference is, how can we have the best
policy over a given time horizon? So not what's the best chess move now? It's what's the best chess move
over the time horizon that I'm considering? And tied up in that question about evaluating the best policy
over a given time horizon is the question of what are the consequences of policies through time and how do we
estimate that? What is our estimate? And what that is related to, how we benchmark, how we compare
policies and their consequences is specifically related to how we understand observations being
mapped to hidden states and our policy, which is how our actions are influencing the way that hidden
states change between each other. That's that B matrix. And the generalized free energy, which is this
par and Friston citation. So another citation link away from the IY paper is that the generalized free
energy basically consists of one model that conditions on policies and then asks about how
state transitions occur. So conditioning on me exercising every day, what kind of state transitions can I expect
to see in the world? And then the second part of the free energy is conditioning on states, what policies
should be selected? So this is the two stroke engine condition on what I know about the world,
world states and causal model of the world, generative model of the world. What is my policy
selection? That's the control theory. That's the cybernetic side. That's the action side, action
looking side. Then this first part is again conditioning on my policies in the world. What are the state
transitions and how do those change? And there's more nuance because of course we're just describing it in
natural language, but this is sort of the broad outline. One half of free energy conditions on
policy. The other part selects policy and basically vice versa with state transitions. So again, we're
conditioning on two different parts and that's also how we end up using this two stroke engine to do
inference in complex settings. Blue, this is the slide that always makes me think about you with where's the
information gain and could the pure information gaining agent get reward or does the pure reward
seeker gain information and like where is this play interplay between the intrinsic values or the
salience and then the intrinsic value in the novelty. So the way that these two and all these different
features and the different representations, I mean, there's so much to say about and think here, but
let's return to that in a second. Blue? So I just wanted to comment while we're here about what you had
said in the introduction video related to why are we always trying to estimate the states when really
we should be trying to estimate the best policy, right? That's what we should be predicting over, not
trying to predict the states. Bring three jackets instead of one jacket and then you'll always be at the right
temperature. Like if we prepare versus trying to predict the state of what's going to be, I think
that that's a just was a neat thought that occurred to me. Exactly. When there's too much emphasis on
precise estimates of world states, it's implicitly like we're focusing so much on precise estimates of
world states that we'll just know what to do when we have that precise estimate. It's like that's not true.
It turns out that maybe again, a one digit of precision or even to the nearest 10 of the
temperature from a policy perspective, you'll be fine. If it's 20 Celsius plus or minus 10,
you might be fine within a broad range of world state predictions from an action selection policy.
Because your action selection could be cool. I'll throw a jacket in my backpack and then I'll be fine,
whatever it is. And so that is this two stroke engine, this handoff or handshake between co-estimating
states and their transitions, like weather in the world and the transitions between different weather
states in the world, given what you know about weather. We're not on Mars. It's not going to do a
acid rain storm at 400 degrees. So it's like, given what we know about the distributions of the niche,
the statistical regularities of the niche, we want to be co-estimating states and our action,
action and perception. And we want to be doing that in a way that suffices, that's adequate to remain
within our preferred areas, our preferred strange attractors, where we're able to basically have
reduced uncertainty about the things that we care about, the physiological variables that matter,
like our temperature. And okay, so it's, it's, is very cool. And again, just for those who haven't
seen these kinds of equations that much, the vertical line means conditions on, and then whatever
follows it, you can think of is like fixed. So it's like you have a spreadsheet and let's just say that
the rows are policies and the columns are like state transitions. And so when you fix on one, it's like
you pick out, you fix a column, and then you're asking about the distribution of that vector fixed
on that column. And similarly, you could fix a row, select one row, condition on one row,
and then ask about the distribution in that line going across. And so that's the full matrix.
And also that's related to what's called marginal likelihood in Bayesian statistics. The margin is
literally the margin of the page when they used to do it with physical pages. So it's like rows by
columns and the marginal likelihood is related to the margin of the page, which is just so fun. It's
kind of like a floppy disk icon, but for statistics. Here's another cool piece of the sophisticated
inference paper. And I think there's a lot I'd like to learn and ask here to Casper and others, but this
formulation of policy pi is related to the argument minimization of free energy of policy. So this is
saying the policy that gets selected pi is going to be like a free energy minimization of G as a function
of all the policies that could be done. So of my affordances, we're going to do a free energy
minimization and that's how I'm going to select my policy. Okay. Well, what is G of pi? G of pi is
related to the expectation of what? Risk minus ambiguity. So high risk is going to be a higher number
and that's worse. In this case, it's like golf. We want to minimize the free energy. And so we're
jointly trading off and there's some natural logs and some negative natural logs. So it's sometimes
a little bit hard to see if you're going like up or downhill, which is why it's so important to
have it phrased like this, but basically risk is bad and ambiguity is bad, but it turns out with the
natural logs that you can just think of them as being combined. So it's like, if we seek to maximize
our precision, minimize risk, minimize ambiguity, what we'll be doing is actually a strict bound
on the intrinsic value and the extrinsic value. So this formulation of search of information gain
related to risk and ambiguity turns out to be strictly bounded by this value driven process.
So worldview related to reward, you see that agents are reward seekers and uncertainty is something
that they just simply deal with or they figure out or they thrive despite. In the active inference
perspective, agents are uncertainty reducers who obtain rewards. And if they don't, they die.
That's evolution. And so we're putting risk and uncertainty first. And so instead of saying like,
well, the agent predicts value and that ends up reducing its uncertainty about the world.
We're actually saying, no, it's a way more tractable calculation for the agent to be reducing
its uncertainty about action. Again, not just about states of the world, but the agent is reducing
its uncertainty about action through action. It's performing optimal foraging experimentation
in order to reduce its uncertainty about best actions. And it turns out that that is going to have
a bound, it's always going to be converging towards this perfect trade-off between intrinsic and extrinsic
value. And special cases fall out of this where certain variables are basically fixed to zero.
So if you have no prior at the beginning and you just let it start, kind of draw from your first
observations. You take those and you use that as your first hyper prior. That's like the parametric
empirical Bayes. And in this case, you end up recapitulating several things from Bayesian
statistics like optimal Bayesian design or just surprise. And then similarly, if you set the ambiguity
to zero. So like the case in active inference eight, where the states were perfectly observable,
like the pendulum and the mountain car, it wasn't like it was getting a noisy estimate of where it
was on the mountain. It knew exactly where it was on the mountain. And in the case of no ambiguity
about states, again, ambiguity is the P distribution of observations given states. So that's like,
given that it's daytime, am I seeing photons? So if you have no ambiguity about that,
then this term is zero, right? Which makes risk minimization the imperative. And that's why when
you get no ambiguity, you find that this whole framework collapses into risk control. Whereas if
you had risk collapse to zero, like it's a no risk situation, the only imperative would be reduction of
ambiguity. So now let's think about that in reward land. If you didn't have to seek value, it could
be purely novelty based search. Whereas if the only imperative were to obtain short term value, you
would not expect that kind of agent to be engaged in a creative search. So it's sort of like each of
these are phrasings, whether risk minus ambiguity or intrinsic minus extrinsic value. You can see how
either one of them could be like zeroed out and what that would do for the other side. Or you could
imagine in realistic situations where they're sort of like tied up in a specific way. So again, you
could imagine a no risk scenario and no ambiguity scenario, no intrinsic or no extrinsic, any of these
combinations. We'll do Steven and then Sasha. Hi, I'm Steven. I'm in Toronto. And yeah, this is really
good the way you're putting this together here and explaining it. I think also this, obviously these
things, sometimes they become really clear when it's an extreme case, and then you've got this blending
that's going on. It's not fully any one of these. But I think that no ambiguity piece is a useful
one to think about what happens sometimes when, well, you could say, I don't know if you'd say Trump, but
that when in the politics, when something is, if people say these people are bad, and you know,
you often see that even at work, and it's a no brainer, things are said to be a no brainer,
then suddenly everyone, the intrinsic value of trying to explore other options is can be collapsed in
people. And they just go with the extrinsic value of gain because there's no, there's supposedly no
ambiguity. And I think that that could be interesting social. Yep. Yep. Like we know
exactly how dangerous this virus is. There's no discussion about what the risk minimizing policy is.
Oh, wait a minute. Is there Sasha? Hi, thank you. My name is Sasha. I'm a neuroscience graduate student
at Davis. In hearing this description, just really thinking about
using this to define play, and really seeing play as the combination of factors that has
very low risk, but like high value of exploration. And so setting up things that have low risk,
but high opportunities for curiosity, that would be, I guess, for me fit into the category of play.
So that's, that's a good way to think about it. And.
And another fun part there is like when value is the imperative, it's difficult to be exploratory
and creative. It's like Maslow's hierarchy of needs, but then when there can be the space where
you can actually engage in a novelty or a curiosity guided search, kind of the prerequisite, the enabling
feature of that is sufficiency at a material level. So that isn't simply related to reward. We don't
just give people reward, kind of dial up their dopamine, and then they'll be acting creatively.
It's actually like we need to scaffold the agent in the environment in a way where it can reduce
its risk so that it can tolerate higher ambiguity or reduce ambiguity so that we can engage with higher
risk scenarios. So sometimes these combinations of features, even though risk and ambiguity,
they're just English words. So really it is about the formalism,
but it helps us reimagine and repartition and fluidly move across partitionings for how to
think about these nuance control problems. Sasha?
Also, it makes me think about how this is commonly studied like in psychology lab spaces and
yeah, really thinking about like how using this framework to figure out what would make the
participants what state that would put them in, as well as how they feel about the risk and ambiguity of
the situation that they're in. Because depending on how you feel about participating in research,
that would change what state of exploration or curiosity you might be in. And that, I don't know,
it just really makes me rethink a lot of research in this field because it's ignoring or, I guess,
excluding one of the key variables of this process.
Yep. And it makes me think about safe spaces. But we could also say, how risky is this space?
That's sort of related to safety. Those are like one over each other. You know, risk equals one over
safety. But also, how ambiguous is this space? We want this space to be safe, so not risky. But we
also want this space to be not ambiguous. We want to be clear and communicating about what this space
is. However, we don't want to make it so unambiguous there's no room to play. So we want to have the
optimal level of ambiguity and risk. That's controlled novelty. That's Pareto optimality. That's free energy
minimization. That's living on the frontier between performance and innovation. So that's where all
these models can take us. And that's why it reimagines the bottom-up versus top-down, the explore versus
exploit, the maximization in the long-term versus the short-term. Those are all coming from the reward
mentality. And when we think about the situated agent reducing uncertainty about action through action,
we have a whole new set of ways to talk about these questions. Stephen, good then, Blue. And bringing it
back as well with this paper and, you know, the way it talks about temple depth and is it, it can also
combine with this, give a way to think about what they call stages of readiness. Because people sort of
think, oh, you know, that could read this and like, okay, we've made it safe. I've said it's safe. I've told
everyone it's safe. Well, it might be you've told them it's safe and you've demonstrated it's safe and the
environment's been safe. But it might be five minutes, it might be three weeks of training before people
start to bring that safety into the way that they balance these different parameters, you know. So
the way that the, and it could be that it crashes quite rapidly, positive or negatively, as we saw
earlier with the way that the dynamics work. And that, that's quite, that shows that the importance
of facilitation and the importance of the way the space is set up, the safety between participants and
all that sort of stuff. Cool. Blue? So I know that you said like, we're not going to unpack the
formalism of the intrinsic value and the extrinsic value, or at least you said that on the intro video.
But those two things, like it makes me think the special cases really, like, you know, they talked
about no intrinsic value, but what about like, no extrinsic value? Like, it just always reminds me of
like, what about just exploring for, you know, info gain or, you know, just out of curiosity to satisfy
that, like for intrinsic value? I did note that I was like, waiting for it. I was like, okay, no risk,
no ambiguity, then no intrinsic, it's like the next one. And I wondered if that actually was related to
maximum entropy. So it's like, given a distribution, if you have no preferences for value, then you're
just sampling, your preference is to sample the distribution unadulterated. And so that is like
max info game, max ent sampling neutrally without preference across a distribution. And so that's very
related to statistical sampling techniques, because those computer algorithms have been designed
actually without a preference vector in mind, just to sample distributions like Monte Carlo,
Markov chain methods. And so now we're taking a sampling agent, you're sampling temperatures,
but you don't want just all temperatures for that distribution. You want to sample repeatedly from
body temperature. However, there might be another distribution where you do want to have a very,
very exploratory sampling pattern. Let's just see if there's any other interesting stuff. So here is
from sophisticated inference, another kind of rollout. And then here we can see observations,
and then there's different actions that can be taken. And each of those actions would lead to
different observations, and so on. And so it gets exponentially large. And this is just like early
chess algorithms. They used to do if this, then that, if this, then that. And they would do these
rollouts and then they'd prune this branch. Oh, that doesn't look that good. So I'm not going to
explore that way. And since those sort of tree pruning chess algorithms, there's been many
developments in chess and other algorithms for this kind of decision-making. And so here's the
structure of the model and here's that rollout. And then one big question is how can we look down
the branches that are most informative, not just fantasize and dream about the one in a billion
branches that's the best, not negatively ruminate and get dwelling on the one branch that's the worst.
But how can we sample information across this branching tree pattern in a way that leads to
effective action in the world? And then here's another integrative plot that they made, which is
showing that basically there's a few ways to think about active inference, what it's doing.
Here it's minimizing risk and minimizing ambiguity. Okay. So that's related to this phrasing on the top.
Minimize risk, minimize ambiguity. Okay. Make it safe, make it clear. Positive way of saying it,
the negative way is minimize risk, minimize ambiguity. So policies, states, observations.
States are the hidden part of the world. Observations, you know, photons on my retina,
policies where my eyes move. And so the risk framing is you should be minimizing free energy in a way that
reduces risk and ambiguity about this chain that connects policy states and outcomes.
Another branch of active inference is actually sort of like a yin and yang. This is like a push pull.
It's saying minimize the entropy of the distribution of outcomes. So achieve predictable outcomes, reduce
big reduction on there, and expand here with the information gain. And so instead of, hey,
do something that's like creative, but FYI, don't be risky and don't be ambiguous. Here is like be as
clear as you can and predictable as you can the outcomes while being as info gain and creative as
you possibly can. And so I like this because it's sort of like, instead of two things that you want
to run away from, and then hopefully you become creative in the gap, this is like something to run
away from and something to run towards. So it rephrases that risk and ambiguity minimization
as actually a push pull of a minimization of surprise about observations and maximization of
the information gain given the policies that can be carried out. We hope you enjoyed this week's
episode. Stay tuned for next time when we're going to have more discussions of the same paper.
