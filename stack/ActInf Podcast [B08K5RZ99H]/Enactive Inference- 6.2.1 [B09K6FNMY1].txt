Welcome to the ACT-INF podcast, where we will present short, digestible segments clipped
from the Active Inference Lab weekly livestreams.
If you like what you hear and you want to learn more, check out the entire livestream
at the Active Inference Lab YouTube channel.
The link to the livestream is provided in the episode description.
My name is Blue Knight and I will be guiding you through this podcast episode, which is
clipped from ACT-INF Lab livestream number 6.2.
This discussion will be loosely structured around the paper, A Tale of Two Densities,
Active Inference is Inactive Inference, by Maxwell Ramstad, Michael Kirchhoff, and Carl Friston.
Daniel Friedman will facilitate this discussion about the relationship between the recognition
model and the generative model, and he starts us off by providing a good description of these
models and the relationship between them.
So, just to take another look at these two cities, the two cities that are being linked,
the densities, we have the Bayesian structural representationalist, and Alex freely added any
details here.
But Bayesian models move between the data parameters, the observations, and hyperparameters, or higher
order parameters.
And from the data to the hyperparameters, we call it a recognition model, and to go from
hyperparameters to a set of data or observations, that is a generative model.
And the outcome is a statistical convergence of a multilevel model that represents structures
of the world, for example, through an expectation maximization scheme.
We can contrast this with the inactivist school of thought, which has linked rather than data
and hyperdata, but rather has focused on the world and agents and how they're linked through
perception and action.
So a lot more adjacencies to areas like niche construction and ecological psychology.
And we remember hearing from Maxwell last week about how it was the desire to mathematically
rigorize some of the inactivism that led him to the Bayesian approach.
And let's now think about how these two densities are linked with a little bit more feedback from
one of our participants.
They wrote, the domino metaphor finally makes sense.
The physical dominoes are the physical state of the system corresponding to the recognition
model and the falling of the dominoes is like the process or the dynamics of the system
reflecting the generative model.
So here we've kind of combined these two different ways of thinking, and we're looking
at the relationship of the world and the agent or the system and the system surroundings,
and directly went ahead and combined perception with the recognition model and action with
the generative model.
This is a quote from the paper that says, on this view, active inference can be read as
a new take on the good regulator theorem proposed by content and Ross Ashby in 1970.
Active inference tells us about the relation between the control system, the generative model
with priors over action policies and the system being controlled, the organism and its adaptive
behavior, the actual actions undertaken in and part of the world.
So what exactly does this clarify about active inference or what do people think about this
metaphor?
Or at least way to think about how, if not necessarily a metaphor, how would we apply active inference
to this system?
What are we getting at here with these similarities and differences between the two models?
Sure.
Mel?
I'm Mel Andrews.
I'm in Ohio.
I'm a doctoral student in philosophy of biology and company of science.
Well, so the good regulator theorem, the idea is that any good control structure for some
larger system has to be a model of that system.
It has to contain in it all of the key variables for that system.
And basically what this says to me about active inference is that in order for a system in an
environment to be able to adequately react to whatever kinds of perturbations are happening
in the environment, it has to contain all of the key existence.
I call them existential variables, the variables in the environment that would lead to the
system continuing to exist or not continuing to exist.
Cool.
And so how exactly, I'm just curious, does the system come to embody all the key variables
of the outside world?
Like, don't the models of the organism reflect a simplified version of the outside world?
Otherwise, there kind of is a Borges story about the map that represents the territory
exactly.
So how does a good regulator arise in the context of an environment like that?
Shannon, and then Maxwell.
Hi, I'm Shannon.
I'm based out of UC Merced in California, but I'm in South Dakota right now.
Well, I don't think the key is to be an exact imitative model of the world or an exact map
of the world, but just a model that is good enough to enable you to act and adjust in the world.
Cool.
Maxwell and then Mel.
Yeah, so I'm Maxwell Ramsted.
I'm based in Montreal.
Formally speaking, there's a difference between external states and hidden states.
And I mean, when everything is going well, the external states that are modeled by the system
coincide with the hidden states that are actually out there.
It is strongly implied by the fact that we're minimizing variational free energy.
that if we're not generating a lot of free energy, then the external beliefs that we have
about the environment tend to reflect the causal structure of that environment.
But I mean, this makes sense also just evolutionarily.
Perceptual systems don't track truth.
They enable adaptive behavioral loops.
So if you're a prey item like a bunny, it's adaptive to generate a lot of false positives compared
to, you know, allowing for false negatives in terms of like predator detection.
It's more advantageous to, you know, make a few less costly mistakes than make a big one.
So, uh, I mean, yellow Breenenberg and Eric Roosevelt make make that point in the paper
that Kate was mentioning just earlier.
But, uh, yeah, the it doesn't actually have to have hook up to anything in the real world,
though.
Usually it will end up doing so.
Cool.
Mel and Sasha.
Yeah, so the the key term in what I said is is key actually.
Um, so the key variables, right?
That's that's just what keeps the system alive.
And we can imagine that we first get a system on the scene.
That needs just basically one parameter in its environment to be correct in order to continue
to exist as a system.
And then we build up from there to systems that are progressively more and more complex
and need progressively more and more complex sort of environmental scenarios to survive.
And then they themselves in order to survive in those more complex regimes need to build in complexity.
Right.
In terms of their their the models of the world that they enact.
Right.
So I think that says something interesting about how we conceptualize of cognition and the onset of cognitive complexity.
Um, as sort of, um, and this is Peter Godfrey Smith line on the subject is is it's a response to a complexifying environmental circumstance.
Right.
Right.
Cool.
Sasha then Alex keeper.
Hi, I'm Sasha.
I'm based out of Davis, California, and I'm a neuroscience graduate student.
Uh, one key phrase that really helped me better understand, um, active inference.
Um, is, uh, good enough.
Um, and that's what really, um, kind of put it all together that, um, while we're going about and reducing uncertainty about our system,
it just has to be good enough to make the next action or in the evolutionary sense to survive.
And that metaphor has really helped me, uh, go through all this, um, all these high level concepts.
So, uh, thank you for mentioning that.
Alex Kiefer, then Alex Vyotkin.
Uh, hi, I'm Alex Kiefer.
Um, I'm, uh, in the philosophy department of Monash University currently in New York City.
I just wanted to just note that there's, there's, you don't need to, to contrast accuracy or, or, you know, answerable to the truthness with, um, uh, enabling adaptive behavior, uh, as, as like a dichotomy.
Um, I'm not sure if that's what was intended or not, but this is one reason I've had trouble getting into understanding the, uh, sort of anti-representationalist viewpoint is that, um, you know,
structural similarity is a matter of degree accuracy is a matter of degree in this sense.
So, um, I mean, yes, your hidden state representation doesn't have to map completely accurately onto reality.
It doesn't have to capture every detail, but if it doesn't do that to any degree, um, you're screwed.
So, you know, um, maybe we need systematically, um, sort of simpler or biased representations in order to do the job.
But I, I think there's still a implied relationship to the truth there.
Cool.
Alex.
Uh, I'm Alex Vyotkin.
I'm a researcher at system management school in Moscow, Russia.
Uh, I want to change the systems level of, uh, consideration of possible application of active inference.
And for me, interesting, uh, on a personal level in terms of, uh, day phenomenology and especially, uh, professional and or working day phenomenology, uh, how it works.
So what is a generative model and the, like an example, uh, and what I want to discuss if I get it correctly or how it's good, uh, possible develop to think about it.
But if, for example, some doctor, he know different disciplines and, uh, with different ontologies.
And when he, uh, met a patient, he started his actions by his, uh, generative model, which activates with, uh, cognition models to serve as a doctor.
Depends on exact situation and the exact case.
Uh, and, uh, if it's so from, for example, from another level, from team level, if person, uh, behave professionally in some discipline,
it's, uh, became like, um, perception model for, uh, generative model of the team.
And, uh, what does it mean in terms of, uh, learning and education of team members?
Because if we can, how some, in some way to link recognition models with ontologies and starting to work with it more, again, cybernetically,
it's possible, could be very interesting, at least for me.
Cool.
Agreed.
The, the ontologies, the, how we think about the world definitely influences our perception.
Okay.
Shannon, go ahead.
Um, I was just going back to Alex Kiefer's point that your model, like there has to be some truth relationship with the world.
And if we're going to the dominoes and the process needs to be that the dominoes fall to make some cool pattern.
If there's an interruption, like, so now this domino can't reach the next domino.
Like that's akin to a disruption and, and how truthy the model is of the world.
And it disrupts the action that you're able to take, or it disrupts the, um, you die instead of being able to survive because your model isn't accurate at that stage for the processes to continue happening, to do the next action or to survive.
Cool.
Cool discussion on this domino system.
We hope you enjoyed this week's episode.
Stay tuned for next time when we will have a hot debate about structural representationalism.
