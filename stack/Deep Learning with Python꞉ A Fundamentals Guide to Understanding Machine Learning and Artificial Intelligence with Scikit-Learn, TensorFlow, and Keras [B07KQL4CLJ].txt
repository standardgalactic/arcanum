This is Audible.
Deep Learning with Python
A Fundamentals Guide to Understanding Machine Learning and Artificial Intelligence
with Scikit-Learn, TensorFlow, and Keras
Written by Sebastian Dark
Narrated by Chris Klein
Introduction
Thank you for purchasing the book Deep Learning.
Deep Learning is a process that widens the range of most artificial intelligence problems
like speech recognition, image classification, question answering, optical character recognition,
and transforming text to speech.
It is true that Deep Learning is a complex subject to learn and understand,
but it is not difficult for most machine learning algorithms.
If you want to learn how to build a Deep Learning model in Python,
you have come to the right place.
Over the course of this book,
you will gather information on what Deep Learning is
and how it ties in to machine learning and artificial intelligence.
You will also learn about the different neural network architectures
that designers use to program a Deep Learning model.
The book provides information on how you can build a Deep Learning model in Python using Keras.
The examples in this book use data sets that are available online.
You can download the data from the links mentioned in the book
and use these data sets to test your model.
By the time you reach the end of the book,
you will have built a solid foundation of building models.
You will also learn to build deep learning models for your projects.
Best of luck!
Chapter 1
What is Machine Learning?
The act of learning is difficult to define.
In its simplest sense,
to learn is to gain knowledge or understanding of a new concept.
One can only gain this through study, experience, or instruction.
There are many correlations that one can draw
between human learning or animal learning and machine learning.
The techniques used in animal learning are often employed to develop machine learning concepts.
Conversely, certain breakthroughs in machine learning have helped us to understand
new perspectives in biological animal learning patterns as well.
To improve the efficiency of a machine, it is often recalibrated structurally.
Doing so often increases performance and quality of output,
and is, in fact, a method in which to learn how the machine functions.
With machine learning, however,
not all recalibration results in new learning.
For instance, suppose a machine is to predict
whether India will win a cricket match against Australia.
The designer or developer can feed some historical information
pertaining to both Indian and Australian players,
their match statistics and run rates, etc.,
into the machine.
The machine can use that input to predict an outcome.
This is a type of machine learning.
The whole concept behind machine learning
is to apply it to something that has artificial intelligence,
that can process large volumes of information
and churn out analyses and results.
As such,
artificial intelligence and machine learning go hand-in-hand.
Engineers build machines with artificial intelligence
to perform the following functions.
Diagnosis,
prediction,
and recognition.
These machines learn to perform these tasks
by learning from the training data set
that the engineer provides it with.
The engineer can use sample data
or historical data
to train the machine to predict the desired output.
The machine identifies patterns in this training data
and teaches itself to perform different types of analyses on the data.
Apart from training data,
there are various other learning mechanisms
that an engineer teaches a machine with artificial intelligence.
The most commonly used are supervised learning
and unsupervised learning.
There are some who deem machine learning
as a questionable exercise.
They do not view machines as capable of anything
beyond the performing of certain tasks.
It is imperative that a machine learns for various reasons.
One of the main reasons is that machine learning
paves the way for further understanding of human learning.
Here are a few other advantages
that machine learning brings with it.
Advantages of machine learning
Due to sheer volume and magnitude of the tasks,
there are some instances where an engineer or developer
cannot succeed no matter how hard they try.
In those cases,
the advantages of machines over humans are clearly stark.
Identifying patterns
When an engineer gives a machine with artificial intelligence
a training data set,
the machine will learn to identify patterns within the data
and produce results for any similar inputs
that the engineers provide the machine with.
This is efficiency far beyond that of a normal analyst.
Due to the strong connection
between machine learning and data science,
which is the process of crunching large volumes of data
and unearthing relationships between underlying variables,
through machine learning,
one can derive important insights into large volumes of data.
Improven efficiency
Humans might have designed certain machines
without the complete appreciation of its capabilities,
since they may be unaware of the different situations
in which a computer or machine works.
Through machine learning and artificial intelligence,
a machine will learn to adapt to environmental changes
and improve its own efficiency,
regardless of its surroundings.
Complete Specific Tasks
A programmer usually develops a machine
to complete certain specific tasks.
These tasks are often an elaborate and arduous program,
where there is scope for the programmer to make errors of omission.
He or she might forget a few steps or details
that they should include in the program.
An artificially intelligent machine that can learn on its own
will not face these challenges,
as it will learn the tasks and processes on its own.
Helps Machines Adapt to the Changing Environment
With ever-changing technology
and the development of new programming languages
to communicate these technological advancements,
it is impossible to convert all existing programs and systems
into these new syntaxes.
Redesigning every program from its coding stage
to adapt to technological advancements is counterproductive.
In such times,
it is highly efficient to use machine learning
so that the machines can upgrade and adapt themselves
to the changing technological climate.
Helps Machines Handle Large Data Sets
Machine learning brings with it
the capability to handle multiple dimensions
and varieties of data simultaneously
and in uncertain conditions.
An artificially intelligent machine
with abilities to learn on its own
can function in dynamic environments.
This emphasizes the efficient use of resources.
Machine learning has helped to develop tools
that provide continuous improvement in quality
in small and larger process environments.
Disadvantages of Machine Learning
It is difficult to acquire data to train the machine.
The engineer must know what algorithm
he or she wants to use to train the machine.
Only then can he or she identify
the data set they will need
to use to train the machine.
There can be a significant impact
on the results obtained
if the engineer does not make the right decision.
It is difficult to interpret the results accurately
to determine the effectiveness
of the machine learning algorithm.
The engineer must experiment
with different algorithms
before he or she chooses one
to train the machine with.
Technology that surpasses machine learning
is being researched.
Therefore, it is important for machines
to constantly learn and change
to adapt to changes in technology.
Chapter 2
An Introduction to Neural Networks
Before we delve into the concepts
of deep learning,
let us understand what neural network architecture is.
engineers built artificial neural networks
to help machines process information
in the same way as the human brain.
The key feature of these networks
is how the layers are structured
to process information.
Each layer consists of a network
of interconnected neurons
that process information.
These neurons work together
to solve a problem.
Since the neural network
processes information
the same way as the human brain,
a machine with neural network architecture
will learn by example.
Engineers can only develop
these machines for a specific purpose,
like pattern recognition
or data classification.
Human beings learn
when the brain makes adjustments
to the neuron.
Neural networks work in the same way.
Historical Background
Although neural networks
were developed before
the invention of computers,
they suffered a setback
since people were not fully adept
at using these networks.
It is because of this reason
that the simulation of neural networks
is a recent development.
Improvements made to computers
have boosted many advances
in the development of neural networks.
People threw themselves into research
when the concept of neural networks
gained importance.
They were unable to obtain information
on how they can use neural networks
to improve the accuracy
and efficiency of machines.
This led to a dip in enthusiasm.
There were some researchers
that continued to study neural networks.
And they worked hard
to develop neural network technology
that people in the technology industry accepted.
Warren McCulloch and Walter Pitts
developed or produced
the first artificial neural network
in 1943.
This neuron is known
as the McCulloch-Pitts neuron.
Since the technology
to develop the network further
was not available to them
in that year,
they were unable to use
this neural network
to perform complex tasks.
Why use neural networks?
Every industry today
uses large volumes of data
to improve the functioning
of the industry.
These data sets
have many variables
that make it difficult
for human beings
to understand the patterns
in the data set.
Neural networks
make it easier
to identify these hidden patterns
in the data set.
Many computers
also find it difficult
to identify trends
in the data set.
if they do not have
neural network architecture.
An engineer can train
the neural network
using large data sets
to help it become
an expert in the field.
The engineer can then
use the network
to predict the output
for any future input.
This gives the engineer
a chance to answer
important questions
about the future.
Some advantages
of neural networks include
neural networks use
a machine learning technique
called supervised machine learning
to adapt and learn
new tasks.
They can represent information
provided to it
during the learning stage.
Since neural networks
can compute data
in parallel,
machines with neural network
architecture
work faster
to provide results.
If a neural network
is partially damaged,
it can lead to
a dip in performance.
However,
the neural network
can retain some of its properties
even when it is damaged.
Neural networks
versus conventional computers.
Conventional computers
and neural networks
do not use the same approach
to solve a problem.
The former only uses algorithms
to solve a given problem.
Conventional computers
can also solve problems
if they know what steps
they should follow
to identify the solution.
This means
that conventional computers
often solve problems
that human beings
can solve.
Computers are useful
when they can solve problems
that human beings
do not know
how to solve.
As mentioned earlier,
neural networks
function in the same way
as the human brain.
The neurons in the network
are interconnected.
These neural networks
always work in parallel
to solve most problems.
Since the network
learns by example,
the engineer
cannot teach the network
to perform a specific task.
This makes it important
for the engineer
to choose the training
datasets carefully.
Otherwise,
it will be difficult
for the machine
to learn how to solve
a problem correctly.
Since the neural network
learns from the dataset,
it can solve
a variety of problems,
including those
that the engineer
does not train
the machine to solve.
This makes these networks
unpredictable.
Computers also solve problems
using cognitive approaches.
The computer should know
the process it should follow
to solve the problem,
and this will only work
when the engineer
provides the computer
with the correct instructions.
The engineer can provide
these instructions
to the machine
using a high-level
programming language.
The computer will decode
the instructions
into a language
that it can understand.
This process helps
the engineer predict
how a computer will work.
If there is an issue
with the result,
the engineer knows
that it is because
of a hardware
or software problem.
Neural networks
and conventional computers
complement each other.
Some tasks,
like arithmetic calculations,
are better suited
for conventional
algorithmic computers,
while complex tasks
are more suited
for neural networks.
Most tasks require
a combination
of these approaches
to ensure that the machine
performs at maximum efficiency.
The architecture
of neural networks
Network layers
The network layer
is the most common type
of neural network architecture.
It has three layers
of interconnected neurons.
The first layer,
known as the input layer,
is connected
to the hidden layer
of neurons,
which is connected
to the output layer.
The input layer
represents the data
or information
that the engineer
provides to the network.
How the input layer
views the raw data
determines the weights
that are placed
on the neurons
in the hidden layer
and the weights
that are placed
on the nodes
connecting the neurons
in the input layer
to the hidden layer.
The behavior
of the output layer
is dependent
on the weights
placed on the neurons
in the hidden layer
and the weights
placed on the nodes
connecting the neurons
in the hidden layer
to the output layer.
This is the structure
for a simple neural network
and it is interesting
since the hidden layers can choose to represent the data in any form.
The weights present on the nodes connecting the input layer to the hidden layer
determine when the hidden network layer must stay active.
The engineer can modify the weights between the input and the hidden layers
to ensure that the hidden unit will represent what the engineer wants.
It is easy to differentiate between a single layer and a multi-layer architecture.
In the single-layer architecture, the neurons are connected at their nodes.
This means that every layer in the network is connected,
which increases the computational power of the network.
In the multi-layer architecture, the network is numbered by layers.
This means that the layers are interconnected and not the neurons.
Feed-forward networks
The feed-forward network is also known as the top-down, or bottom-up, network.
Signals in the feed-forward network only travel in one direction,
from the input to the output.
The network does not have a feedback look,
which means the output derived from one layer
does not affect the output derived from other layers.
The network is straightforward,
in the sense that the input is associated with the output.
This type of network is used in pattern recognition.
Feedback networks
Signals in the feedback network will travel in both directions.
This introduces a loop within the network.
These networks are complicated to build,
but are extremely powerful.
The state of the feedback network will change
until the signals reach a point of equilibrium.
The network will remain in this state of equilibrium
until the input dataset changes.
When the engineer feeds a new dataset to the machine,
the network will need to identify a new equilibrium point.
These networks are interactive and recurrent.
However, only single-layer networks are called recurrent.
We will learn more about recurrent neural networks in this chapter.
Perceptrons
The term perceptrons was coined in 1960 by Frank Rosenblatt
when there were significant developments being made in neural network architecture.
A perceptron is a type of McCulloch and Pitts model.
In this model, every neuron is assigned a fixed or pre-processing weight.
A perceptron is similar to the visual system or function in human beings.
Perceptrons are used only in pattern recognition,
although they can be used for other functions.
Recurrent neural networks
In recurrent neural networks,
the information always goes through a loop.
When the network makes a decision,
it will consider the current input
and also assess what it has learned from previous inputs.
The usual RNN has a short-term memory,
and when it is combined with the LSTM,
long-short-term memory,
it will have a long-term memory.
This is further discussed below.
A good way to illustrate the concepts of RNN
is by using an example.
Consider that you are using a regular feed-forward neural network
and give the network the word brain as its input.
This network will process that word character by character,
and by the time it reaches the letter I,
it will forget about the first three characters.
This makes it impossible to use this model
to predict what the next character will be.
An RNN can remember the previous characters
and also predict the next character,
since it has an internal memory.
It produces outputs
and copies those outputs back into the network.
This means that RNNs
always add the immediate past information
to the present information.
Therefore, an RNN network
has the following inputs.
The present data.
The recent data.
An engineer must be careful
about the training dataset he uses
to teach the model,
since the model uses the sequences
to predict characters in the text.
This means that an RNN
can perform functions
that most other algorithms cannot.
A feed-forward neural network
will assign weights to the neurons
in every layer
to produce the output.
The RNN applies weight
to the current and previous input data
and also tweaks the weights
assigned to the neurons.
Chapter 3
An Introduction to Deep Learning
Most engineers and developers
help machines learn
using different machine learning techniques.
Deep learning is one of the many techniques
used to help a machine learn by example.
It is one of the key technologies
used to help a car
navigate the streets without a driver.
Cars can now identify
stop signs, pedestrians, and lampposts.
Deep learning is also used
in voice control devices
like phones, tablets, televisions,
and other hands-free devices
like Alexa.
Through deep learning,
the technology industry
can achieve results
that were never possible before.
Computer models
using deep learning techniques
learn to perform classification tasks
from text, sound, or images.
A machine using deep learning models
can achieve high levels of accuracy
that human beings cannot.
These machines also perform
better than human beings.
Engineers train the machines
using large data sets
called the training data set
and neural network architecture.
How does deep learning
attain such impressive results?
Deep learning can attain these results
because of accuracy.
Through deep learning techniques,
engineers can ensure
that the machines they develop
have a higher level
of recognition accuracy.
This helps the machine
to meet the demands
of the customer,
and it is crucial
for devices like driverless cars
to have this function.
Recent developments
in deep learning
have improved machines
to a point where
they perform some tasks,
like the classification of images,
better than human beings.
Although deep learning
was studied in the early 1980s,
there are two reasons
why it is a popular
machine learning technique now.
Deep learning uses large volumes
of labeled data
to train machines.
For example,
it takes thousands of hours
of video and millions of images
to train a driverless car
to perform well on roads.
Deep learning also requires
computing power.
Most machines have
a parallel architecture
that improves the function
of the deep learning technique.
When deep learning
is combined with cloud computing
or clusters,
it reduces the time
the engineer must spend
on training the machine
from a few weeks
to hours or less.
Examples of deep learning
Deep learning techniques
and applications
are used in a wide range
of industries.
Automated driving
Automotive researchers
use deep learning techniques
to help a machine
detect objects
like traffic lights
and stop signs.
In addition to this,
the techniques are used
to detect pedestrians
and other cars,
which helps to decrease
the number of accidents.
Defense and aerospace
Deep learning techniques
are used to identify safe
or unsafe zones
for people or troops.
These techniques
also help machines
identify the areas of interest
using satellite imagery.
Medical research
Cancer researchers
use deep learning techniques
to detect cancer cells
in the human body.
Many teams at UCLA
built a microscope
that uses deep learning techniques
to yield a high-dimensional data set.
This microscope
can detect cancer cells
anywhere in the human body.
Industrial automation
Deep learning techniques
help organizations
improve the safety
of workers
around heavy machinery.
The machines
using these techniques
detect when an object
or a person
is within an unsafe distance
of the heavy machinery.
Electronics
Deep learning techniques
are now used
in automated speech
and hearing translation.
For instance,
home assistance devices
like Alexa
can respond to your voice
and understand your preferences
using deep learning algorithms.
How does deep learning work?
The neural network architecture
is used in most
deep learning techniques
and is, for this reason,
that deep learning models
are known as
deep neural networks.
The concept of neural networks
is covered in the previous chapter.
The term deep
refers to the many
hidden layers
in the network.
A traditional neural network
can only have
up to two hidden layers.
But a deep neural network
can have close to
150 layers.
Deep learning models
use large data sets
called the training data set
and neural networks
to learn features
from the data.
This means that
there is no need
for the engineer
to manually extract features
from the data
to train the machine.
One of the most
common types
of deep networks
is called
the convolutional neural network
or CNN.
This type of network
is well suited
to process
two-dimensional data
like images.
A CNN
combines the features
it has learned
from the features
in the input data
and uses
the two-dimensional
convolutional layers
to process information.
A CNN
eliminates the need
to extract features
from data manually.
This means that
the engineer
does not need
to classify the features
or identify the images
to help the network
categorize them.
The CNN
extracts the features
directly from the images
or input data.
The engineer
does not train the data
to choose some features
from the information
provided to it.
The CNN
learns what features
it needs to look for
when the engineer
feeds it the training
data set.
It is for this reason
that computers
or models
with CNN
are used
to classify objects.
A CNN
learns to identify
the different characteristics
and features
of an image.
It does this
by using the hundreds
of hidden layers
within the network.
Every hidden layer
identifies complex features
of the image
and the complexity
increases
as the hidden layers
increase.
For example,
the first hidden layer
can detect colors
in the image
while the last layer
can identify
different objects
in the background.
Difference between
deep learning
and machine learning
Deep learning
is a type
of machine learning.
Let us consider
an instance
where an engineer
is training
a machine learning
and deep learning
model
to categorize
these images.
In machine learning,
the engineer
trains the model
by extracting
the relevant features
from the training
data set
and providing
that information
to the machine.
The machine
then uses
these features
to categorize
objects present
in the images.
As mentioned earlier,
a deep learning model
will identify
the features
from the training
data set.
In addition to this,
deep learning
also performs
end-to-end learning.
In this process,
the network
is given the
training data set
and is asked
to perform a task
like classification
and the network
learns to do this
without the help
of the engineer.
Another difference
is that shallow
learning algorithms
converge with the
increase in data
while a deep
learning algorithm
will scale with data.
This means that
the hidden layers
in the deep
neural network
continue to learn
and improve
in their functioning
as the size
of the data set
increases.
Shallow learning
algorithms refer
to those machine
learning algorithms
that plateau
at a specific
performance level
when the engineer
adds more training
data or examples
to the network.
In simple words,
in machine learning,
the engineer
must give the machine
a classifier
and a feature
to sort images.
While with deep
learning,
the machine learns
to perform
these functions
by itself.
Choosing between
deep learning
and machine learning.
Machine learning
algorithms offer
different models
and techniques
that the engineer
can choose from
depending on
the application,
the type of problem
the machine should solve,
and the data
it is processing.
Deep neural networks
require a huge volume
of data
that the engineer
can use to train
the model,
as well as a
graphics processing unit
or GPU
which will process
the data quickly.
When you need
to choose between
deep neural networks
and machine learning,
you should consider
whether you have
labeled data
and a high-performance
GPU.
If you do not
have either,
you should stick
to machine learning
algorithms.
Since deep learning
is more complex,
you will need
at least a few
thousand variables
to ensure that
the algorithm provides
the necessary output.
If you have
GPU with high performance,
you can be certain
that the model
will analyze
the data quickly.
Subjects involved
in machine
and deep learning.
Deep learning
is not a
standalone science,
but rather
borrows its fundamentals
and concepts
from multiple subjects.
Together,
all these subjects
help a programmer
to develop new methods
of deep learning
and collectively form
the interdisciplinary subject
of deep learning.
Below are some
of the disciplines
and languages
that find their application
in deep learning.
Statistics
Statistics is the science
of data,
and,
where there is data,
there is deep learning.
One of many
statistical concepts
that lends itself
to deep learning
is that of
hypothesis testing,
whereby,
the statistician
predicts the parameters
of an unknown
data set.
Another problem
from statistics
that can be solved
with deep learning
is predicting
the values
of functions
using certain
sample values
of that function.
Deep learning
solves this
and other
statistical problems
by using
historical information
and past data
to predict
future outcomes.
statistics are
indeed
an important
facet
of deep learning.
Brain modeling
Both machine learning
and deep learning
use neural networks
to derive solutions
from data.
We read about
neural networks
and their architecture
in the previous chapter.
Scientists have suggested
that non-linear elements
with weighted inputs
can be used
to create
a neural network.
Extensive studies
are being conducted
to assess
these non-linear elements.
Scientists
and psychologists
alike
are trying to gather
more information
about the human mind
through these
neural networks.
Connectionism,
sub-symbolic processing,
and brain-style
computations
are a few spheres
associated
with these types
of studies.
Adaptive Control Theory
Adaptive Control Theory
is a subject
that is closely
associated
with the control
of systems.
As mentioned earlier,
it is difficult
for the system
to adapt
to a change
in its surrounding
environment.
Adaptive Control Theory
is a part
of this subject
that deals
with methods
that help the system
adapt to such changes
and continue
to perform optimally.
The idea
is that systems
should anticipate
the changes
and modify
themselves accordingly.
Psychological Modeling
For years,
psychologists
have tried
to understand
human learning.
They use
EPAM networks
to understand
and identify
the patterns
in human learning.
This network
stores and retrieves
words from a database
when the engineer
gives the machine
a problem.
The concepts
of semantic networks
and decision trees
came into existence
much later.
Artificial intelligence
has affected
research in psychology
since most research
studies include
the use of a machine
that helps them
to understand
the human mind.
Engineers
use the concept
of reinforcement learning
to help a network
or machine learn.
Artificial Intelligence
As mentioned earlier,
deep learning
is greatly concerned
with the subject
of artificial intelligence.
While studying
artificial intelligence,
great focus
has been laid
in the past
on using analogies
for learning
and how past experiences
influence
acclimatization
of and accommodation
for future events.
More recent studies
have focused
on using the concepts
of inductive
logic programming
and decision tree
methods.
Evolutionary Models
Famous Darwinian ideology
states that animals
not only display
a preference
for learning new things
in life,
but are also driven
to adapt
to their surroundings,
thereby enhancing
their performance.
Early men
fashioned weapons
out of materials
available in their surroundings
to protect themselves
from larger,
stronger,
and faster predators.
For machines,
the concepts
of evolution
and learning
mean one
and the same.
Evolutionary models
and ideologies
can be used
to devise
deep learning techniques.
The most prominent
technique
that has been developed
using evolutionary models
is the genetic algorithm.
Chapter 4
Supervised Learning
An important part
of the learning process
is called training,
where a machine
is provided with data
about historic events
that will help
the machine anticipate
future events.
This type of learning
is called supervised learning,
when the training data
fed is supervised.
The data fed
essentially consists
of training examples.
These examples consist
of inputs
and desired outputs.
These desired outputs
are also known
as supervisory signals.
The machine then uses
a supervised machine algorithm
that creates
an inferred function
that is used
to forecast events.
If the outputs
are discrete,
the function
is called a classifier,
and if the outputs
are continuous,
the function
is known
as a regression function.
The function
is used
to predict
the outputs
for future inputs, too.
This algorithm
is used
to generate
a generalized method
to reach the output
from the data
that was fed in
as input.
An analogy
that can be made
in the spheres
of human and animal learning
is concept learning.
Overview
This type of learning
always uses
a fixed algorithm.
The steps
that are involved
involved in the process
are listed below.
Step 1
The first step
in this process
of learning
is to determine
the type of examples
that would need
to be used
to train the machine.
This step
is crucial,
since the engineer
will need to know
what kind of data
he wants to use
as an example
for his machine.
For instance,
a speech recognition system,
the engineer
could either use
simple words,
small sentences,
or entire paragraphs
to train the machine.
Step 2
The engineer
has decided
the type of data
that will be used.
He will need
to collate that data
and create
a training set.
This set
will be used
to represent
the possibilities
of any function
that can be used.
The second step
requires the engineer
to collect
the desired inputs
and outputs
that will need
to be used
for the training process.
Step 3
The next step
is to determine
how to represent
the input data
to the machine.
This is very important,
since the accuracy
of the machine
depends on the input
representation
of the function.
Normally,
the representation
is done
in the form
of a vector.
This vector
contains information
about various
characteristic features
of the input.
However,
the vector
should not include
information
on too many features,
since this will
increase the time
taken for training.
A large number
of features
might also lead
to mistakes
made by the machine
in prediction.
The vector
needs to contain
exactly enough data
to predict outputs.
Step 4
Once the engineer
has decided
what data
to use
as input data,
a decision
will need
to be made
on the structure
of the function
that will need
to be created.
The learning algorithm
must be decided
too.
The algorithms
that are used
are often supported
vectors
and decision trees.
Step 5
The engineer
will now need
to complete
the design.
The algorithm
that is chosen
will need
to be run
on the data set
that has been used
as the training
input data.
There are some
algorithms
that will need
the engineer
to identify
control parameters
to verify
that the algorithm
is running well.
These parameters
can be estimated
by testing
on the smaller subset
or by using
the method
of cross-validation.
Step 6
Once the algorithm
has run
and the function
has been generated,
the accuracy
and effectiveness
of the function
will need
to be calculated.
Engineers
use a testing
set for this.
This data set
is different
from the training
data set
and the corresponding
outputs to the input
data are already known.
Test set inputs
are sent to the machine
and the outputs
obtained
are checked
with those
in the test set.
Supervised learning
algorithms
are often used
and each of these
has their strengths
and weaknesses.
Since there is
no definitive algorithm
that can be used
for all instances,
the selection
of the learning algorithm
is a major step
in the procedure.
Issues to consider
in supervised learning
With the usage
of supervised learning
algorithms,
a few issues arise.
Given below
are four major issues.
Bias-variance trade-off
The issue that will need
to be kept in mind
is that,
while working with learning
is the bias-variance trade-off,
consider yourself
in a situation
where you have
some good training sets.
When a machine
is trained
using some
training data sets,
it gives predictions
that are systematically
incorrect
for certain outputs.
It can be said
that the algorithm
is biased
towards the input data set.
A learning algorithm
can also be considered
to have a high variance
for input.
This occurs
when the algorithm
causes the machine
to predict different outputs
for that input
in each training set.
The sum of the variance
and the bias
of the learning algorithm
are known
as the prediction error
for the classifier function.
There is a trade-off
that exists
between the variance
and the bias.
A requirement
for learning algorithms
with low bias
is that they need
to be flexible enough
to accommodate
all the data sets.
If they are too flexible,
the learning algorithms
might end up giving
varying outputs
for each training set
and hence
increases the variance.
The trade-off
will need to be adjusted
using supervised learning algorithms.
This is done automatically
or by using
an adjustable parameter.
Function complexity
and amount of training data.
The second issue
is concerned
with deciding
on the amount
of training data
based on the complexity
of the classifier
or regression function
to be generated.
Suppose the function
to be generated
is simple.
A learning algorithm
that is relatively inflexible
with low variance
and high bias
will be able to learn
from a small amount
of training data.
On many occasions,
the function
will be complex.
This will be the case
due to a large number
of input features
being involved
or due to the machine
being expected
to behave differently
for different parts
of the input vector.
In such cases,
the function
can only be learnt
from a large amount
of training data.
These cases
also require
the algorithms used
to be flexible
with low bias
and high variance.
Therefore,
efficient learning algorithms
automatically arrive
at a trade-off
between the bias
and the variance
depending on the complexity
of the function
and the amount
of training data required.
Dimensionality
of the input space
Another issue
that needs to be dealt with
is the dimensionality
of the input vector space.
If the input vector
includes several features,
the learning problem
will become difficult
even if the function
only considers
a few of these features
are valuable inputs.
This is because
the extra
and unnecessary dimensions
could learn to confusion
and could cause
the learning algorithm
to have high variance.
So when the input dimensions
are large,
the classifier
is adjusted
to offset the effects
by having low variance
and high bias.
In practice,
the engineer
could manually remove
the irrelevant features
to improve
the accuracy
and efficiency
of the learning algorithm.
This might not always
be a practical solution.
Recently,
many algorithms
have been developed
which are capable
of removing
unnecessary features
and retaining
only the relevant ones.
This concept
is known as
dimensionality reduction,
which helps to map
input data
into lower dimensions
to improve the performance
of the learning algorithm.
Noise in the output values
Last but not least
are the interference
of noise,
white noise,
in the output values
provided by the machine.
The output values
can be wrong
due to the white noise
that is added
to the output values.
These values
can also be wrong
due to human error.
In such cases,
the learning algorithm
should not look
to match
the training inputs
with their exact outputs.
Algorithms
with a low variance
and high bias
are the most desired.
Chapter 5
Unsupervised Learning
Now that we have covered
supervised learning,
you should be able
to define it with ease.
If not,
please take a step back
and read the previous chapter.
Apart from supervised learning,
there are other concepts
of learning,
like unsupervised learning,
that is gaining importance.
In this technique,
the machine is designed
to interact
with its ambient environment
through actions.
Based on the environment's
response to these actions,
the machine receives rewards
if the environment
reacts positively,
or punishments
if it reacts negatively.
The machine will learn
from the reactions,
and this machine
is taught to react
in a manner
by which it can maximize rewards
and identify future events.
The objective could also be
to minimize future punishments.
This technique of learning
is related to the subjects
of control theory
in engineering
and decision theory
in statistics
and management sciences.
The main problems
studied in these two subjects
are more or less equivalent,
and the solutions
are similar as well,
but both subjects
focus on different aspects
of the problem.
There is also another technique
that uses unsupervised learning
and game theory.
The idea here
is similar to that
in unsupervised learning.
The machine produces actions
that affect
the surrounding environment,
and it receives rewards
or punishments
depending on the reactions
of the environment.
However,
the main difference
is that the environment
is not static.
It is dynamic,
and can include
other machines as well.
These other machines
are also capable
of producing actions
and receiving rewards,
or punishments.
So,
the objective of the machine
is to maximize
its future rewards,
or minimize
its future punishments,
taking into account
the effects
of the other machines
in its surroundings.
The application
of game theory
to such a situation
with multiple dynamic systems
is a popular area
of research.
Finally,
the fourth technique
is called
unsupervised learning.
In this technique,
the machine receives
training inputs,
but it does not receive
target outputs
or rewards
and punishments
for its actions.
This begs the question,
how can the machine
possibly learn anything
without receiving
any feedback
from the environment,
or having information
about target outputs?
However,
the idea is
to develop
a structure
in the machine
to build representations
of the input vectors
in such a manner
that they can be used
for other applications,
such as prediction
and decision making.
Essentially,
unsupervised learning
can be looked at
as the machine
identifying patterns
in input data
that would normally
go unnoticed.
Two of the most
popular examples
of unsupervised learning
are dimensionality reduction
and clustering.
The technique
of unsupervised learning
is closely related
to the fields
of information theory
and statistics.
Chapter 6
How to Create
and Train
Deep Learning Models
This chapter
provides information
on the three ways
an engineer
can train
a deep learning model
to classify objects.
Training from Scratch
If you want to train
the deep neural network
from scratch,
you should collate
a large volume
of labeled data.
You must then
design the network
to ensure
that it will learn
all the features
in the data set.
This is a good practice
for new applications
or for applications
that have multiple
output categories.
Most engineers
do not use this approach
since the network
will take a few days
or weeks
to learn the process
due to the large volume
of training data.
Transfer Learning
Most engineers
train deep learning networks
using the
transfer learning approach.
In this approach,
a pre-existing
or pre-trained model
is fine-tuned.
You can start
with networks
like GoogleNet
and AlexNet
and feed these networks
new data
containing unknown classes.
You can make
a few tweaks
to the network
which will allow
the network
to perform new tasks.
This means that
you do not require
large volumes of data
to train the network
which reduces
the computation time
to a few hours
or minutes.
If you want to use
transfer learning
to train a model,
you will need
an interface
that will allow you
to connect
to the pre-existing network.
This interface
will help you
enhance and modify
the network
to perform the task.
Software like
MATLAB
has functions
and tools
that you can use
for this purpose.
Feature extraction
This is a specialized
and slightly less
common approach
to training
a deep neural network.
Every layer
in the network
needs to identify
the different features
in the large dataset.
You can extract
these features
from the network
at any time
during the training process.
These features
can be used
to train
support vector machines
or a machine learning model.
Chapter 7
Applications of Deep Learning
Automatic Colorization
of Images
It is difficult
to add color
to black and white photographs.
Human beings
traditionally did this
since it was
a difficult task to do.
You can now use
deep learning networks
to automatically
add color
to photographs.
A deep learning network
will identify
the objects in the image
and their context
within the photograph.
It will then
add color
to the image
using that information.
This is a highly
impressive feat.
This capability
increases the use
of large
convolutional
and high-quality
neural networks
like ImageNet.
The approach
involves the use
of supervised layers
and CNNs
to recreate an image
by adding color
to it.
adding sound
to silent movies
In this task,
the deep neural network
must develop
or recreate sounds
that will match
a silent video.
Let us consider
the following example.
We need the network
to add sounds
to a video
where people
are playing drums.
The engineer
will provide the network
with close to
1,000 videos
with the sound
of the drum
striking many surfaces.
The network
will identify
the different sounds
and associate
the video frames
from the silent video
or movie
with the pre-recorded sounds.
It will then
select the sound
from the database
that matches
the video in the scene.
This system
is then evaluated
using a Turing test
where human beings
were asked
to differentiate
between the real
and synthesized video.
Both CNN
and LSTM
neural networks
are used
to perform
this application.
Automatic translation
Deep neural networks
can translate words,
phrases,
or sentences
from one language
to another
automatically.
This application
has been around
for quite some time now,
but the introduction
to deep neural networks
has helped the application
achieve great results
in the following areas.
Translation of images
Translation of text
For the translation of text,
the engineer
does not have to feed
the deep neural network
with a pre-processing sequence.
This allows the algorithm
to identify
the dependencies
between the words
in a sentence
and map them
to a new language.
The stacked networks
in a large LDTM
recurrent neural network
is used for this purpose.
You may have guessed
from previous sections
of the chapter
that a CNN
is used to verify
if an image has letters
and where it can find
those letters
in the scene.
Once the network
identifies these letters,
it can transform
the letters into text,
translate it
into different languages,
and recreate the image
with translated text.
This process is known
as instant visual translation.
Object detection
and classification
and classification
in images.
In this application,
the deep neural network
identifies and classifies
the objects in an image.
The network
classifies the images
into a set
of previously known objects.
Very large CNNs
have been used
to achieve accurate results
when compared
to the benchmark examples
of the problem.
Automatic handwriting generation
For this application,
the engineer must feed
the deep neural network
with a few handwriting examples.
This helps the network
generate new handwriting
for a given word,
phrase,
or sentence.
The data set
that the engineer
feeds the network
should provide
a sequence of coordinates
that the writer uses
when writing with a pen.
From the data set,
the network identifies
and establishes
a relationship
between the movement
of the pen
and the letters
in the data set.
The network
can then generate
new handwriting examples.
What is fascinating
is that the network
can learn different styles
and mimic those styles
whenever necessary.
Automatic text generation
The engineer
will feed the network
with a data set
that only includes text.
The network learns
this text
and can generate
using new text
character by character
or word by word.
This network
can learn
how to punctuate,
spell,
form sentences,
differentiate between paragraphs,
and capture the style
of the text
from the data set.
An engineer
uses large,
recurrent neural networks
to perform this task.
The network
establishes the relationship
between the items
in the many sequences
in the data set
and then generates
new text.
Most engineers
choose the LSTM
recurrent neural network
to generate text,
since these networks
use a character-based model
and generate
only one character
at a time.
Automatic image
caption generation
As the name suggests,
the model,
when given an image,
must describe
the contents
of the image
and generate
a caption.
In 2014,
many deep learning algorithms
use the models
for object detection
and object classification
in images
that generate a caption.
Once the model
detects objects
in the image
and categorizes them,
it will need
to label those objects
and form
a coherent sentence.
This is an impressive
application
of deep learning.
The models used
for the application
use large CNNs
to detect the objects
in the images
and a recurrent neural network
like the LSTM
to generate
a coherent sentence
using the labels.
Automatically playing a game
This is an application
where a machine
learns how it can play a game
using only the pixels
on the screen.
Engineers use
deep reinforcement models
to train a machine
to play a computer game.
Deep Mind,
which is now a part of Google,
works primarily
on this application.
Chapter 8
An Introduction to Python
Running Python
Python is a software
that can be installed
and run
on multiple operating systems,
including
macOS
or OS2,
Linux,
Unix,
and Windows.
If you are running Python
on GNU
slash Linus
or MacsOS X,
you may already have
the software installed
in the system.
It is recommended
that you use
this type of system
since it already has Python
set up
as an integral part.
The programs
in this book
work on any operating system.
Installing on Windows
If you are using Windows,
you will need to install Python
and configure certain settings
correctly
before you start working
on the examples
given in this book.
To do that,
you must refer to
specific instructions
provided for your operating system
on the following Python web pages.
www.wiki.python.org
slash m-o-i-n
slash beginnersguide
slash download
www.python.org
slash doc
slash faq
slash windows
docs.python.org
slash dev
slash 3.0
slash using
slash windows
dot html
You will first need to download
the official installer.
Alternative versions
for AMD
and Itanium machines
are available
at python.org
slash download.
This file,
which has a dot msi
extension,
must be saved
at a location
that you can find easily.
You can double-click this file
to start the Python
installation wizard,
which will take you
through the installation.
It is best to choose
the default settings
if you are unsure
of the answers.
Installing on other systems
You may choose
to install Python
on other systems
if you want to take advantage
of the latest versions
of Python.
The instructions
for Unix-like
and Linux systems
can be found
at the following links.
docs.python.org
slash dev
slash 3.0
slash using
slash unix.html
If you're using
OX10,
your instructions
are here.
www.python.org
slash download
slash mac
docs.python.org
slash dev
slash 3.0
slash using
slash mac
dot html
Choosing the right version
Different installers
include different numbers
after the word Python,
which refers to
the version number.
If you look at
the archives
on multiple websites,
the version numbers
will range
from 2.5.2
to 3.0
where the former
is an old
but usable version
of Python
while the latter
is the latest version.
The Python team
released version 2.6
at the same time
that it released
version 3.0.
There are some people
who may want to stick
to version 2
of Python
since they want
to continue
to write code
the old way
but still want to
benefit from
general fixes
and some of the
new features
introduced in
version 3.0.
The Python language
is continuously evolving.
Version 3.0
has become the norm
and has evolved
into version 3.1.1.
The newer versions
of 3.0
are refinements
of version 3.0.
Version 3.0
includes several changes
to the programming language
that is incompatible
with version 2.0.
You do not have to worry
about programming
using different versions
of Python
since there is only
a subtle difference
in the language
or syntax.
There may be some
differences running
Python on other
operating systems
which will be pointed
out in the book
wherever necessary.
Otherwise,
the codes in the book
will work in the same way
across different
operating systems.
This is one of the
many good points
of Python.
For the most part,
this book will concentrate
on the fun part,
learning how to write
programs using Python.
If you wish to learn
more about Python,
you should read
the documentation
prepared by the developers,
which is free
and well written.
It is available at
python.org
slash doc.
Learning while having fun
On most occasions,
people do not want
to have fun
when they work
in technical disciplines
since fun
is underestimated.
Every human being
only learns a subject
well when he or she
is having fun with it.
Developing software
using Python
is often an engrossing
and enjoyable experience,
partly because you can
test out your changes
as soon as you have
made them
without having to
perform any
intermediary steps.
Unlike other languages,
Python takes care
of most background tasks,
making it easier
for you to focus
on the code
and the design
of your code.
This makes it easy
for the user
to stay in the
creative flow
and continue to develop
and refine the program.
Python is easy
to read
and is one of many
languages
that use a syntax
that is closer to English.
Therefore,
you spend less time
trying to understand
what you have written,
which means you have
more time on hand
to understand
how the code
can be improved
and how you can
expand the code
to encompass
different aspects.
Another good thing
about Python
is that it can be useful
to complete any task,
regardless of how big
or how small
the task may be.
You can develop
simple text-driven
or numerical-based
programs,
as well as major
graphical applications.
There are some
limitations to the language,
but before you
identify them,
you will already
become adept
at programming,
which will help you
know how to work
around that limitation.
Chapter 9
Python Environment
for Deep Learning
This chapter
will provide information
on the environment
that you need to set up
to use Python
for deep learning.
You might install
the following software
to build deep learning
algorithms in Python.
Python 2.7 Plus
Matplotlib
SciPy
with NumPy
Theono
Keras
TensorFlow
Experts recommend
that you install
Python,
Matplotlib,
SciPy,
and NumPy
through the
Anaconda distribution,
since it comes
with all the packages.
You must ensure
that the different
software you download
is installed correctly.
To import the libraries
from the software
listed above
into Python,
type the following
commands in the
command line program
in Python.
$python
Python 3.6.3
Break
Anaconda
Custom
Parentheses
32-bit
End Parentheses
Break
Parentheses
Default
Comma
October 13th
Comma
2017
Comma
14
Colon
21
Colon
34
End Parentheses
Bracket
GCC
7.2.0
End Bracket
On Linux
Now import
the necessary libraries
and print the versions
in the output window.
Import
Nuppie
Print
Nuppie
Period
Underscore
Version
Underscore
You will receive
the version number
of the installed software.
Installation of
Keras,
TensorFlow,
and Thano
Before you install
the packages from
Keras,
TensorFlow,
and Thano,
you should confirm
your machine has
pip installed.
Pip is the
package management
system in
Anaconda.
You should type in
the following command
in Python
to confirm if the pip
has been installed.
$pip
When Python confirms
that pip has been
installed,
you should install
Keras and
TensorFlow
using the following
command.
$pip
install
Thano
$pip
install
TensorFlow
$pip
install
Keras
To confirm
if Thano
has been installed
in Python,
you should type
the following code.
$python
$python
$python
$python
$python
$python
$python
$python
$python
$python
$python
$python
$python
$python
Once you obtain
the version number
of Thano
that has been
installed on your
system,
you should confirm
if TensorFlow
has been installed
in Python
by executing
the following
line of code.
$python
hyphen
c
quote
import
TensorFlow
colon
print
TensorFlow
period
underscore
version
underscore
end quote
Now to confirm
the installation
of Keras
in your system,
execute the
following line
of code.
$python
hyphen
c
quote
import
Keras
colon
print
Keras
period
underscore
version
underscore
end quotation
using
TensorFlow
back end
You will receive
the version number
of TensorFlow
that is installed
on your system.
Chapter 10
How to develop
a neural network
in Python
using Keras
Now that you have
learned the necessary
libraries in Python,
you can build your
first neural network
using Keras.
Load data
If you want to use
a machine learning
algorithm that uses
stochastic models
to generate output,
you must use
a seed.
This is to ensure
that you can run
the same code
repeatedly
and still generate
the same result.
You should use
this method
if you want to
compare the results
from different models
or algorithms
for the same data
or to debug
some part of the code.
You can initialize
a random number generator
with any number
as the seed.
For example,
from Keras.models
import sequential
from Keras.layers
import dense
import numpy
numpy.random.seed
parentheses 7
end parentheses
The last statement
in the code above
allows you to fix
a random seed
that you can use
to reproduce
the results.
Now, you can load
the data into Python.
For the examples
in this chapter,
we will use
a data set
from the UCI
Machine Learning Repository.
The data set
we will be using
is the Pima Indians
onset of diabetes.
This data set
contains medical records
of multiple patients
from Pima India
and provides information
on whether they
developed diabetes
in five years
or not.
This is a binary
classification problem
since you are only
checking whether
the patient has
developed diabetes
or not,
where the onset
of diabetes
is labeled 1
and no diabetes
is labeled 0.
The input variables
used in the data set
are numerical,
which means that
you can use the data set
when you work
with neural networks
that need numerical
input and output values.
This is an ideal
example to use
for your first
neural network.
Please download
the data set
from the following
location.
raw.githubusercontent.com
slash jbrownlee
slash datasets
slash master
slash pima
hyphen indians
hyphen diabetes
dot data
dot csv
move the data set
to the local
working directory
where you have
saved the Python file
and save it
with a file name.
Now,
you can load
the file into Python
using the function
load text
parentheses
and parentheses
which is a
num5 function.
Go through the data set
and understand
the different variables
in the set.
The data set
has one output
variable in the last
column
and eight input
variables.
Once you load
the data
into Python,
you can split
the data
into input
and output
variables.
Data set
equals
numpy dot load
text
parentheses
quote
file
underscore name
dot csv
end quote
comma
delimiter
equals
quote
comma
end quote
end parentheses
X equals
data set
bracket
bracket
colon
comma
zero
colon
eight
end bracket
y equals
data set
bracket
colon
comma
eight
end bracket
In the first statement
of the code above,
you are loading the data set
into Python.
Remember to name the data set
once you download it.
The second and third statements
of the code split the data
into the input X and output Y variables.
There are nine columns in the data set,
and the range in the second line of the code,
zero colon eight,
will select the columns with their index between zero and seven.
In the first part of this section,
we defined the seed.
Now,
you can determine the neural network model.
Define the model.
You will use a sequence of layers when you define a model in Keras.
In this section,
you will learn to develop a sequential model and add layers to that model one at a time.
You can stop adding layers to the model once you are happy with the topology of the network.
You will first need to ensure that the input layer in your model has the correct number of inputs.
You can specify this when you create the first layer using the argument input dim,
and set it to eight for each of the input variables.
You may wonder how you will know the number of layers and what their types are.
This is a difficult question to answer,
but you can use the trial and error method to understand the network structure better.
You only need to worry about building a model that will capture the problem
and understand if the network can solve the problem or not.
In the example in this section,
we will use a network structure that has three layers and is fully connected.
You can define the fully connected layers in Python using the dense class.
You can specify the number of neurons in each layer as the first argument,
in it,
the initialization layer,
as the second argument,
and use the activation argument to specify the activation function.
In the example below,
we will assign weights to the network using a small number generated from the uniform distribution.
The weight is between the numbers 0 and 0.05 in the example below,
since that is the default weight assigned to the neurons in Keras.
An alternative would be to generate a random number from the normal or Gaussian distribution.
In this example,
we will use the sigmoid function for the outer layer
and the rectifier function for the first two layers of the network.
Earlier,
the tan and sigmoid functions were used for all layers.
Most engineers have moved from the tan activation function
to the rectifier activation function
to enhance the performance of the network.
The sigmoid function is used on the output layer
to ensure that the output is only between 0 and 1.
The function makes it easy for the network to map the data into different classes,
with probabilities 0 and 1,
using a threshold of 0.5.
Model equals sequential, parenthesis and parenthesis.
Model dot add, parenthesis, dense, parenthesis,
12, comma, input underscore dim,
equals 8, comma, activation equals,
quote, R-E-L-U, quote, and parenthesis and parenthesis.
Model dot add, parenthesis, dense, parenthesis, 8, comma, activation equals,
quote, R-E-L-U, end quote, and parenthesis and parenthesis.
Model dot add, parenthesis, dense, parenthesis 1, comma, activation equals,
quote, sigmoid, end quote, and parenthesis and parenthesis.
In the model we have created above,
the first layer has 8 input variables and 12 neurons.
The second layer, which is the hidden layer,
has 8 neurons,
and the output layer has only 1 neuron
to predict whether the patient has diabetes or not.
Compile the model.
Once you have defined the model, you can compile it.
Python uses the libraries Theono and TensorFlow
to compile the model since they have efficient numerical libraries.
These libraries are at the back end,
and they decide how the system should represent the network
for training or making predictions.
The back end will decide if the system should distribute
the functions of the network evenly between the CPU and GPU,
or if it should only use the CPU or GPU.
When you want to compile the model,
you should define some additional properties
that are necessary to train the network.
When I say training a network,
I mean that you should let the network identify the weights
or a set of weights it should use
to predict the results of the problem.
You should also specify the loss function
that you will use to evaluate the calculated weights,
the optimizer to search for different weights in the network,
and any other metrics that you want to collect during the training.
In the example below,
we use logarithmic loss,
which is defined as binary underscore crossentropy
in Keras for a binary classification problem.
We will also use atom,
which is the gradient descent algorithm,
since it is an effective default.
Since we are solving a classification problem,
we will report the accuracy of classification as the metric.
The code above is used to compile the model.
Now that the model is defined and compiled,
you can use it for computation.
Enter some data to execute the model.
You can either fit or train your model by using the fit function.
When you train the network,
the training process will run for a specific number of iterations,
called epochs.
You can specify the number of iterations using the epochs statement.
The model will run through the dataset and assign weights to the different layers in the network.
You can set the number of instances that the model can evaluate using the batch underscore size argument.
In the example below,
we will only run 150 iterations and use a batch size of 10.
This means we will only allow the model to define 10 instances.
You can either increase or decrease the size once you have run the model a few times.
Model.fit
The code above fits the model.
Depending on what the model decides,
the work will happen either on the CPU or GPU.
For the example we are using,
we do not require a GPU,
since the dataset is not vast.
If you want to run the code for a large dataset,
you can use GPU hardware that is available in the cloud.
You have trained the neural network and let it run through the dataset 150 times.
Now, you can evaluate how the neural network performs using the same dataset.
This will help you understand how well you have modeled the dataset,
but you will not gather any information on how the model will perform on a new dataset.
In this example,
we are using the same dataset to train and evaluate the performance of your model,
since it makes it easier for you to understand how to test the model.
You can split the dataset into training datasets,
and test datasets to train and evaluate your model.
You can evaluate the model using your training dataset and the evaluate function.
Pass the same input and output that you use to train the model.
The model will generate a prediction for every input and output pair.
It will also collect scores on any metrics that you have defined,
such as average loss or accuracy.
You can use the following code to evaluate the model.
Scores equal
model.evaluate
parentheses x, y
end parentheses
print
parentheses
quote
backslash
n
percentage
s
colon
space
percentage
dot
two f
percentage
percentage
end quote
percentage
start parentheses
model.metrics underscore names
bracket one
end bracket
comma
scores
bracket one
end bracket
asterisk
100
end parentheses
end parentheses
end parentheses
tie it all together
now that you know how to create a neural network in charis
let us tie all the steps up and write the complete code
the sentences beginning with the pound symbol represent the comments
these comments will help you understand what the statement in the code means
pound
create your first network in charis
from charis dot models
import sequential
from charis dot layers
import dense
import numpy
pound
fix random seed to reproduce the results
numpy dot random dot seed
parentheses
seven
end parentheses
pound
load the data set
data set
equals
numpy dot load
txt
parenthesis
quote
file underscore name
dot csv
end quote
comma
delimiter
equal
quote
comma
end quote
end parenthesis
pound
split the data into input and output class variables
x equals
data set
bracket
colon
comma
zero colon
eight
y equals data set
bracket
colon
comma
eight
end bracket
pound
create the model
model equals
sequential
parentheses
and parentheses
model dot add
parentheses
dense
parentheses
12
comma
input underscore
dim
equals
eight
comma
activation
equals
quote
relu
end quote
parentheses
and parentheses
model dot add
parentheses
dense
parentheses
eight
comma
activation
equals
quote
relu
quote
end parentheses
and parentheses
model dot add
parentheses
dense
parentheses
one
comma
activation
equals
quote
sigmoid
end quote
end
parentheses
and parentheses, pound, compile the model, model.compile, parentheses, loss equals, quote,
binary underscore, C-R-O-S-S-E-N-T-R-O-P-Y, end quote, comma, optimizer equals, quote, atom,
end quote, comma, metrics equal, bracket, quote, accuracy, end quote, end bracket, end
parentheses, pound, fit the model, model.fit, parentheses, x, comma, y, comma, epochs equals
150, comma, batch underscore size equals 10, end parentheses, pound, evaluate the model,
scores equal, model.evaluate, parentheses, x, comma, y, end parentheses, print, parentheses,
quote, backslash, n, percentage, S, colon, space, percentage, dot, 2F, percentage, percentage,
end quote, percentage, parentheses, model.metrics underscore name, bracket, one, end bracket,
comma, scores, bracket, one, end bracket, asterisk, 100, end parentheses, end parentheses.
When you run this model, you should always read the message that you receive at the end
of every iteration or epoch.
This message provides information on the accuracy of the data and evaluates the trained model
on the data set used for training.
If you use a Theano backend on your CPU, this code will take 15 seconds to run.
If you use Jupyter Notebook or IPython to run this program, you will receive an error since
the output progress on these platforms is barred during training.
You can turn off the output progress bars by making a change to the function verbose when
you fit the model.
You should also remember that the skill of your model will vary.
Neural networks are stochastic algorithms, which means that you can train a different
model with a different skill using the same data and algorithm.
This is not an error or a bug, but a feature of the neural network.
You should not worry if you do not receive the same output as the one above, although we
did fix the same seed.
Make predictions.
Now that you have trained the model, you may wonder how you can use the model to predict
the output on new data sets.
You can adapt the example above to generate predictions on the training data set.
It is easy to instruct the model to make predictions.
All you need to do is use the function model.predict.
You can use the signoid function to activate the output layer to ensure that the results are
only between 0 and 1.
You can convert the result into a binary prediction by using the rounding function on the data.
You can use the following code to make predictions.
Now that you have the choice of the model to make true model.predict.
Goad.p Felix model.parest. sdia.p Whats fully model.parest.
Create your first network in Keras.
From Keras.models, import sequential.
From Keras.layers, import dense.
Import numpy.
Pound.
Fix random seed to reproduce the results.
numpy.random.seed, parentheses, 7, end parentheses.
Pound.
Load the data set.
Data set equal numby.load.txt, parentheses, quote, file underscore name dot csv, end quote, comma, delimiter equal, quote, comma, end quote, end parentheses.
Pound.
Split the data into input and output class variables.
X equal data set, bracket, colon, comma, 0, colon, 8, end bracket.
Y equal data set.
Bracket, colon, comma, 8, end bracket.
Pound.
Create the model.
Model equals sequential, parentheses, end parentheses.
Model.add, parentheses, dense, parentheses, 12, comma, input underscore dim, equals 8, comma, activation equals, quote, RELU, end quote, end parentheses, end parentheses.
Model.add, parentheses, dense, parentheses.
Model.add, parentheses, dense, parentheses, 1, comma, activation equals, quote, sigmoid, end quote, end parentheses, end parentheses.
Pound. Compile the model.
Model dot compile. Parentheses. Loss equals. Quote. Binary underscore. C-R-O-S-S-E-N-T-R-O-P-Y. End quote. Comma.
Optimizer equals. Quote. Atom. End quote. Comma. Metrics equal. Bracket. Quote. Accuracy. End quote. End bracket. End parenthesis. Pound. Fit the model. Model dot fit. Parentheses. X comma Y comma. Epochs equals 150. Comma. Batch underscore size equals 10. End parenthesis.
Pound. Evaluate the model. Scores equal. Model dot evaluate. Parentheses. X comma Y. End parenthesis. Print. Parentheses. Quote. Backslash. N. Percentage. S. Colon. Space. Percentage dot 2F. Percentage. Percentage. End quote. Percentage. Parentheses. Model dot metrics underscore name. Bracket 1. End bracket. Comma.
Scores. Scores. Bracket 1. End bracket. Asterix. 100. End parenthesis. End parenthesis. Pound. Making predictions.
Predictions equal. Model dot predict. Parentheses. X. End parenthesis.
Rounded equal. Bracket. Round. Parentheses. X. Bracket 0. End bracket. End parenthesis. For X in predictions. End bracket.
Print. Parentheses. Rounded. End parenthesis.
In the above example, we are rounding the predictions to obtain whole numbers as results.
When you run the complete program, you will receive an output for every input pattern.
You can use these predictions in your application if necessary.
Chapter 11. How to evaluate the performance of a deep learning model.
You will need to make a lot of decisions when you design and configure the deep learning models.
Most of the decisions you make should be resolved through the trial and error method.
You should then evaluate the model on real data or test data.
Therefore, it is important that you identify a robust way to evaluate the performance of a deep neural network.
This chapter will provide some information on the different ways you can evaluate the model you are building using Keras.
Empirically Evaluate Network Configurations
There are many decisions that you should make when you design and configure deep learning models.
You can make most of these decisions by copying a pre-existing model and making changes to the model depending on your requirement.
The best technique is to design small segments of the model and evaluate the smaller segments using real data.
If the model performs well, you can combine the small segments of the model to develop the final model.
The decisions you need to make include the size of the network, the number of layers in the network, and their types,
activation function, number of iterations or epochs, choice of the loss function, and optimization procedure.
You often use deep learning models to solve problems where the input data is large.
That is, the data set has at least a million instances.
Therefore, you need to identify some criteria which will help you estimate the performance of different configurations of the network on unseen data.
Data Splitting
When you use large volumes of complex data, you need to give the deep neural network enough time to learn.
Instead of using a large data set to train the data, you can split the data into two sets, the training data set and the testing or validation data set.
Keras allows you to evaluate the deep learning algorithms using the following methods.
Use a manual data set for verification.
Use an automatic data set for verification.
Use an automatic data set for verification.
You can use Keras to split the data set into a training data set and a validation data set.
You can then use this validation data set to evaluate how your model performs, and validate the outputs after each epoch.
You can split the data including the validation split argument in the fit function of the code.
You can let Python know how much of the data you want to split into the validation data set.
For example, you can tell Python to hold back at least 20% or 30% of the training data set into the validation data set.
The example in this section shows how you can use Keras to split the training data set into a validation data set.
The data set used in the examples below is the same as the one we used in Chapter 6.
Pound
Neural Network with an Automatic Validation Set
From Keras.Models
Import Sequential
From Keras.Layers
Import Dense
Import Numpy
Pound
Fix Random Seed to reproduce the results
Numpy.Random.Seed
Parentheses 7
Pound
Load the data set
Data set equals
Numpy.Lowat.TXT
Numpy.Lowat.TXT
Parentheses
Quote
File underscore name
Dot CSV
End quote
Comma
Delimiter
Equal
Quote
Comma
End quote
End parentheses
Pound
Split the data into input and output class variables
Dim.Lowat.TXT
X equals data set
X equals data set
Bracket
Bracket
Colon
Comma
Zero
Colon
Eight
End bracket
Y equals data set
Bracket
Colon
Comma
Eight
End bracket
Pound
Create the model
Model equals sequential
Parentheses and parentheses
Model dot add
Parentheses
Parentheses
Dense
Parentheses
One
Comma
Activation
Equals
Quote
R.E.L.U.
End quote
End parentheses
End parenthesis
and parentheses
Model dot add
Dense
Parentheses
Eight
Comma
Activation
Equals
Quote
R.E.L.U.
Quote
End parenthesis
End parenthesis
Model dot add
Parentheses
Dense
Parenthesis
One
Comma
Activation equals, quote, sigmoid, end quote, end parenthesis, end parenthesis, pound, compile the model, model.compile, parentheses, loss equals, quote, binary underscore, C-R-O-S-S-E-N-T-R-O-P-Y, end quote, comma,
optimizer equals, quote, atom, end quote, comma, metrics equal, bracket, quote, accuracy, end quote, end bracket, end parenthesis, pound, fit the model, model.fit, parentheses, x, y, comma, validation underscore split, equal, 0.33, comma, epochs equal, 150.
comma, batch underscore size equal, 10, end parenthesis, end parenthesis, when you run the example above, you will receive the following output for each iteration or epoch.
This epoch will show the accuracy and loss on both the validation and training data sets.
Use a manual data set for verification.
Through Keras, you can manually specify the data set you want the model to use for validation during the training period.
In the example below, we will use the train underscore test underscore split function in the scikit library and separate the data into a training and validation data set.
The model will use 67% of the data set for training and the remaining 33% for validation.
You can specify the validation data set in the fit function using the validation data argument.
This argument takes the tuple of both the input and output data sets.
Pound.
Neural network with manual validation set.
From Keras.models, import sequential.
From Keras.layers, import dense.
Import numpy.
Pound.
Fix random seed to reproduce the results.
Numpy.random.seed, parenthesis, seven, end parenthesis.
Pound.
Load the data set.
Data set equals numpy.loadtxt, parenthesis, quote, file underscore name, dot csv.
End quote, comma, delimiter, equal, quote, comma, end quote, end parenthesis.
Pound.
Split the data into input and output class variables.
X equals data set.
Bracket, colon, comma, 0, colon, eight, end bracket.
Y equals data set.
Bracket, colon, comma, eight, end bracket.
Pound.
Split into 67% for train and 33% for validation.
X underscore train, comma, X underscore test, comma, Y underscore train, comma, Y underscore
test, equal, train underscore test, underscore split, parenthesis, X, comma, Y, comma, test
underscore, x underscore, afect, guess, lit, F mariage, while Dad, seventh, name nav, entit, researched,
, s underscore, Kkers, eight, comma, Y, comma, y, untold, dot, four,
Parentheses, dense, parentheses, 12, comma, input, underscore, dim, equals 8, comma, activation, equals, quote, relu, end, parentheses, end, parentheses.
Model.add, parentheses, dense, parentheses, 8, comma, activation, equals, quote, relu, quote, end, parentheses, end, parentheses.
Model.add, parentheses, dense, parentheses, 1, comma, activation, equals, quote, sigmoid, end, quote, end, parentheses, end, parentheses.
Pound.
Compile the model.
Model.compile, parentheses, loss, equals, quote, binary underscore, C-R-O-S-S-E-N-T-R-O-P-Y, end, quote, comma, optimizer, equals, quote, atom, end, quote, comma, metrics, equal, bracket, quote, accuracy, end, quote, end, bracket, end, parentheses.
Pound.fit model.model.fit, parentheses, x underscore train, comma, y underscore train, comma, validation underscore data, equals, x underscore test, comma, y underscore test, end, parentheses, comma, epochs, equal, 150, comma, batch underscore size, equal, 10, end, parentheses.
As with the earlier example, you will receive an output for every epoch, which includes the accuracy and loss for every model for both the training and validation data sets.
Manual K-Fold Cross-Validation.
One of the best ways to evaluate a deep learning model is using the K-Fold Cross-Validation method.
This method provides the designer with an estimate of how the model performs on unseen data.
This method splits the data into K-subsets and then runs the training data set through each of these subsets, except for the one that it holds out.
It then evaluates the model's performance on the data set that it has held out.
This process is repeated until every subset is held out of iteration at least once.
The model calculates an average of the estimates and takes that as the performance measure.
This method is often not used to evaluate a deep learning model, since it is expensive.
For instance, the K-Cross-Validation method is used for either 5 or 10 folds.
This means that 5 or 10 models must be constructed and evaluated, which increases the time taken to evaluate the model.
If you have a problem that is small and have enough resources to compute the number of models,
this method will help you obtain a less biased estimate of the model.
In the example below, we will use the Stratified K-Fold class from the Scikit library in Python
to split the training data set into 10 folds.
The algorithm tries to maintain a balance between the number of instances in each class in every fold.
In this example, we will create and evaluate the 10 models
and use the 10 splits of data to collect the scores.
The algorithm prints the performance for each model and stores it.
At the end of each cycle, the algorithm will calculate the average and the standard deviation of the model performance
and print it in the output window after every run.
This gives you a robust estimate of the performance of the model.
Pound.
Neural network for data set with 10-fold cross-validation.
From keras.models, import sequential.
From keras.layers, import dense.
From sklearn.model underscore selection, import stratified k-fold.
Import numpy.
Pound.
Fix random seed to reproduce the results.
Seed equal 7.
Numpy.random.seed, parentheses, seed, end parentheses.
Pound.
Load Pima Indians data set.
Data set equal
Numpy.loadtxt, parentheses, quote, file underscore name, dot csv, end quote, comma.
Delimiter equal
Quote, comma, end quote, end parentheses.
Pound.
Split the data into input and output class variables.
X equals data set.
Bracket, colon, comma, 0, colon, 8, end bracket.
Y equals data set.
Bracket, colon, comma, 8, end bracket.
Pound.
Define 10-fold cross-validation test harness.
K-fold equal
K-fold equal
Stratified k-fold, parentheses, n underscore splits, equal 10, comma, shuffle equal true, comma,
random underscore state, equal, seed, end parentheses.
CV scores equal
Bracket, end bracket.
For train, test in k-fold, dot split, parentheses, x, comma, y, end parentheses, colon.
Model equals
Model equals
Sequential, parentheses, end parentheses.
Model dot add, parentheses, dense, parentheses, 12, comma, input underscore dim, equals 8, comma,
Activation equals
Quote, R-E-L-U, end quote, end parentheses, end parentheses.
Model dot add, parentheses, dense, parentheses, 8, comma, activation equals, quote, R-E-L-U, quote, end
parentheses, end parentheses.
Model dot add, parentheses, dense, parentheses, 1, comma, activation equals, quote, sigmoid, end
quote, end parentheses, end parentheses.
Pound.
Compile the model
Compile the model
Model dot compile
Parentheses
Loss equals, quote, binary underscore, C-R-O-S-S-E-N-T-R-O-P-Y, end quote, comma,
Optimizer equals, quote, atom, end quote, comma,
Metrics equal, bracket, quote, accuracy, end quote, end bracket, end parentheses.
Pound.
Pound.
Fit the model
Model dot fit, parentheses, X, bracket, train, end bracket, comma, Y, bracket, train, end bracket,
comma, epochs, equal, 150, comma, batch underscore size equal, 10, comma, verbose equal, 0, end parentheses.
Pound.
Pound.
Pound.
Pound.
Scores equal, model dot, evaluate, parentheses, X, bracket, test, end bracket, comma, Y, bracket,
test, end bracket, comma, verbose equal, 0, end parentheses.
Print.
Parentheses, quote, percentage, S, colon, percentage, 0.2, F, percentage, percentage, end quote, percentage.
Parentheses, Parentheses, model dot, underscore, names, bracket, one, end bracket, comma,
Scores bracket, one, end bracket, asterix, 100, end parenthesis, end parenthesis.
Cvscores dot, append, parentheses, Scores bracket, one, end bracket, asterix, 100, end parenthesis.
Print.
Parentheses, quote, percentage, 0.2, F, percentage, percentage.
Parentheses, plus, slash, minus, percentage, 0.2, F, percentage, percentage, end parenthesis,
end quote, percentage, parentheses, Numpy dot, mean, parentheses, Cvscores, end parenthesis,
comma, Numpy dot, std, parentheses, Cvscores, end parenthesis, end parenthesis, end parenthesis.
You will receive the following output when you run the algorithm.
You will receive the following output when you run the algorithm.
Chapter 12. How to Save and Load Deep Learning Modules
Since deep learning models take hours, days, weeks, and sometimes months to train,
it is important to know how you should save the model and load it off the disk whenever necessary.
This chapter provides information on how you can save a Keras model to your disk
and load it into Python when you need to make any predictions.
With Keras, you do not have to worry about saving the architecture and model weights.
All model weights are saved in the HDFI format that is a grid format used to store multidimensional data.
You can save the model structure using the following formats, JSON and YAML.
We will look at the following examples in this chapter.
Save the model in the JSON format.
Save the model in the YAML format.
The examples in the chapter will also demonstrate how you can save and load the model weights to the HDFI format.
We will continue to use the dataset we used in the previous chapter for these examples.
You will first need to install H5PY in Python.
JSON is a simple format that allows you to use a hierarchy to describe the data.
Keras allows you to save the file and later load the file using the model underscore from underscore JSON function.
This will create a new model using the JSON specification or format.
The weights from the model are saved directly using the function save underscore weights
and are later loaded into Python using the load underscore weights function.
In the example below, you will load the model using the saved files and then create a new model.
It is important that you compile the loaded model before you begin to use it.
This is done to ensure that the predictions made using the efficient and appropriate computations from the Keras back-end.
Pound.
Neural network for the dataset serialized to JSON and HDF5.
From Keras.models.
Import.
Sequential.
From Keras.layers.
Import.
Dense.
From Keras.models.
Import.
Model underscore from underscore JSON.
Import.
Import.
Numpy.
Import.
OS.
Pound.
Fix random seed to reproduce results.
Numpy.
Random.
Seed.
Parentheses.
Seven.
End.
Parentheses.
Pound.
Load Pima Indians dataset.
Dataset equal.
Numpy.
Load.
TXT.
Parentheses.
Quote.
File underscore name.
Dot CSV.
End.
Quote.
Comma.
Delimiter equal.
Quote.
Comma.
End quote.
End parentheses.
Pound.
Split into input X and output Y variables.
X equal.
Dataset.
Bracket.
Colon.
Comma.
Zero.
Colon.
Eight.
End bracket.
Y equal.
Dataset.
Bracket.
Colon.
Comma.
Eight.
End bracket.
Pound.
Create model.
Model equal.
Sequential.
Parentheses.
End parentheses.
Model dot add.
Dense.
Parentheses.
Twelve.
Comma.
Input underscore dim.
Equal.
Eight.
Comma.
Kernel underscore initializer.
Equal.
Quote.
Uniform.
End quote.
Comma.
Activation.
Equal.
Quote.
RELU.
End quote.
End parentheses.
End parentheses.
Model dot add.
Parentheses.
Dense.
Parentheses.
8.
Comma.
Kernel underscore initializer.
Equal.
Quote.
Uniform.
End quote.
Comma.
Activation equal.
Quote.
RELU.
End quote.
End parentheses.
End parentheses.
Model dot add.
Parentheses.
Dense.
Parentheses.
One.
Comma.
Kernel underscore initializer.
Equal.
Quote.
Uniform.
End quote.
Comma.
activation equal, quote, sigmoid, end quote, end parenthesis, end parenthesis, pound, compile
model, model.compile, parenthesis, loss, equal, quote, binary underscore crocentropy, end quote,
comma, optimizer equal, quote, atom, end quote, comma, metrics equal, bracket, quote, accuracy,
end quote, end bracket, end parenthesis, pound, fit the model, model.fit, parenthesis, x, comma,
y, comma, epox equal, 150, comma, batch underscore size equal, 10, comma, verbose equal, 0, end
parenthesis, pound, evaluate the model, scores equal, model.evaluate, parenthesis, x, comma,
y, comma, verbose equal, 0, end parenthesis, print, parenthesis, quote, percentage, s, colon,
percentage, point, 2, f, percentage, percentage, end quote, percentage, parenthesis, model.metrics
underscore names, bracket, 1, end bracket, comma, scores, bracket, 1, end bracket, asterick,
100, end parenthesis, end parenthesis, pound, serialize model to json, model,
underscore, json, equal, model.to, underscore, json, parenthesis, end parenthesis, with open,
parenthesis, quote, model.json, quote, comma, end quote, w, end quote, parenthesis, as json,
underscore, json, underscore, file, colon, json, underscore, file, dot, write, parenthesis, model,
underscore, json, end parenthesis, pound, serialize weights to hdf5, model.save, underscore,
weights, parenthesis, quote, model.h5, end quote, end parenthesis, print, parenthesis, quote,
saved, model.to, end quote, end parenthesis, pound, load json, and create model, json, underscore,
underscore, file, equal, open, parenthesis, quote, model.json, end quote, comma, quote,
R, end quote, end parenthesis, loaded, underscore, model, underscore, json, equal, json, underscore,
file, dot, read, parenthesis, end parenthesis, json, underscore, file, dot, close, parenthesis,
end parenthesis, loaded, underscore, model, equal, model, underscore, from, underscore, json,
parenthesis, loaded, underscore, model, underscore, json, end parenthesis, pound, load weights into
new model.load, underscore, model, dot, load, underscore, weights, parenthesis, quote, model.h5, end quote,
and parentheses. Print parentheses, quote, loaded model from disk, end quote, end parentheses.
Pound. Evaluate loaded model on test data. Loaded underscore model dot compile, parentheses,
loss equal, quote, binary underscore, cross entropy, end quote, comma. Optimizer equal,
quote, rmsprop, end quote, comma. Metrics equal, bracket, quote, accuracy, end quote, end bracket,
end parentheses. Score equal, loaded underscore model dot evaluate, parentheses, x, comma, y,
comma, verbose, equals zero, end parentheses. Print parentheses, quote, percentage s, colon,
percentage, point, two, f, percentage, percentage, end quote, percentage, parentheses, loaded underscore
model dot metrics underscore name, bracket, one, end bracket, comma, score, bracket, one, end bracket,
asterisk, 100, end parentheses, end parentheses. When you run the example, you will find the
following output. ACC, 78.78%. Saved model to disk. Loaded model from disk. ACC, 78.78%.
The format of the model in JSON looks as follows. Brace, quote, keras underscore version, end quote,
colon, quote, colon, quote, 2.02, end quote, comma, quote, back end, end quote, colon, quote,
zung, end quote, comma, quote, config, end quote, colon, bracket, brace, quote, config, end quote,
colon, brace, quote, dtype, end quote, colon, quote, float 32, end quote,
Bias underscore regularizer, colon, null, comma.
Activation, colon, quote, relu, end quote, comma.
Bias underscore constraint, end quote, colon, null, comma.
Use underscore bias, end quote, colon, true, comma.
Quote, bias underscore initializer, end quote, colon, brace.
Quote, config, end quote, colon, brace, end brace, comma.
Quote, class underscore name, end quote, colon, quote, zeros, end quote.
End brace, comma.
Quote, kernel underscore regularizer, end quote, colon, null, comma.
Quote, activity underscore regularizer, end quote, colon, null, comma.
Quote, kernel underscore constraint, end quote, colon, null, comma.
Quote, trainable, end quote, colon, true, comma.
Quote, name, end quote, colon, quote, dense underscore one, end quote, comma.
Quote, kernel underscore initializer, end quote, colon, brace.
Quote, config, end quote, colon, brace.
Quote, max vol, end quote, colon, 0.05, comma.
Quote, min vol, end quote, colon, minus 0.05, comma.
Quote, seed, end quote, colon, null, end brace, comma.
Quote, class underscore name, end quote, colon, quote, random uniform, end quote, end brace, comma.
Quote, batch underscore input underscore shape, end quote.
Quote, colon, colon, colon, colon, colon, brace, comma.
Quote, class underscore name, end quote, colon, quote, dense, end quote, end brace, comma.
Brace, quote, config, end quote, colon, brace.
Quote, kernel underscore regularizer, end quote, colon, null, comma.
Quote, bias underscore regularizer, end quote, colon, null, comma.
Quote, activation, end quote, colon, quote, RELU, end quote, comma.
Quote, bias underscore constraint, end quote, colon, null, comma.
Quote, use underscore bias, end quote, colon, true, comma.
Quote, bias underscore initializer, end quote, colon, brace.
Quote, config, end quote, colon, brace.
End brace, comma.
Quote, class underscore name, end quote, colon, quote, zeros, end quote.
End brace, comma.
Quote, activity.
Activity underscore regularizer, end quote, colon, null, comma.
Quote, kernel constraint, end quote, colon, null, comma.
Quote, trainable, end quote, colon, true, comma.
Quote, name, end quote, colon, quote, dense underscore two, end quote, comma.
Quote, kernel underscore initializer, end quote,
colon, colon, colon, brace.
Quote, config, end quote, colon, brace.
Quote, max val, end quote, colon, 0.05, comma.
Quote, min val, end quote, colon, minus 0.05, comma.
quote, seed, end quote, colon, null, end brace, comma, quote, class underscore name, end quote,
colon, quote, random uniform, end quote, end brace, comma, quote, units, end quote, colon,
8. End brace, comma. Quote, class underscore name, end quote, colon, quote, dense, end quote.
End brace, comma. Brace. Quote, config, end quote, colon, brace. Quote, kernel underscore
regularizer, end quote, colon, null, comma. Quote, bias underscore regularizer, end quote, colon,
null, comma. Quote, activation, end quote, colon, quote, sigmoid, end quote, comma. Quote, bias
underscore constraint, end quote, colon, null, comma. Quote, use underscore bias, end quote,
true, comma, quote, bias underscore initializer, end quote, colon, brace, quote, config, end
quote, colon, brace, end brace, comma, quote, class underscore name, end quote, colon, quote,
zeros, end quote, end brace, comma, quote, activity underscore regularizer, end quote,
colon, null, comma, quote, kernel underscore constraint, end quote, colon, null, comma,
quote, trainable, end quote, colon, true, comma, quote, name, end quote, colon, quote, dense underscore
three, end quote, comma, quote, kernel underscore initializer, end quote, colon, brace, quote,
config, end quote, colon, brace, quote, maxval, end quote, colon, 0.05, comma, quote, minval,
end quote, colon, minus, 0.05, comma, quote, seed, end quote, colon, null, end brace, comma,
quote, class underscore name, end quote, colon, quote, random uniform, end quote, end brace, comma,
quote, units, end quote, colon, 1, end brace, comma, quote, class underscore name, end quote,
colon, quote, dense, end quote, end brace, end bracket, comma, quote, class underscore name,
end quote, colon, quote, sequential, end quote, end brace.
Save your neural network model to YAML. The example in this section is the same as the one above,
except for the fact that we will use the YAML format to specify the model. The model described
below uses YAML, saves the model to the file named neural underscore YAML, and loads the new model
using the function model underscore from underscore YAML. The weights are handled in the same way as
the example above in the HDF5 format as model.h5. Pound. Neural network for the Pima Indians data set to
serialize to YAML and HDF5. From keras.models, import sequential. From keras.layers, import dense.
From keras.models, import model underscore from underscore YAML. Import numpy.import os.
Pound. Fix random seed to reproduce results. Numpy.random.seed, parentheses, 7, end parentheses.
Pound. Load Pima Indians data set. Dataset equal numby.loadtxt, parentheses, quote, file underscore name,
dot csv, end quote, comma. Delimiter equal, quote, comma, end quote, end parentheses.
Pound. Split into input X and output Y variables.
X equal data set, bracket, colon, comma, 0, colon, eight, end bracket.
Y equal data set, bracket, colon, comma, eight, end bracket.
Pound. Create model. Model equal, sequential, parentheses, end parentheses.
Model dot add, dense, parentheses, 12, comma, input underscoreendenm
Add dense, parentheses, 12, input underscore dim equal 8, kernel underscore initializer equal, quote, uniform, end quote, comma, activation equal, quote, RELU, end quote, end parenthesis, end parenthesis.
Model dot add, parentheses, dense, parentheses, 8, comma, kernel underscore initializer equal, quote, uniform, end quote, comma, activation equal, quote, RELU, end quote, end parenthesis, end parenthesis.
Model dot add, parentheses, dense, parentheses, 1, comma, kernel underscore initializer equal, quote, uniform, end quote, comma, activation equal, quote, sigmoid, end quote, end parenthesis, end parenthesis.
Pound.
Compile model.
Model dot compile, parentheses, loss, equal, quote, binary underscore crocentropy, end quote, comma, optimizer equal, quote, atom, end quote,
comma, metrics, equal, bracket, quote, accuracy, end quote, end bracket, end parenthesis.
Pound.
Fit the model.
Model dot fit, parentheses, x, comma, y, comma, epochs equal, 150, comma, batch underscore size equal, 10, comma, verbose equal, zero, end parenthesis.
Pound.
Evaluate the model.
Scores equal, model dot evaluate, parentheses, x, comma, y, comma, verbose equal, zero, end parenthesis.
Print, parentheses, quote, percentage S, colon, percentage point two F, percentage percentage, end quote, percentage, parentheses, model dot metrics underscore names, bracket one, end bracket, comma, scores, bracket one, end bracket, asterick, 100, end parenthesis, end parenthesis.
Pound.
Pound.
Serialize model to YAML.
Model underscore YAML equal, model dot T O underscore YAML, parentheses, end parenthesis.
Pound.
Pound.
Pound.
Pound.
Serialize weights to HDF5.
Model dot C.
Pound.
Pound.
Pound.
Pound.
Pound.
Pound.
L
model. YAML underscore file equal open parentheses quote model dot YAML end quote comma quote
R end quote end parentheses. Loaded underscore model underscore YAML equal YAML underscore
file dot read parentheses end parentheses. YAML underscore file dot close parentheses
end parentheses. Loaded underscore model equal model underscore from underscore YAML
parentheses loaded underscore model underscore YAML end parentheses. Pound. Evaluate loaded
model on test data. Loaded underscore model dot compile parentheses loss equal quote binary
underscore cross entropy end quote comma. Optimizer equal quote RMS prop end quote comma. Metrics
equal bracket quote accuracy end quote end bracket end parentheses. Score equal loaded underscore
model dot evaluate parentheses X comma Y comma verbose equals zero end parentheses. Print
parentheses quote quote. Percentage S colon percentage point two F percentage percentage
end quote percentage. Parentheses loaded underscore model dot metrics underscore name bracket one
end bracket comma. Score bracket one end bracket asterisk 100 end parentheses end parentheses.
When you run the example you will find the following output. ACC 78.78 percent.
Saved model to disk. Saved model to disk. Loaded model from disk. ACC 78.78 percent.
The model that you described in the YAML format will look like this.
Backend colon Theano. Class underscore name colon sequential. Config colon.
Hyphen. Hyphen. Class underscore name. Colon. Dense. Config. Colon. Activation. Colon. R-E-L-U.
Activity underscore regularizer. Colon. Null. Batch underscore input underscore shape. Colon.
Exclamation point. Exclamation point. Python slash tuple. Bracket null comma eight end bracket.
Bias underscore constraint. Bias underscore constraint. Colon. Null. Bias underscore initializer. Colon.
Class underscore name. Colon. Zeros. Config. Colon. Brace end brace. Bias underscore regularizer. Colon. Null.
D-type. Colon. Float 32. Kernel underscore constraint. Colon. Null.
Colon. Kernel underscore initializer. Colon. Class underscore name. Colon. Random uniform.
Config. Colon. Brace. Max val. Colon. 0.05 comma. Min val. Colon. Minus 0.05 comma. Seed. Colon. Null. End brace.
Colon. Kernel underscore regularizer. Colon. Null. Name. Colon. Dense underscore one.
Trainable. Colon. True. Units. Colon. 12. Use underscore bias. Colon. True. Hyphen. Class underscore name. Colon. Dense.
Config. Config. Config. Colon. Activation. Colon. Relu. Activity underscore regularizer. Colon. Null.
Bias underscore constraint. Colon. Null. Bias underscore initializer. Colon. Class underscore name. Colon. Zeros.
Config. Colon. Brace. End brace. Bias underscore regularizer. Colon. Null.
Colon. Kernel underscore constraint. Colon. Null. Kernel underscore initializer. Colon.
Class underscore name. Colon. Random uniform. Config. Colon. Brace.
Max val. Colon. 0.05. Comma. Min val. Colon. Minus 0.05. Comma. Seed. Colon. Null. End brace.
Kernel underscore regularizer. Colon. Null. Name. Colon. Dense underscore two.
Trainable. Colon. True. Units. Colon. 8. Use underscore bias. Colon. True.
Hyphen. Class underscore name. Colon. Dense. Config. Colon. Activation. Colon. Signoid.
Activity. Activity underscore regularizer. Colon. Null. Bias underscore constraint. Colon. Null.
Bias underscore initializer. Colon. Class underscore name. Colon. Zeros. Config. Colon. Brace. End brace.
Bias underscore regularizer. Colon. Null. Kernel underscore constraint. Colon. Null.
Kernel underscore initializer. Colon. Class underscore name. Colon. Random uniform.
Config. Colon. Brace. Max val. Colon. 0.05. Comma. Min val. Colon. Minus 0.05. Comma. Seed. Colon. Null. End brace.
Kernel underscore regularizer. Colon. Null. Name. Colon. Dense underscore three.
Trainable. Colon. True. Units. Colon. One. Use underscore bias. Colon. True.
Keras underscore version. Colon. 2.0.2.
Chapter 13. Reducing Dropouts in Deep Learning Models.
One of the most powerful and simple techniques you can use to regularize a neural network in deep learning is the dropout.
In this chapter, you will gather information on what the dropout regularization technique is and how you can apply it to your deep learning models in Python using Keras.
Dropout regularization for neural networks.
Srivastava and his team proposed the use of dropouts to regularize neural networks.
They explained the concept of dropouts and how they help to regularize neural networks in the paper.
Dropout. A simple way to prevent neural networks from overfitting.
Dropout is a technique where the model ignores some neurons in every iteration during the training period.
These neurons are dropped out at random.
This means that these neurons do not contribute to obtaining the results.
Their contribution is not taken into account on the forward pass.
And any updates made during the iteration are not pushed back towards these neurons.
As the neural network learns, every neuron in the network is assigned a weight.
Some neurons in the network will be assigned an additional weight to represent a special feature in the training data set.
The neighboring neurons will rely on this special weight.
And if this weight is too high, it can result in a sensitive model that is specialized to the training data.
The phenomenon where many neurons rely on one neuron in the network is known as co-adaptation.
If neurons are dropped out of the network at random during training,
other neurons will need to step up and handle the problem.
They should learn to make predictions even if one neuron is missing.
This ensures that the network can learn to make multiple independent internal representations of the data set.
The effect of this is that the neurons in the network will become less sensitive to a change in the specific weights of other neurons.
This will help you develop a model that can make better predictions.
These models will not overfit the training data, which will reduce the effect of co-adaptation.
Regularizing dropouts in Keras
You can easily implement dropouts by selecting, at random, which notes should be dropped out.
Given a specific probability in each epoch or weighted cycle.
This is how you can implement dropouts in Keras.
It is important to remember that dropouts can only be used when you are training the model,
and not when you want to evaluate the model.
We will now look at different ways to use dropout in Keras.
The examples use the Sonar data set.
You can download this data set from the following location.
Archive.ics.uci.edu
Slash ml
Slash
Datasets
Slash
Connectionist
Plus
Bench
Plus
Parentheses
Sonar
Comma
Plus
Mines
Plus
VS
Dot
Plus
Rocks
The data set is taken from the UCI Machine Learning Repository.
Like the examples used above, this data set provides a binary classification problem.
The model needs to differentiate between mock mines and rocks from a sonar chirp that returns
in the data.
This is a good training data set to use, since the input values have the same scale and are
numerical.
The data set has one single output value, and 60 input values.
You must standardize these input values before you use the data set in the network.
The neural network model has an input layer, two hidden layers, and one output layer.
The first hidden layer has 60 units, and the second level has 30 units.
We train the model with a low momentum and learning rate using the Stochastic Gradient
Descent.
Once you download the data, move it to the working directly and give the file a name.
In the example below, we will evaluate the model using the scikit-learn method and tenfold
cross-validation.
This will help you identify the differences in the results.
The final code will develop the neural network model.
Pound.
Pound.
Pound.
Pound.
Pound.
Pound.
Taewon.
Pound.
Tawaiya.
Tawaiya.
Tawaiya.
Loan.
From keras.layers, import dense.
From keras.layers, import dropout.
From keras.wrappers.scikit-learn, import kerasclassifier.
From keras.constraints, import maxnorm.
From keras.optimizers, import sgd.
From sklearn.model.selection, import cross-val-score.
From sklearn.preprocessing, import label encoder.
From sklearn.model.selection, import stratified k-fold.
From sklearn.preprocessing, import standard scaler.
From sklearn.pipeline, import pipeline.
Pound.
Fix random seed for reproductibility.
Seed equals 7.
numpy.random.seed.seed.
Pound.
Load dataset.
DataFrame equal.
Read underscore csv.
Parentheses.
File underscore name.csv.
End quote.
Comma.
Header equal none.
End parentheses.
Dataset equal.
DataFrame dot values.
Pound.
Split into input x and output y variables.
x equal dataset bracket colon comma 0 colon 60.
End bracket.
Dot as type.
Parentheses float.
End parentheses.
Y equal dataset bracket colon comma 60.
End bracket.
Pound.
Encode class values as integers.
Encoder equal label encoder.
Parentheses and parentheses.
Encoder dot fit.
Parentheses y.
End parentheses.
Encoded underscore y equal.
Encoder dot transform.
Parentheses y.
End parentheses.
Pound.
Baseline.
D-E-F space create underscore baseline.
Parentheses and parentheses colon.
Pound.
Create model.
Model equals sequential.
Parentheses and parentheses.
Model dot add parentheses dense.
Parentheses 60.
Comma.
Input underscore dim.
Equals 60.
Comma.
Kernel underscore initializer.
Kernel underscore initializer.
Equal.
Quote normal.
End quote.
Comma.
Activation equal.
Quote.
R-E-L-U.
End quote.
End parentheses.
End parentheses.
Model dot add.
Parentheses dense.
Parentheses 30.
Comma.
Kernel underscore initializer.
Equal.
Quote normal.
End quote.
Comma.
Activation equal.
Quote.
R-E-L-U.
End quote.
End parentheses.
End parentheses.
Model dot add.
Parentheses.
Dense.
Parentheses.
One.
Comma.
Kernel underscore initializer.
Equal.
Quote.
Normal.
End quote.
Comma.
Activation equal.
Quote.
Sigmoid.
End quote.
End parentheses.
End parentheses.
Pound.
Compile the model.
S-G-D.
Equal.
S-G-D.
Parentheses.
I-R.
Equal.
0.01.
Comma.
Momentum equal.
0.8.
Comma.
Decay equal.
0.0.
Comma.
N-E-S-T.
E-R.
O-V.
Equal.
False.
End parentheses.
Model dot compile.
Parentheses.
Loss equal.
Quote.
Binary underscore crocentropy.
End quote.
Comma.
Optimizer equal.
S-G-D.
Comma.
Metrics equal.
Bracket.
Quote.
Accuracy.
End quote.
End bracket.
End parentheses.
Return.
Model.
Numpy dot.
Random dot.
Seed.
Parentheses.
Seed.
End parentheses.
Estimators.
Equal.
Bracket.
End bracket.
Estimators dot.
Append.
Parentheses.
Parentheses.
Quote.
End quote.
Comma.
Standard Scalar.
Parentheses.
End parentheses.
End parentheses.
End parentheses.
Estimators dot.
Append.
Parentheses.
Parentheses.
Quote.
M-L-P.
End quote.
Comma.
Keres Classifier.
Quote.
Build underscore F-N.
Equal.
Create underscore Baseline.
Comma.
Epochs equal.
Three hundred.
Comma.
Batch underscore Size.
Equal.
Sixteen.
Comma.
Verbose.
Equal.
Zero.
End parentheses.
End parentheses.
End parentheses.
Pipeline.
Equal.
Pipeline.
Parentheses.
Estimators.
End parentheses.
K-Fold.
Equal.
Stratified K-Fold.
Parentheses.
N underscore Splits.
Equal.
Ten.
Comma.
Shuffle.
Equal.
True.
Comma.
Random.
Underscore State.
Equal.
Seed.
End parentheses.
Results.
Equal.
Cross underscore Val.
Underscore score.
Parentheses.
Pipeline.
Comma.
X.
Comma.
Encoded underscore Y.
Comma.
Cv equal.
K-Fold.
End parentheses.
Print.
Parentheses.
Quote.
Baseline.
Colon.
Percentage.
Point two.
F.
Percentage.
Percentage.
Parentheses.
Percentage.
Point two.
F.
Percentage.
Percentage.
End parentheses.
End quote.
Percentage.
Parentheses.
Results dot mean.
Parentheses and parentheses.
Asterix.
100.
Comma.
Results dot std.
Parentheses and parentheses.
Asterix.
100.
End parentheses.
End parentheses.
This code will generate results with an 86% estimated classification accuracy.
Using dropout on the visible layer.
You can apply dropouts to the input neurons in the visible layer.
In the example below, we have included a dropout layer between the visible layer and
the first hidden layer.
The rate is set to 20%, which means that one in five inputs will be excluded from every
update cycle.
The input will be selected at random.
Additionally, we have imposed a constraint on the weights in every hidden layer.
This ensures that the maximum of the weights in the layers does not exceed three.
We do this by including the kernel underscore constraint argument in the dense class when you
build the model.
The momentum is increased to 0.9, and the learning rate is increased by one.
The paper mentioned in the first section of the book recommends that you increase the
learning rate when you want to use dropouts in your model.
Continuing from the example above, the code includes an input dropout for the same network.
Pound.
Dropout in the input layer with weight constraint.
DEF space create underscore model parentheses and parentheses colon.
Pound.
Create model.
Model equal sequential parentheses and parentheses.
Model dot add parentheses dropout parentheses 0.2 comma.
Input underscore shape equal parentheses 60 comma.
End parentheses and parentheses and parentheses.
Model dot add parentheses dense parentheses 60 comma.
Kernel underscore initializer equal quote normal end quote comma.
Activation equal quote RELU end quote comma.
Kernel underscore constraint equal max norm parentheses 3 end parentheses.
End parentheses and parentheses.
Model dot add parentheses dense parentheses 30 comma.
Kernel underscore initializer equal quote normal end quote comma.
Activation equal quote RELU end quote comma.
Kernel underscore constraint equal max norm parentheses 3 end parentheses.
End parentheses and parentheses.
Model dot add parentheses dense parentheses 1 comma.
Kernel underscore initializer equal quote normal end quote comma.
Activation equal quote sigmoid end quote end parentheses and parentheses.
Pound.
Compile model.
SGD equal SGD parentheses.
IR.
IR equal 0.1 comma.
IR equal 0.1 comma.
IR equal 0.1 comma.
IR equal 0.1 comma.
Momentum equal 0.9 comma.
IR equal 0.9 comma.
Decay equal 0.0 comma.
NESTEROV equal 0.0 comma.
NESTEROV equal false.
End parentheses.
Model dot compile.
Parentheses.
Loss equal quote binary underscore crocentropy end quote comma.
Optimizer equal 0.1 comma.
Optimizer equal SGD comma.
Metrics equal bracket quote.
Accuracy end quote end bracket end parentheses.
Return model.
Numpy dot random dot seed parentheses 7 end parentheses.
Estimators equal bracket end bracket.
Estimators dot append parentheses parentheses quote.
Standardize end quote comma.
Standard scalar parentheses end parentheses end parentheses end parentheses.
Estimators dot append parentheses parentheses quote.
MLP end quote comma.
Keras classifier parentheses.
Build underscore FN equal.
Create underscore model comma.
Epox equal 300 comma.
Batch underscore size equals 16 comma.
Verbose equals 0.
End parentheses end parentheses end parentheses.
Pipeline equal pipeline parentheses estimators end parentheses.
Kfold equal stratified Kfold parentheses.
N underscore splits equal 10 comma.
Shuffle equal true comma.
Random underscore state equal seed end parentheses.
Results equal cross underscore val underscore score.
Parentheses.
Pipeline comma X comma.
Encoded underscore Y comma.
CV equal Kfold end parentheses.
Print parentheses quote.
Quote visible colon percentage point two F.
Percentage percentage percentage.
Parentheses.
Percentage point two F.
Percentage percentage.
End parentheses.
End quote.
Percentage.
Parentheses.
Results dot mean.
Parentheses and parentheses.
Asterix 100.
End parentheses.
Results dot STD.
Results dot STD.
Parentheses end parentheses.
Asterix 100.
End parentheses.
End parentheses.
When you run the example above, you will notice that the classification accuracy has reduced
in at least one test run.
You will receive the following output when you run the code.
Visible colon 83.52 percent.
Parentheses.
7.68 percent.
End parentheses.
Using dropout on hidden layers.
You can apply dropouts to the hidden layers and neurons in the model.
The example below shows you how you can apply a dropout between two hidden layers, and between
the output layer and the last hidden layer.
The dropout rate used is 20 percent, and a constraint is placed on the weights used in these
layers.
DEF space create underscore model parentheses and parentheses colon pound create model
model equal sequential parentheses and parentheses model dot add parentheses dense
parentheses 60 comma input underscore dim equal 60 comma kernel underscore initializer
equal equal equal quote normal end quote comma activation equal quote relu end quote comma
kernel underscore constraint equal max norm parentheses three end parentheses end parentheses
model dot add parentheses dropout parentheses 0.2 end parentheses end parentheses
model dot add parentheses dense parentheses 30
Kernel underscore initializer equal
Normal end quote comma
Activation equal
RELU end quote comma
Kernel underscore constraint equal
Max norm parenthesis 3
End parenthesis end parenthesis end parenthesis
Model dot add parenthesis
Dropout parenthesis 0.2
End parenthesis end parenthesis
Model dot add parenthesis dense
Parenthesis 1 comma
Kernel underscore initializer equal
Quote normal end quote comma
Activation equal
Quote sigmoid end quote
End parenthesis end parenthesis
Pound
Compile the model
SGD equal
SGD parenthesis
IR equal 0.1 comma
Momentum equal 0.1
9 comma
Decay equal 0.0 comma
NESTEROV equal
False end parenthesis
Model dot compile
Parenthesis
Loss equal
Quote
Binary underscore cross entropy
End quote
Comma
Optimizer equal
Optimizer equal
Optimizer equal
SGD comma
Metrics equal
Bracket quote
Accuracy end quote
End bracket
End parenthesis
Return model
Numpy dot random dot seed
Parenthesis 7
End parenthesis
Estimators equal
Bracket end bracket
Estimators dot append
Parenthesis parentheses
Parenthesis quote
Standardize end quote
Comma
Standard scalar
Parenthesis end parenthesis
End parenthesis end parenthesis
Estimators dot append
Parenthesis parenthesis
Quote
MLP end quote
Comma
Keras classifier
Parenthesis
Build underscore
FN equal
Create underscore
Model
Comma
Epox equal
300
Comma
Batch underscore
Size equals
16
Comma
Verbose equals
Zero
End parenthesis
End parenthesis
End parenthesis
Pipeline equal
Pipeline
Parenthesis
Estimators
End parenthesis
Kfold
Equal
Stratified Kfold
Parenthesis
N underscore
Splits
Equal
Ten
Comma
Shuffle
Equal
True
Comma
Random
Underscore
State
Equal
Seed
End parenthesis
Results
Equal
Cross
Underscore
Val
Underscore
Score
Parenthesis
Pipeline
Comma
X
Comma
Encoded
Underscore
Y
Comma
CV
Equal
Kfold
End parenthesis
Print
Parenthesis
Quote
Hidden
Colon
Percentage
0.2f
Percentage
Percentage
Parenthesis
Percentage
0.2f
Percentage
Percentage
End parenthesis
End quote
Percentage
Parenthesis
Results
Results
Dot
Mean
Parenthesis
End
Parenthesis
Asterisk
100
Comma
Results
Dot
Std
Parenthesis
End
Parenthesis
Asterisk
100
End
Parenthesis
End
Parenthesis
The output
for the above
code is
Hidden
Colon
83.59%
Parenthesis
7.31%
End
Parenthesis
You will find
that using
dropouts
for the problem
and the chosen
network did not
improve the
performance of
the model.
The performance
was worse than
the benchmark.
It is possible
that you may need
to increase the
number of epochs
during the
training stage.
Alternatively,
you may need to
fine-tune the
learning rate in
the code.
Tips for
using dropout
The paper
mentioned above
provides information
on the many
machine learning
problems.
They also provide
information on what
you should consider
when you use
dropouts in your
model.
It is better to
use a small
dropout value
anywhere between
20% and 40%
of the neurons.
It is best to
use 20% as the
starting point.
If you use a
very small or
very big value of
probability, the
network will not
learn enough.
If you want to
include dropouts in
your model, you
should use a large
model since you
will get better
performance.
Dropouts help the
model learn how to
represent the data
without the use of
some neurons in the
layers.
You should use
dropouts on hidden
and visible units.
To obtain good
results, you should
apply dropouts at
each layer in the
network.
You can use a
large learning rate
with large momentum
and decay.
Increase the
learning rate in
the model by a
factor of 100 and
use a momentum
value of 0.99 or
0.9.
Maintain the size of
the weights in the
network.
If you have a large
learning rate, it
can result in a
very large weight.
When you impose or
maintain the size of
the weight, it will
improve the results
of the model.
Conclusion
Thank you for
purchasing the book.
Over the course of
the book, you will
learn more about
neural networks and
deep learning.
You will also learn
how to build a deep
neural network in
Python using
Keras.
The examples in
this book are easy
to follow, and you
can use these
examples as a base
when you want to
build your own deep
neural network.
If you do not know
how to develop a
deep learning model,
or are unsure of
how to build the
model, you can use
pre-existing codes
and tweak them to
help you obtain the
desired output.
I hope you can
build a basic deep
learning model using
the information
present.
The End
This has been
Deep Learning with
Python, a
Fundamentals Guide to
Understanding Machine
Learning and
Artificial Intelligence
with Scikit-Learn,
TensorFlow, and
Keras.
Written by
Sebastian Dark.
Narrated by
Chris Klein.
Copyright 2018
by Sebastian Dark.
Production
Copyright by
Sebastian Dark.
Audible hopes you
have enjoyed this
program.
Thank you.
