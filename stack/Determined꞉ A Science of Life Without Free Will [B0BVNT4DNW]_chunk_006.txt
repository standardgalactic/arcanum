Neuron being polarized in a negative direction.
It is briefly hyper-polarized
into what is called the refractory period.
Yep.
During it, the neuron has trouble getting it up
to a positively charged action potential.
It's all over with indeed.
Suppose there's a problem with this system.
Some protein is out of whack
so that the refractory period doesn't occur.
Consequence?
There are abnormal bursts of high-intensity clusters
of action potentials,
one on top of the other.
Or suppose some inhibitory neurons stop working.
The result is a different route to neurons
having abnormal clusters of excitation.
What we have just described
are the two broad underlying causes
of epileptic seizures.
Too much excitation
or too little inhibition.
Scores of textbooks
and tens of thousands of research papers
have explored the causes of such
synchronized overexcitation.
faulty genes,
concussive head injury,
birth complications,
high fevers,
some environmental toxins.
Amid all this complexity,
this disease,
which afflicts 40 million people worldwide
and kills more than 100,000 a year,
is about too much excitation
and or
too little inhibition
in the nervous system.
Predictably,
all this was discovered
only recently.
But epilepsy
is an ancient disease.
The subtype of seizures
that most people are familiar with
is a grand mal seizure,
where the sufferer convulses
and writhes
with automatic movements,
frothing at the mouth,
and the eyes roll up.
All sorts of opposing muscle groups
are stimulated at once.
The person falls to the ground,
explaining the name given
to epilepsy
by many of the ancients.
The falling sickness.
Clinically accurate descriptions
of seizures
go back to at least
the Assyrians,
almost 4,000 years ago.
Some of the insights generated
were remarkably prescient.
Ancient Greek physician
Hippocrates, for example,
noted that chronic seizures
often arise with a delay
after a traumatic brain injury,
something we're still trying
to sort out
on a molecular level.
Mind you, though,
there are plenty
of scientific missteps.
There was epilepsy
supposedly being caused
by faces of the moon
and their influence
on brain fluids.
With 1,600 years going by
before someone was able
to statistically disprove
a link between epilepsy
and lunar phases.
Pliny the Elder
thought someone got epilepsy
from eating an epileptic goat,
sidestepping the issue of,
okay, but where did that goat
get its epilepsy from?
Carnivorous epileptic goats
all the way down.
The 2nd century physician Galen
worked with the prevailing wisdom
that the body is built
on the four humors.
Black bile,
yellow bile,
phlegm,
and blood.
Galen's theory centered
on the ventricles
of the brain.
According to him,
phlegm could occasionally thicken
into a plug
in the ventricles,
and a seizure was the brain's attempt
to shake it loose.
Note that in this framework,
the clotted phlegm
is the disease,
and a seizure
is a protective response
that just happens
to cause more problems
than it solves.
These first hints
of scientific explanation
also produced stabs
at treatment.
In Greece in the 4th century BC,
one involved the person
with epilepsy
drinking a concoction
made of the genitals
of seals
and hippos,
the blood of a tortoise,
and the feces
of a crocodile.
Other supposed cures
included
drinking the blood
of a gladiator
or of someone
who had been decapitated.
There was
rubbing the sufferer's feet
with menstrual blood
or consuming
burned human bones.
Just to put our current
single-payer health insurance
debates in perspective,
Athenaeus of Naukratus,
another 2nd century sage,
reported on one physician
who claimed to be able
to cure epilepsy,
details unclear,
but who would do so
only if the patient
agreed to become
his slave afterward.
These primitive attempts
at understanding
the disease
produced plenty of horrors.
There was the erroneous belief
that epilepsy
was an infectious disease,
leading to people
with epilepsy
being marginalized
and stigmatized,
unable to share food
with others,
unwelcome and sick
in awkward places.
Even worse
was the mostly erroneous belief
that epilepsy
was heritable.
Only a tiny percentage
of cases
are due to
heritable mutations.
This led to prohibitions
on people
with epilepsy marrying.
In various
European locales,
men with epilepsy
would be castrated,
a practice lasting
into the 19th century.
Among the 16th century Scots,
if a woman with epilepsy
became pregnant,
she would be buried
alive.
And by the 20th century,
the same medical ignorance
led to the compulsory
sterilization
of thousands
with epilepsy.
In the U.S.,
the landmark case
was Buck v. Bell,
1927,
where the Supreme Court
upheld the legality
of the state of Virginia
forcibly sterilizing
the feeble-minded
and epileptic
in a law
that was not repealed
until 1974.
The practice was legal
in most states
during the 20th century
and was particularly
common in the South,
where it was sardonically
known as
a Mississippi appendectomy.
The same was the case
throughout Europe,
with the practice
peaking, naturally,
in Nazi Germany.
In 1936,
the Third Reich
arranged for an honorary
doctorate
for Harry Laughlin,
the American eugenicist
who was the architect
of the Virginia law,
and at the Nuremberg trials,
Nazi doctors
explicitly cited
Buck v. Bell
in their defense.
Now, these were all
the horrors
generated by wrong science.
But science,
wrong or otherwise,
was an obscure sideshow
when it came to epilepsy.
Because starting
millennia ago,
for most people,
ranging from peasants
to sages,
the explanation
for seizures
was obvious.
Demonic possession.
The Mesopotamians
called epilepsy
the hand of sin,
considering it to be
a sacred disease,
and were impressively
attuned to the
heterogeneity of seizures.
People with what was
probably petty
mal epilepsy
with auras
were viewed as having
a good kind
of sacred possession,
often associated
with prophecy.
But what were
most likely
grand mal seizures
were the doings
of demons?
Most Greek
and Roman physicians
believe the same,
with the most
cutting-edge
integrating demonic
interpretations
with materialistic
medical notions.
Demons made the soul
and body
become unbalanced,
producing
the falling disease.
Among Galen's followers,
demons caused
phlegm to thicken.
Christianity
got on the bandwagon
thanks to a
New Testament precedent.
In Mark 9,
verses 14-29,
a man brings
his son to Jesus,
saying there is
something wrong
with him.
Since he was a child,
a spirit comes
and seizes him,
making him mute.
And then the spirit
throws him to the ground,
where he foams
at the mouth,
grinds his teeth,
and becomes rigid.
Can you cure him?
Of course,
says Jesus.
The man presents
his son,
who is promptly seized
by that spirit
and falls to the ground,
convulsing and foaming.
Jesus perceives
that the boy
is infested
with an unclean spirit
and commands it
to come out
and be gone.
The seizing ceases,
and thus the epilepsy
demonic possession link
was established
in Christianity
for centuries to come.
Now,
harboring a demon
inside you
can cut a couple
of different ways.
One is where
an innocent bystander
is cursed into possession
by some witch
or warlock.
I saw this attribution
in the parts
of rural East Africa
where I worked,
usually leading to efforts
to identify
and punish
the perpetrator.
But the other
is where epilepsy
is a sign
of the person
themselves
having welcomed
in Satan.
This view
predominantly held sway
throughout Christendom.
Naturally,
a late medieval
period Christian
did not have
Jesus' power
to purge epileptics
of their demon.
Instead,
a different sort
of solution emerged,
made most consequential
by a pair
of German scholars.
In 1487,
the two Dominican friars,
Heinrich Kramer
and Jakob Sprenger,
published
Malleus Maleficarum,
Latin for
Hammer of the Witches.
It was in part
a religio-political
polemic,
a vigorous refutation
of any bleeding hearts
of that time,
who suggested
that there was actually
no such thing
as witches.
And once that liberal
tomfoolery
was out of the way,
the book was
an instruction manual,
the definitive guide
for both religious
and secular authorities
to recognize witches
for who they were,
get them to confess,
and then
dole out justice.
One reliable indicator
that someone was a witch?
Seizures,
of course.
Hundreds of thousands
of people,
almost all female,
were persecuted,
tortured,
killed
during this period
of witch-hunting.
Malleus Maleficarum
arrived just in time
to take advantage
of the recently invented
printing press,
went through 30 editions
over the subsequent century,
and was read
throughout Europe.
While the focus
of the book
was not remotely epilepsy,
its message was clear.
Epilepsy was brought on
by someone's own,
freely chosen evil.
And such demonic possession
represented a danger
to society
and needed to be dealt with.
And masses of people
with some haywire
potassium channels
in their neurons
were burned at the stake.
With the enlightenment
of the enlightenment,
witch hunts began
to be more metaphorical.
But epilepsy
was no less burdened
with a perception
of its sufferers
being at fault
in some manner.
It was a disease
of moral turpitude.
It joined going blind
and growing hair
on your hands
as the supposed wages
of sinful masturbation,
excessive
and synchronized
action potentials
in neurons,
all because someone
was pleasuring themselves
too often.
For women,
it could be caused
by an unseemly interest
in sex,
and occasionally cured
in the 19th century
by genital mutilation.
Sex outside of holy matrimony
was a risk factor as well.
In 1800,
the British physician
Thomas Beddows
came up with
one of the most
low-energy versions
of blaming the victim
I have ever heard of,
positing that seizures
were caused by people
being excessively sentimental
and reading too many novels,
instead of
living the vigorous
outdoor life of gardening.
In other words,
over the course
of a few centuries,
we've gone from epilepsy
being caused
by grasping Beelzebub
to your bosom
to its being caused
by reading too many
Harlequin romances.
Or not.
Amid the continuity
of blaming the victim,
there was also
the continuity
of those with epilepsy
being viewed
as a threat,
but on medico-legal
rather than theological grounds.
We live in a remarkable time,
with an array of medications
available that prevent
most seizures
in most people
with epilepsy.
But prior to the early
20th century,
a person with epilepsy
might experience
many hundreds of seizures
in their lifetime.
Temkin describes
one survey
in the early 19th century
documenting that
chronically hospitalized
people with epilepsy
averaged two seizures
a week
for years.
One consequence of this
is the eventual emergence
of considerable amounts
of brain damage.
My lab spent decades
studying how seizures
can damage
or kill neurons,
and trying,
mostly unsuccessfully,
to develop gene therapy
strategies
to try to protect
such neurons.
Basically,
the repeated bursts
of firing
deplete neurons
of energy,
leaving the cells
without the energetic
means to clean up
damaging things like
oxygen radicals
in the aftermath.
Decades of damaging
seizures typically produced
extensive cognitive decline,
accounting for the numerous
19th century hospitals
and institutes
devoted to the
epileptics
and feeble-minded.
In addition,
seizure-induced damage
often occurred
in frontal cortical regions
involved in impulse control
and emotional regulation,
accounting for another
flavor of institution,
that devoted to the
epileptic insane.
Independent of people
with epilepsy
undergoing a vastly
larger number of seizures,
than is commonplace today.
The prevalence of epilepsy
was higher,
thanks to higher rates
of head injuries
and of febrile epilepsy
due to infectious diseases
that we are now spared.
The higher prevalence,
coupled with someone
with epilepsy
typically experiencing
far more seizures
than we are accustomed
to today,
made people back then
more aware of the
extraordinarily rare cases
of epilepsy
being associated
with violence.
This can involve
automatisms of
aggressive behavior
during a psychomotor seizure,
which was given
the Victorian label
of Führer epilepticus.
More common
is aggression
immediately following
a seizure,
where the person,
in a state of
agitated confusion,
violently resists
being constrained.
Rarer are bursts
of violence
coming hours later.
The violence
typically follows
a cluster of seizures,
shows no evidence
of premeditation
or motive,
and comes in a rapid,
fragmentary burst
of stereotyped movement
that lasts for less
than 30 seconds.
Afterward,
the person is stricken
with remorse
and remembers nothing.
A 2001 paper
describes one such case
of a woman
whose rare,
intractable epilepsy
produced seizures
virtually daily
that were associated
with outbursts
of agitated aggression.
She had been arrested
32 times
for such violent incidents.
The severity
of violence escalated,
culminating in a murder.
The seizure focus
was near the amygdala,
and after surgical removal
of that part
of her temporal lobe,
both the seizures
and the aggressive outbursts
stopped.
Cases like these
are so immensely rare
that a single example
merits a paper
being published.
The millions of people
with epilepsy
have no higher rates
of violence
than anyone else,
and the majority
of any such violence
is unrelated
to the disorder.
Nonetheless,
by the 19th century,
there was a widespread
public association
of epilepsy
with violence
and criminality.
Malleus Maleficarum Redux
People with this disease
brought it on themselves
with their moral failings
and constitute
a threat to society
for which they must
be held responsible.
But there was
a glimmer of hope.
Nineteenth-century science
was advancing
in such a way
that you could imagine
the chain of insights
that would link
time's knowledge
to the present.
Autopsy studies
had finally eliminated
the notion
of plugs of phlegm.
Statisticians
had finally eliminated
the moon
from the picture.
Neuropathologists
were beginning
to note extensive damage
in the post-mortem brains
of people
with a history
of repeated seizures.
This was the era
of galvanism
and animal electricity,
the growing recognition
of the electrical nature
of the signals
by which the brain
made muscles move,
that the brain itself
was some manner
of electrical organ,
which suggested
that epilepsy
might involve
some manner
of electrical problem.
A giant among
neurologists
named Hugh Lings Jackson,
an utter genius,
introduced the idea
of localization,
where in the body
convulsive twitching
and movements
at the start
of a seizure
could tell you
where in the brain
the problem
was centered.
But something
arguably even more
important was happening,
the whispers
of modernity,
the first time
that people were
starting to say,
it's not him,
it's his disease.
In 1808,
a person who had killed
while having a seizure
was acquitted,
with more such cases
to follow.
By mid-century,
psychiatry heavyweights
like Benedicte Morel
and Louis de la Sueve
were more generally
arguing that people
with epilepsy
could not be held
responsible for their actions.
In a key publication
in 1860,
the psychiatrist
Jules Follray
wrote,
He's teetering
on the edge
of the first half
of this book,
but he can't quite
follow through
and concludes,
oxymoronically,
Still,
when we do not
limit our observations
to those with epilepsy
secluded in the mental asylum,
when we also take into account
all those who live in society,
without anyone suspecting
the existence
of their illness,
it becomes impossible
not to attribute
to some of them
the privilege
of moral responsibility,
if not for the entirety
of their lives,
then at least
for significant periods
of their existence.
thus,
someone has not
the slightest responsibility,
while still having
moral responsibility.
You're sure
you still want
to hitch a wagon
to modern versions
of this impossible
compatibilism?
Which brings us
to the present.
Imagine the tragic scenario
of some middle-aged man
on his way to work,
who,
in the middle of driving,
suddenly has
a grand mal seizure.
He's otherwise
perfectly healthy,
zero prior history
of anything
that could have
predicted this,
utterly from out of nowhere.
In his convulsing,
arms twisting the wheel
every which way,
foot repeatedly slamming
on the gas,
he loses control
of the car.
He strikes a child,
who is killed.
Here are some of the things
that are unlikely
to happen.
The man,
slumped over the wheel,
still convulsing
and frothing,
is pulled from the car
and beaten to death
by the witnesses.
The man,
when eventually brought
to court for a hearing,
has to be spirited
in the back way,
wearing a bulletproof vest,
because of the vengeful mob
on the courthouse steps
threatening to string him up
if he is not punished
appropriately.
The man is convicted
of anything like murder,
manslaughter,
or vehicular homicide.
Instead,
the loved ones
of that child
with their lives
dripped apart by pain
will lament forever
the monumentally bad luck
of what happened,
akin to
if the driver
had had a fatal heart attack
from out of the blue,
if a comet
had fallen from the sky,
if an earthquake
had come and split
the earth open,
swallowing their baby.
Oh,
it isn't that clean,
of course.
We desperately
search for attribution.
Wait,
he had no medical history
of anything?
Was he taking
some sort of medicine
at the time
that was the cause
and no one warned him?
Was he drinking,
and that somehow
triggered a seizure?
When did he have
his last checkup?
Why didn't the doctor
spot this brewing?
He had to have been
acting oddly that morning.
No one at home
stopped him from driving?
Was there some
blinking strobe light
at the time
that triggered the seizure?
Someone who should have
known that that was unsafe?
On and on.
We seek attribution.
We seek blame.
And if we are lucky,
the facts become
emotionally acceptable
as well.
And we reach a conclusion
that would have been
unthinkable to a
sixteenth-century parent
grieving over the
febrile death
of their child,
convinced that some
witch caused it.
It is not the driver's
fault that this happened,
that he lost control
of the car.
There is no one
who had the freedom
to have willed this
not to have happened.
Just the most
sickening bad luck
that any parent's
heart should have
to bear.
And this is some
approximation of
what now happens,
in that the driver
would not be charged
with anything.
We've done it.
We now think differently
than people did
in the past.
Of course,
there is still
massive societal stigma
about epilepsy,
particularly among those
who are less educated.
Because of a still
widespread belief
that epilepsy is
contagious and
or a form of
mental illness,
half of people
with the disease
report feeling
stigmatized.
When this happens
to children,
it predicts lower
performance and more
behavior problems
in school.
In the developing
world,
there is still a
common belief that
epilepsy has
supernatural causes,
and nearly half of
the people queried
would object to
sharing a meal with
someone with epilepsy.
To quote the
Indian neurologist
Rajendra Kale,
the history of
epilepsy can be
summarized as
4,000 years of
ignorance,
superstition,
and stigma,
followed by
100 years of
knowledge,
superstition,
and stigma.
Nonetheless,
there has been a
massive shift from
the past.
After those
four millennia,
we've left behind
the Mesopotamians
and Greeks,
Kramer and
Spranger,
Lombroso and
Bedos.
Most people in
the westernized
world have
subtracted free
will,
responsibility,
and blame out
of their thinking
about epilepsy.
This is a
stunning accomplishment,
a triumph of
civilization and
modernity.
So the shifting
views of epilepsy
provide a great
model for the
more global task
that is at the
center of this
book.
But that's only
half the challenge,
because whether
one thinks about
witches or
thinks about
overly synchronized
neurons,
someone having a
seizure can still
be dangerous.
It's that
canard again.
Oh, so you're
saying that
murderers and
thieves and
rapists aren't
responsible for
their behavior?
You're just going
to have them out
on the streets,
preying on all
of us?
No, that half
of the issue has
been solved as
well, in that
people with
uncontrolled
seizures are not
supposed to
operate dangerous
things like cars.
someone who has
a seizure in the
sort of circumstance
described would
have their license
suspended until
they have been
seizure-free for an
average of six
months.
It's how things
work these days.
When someone has
had a first seizure,
mobs of parasite-
riddled Yahoo
peasants with
pitchforks don't
gather to witness
the ritualistic
burning of the
epileptic's driver's
license.
The heartbreak
of a tragedy
doesn't get
translated into a
frenzy of
retribution.
We have been
able to subtract
blame and the
myth of free
will out of the
entire subject,
and nonetheless
have found
minimally constraining
ways of protecting
people who suffer,
directly or
secondarily,
from this terrible
disease.
A learned,
compassionate person
from centuries
past, steeped in
Malleus Maleficarum
would be flabbergasted
at how we've come
to think this way.
We've changed.
Sorta.
Putting our money
where our mouth is.
On March 5th,
2018, Dorothy
Bruns, driving her
Volvo sedan on a
commercial street in
Brooklyn, had a
grand mal seizure.
She seemingly
slammed her foot on
the accelerator, and
her car went through
a red light,
striking a group of
pedestrians in a
crosswalk.
Twenty-month-old
Joshua Liu and
four-year-old Abigail
Blumenstein were
killed.
And their mothers,
along with another
pedestrian, were
seriously injured.
Bruns' car dragged
Joshua's stroller
350 feet before it
swerved into a
parked car and
stopped.
In the altar of
flowers and teddy
bears placed there by
community members,
someone included a
stroller painted
white, a ghost
stroller, akin to
the ghost bikes that
are often placed to
mark where a
bicyclist has been
killed.
There was initially
some skepticism that
she had actually had
a seizure.
One neighborhood
resident stated that
Bruns didn't look like
she had a seizure at
all.
She was saying,
hello, hello, what
happened?
What happened?
When you have a
seizure, you're out.
And she was active.
But it was a
seizure.
Bruns was still
twitching and foaming
at the mouth when
police got there, and
she had two more
seizures in the
subsequent hours.
Despite what was
just described, Bruns
was charged with
involuntary manslaughter
and criminally
negligent homicide.
Eight months later,
awaiting trial, she
killed herself.
Why the different
outcome?
Why not?
It's not her, it's
her disease?
Because Bruns' case
was not the
hypothetical one
outlined previously,
where the perfectly
healthy individual
from out of nowhere
had a seizure.
Bruns had a history
of seizures that
were resistant to
medication, along
with multiple
sclerosis, strokes,
and heart disease.
In the previous two
months, three doctors
had told her that she
was not safe to
drive.
And yet, she
did.
And there have
been other versions
of this theme.
In 2009,
Auburn Scarlett
was convicted of
murder.
He had failed to
take his medications
for his epilepsy,
had a seizure,
and struck and
killed two
pedestrians in
Manhattan.
In 2017,
Emilio Garcia,
a New York City
taxi driver,
pleaded guilty
to murder.
He hadn't taken
his mans for
his disorder,
had a seizure
while driving,
and killed two
pedestrians.
And in 2018,
Howard Unger
was convicted
of manslaughter.
He failed to
take his meds,
had a seizure,
and lost control
of his car,
killing three
pedestrians in
the Bronx.
Look, if you're
taking this book
seriously, as I
mostly do, it is
clear where this
must head.
At every one of
those junctures,
these individuals
had to make a
decision.
Should I drive
even though I
didn't take my
meds?
A decision like
any other,
whether to pull
a trigger,
participate in
mob violence,
pocket something
that isn't yours,
forego a party
in order to
study, tell the
truth, run into
a burning building
to save someone.
All the usual.
And we know
that that decision
is as purely
biological as
when you fling
your leg out
when hit on the
right spot on
your knee.
Just vastly more
complicated biology,
most dramatically
in its interaction
with environment.
So you sit at
the juncture of
deciding, should
I drive without
my meds or do
the harder,
right thing?
It's back to
chapter 4.
How many neurons
are there in your
frontal cortex and
how well do they
work?
What do the
underlying disease
and the drugs
taken for it do
to your judgment
and frontal
function?
Is your frontal
cortex a little
light-headed and
sluggish because
you skipped
breakfast and now
your blood sugar
levels are low?
Have you had a
sufficiently lucky
upbringing and
education to have
a brain that has
learned about the
effects of blood
sugar on decision
making and frontal
function and a
frontal cortex
functional enough to
make you have
decided to eat
breakfast?
What are your
gonadal steroid
hormone levels that
morning?
Has stress in the
previous weeks to
months neuroplastically
impaired your frontal
function?
Do you have a
toxoplasma infection
latent in your brain?
At one point in
adolescence, were
your meds working
well enough that
you could finally do
the single thing that
made you feel normal in
the face of a
shattering disease,
namely driving a
car?
What were your adverse
childhood experiences
and ridiculously lucky
childhood experiences?
Did your mother drink a
lot when you were a
fetus?
What sort of dopamine
D4 receptor gene
variant do you have?
Did the culture that
your ancestors developed
glorify following rules?
Or thinking of
others?
Or taking risks?
On and on.
We're back to the
table in chapter 4.
Having seizures and
deciding to drive, even
though you haven't taken
your meds, are equally
biological, equally the
product of a nervous
system sculpted by
factors over which you
had no control.
And despite that, this is
so hard.
When Garcia didn't take
his meds, one of those
killed was a child.
When Unger didn't, it
was a child and her
grandfather, out
trick-or-treating.
It turned out that the
reason that Scarlett
wasn't taking his meds was
because it interfered with
his enjoyment of liquor.
The judge, at sentencing,
called him an abomination.
I feel crazy, embarrassed,
trying to make the argument
anchored in the last
paragraph's Science, and
in chapter 4, that not only
does someone not deserve to
be blamed or punished for
having seizures, but it is
equally unjust and
scientifically unjustifiable
to make someone's life a
living hell because they
drove despite not having
taken their meds, even if
they did that because they
didn't want those meds
interfering with their
getting a buzz when
drinking.
But this is what we must
do if we are to live the
consequences of what
science is teaching us,
that the brain that led
someone to drive without
their meds is the end
product of all the things
beyond their control from
one second, one minute,
one millennium before.
And likewise, if your brain
has been sculpted into one
that makes you kind, or
smart, or motivated.
This multi-century arc of the
changing perception of
epilepsy is a model for what
we have to do going forward.
Once, having a seizure was
steeped in the perception of
agency, autonomy, and freely
choosing to join Satan's
minions, now we effortlessly
accept that none of those
terms make sense, and the
sky hasn't fallen.
I believe that most of us
would agree that the world
is a better place because
sufferers of this disease are
not burned at the stake.
And even though I am
hesitant to continue this
writing here, oh no, I'm
going to alienate the
listener into thinking that
all this is simply too way
out there.
The world will be an even
more just place when we
make the same transition
and attribution, when
thinking of these people
who drove, despite not
having taken their meds.
There is no place for
burning at the stake here
either.
This history of epilepsy
frustrates me a bit.
It is great to be able to
pinpoint just when 19th
century physicians and
legal scholars were first
embarking on subtracting out
responsibility.
To track down the perfect
paper in some 1860s French
medical journal and get
it translated.
But simply because of the
antiquity, there's no way
to know something even
more important.
When did the average
person begin to think
differently about epilepsy?
When would someone at a
dinner party have discussed
a newspaper article about
how epilepsy was being
viewed in a new light?
When did well-informed
teenagers start feeling
contemptuous that their
clueless parents still
believe that masturbation
caused epilepsy?
When did most people
begin to think that
epilepsy is caused by
demons was as silly as
hellstorms are caused by
witches?
Those are the transformations
that matter.
And to get a feel for what
change like that looks like,
we have to examine the more
recent history of another
tragic misconception.
Generators and Refrigerators
While every mental illness
on earth exacts a massive
toll, you really, really do
not want to have
schizophrenia.
There have been idiotic,
new-agey fads that have
somehow arrived at a view
of the disease as having
all sorts of hidden
blessings.
Notions of schizophrenia as
being the label given to the
truly sane people in an
insane world.
Schizophrenia as a
wellspring of creativity,
spirituality, or of deep,
shamanistic spirituality.
These pronouncements have the
nostalgic, neo-60s tinge of
people in cranberry bell-bottoms
doling out a lot of bread for
their primal scream therapy.
Some are advanced by people
whose credentials have made
their prattling truly dangerous.
There are no hidden blessings
in schizophrenia.
It is a disease that devastates the
lives of its sufferers and their
families.
Schizophrenia is a disease of
disordered thought.
If you meet someone whose
individual sentences sort of
make sense, but are juxtaposed
with meandering incoherence,
where, after 30 seconds, you can
already tell something is not
right with them, there's a good
chance it's schizophrenia.
And if it is a homeless person
muttering in fragments of
thought, they are likely to have
been deinstitutionalized and dumped
out on the streets, for lack of an
alternative.
It affects 1-2% of the population,
regardless of culture, gender,
ethnicity, or socioeconomic status.
A remarkable thing about the disease
is that the chaotic thought has some
consistent features to it.
There's tangential thought and loose
associations, where a logical sequence
of A to B to C, instead, veers off
every which way, the person
ricocheting about, pulled by the
sounds of words, their homonyms,
vaguely discernible leaps of
connectiveness, tangenting loosely
with elements of delusion, of
paranoid persecution.
Add to that, the hallucinations.
Most of them are auditory, taking the
form of hearing voices, incessant,
often taunting, threatening,
demanding, demeaning.
These are some of the major
positive symptoms of schizophrenia.
Traits that appear in its sufferers
are not normally found in others.
The negative symptoms of the disease,
the things that are absent,
include strong or appropriate
emotions, expression of affect,
and social connections.
Add to that high rates of suicide,
self-mutilation, and violence,
and the hidden blessings nonsense
is hopefully expunged.
A strikingly consistent feature of
schizophrenia is that the onset is
typically in late adolescence or early
adulthood.
However, in retrospect, there are
milder abnormalities stretching back
to infancy.
Individuals destined for a
schizophrenia diagnosis have higher
rates of soft, neurological signs in
early life, such as late standing and
walking, delayed toilet training,
sustained problems with bedwetting.
Moreover, there are behavioral
abnormalities early in childhood.
In one study, trained observers who
watched home movies were able to
identify children destined for the
disease.
Amid most people with schizophrenia
being no more violent than anyone
else, the elevated levels of
violence take us in an obvious
direction.
If someone commits a violent act during
a schizophrenic delusion, should they be
held accountable?
When did average people start thinking,
it's not him, it's his disease?
In 1981, John Hinckley, long
suffering from schizophrenia, attempted to
assassinate Ronald Reagan, which injured
Reagan, along with a police officer and a
secret service agent, and eventually caused
the death of Press Secretary James Brady.
When he was found not guilty by reason of
insanity, much of the country erupted in
outrage.
Three states banned the insanity defense.
Most other states made it more difficult to
mount.
Congress accomplished the same by passing the
Insanity Defense Reform Act, signed into
law by Reagan.
So we still have a ways to go.
But the point of this section isn't the
demonization and criminalization of
schizophrenia and its parallels to
epilepsy.
Instead, it has to do with its cause.
You're a woman in the early 1950s.
The war years were, of course, immensely
hard, raising three small kids on your own
with your husband in the service.
But thank God, he came back safe and
sound.
You have a home in the new American Eden,
the suburbs.
The economy is booming, and your husband
recently got a promotion as he's rising up the
corporate ladder.
Your teenagers are thriving, except for your
oldest, the 17-year-old, who is increasingly
worrying you.
He's always been different from the rest of
you, who are so, well, normal.
Extroverted.
Athletic.
Popular.
With each passing year since he was little,
he's become more withdrawn, disconnected,
saying and doing odd things.
He had imaginary friends until a much older age
than his peers, but hasn't had an actual
friend in years.
You have to admit that it makes sense that
he's shunned given his peculiarities.
He talks to himself a lot, often showing
emotions completely inappropriate to the
circumstances.
And recently, he has become obsessed with the
idea that the neighbors are spying on him,
even reading his thoughts.
This is what finally prompts you to take him to
the family doctor, who refers you to a specialist
in the city, a psychiatrist, with a stern manner and
European accent.
And after a variety of tests, the doctor gives you a
diagnosis.
Schizophrenia.
You've barely heard of the disease, and the little that you
know evokes nothing but horror.
Are you sure?
You ask repeatedly.
With absolute certainty.
Is there a treatment?
You are given a few options, all of which will eventually turn out to be
useless.
And then you ask the key question.
What caused this disease?
Why is he sick?
And there's an assured answer.
You did.
You caused this disease because of your terrible
mothering.
It was called
schizophrenogenic mothering.
And it had become the dominant explanation for the disease,
rooted in Freudian thinking.
The first wave of Freudian influences in America,
early in the 20th century,
was a fairly inconsequential fad,
mostly for New York intellectuals.
Titillating and mildly scandalous
because of its focus on sex.
It was already waning by the 1920s.
Then the 1930s brought the European intelligentsia fleeing Hitler,
a bounty of refugees that turned the U.S.
into the center of the intellectual universe.
And this included most of the leading lights of Freudian thinking,
the next generation of psychodynamic royalty.
With their confident, authoritative air of European intellectual superiority,
they proceeded to wow the yokels of American psychiatry
and become the dominant model of thought.
By 1940,
the chair of every major American medical school's psychiatry department
was a Freudian psychoanalyst.
A stranglehold that was to last many decades.
In the words of the influential psychiatrist E. Fuller Torrey,
the transformation of Freud's theory from an exotic New York plant
to an American cultural kudzu
is one of the strangest events in the history of ideas.
And these were not the Freudians of yore,
going on in a charmingly scandalous way about penis envy.
Freud himself had little interest in schizophrenia
or in psychoses in general,
greatly preferring genteel, neurotic, educated clients
who were the worried well.
The next generation of Freudians,
who helped instill what became the psychodynamic cliché
of blaming your parents for your psychological problems,
had many in their cadre with a strong interest in psychoses.
The schizophrenogenic mothering notion
emerged from a chilling hostility toward women,
often propounded by female analysts.
The refugee Freudian Frida Frommreichmann wrote in 1935
that the schizophrenic is painfully distrustful
and resentful of other people
due to the severe early smothering and rejection
he encountered in important people of his infancy and childhood.
As a rule, mainly in a schizophrenogenic mother.
The analyst Melanie Klein,
a refugee in the UK rather than the US,
wrote of psychosis,
it arises in the first six months of life
as the child spits out the mother's milk,
fearing the mother will revenge herself
because of his hatred of her.
Strange, toxic gibberish.
Every accusing psychoanalyst had a slightly different notion
of just what was pathological
about schizophrenogenic mothering.
But the general theme centered on mothers supposedly being rigid,
rejecting and unloving,
domineering,
or anxious.
And in the face of all that,
all the child can do is retreat
into schizophrenic delusions
and fantasy.
A theoretical elaboration was soon added
by the anthropologist
Gregory Bateson,
working with psychoanalysts
in the form of the double-bind theory of schizophrenia.
In that view,
the core of all of those supposedly malign maternal traits
became the generation of emotional double-binds,
highly aroused circumstances
where the child is damned if he does,
damned if he doesn't.
This would be produced by the mother
who harangues the child,
saying,
Why don't you ever say you love me?
Why don't you ever say you love me?
I love you,
says the child,
and the mother retorts,
How is that supposed to mean anything
when I have to ask for it?
And in the face of unwinnable emotional assaults like that,
schizophrenia serves as a protective retreat of a child
into their own fantasy world.
There were soon elaborations on the theory
and ones that could be vaguely considered
to be liberal or humane.
Theoreticians in the psychodynamic fold
broadened their thinking to include the possibility
that a kid could be sufficiently screwed up
to become schizophrenic
thanks to being double-binded by the father.
Nonetheless,
the more general picture was of the father
as passive and hen-picked.
Culpable only insofar as
he didn't reign over
that schizophrenogenic harpy of a wife
loose in the house.
Things expanded even further outward
with the possibility
that the culprit was the entire family.
By the 1970s,
this family systems approach
was embraced by the first wave
of feminist psychiatrists,
one proponent writing approvingly that
only recently have psychiatrists
been talking about schizophrenogenic families.
Wow!
Progress!
Progress!
So what is actually wrong?
Naturally,
there is no empirical evidence whatsoever
in support
of schizophrenogenic mothering
or any of its variants.
Our modern understanding of schizophrenia
bears no resemblance
to these earlier Brothers Grimm fairy tales.
We now know that schizophrenia
is a neurodevelopmental disorder
with strong genetic components.
A great demonstration of this
is the fact that if someone has the disease,
their identical twin,
who shares all their genes,
has a 50% chance of having it as well,
versus the usual 1-2% risk
in the general population.
The genetics of schizophrenia,
the genetics of schizophrenia, however,
are not about a single gene that has gone awry,
as compared with classic single gene disorders,
such as cystic fibrosis,
Huntington's disease,
or sickle cell anemia.
Instead,
it arises from an unlucky combination
of the variants
of an array of genes,
many of which are related
to neurotransmission
and brain development.
However,
the collection of genes
does not cause schizophrenia,
but instead,
increases the risk for it.
This is implicit
in flipping the finding
just mentioned on its head.
If someone has the disease,
their identical twin
has a 50% chance
of not having it.
In a classic gene-environment interaction,
getting the disease
basically requires
a combination
of the genetic vulnerability
plus a stressful environment.
What sort of stress?
During fetal life,
disease risk many years later
is raised by prenatal malnutrition.
For example,
the Dutch Hunger winter famine
of 1944
greatly boosted
the incidence of schizophrenia
among individuals
who had been fetuses
at that time.
Exposure to any
of a number of viruses
by way of maternal infection,
placental bleeding,
maternal diabetes,
or infection
with the protozoan parasite
Toxoplasma gondii.
Perinatal risk factors
include premature birth,
low birth weight,
and small head circumference,
hypoxia during delivery,
emergency C-section,
and being born
during winter months.
Later during development,
the risk is raised
by psychosocial stressors
such as loss of a parent
to death,
parental separation,
early adolescent trauma,
migration,
and urban living.
So the disease arises
from genetic risk
that leaves someone's brain
teetering on a cliff
coupled with a stressful environment
that then pushes it
over the edge.
What abnormalities
are in the brain
after it's been pushed off?
The most dramatic
and reliable one
involves an excess
of the neurotransmitter
dopamine.
This chemical messenger
plays a role
particularly
in the frontal cortex,
in marking the salience
of an event.
Unexpected reward
and we think,
whoa, that's great!
What can I learn
about what just happened
to make it more likely
to happen again?
Unexpected punishment
and it's,
whoa, awful!
What can I learn
to make it less likely?
Dopamine is the mediator
of the message,
pay attention,
this is important.
The best evidence
is that not only
are dopamine levels
elevated in schizophrenia,
but this is due
to random bursts
of its release,
producing random bursts
of salience.
For example,
if you have schizophrenia
and a pointless dump
of dopamine
just happens to occur
when you are noting
someone glancing at you,
then,
heavy with this faux feeling
of significance
in the glance,
you conclude
that they are monitoring you,
reading your mind,
Schizophrenia
is a thought disorder
of,
as it's termed,
aberrant salience.
Aberrant salience
is thought
to also contribute
to another defining feature
of the disease,
namely,
the hallucinations.
Most people have
an internal voice
in their heads,
narrating events,
reminding us of things,
intruding with
unrelated thoughts,
have a random burst
of dopamine
along with one of those,
and it becomes marked
with so much salience,
so much presence,
that you perceive it,
respond to it
as an actual voice.
Most schizophrenic
hallucinations
are auditory,
reflecting how much
of our thinking
is verbal,
and as a truly
remarkable exception
that proves the rule,
there have been reports
of congenitally deaf
individuals
with schizophrenia
whose hallucinations
are in American
sign language,
where some
hallucinate
a pair of
disembodied hands
signing to them,
or being signed
to by
God.
The disease
also involves
structural changes
in the brain.
This is a bit
tricky to demonstrate.
The first evidence
came from
post-mortem comparisons
of the brains
of people
with schizophrenia
with control brains
after death.
The nature
of the structural
abnormalities
raised the possibility
that the finding
was a post-mortem
artifact.
That is,
brains of people
with schizophrenia
for some reason
were more likely
than control brains
to get squished
from being removed
during autopsy.
Though a little
far-fetched,
this worry
was eliminated
when neuroimaging
came along,
showing the same
structural problems
in the brains
while people
were still alive.
The other
potential con
found that still
needed to be
eliminated
concerned medications.
If you observe
something structurally
different in the brain
of, say,
a 40-year-old
with schizophrenia,
is the difference
due to the disease
or to the fact
that they have been
taking various
neuroactive drugs
for decades?
As a result,
the gold standard
in the field
emerged to be
neuroimages
of the brains
of adolescents
or young adults
just diagnosed
with a disease
who had not
been medicated yet.
And eventually,
once it was possible
to identify
those genetically
at risk
and follow them
from childhood,
seeing who would
develop the disease
and who would not,
it became clear
that some of the
brain changes
were happening
well before
the most serious
symptoms
were emerging.
So these brain
changes preceded
and predicted
the disease.
The most dramatic
change
is that the cortex
is abnormally
thin,
compressed,
hence the worry
about squishing.
There are logical
differences as well
in the ventricles,
those fluid-filled
caverns inside the
brain.
Specifically,
if the cortex
is thin,
compressed,
the ventricles
enlarge,
pressing outward.
This raises
the question
of whether the
problem is
enlarged ventricles
that squish
the cortex
from within
or a thinned-out
cortex that
allows the
ventricles to
fill the empty
space.
As it turns
out,
the cortical
thinning
comes first.
Very tellingly,
the cortical
changes are
most dramatic
in the frontal
cortex.
The thinning
turns out
not to be
due to
loss of
neurons.
Instead,
there's loss
of the
complex cables,
the axons
and dendrites,
that allow
neurons to
communicate
with each
other.
The frontal
cortex has
a lessened
ability for
its neurons
to communicate
with each
other,
to coordinate
their actions,
to function
in logical
sequential
ways.
And in
support of
that,
functional
brain imaging
shows that
the thinned-out
impoverished
frontal cortex
in someone
with schizophrenia
has to work
harder to
pull off the
same degree
of efficacy
at tasks
than the
frontal cortex
of a
control subject.
So if one
were forced
to come up
with a grand
synthesis of
the disease,
based on
current knowledge,
it would run
something like
this.
In
schizophrenia,
an array of
gene variants
constitute a
risk for
the disease,
and certain
times of
major stress
early in
life regulate
those genes
in such a
way that
things divert
onto the
road leading
to schizophrenia.
These
manifestations
then include
an excess
of dopamine
and sparse
neuron-to-neuron
connections in
the frontal
cortex.
Why the
late-adolescent
early-adult
onset typical
of the
disease?
Because that's
when the
frontal cortex
is having its
final burst
of maturational
growth, and
with that being
impaired in
schizophrenia.
Things wrong
with genes,
neurotransmitters,
the amount of
axonal wiring
connecting neurons.
The purpose of
going through
this overview of
our current
understanding of
the disease is
to hammer in
this point.
It's a
biological
problem.
It's a
biological
problem.
It's the
world of
people in
lab coats
with test
tubes,
rather than
V&E
psychoanalysts
whose modus
operandi would
be to tell
the mother
that she
sucks at
mothering.
A universe
away from
the idea
that if
you're a
teenager
cursed with
a
schizophrenogenic
mother, a
descent into
schizophrenic
madness is
your escape.
In other
words, this
is another
domain where
we have
managed to
subtract out
the notion
of blame
from the
disease, and
in the
process, become
vastly more
effective at
treating the
disease than
when mothers
were being
given scarlet
letters.
As I said,
learning about
the transition
of epilepsy
from being
what happens
when you
enlist with
Satan to
being a
neurological
disorder is
frustrating,
because there's
next to no
information about
how the
average person
started thinking
about the
disease differently
in the 18th
and 19th
centuries.
But we know
about how the
transition most
likely occurred
in the case of
schizophrenia.
A picture is
worth a thousand
words on
television.
The change in
the view of
schizophrenia should
have happened in
the 1950s,
when the first
drugs that
helped lessen
the symptoms
of schizophrenia
came online.
When dopamine
is released by
a neuron,
intent on
sending a
dopaminergic
message to
the next
neuron in
line, it
works only
if that next
neuron has
receptors that
bind and
respond to
dopamine,
basic
neurotransmitter
signaling.
And the
first effective
drugs were
ones that
blocked
dopamine
receptors.
These were
termed
neuroleptics
or
antipsychotics,
the most
famous being
thorazine,
a.k.a.
chlorpromazine,
and
haldol.
What happens
when you block
dopamine receptors?
The first neuron
in line can
release dopamine
until the cows
come home,
and still no
dopaminergic
signal is going
to get through.
And if people
with the disease
start acting less
schizophrenic at
that point,
you have to
logically conclude
that the problem
was too much
dopamine on
the scene in
the first place.
The case was
strengthened even
more by the
demonstration of
the flip side.
Take a drug
that drastically
increases dopamine
signaling,
and people develop
many schizophrenia-like
symptoms.
This is an
amphetamine
psychosis.
Findings like
these jump-started
the dopamine
hypothesis,
still the most
credible explanation
for what is going
wrong in the
disease.
It also caused
a drastic
reduction in
the numbers
of people
with schizophrenia
warehoused for
life in
psychiatric
institutions
tucked away
at a genteel
distance from
everyone else.
It was the
end of
asylums.
This should
have stopped
the
schizophrenogenic
voodoo right
in its tracks.
High blood
pressure can
be lessened
with a drug
that blocks
a receptor
for a different
type of
neurotransmitter,
and you conclude
that a core
problem was too
much of that
neurotransmitter.
But
schizophrenic
symptoms can
be lessened
with the
drug that
blocks dopamine
receptors,
and you still
conclude that
the core
problem is
toxic
mothering.
Remarkably,
that's what
psychiatry's
psychoanalytic
ruling class
concluded.
After fighting
the introduction
of the
medications tooth
and nail in
America,
and eventually
losing,
they came up
with an
accommodation.
Neuroleptics
weren't doing
anything to the
core problems
of schizophrenia.
They just
sedated patients
enough so
that it is
easier to
psychodynamically
make progress
with them
about the
scars from
how they
were mothered.
The psychoanalytic
scumbags even
developed a
sneering pejorative
term for
families,
that is,
mothers,
of schizophrenic
patients who
tried to dodge
responsibility by
believing that it
was a brain
disease.
Dissociative
Organic Types
The influential
1958 book
Social Class
and Mental
Illness,
a community
study,
John Wiley,
by the V&E
psychiatrist
Frederick Redlich,
who chaired
Yale's psychiatry
department for
17 years,
and the
Yale's
sociologist
August Hollingshead,
explained it
all.
Dissociative
Organic Types
were typically
lower-class,
less-educated
people for
whom it's
a biochemical
disorder,
was akin to
still believing
in the
evil eye,
an easy,
erroneous
explanation for
those not
intelligent enough
to understand
Freud.
Schizophrenia
was still
caused by
lousy parenting
and nothing
was to change
in the
mainstream
for decades.
The breakthrough,
in the late
1970s,
came at the
intersections of
public advocacy,
neuroimaging,
the influence of
the media,
money,
and schizophrenia
in the family's
closet of
powerful people.
In some ways,
it started with
a murder.
In the early
70s, a young
man suffering
from schizophrenia
killed two
people in
Olympia,
Washington,
while in a
delusional state.
A local woman
named Eleanor
Owen, the
mother, sister,
and aunt of
people with
schizophrenia, did
something that
was a catalyst.
She resisted
the usual
response of
someone touched
by the disease,
which was to
retreat into
the shame and
guilt that was
always there,
but particularly
searing when the
rare violence
committed by
someone with
schizophrenia
confirmed the
stereotype.
Owen contacted
seven other
local people she
knew who had a
close family member
with the disease,
and they contacted
the family of the
killer to offer
support and
comfort.
Owen and
Cohort felt
empowered by the
act, and
rather than shame
and guilt,
the main emotion
they felt was
rage.
The anti-psychotic
revolution had
emptied the
psychiatric hospitals
of chronic care
schizophrenic patients
who were not much
more normal and
healthy in their
behavior.
The laudable plan
was to construct
community mental
health clinics
throughout the
country that
would care for
these individuals
and help them
reintegrate into
their communities,
except the funding
was way slower
in coming than
what was needed
to keep pace
with the numbers
of people being
deinstitutionalized.
By the Reagan
years, funding
had basically
completely stopped.
Most of the
people deinstitutionalized,
if they were
lucky, wound up
being dumped back
on their families,
otherwise, the
streets.
Thus, the rage
was at the irony
of this.
We're such toxic
family members that
we caused the
disease in the
first place, and
now we're being
entrusted with their
care because various
agencies couldn't
figure out what
else to do with
them.
Moreover, as a
group, it was
easier for them to
air the real source
of their rage,
their increasing
conviction that the
idea of a
schizophrenogenic
mother or family
was sheer nonsense.
I had the
opportunity to talk
with Owen a few
years ago, a
two-hour conversation
with this 99-year-old
who remembered it
all well.
On a primal level,
I knew it was not
my fault.
I was operating on
sheer emotional
rage.
Her group soon
formed the
Washington Advocates
for the Mentally
Ill, basically a
support group tiptoeing
in the realm of
advocacy.
Meanwhile, a
similar group called
the Parents of
Adult Schizophrenics
had formed in
San Mateo,
California.
It scored an
early victory in
winning the right
for family members
of individuals with
schizophrenia to be
on every county
mental health board
in the state.
In Madison,
Wisconsin, another
group had formed,
founded by Harriet
Shetler and
Beverly Young.
They all eventually
got word of each
other, and by
around 1979, the
National Alliance on
Mental Illness,
NAMI, had formed.
One of their first
actual hires was
Lori Flynn, who
became director from
1984 to 2000, a
homemaker with some
experience with
community volunteering.
She had a daughter
who had starred in
her high school musical
and been on track to
be valedictorian, when
a variant of
schizophrenia destroyed
her.
She and Owen were
soon joined by Ron
Hanberg, a lawyer and
social worker who
wound up running
NAMI's policy work
for 30 years,
despite having no
family member touched
by schizophrenia.
The pull for him
was a sense of
justice.
Someone's kid gets
diagnosed with
cancer, that's one
thing.
Someone's kid gets
diagnosed with
schizophrenia.
Neighbors did not
come over with
casseroles.
They had some
successes, getting a
few state legislatures
to push in the
direction of more
medical insurance
coverage of
schizophrenia.
Owen was the
bulldog.
I have no idea how
I managed to
threaten them.
Legislators, she
recalled later.
I was a monster.
It was from the
pain.
Flynn described the
members as furious
in their nice
Midwestern way.
And then a
catalyst happened
when NAMI connected
with the perfect
hybrid of an
individual, a
first-degree family
member of a
schizophrenic person
who also happened
to be one of the
world's experts in
the emerging field
of biological
psychiatry.
Eve Fuller-Torey,
mentioned earlier,
had decided to
become a
psychiatrist when
his younger
sister was
diagnosed with
schizophrenia.
Schizophrenogenic
theorizing struck
him as deeply
wrong for the
same reason it
did the early
NAMI members,
with a number of
them in effect
saying,
wait, my mother
mothered nine of
us kids, but she
only schizophrenogenically
mothered one of
us?
It turned him
into a scathing
critic of the
psychoanalytic
school of
psychiatry.
With degrees
from Princeton,
McGill, and
Stanford, he
could have
settled into a
comfortable,
lucrative private
practice.
Instead, he
spent some years
as a physician
in Ethiopia,
then the
South Bronx,
and then an
Inuit community
in Alaska.
He eventually
became a
psychiatrist at
the National
Institute of
Mental Health
and at
St. Elizabeth's,
the oldest
federal psychiatric
hospital in the
U.S.
In the
process, he
became a
fierce critic
of the
psychodynamic
stranglehold,
authoring the
superb books
The Death of
Psychiatry and
Freudian Fraud,
along with the
highly regarded
biography of
Ezra Pound,
a long-time
patient at
St. Elizabeth's,
and 18
other books.
His
outspokenness
cost him at
least one
position, and
he eventually
quit the
federal psychiatry
establishment, as
well as the
psychodynamically
dominated American
Psychiatric
Association, and
founded his
own mental
health research
institute, with
a focus on the
biological causes
of schizophrenia.
It was
inevitable that he
and NAMI would
connect.
Tory was a
godsend to
them.
Fuller spoke
for us when
no one in the
medical community
would, said
Flynn, because
he was one of
them.
He became
NAMI's medical
spokesperson, lectured
and taught NAMI
groups all over
the country,
including getting
many of its
members to drop
their embrace of
various unproven
alternative medicine
treatments for the
disease, such as
megavitamin therapy.
He wrote the
best-selling
primer, Surviving
Schizophrenia, a
manual for families,
consumers, and
providers, Harper
Perennial 1995, which
has gone through
five editions.
Tory donated more
than $100,000 in
royalties from the
book to NAMI and
persuaded a
philanthropist to
hire a D.C.
lobbyist for NAMI
instead of funding
Tory's own
research.
And then another
piece of the puzzle
fell into place, one
that I suspect is
enormously important
for the battles to
come in removing
blame from our
thinking about the
worst and most
troubled human
behavior.
It's what Harvard
biologist Brian
Farrell would label
a case of applied
celebrity, famous
and or powerful
people touched
by schizophrenia
in their own
families who
became involved.
Two were
Senators Paul
Wellstone, Democrat
Minnesota, and
Pete Domenici,
Republican New
Mexico.
Flynn recalls
thinking, oh
good, a
Republican.
Both became
supporters in
Congress, pushing
for more medical
insurance coverage
of schizophrenia
care and
advocating in
other ways.
Hanberg recalls
the day he
rented a truck,
filled it with
more than half a
million paper
petitions calling
for more federal
funding for the
biological roots of
mental illness, and
deposited them on the
steps of the
Capitol, standing
alongside Domenici.
And then lightning
really struck.
On December 9,
1988, Tory appeared
on the Phil
Donahue show.
Donahue was then
the king of daytime
talk shows and
quietly had a family
member with the
disease.
Guests included
Lionel Aldridge, the
famed Green Bay
Packer, who had
descended into
misdiagnosed
schizophrenia and
homelessness after his
Super Bowl days.
He was now
successfully medicated,
as were a number of
other guests on the
show, who, along
with similar audience
members with comments
and testimonies,
appeared, well,
fairly normal.
And then there was
Tory, emphasizing how
schizophrenia was a
biological disease.
It has nothing to do
with what your mother
did to you.
Just like multiple
sclerosis.
Like diabetes.
Not because of an
unloved childhood.
He showed the brain
scans of a pair of
twins, one with the
disease, and one
without.
The enlarged
ventricles jumped out
in a powerful
demonstration of a
picture being worth
at least a thousand
words.
At the end, Tory gave
a shout-out to
NAMI.
In the days afterward,
NAMI received a
dozen bags of
mail a day, from
family members of
people with
schizophrenia.
Membership soared to
more than 150,000.
Donations poured in.
And NAMI became a
powerful lobbying
force, pushing for
public education about
the nature of the
disease, advocating
for medical schools to
change their curriculum
about schizophrenia, and
to shift psychiatry
departments away from
psychoanalysis and
toward biological
psychiatry, funding
the next generation of
young researchers in
the field.
Tory and Flynn appeared
repeatedly on
Donahue, on Oprah, and
in an influential PBS
documentary.
Celebrities came
forward with stories
about the mental
illness struggles that
they or family members
had endured.
A Beautiful Mind
won a Best Picture
Oscar for its
depiction of John
Nash, the Nobel
laureate economist
who struggled his
entire adult life
with schizophrenia.
And along the way,
the myth of
schizophrenogenic
mothers, fathers,
and families died.
No credible
psychiatrist would
counsel someone
anymore that their
toxicity caused
their loved ones
schizophrenia, or take
a schizophrenic patient
on a journey of
free associative
psychoanalysis, to
uncover the sins of
the mother.
No medical schools
teach it.
Close to no one in
the public believes
it.
We're still maddeningly
unsuccessful in
understanding the
nuts and bolts of the
disease, and in
devising new and more
effective treatments.
Our streets teem with
homeless,
deinstitutionalized
schizophrenia sufferers,
and families are still
devastated by the
disease, but at least
no family member is
being taught that it
is all their damn
fault.
We've subtracted out
the blame.
The picture isn't
perfect, of course.
A few gray eminences
of psychoanalysis
recanted their views
in technical journals,
and some, even did
studies showing that
psychoanalytic
approaches, did
nothing to help with
the disease.
But to the bitterness
of the NAMI members
I spoke to, no
leader in that field
ever came to them
to apologize, bringing
to mind the quip of
physicist Max Planck
that science progresses
one funeral at a time.
The bitterness still
resonates 43 years
later from a brilliant
piece of sociopolitical
theater by Torrey,
published in 1977, in
Psychology Today.
In A Fantasy Trial About
a Real Issue, he
imagined a trial of
the psychoanalytic
establishment for the
harm done to mothers
of people with
schizophrenia.
No trial since
Nuremberg has stirred
so much public
interest, he
facetiously reported
about the supposed
mass trial held in
a stadium in D.C.
He noted the charges.
The accused did
willfully and with
forethought, but no
scientific evidence,
blame the parents of
patients with
schizophrenia for
their children's
condition, thereby
causing great anguish,
guilt, pain, and
suffering.
Defendants included
Fromm Reichman,
Klein, Bateson, and
Theodore Leeds, who
claim that parents of
schizophrenics are
narcissistic and
egocentric.
All were convicted and
sentenced to spend
ten years reading
their own writings.
He finished with an
acidic flourish.
Relatives wept
openly.
Nobody had expected
that harsh a
sentence.
Eleanor Owen had a
movingly different
take on it.
Despite the fury that
drove the advocacy
that ultimately helped
move mountains,
despite the shame and
guilt heaped on
people like her by
ideologues preaching a
judgmental pseudo-
religion free of
facts, she still
says,
But there were no
villains.
Snapshots mid-
metamorphosis.
There have been other
success stories as
well.
Autism has undergone a
remarkably similar
shift.
Once loosely termed
childhood
schizophrenia, it was
formalized into the
diagnosis of early
infantile autism by
psychiatrist Leo
Connor, after
considering the
possibility of
biological, specifically
genetic, roots to the
disease, he settled
into the thinking of
the time, which was
of course, once again,
blaming the mother.
In this case, the
presumed maternal
toxicity was a
coldness and
inability to love.
Connor's soundbite
that haunted
generations of
parents was
refrigerator mothers.
There then followed
the usual story.
Decades of shame
and guilt, increasing
scientific insight
showing that there is
zero evidence for the
refrigerator mothering
concept.
First hints of
advocacy and pushing
back against the
accusation.
Increasing public
awareness of the
prevalence of the
disease, making the
refrigerator accusation
tougher to maintain,
with some applied
celebrity thrown in.
And the role of
blame in autism has
disappeared, as we
now know it to be an
alarmingly common
neurodevelopmental
disorder.
Moreover, many with
milder versions of
autism, what used to
be called Asperger's
syndrome and is now
labeled something like
high-functioning
autism spectrum
disorder, ASD, object
to being pathologized
with the concept of
disorder.
Instead, they argue
that ASD should
instead be viewed as
merely an extreme in
the normal variation in
human sociality, and
that it brings many
cognitive traits that
compare favorably with
those of neurotypicals,
that is, everyone
else.
A remarkably similar
story with three
interesting differences.
The first involved
Connor.
He was as much of a
dead white male
authority as you could
find.
Professor at Johns
Hopkins School of
Medicine, the first
certified child
psychiatrist in the
country, the author of
the first textbook on
the subject, and he
appeared to have been a
really good person.
As another of the
intellectuals who was
able to escape Europe,
he helped save the
lives of many others,
sponsoring their entry
into the U.S.,
supporting them
materially.
He had a deep vein of
social activism
concerning psychiatric
public health and
community psychiatry
outreach programs.
Remarkably, he changed
his view as more
knowledge accrued, and
in 1969, he did
something extraordinary.
He appeared at the
annual meeting of the
parent advocacy group
Autism Society of
America, and
apologized.
Herewith, I acquit
you people as
parents.
Next, while Owen felt
that there were no
villains in the
schizophrenogenic
mothering saga, that
of refrigerator
mothering indeed had
one, in my opinion.
Bruno Bettelheim had
survived the
concentration camps
and made it to
America, an
Austrian intellectual
of the psychoanalytic
stripe who became
the supposed
definitive expert on
the causes and
treatment of autism.
He also wrote
influential books on
the psychodynamic
roots of fairy tales
in The Uses of
Enchantment, and on
child-rearing practices
on Israeli
kibbutzim in The
Children of the
Dream.
He founded the
Orthogenic School for
Autistic Children,
associated with the
University of Chicago,
and became the
recognized pioneer in
their successful
treatment.
He was lauded and
revered, and he
embraced refrigerator
mothering with a
venom that would have
made Fromm Reichman or
Klein blanch.
Torrey included
Bettelheim as a
defendant in his
fantasy show trial.
In his widely read
book about autism, The
Empty Fortress, Free
Press, 1967, his
stated belief was
that the precipitating
factor in infantile
autism is the
parents' wish that
his child should not
exist.
In words that take
one's breath away, he
wrote,
whether in the death
camps of Nazi
Germany, or while
lying in a possibly
luxurious crib, but
they're subjected to
the unconscious death
wishes of what overtly
may be a conscientious
mother, in either
situation, a living
soul has death for a
master.
He was also emptier
than the supposed
fortress of autism.
He faked his European
credentials and training
history.
He plagiarized
writing.
His school actually
had very few kids
with autism, and he
fabricated his supposed
successes.
He was a tyrannical
bully to a staff.
I have heard people who
had been in his
training orbit refer
to him sarcastically
as Beto Brutelheim.
And, as is well
documented, he
repeatedly physically
abused the children.
And, of course, he
apologized for nothing.
It was only after his
death that a spate of
articles, books, and
testimonials of scores
of survivors of his
wisdom came forward.
The final difference
from the schizophrenia
story is why I
consider the vanquishing
of blame regarding
autism to still be
mid-metamorphosis.
This is the anti-vaxxer
movement, which insists
in the face of every
possible scientific
refutation that autism
can be caused by
vaccinations gone awry.
Amid these often
well-educated and
privileged medieval
witch hunters being
responsible for decreased
vaccination rates, a
resurgence of measles, and
the deaths of children, I
note what is often a
secondary theme.
There is, of course, the
primary conspiracy theory of
some sort of medico-pharmaceutical
willingness to shower
autistic hell down on the
innocent for the sake of
vaccine profits, but there
is also often some
additional, familiar
finger-pointing.
If your child has
autism, it's your own
damn fault, because you
didn't listen to us about
vaccines.
We are in the midst of
other transitions as well.
In 1943, General George
Patton famously slapped a
soldier in the hospital for
what we would now call
post-traumatic stress
disorder, PTSD, but which
Patton interpreted as
cowardice.
Patton ordered his
court-martial, which was
fortunately overruled by
Ike.
Even well after Vietnam,
PTSD was officially
viewed as psychosomatic
malingering by most of
governmental powers that be,
and afflicted veterans were
often denied health benefits
to treat it.
And then the usual, genetic
links, identification of
early developmental
neurological issues, and
types of childhood
adversity that increase the
risk of succumbing to it.
Neuroimages showing brain
abnormalities.
Things are slowly changing.
In the early 1990s, about a
third of the soldiers deployed
in the first Gulf War
complained of being
never quite right again,
with a constellation of
symptoms, exhaustion,
chronic unexplained pain,
cognitive impairments.
Gulf War syndrome was
generally viewed as being
some sort of psychological
disorder.
That is, not for real, a
marker of psychologically
weak, self-indulgent
veterans.
And then, science trickled
in.
Soldiers had been
administered a heavy-duty
class of drugs related to
pesticides as protection
against the nerve gas that
Saddam Hussein was
expected to use.
While these drugs could
readily explain the
neurological features of
Gulf War syndrome, this
was discounted.
Careful research in the
run-up to the war had
identified what doses could
be given safely, would not
damage brain function.
But then it turned out that
the drugs became more
damaging to the brain during
stress, something that was
not considered beforehand.
One of the mechanisms
implicated was that stress,
in this case, body heat
generated by carrying 80
pounds of gear in 120
degree desert weather,
coupled with basic combat
terror, could open up the
blood-brain barrier,
increasing the amount of
drug getting into the brain.
It was not until 2008 that
the Department of Veterans
Affairs officially declared
Gulf War syndrome to be a
disease, not some
psychological malingering.
So many fronts of
advances.
Kids who are having trouble
learning to read and keep
reversing letters aren't lazy
and unmotivated.
Instead, there are cortical
malformations in their brains
that cause dyslexia.
Issues of free will and
choice are irrelevant when it
comes to any scientifically
informed read of someone's
sexual orientation.
Someone insists that,
despite evidence from their
genes, gonads, hormones,
anatomy, and secondary sexual
characteristics that they are
the sex they were assigned
at birth, that is not who
they are, has never been from
as far back as they can
remember, and the neurobiology
agrees with them.
And even further reaching,
sneaking into everyday life so
subtly that we cannot readily
see the change in mindset
implied, someone doesn't
help you carry something
heavy, and rather than being
irritated, you recall their
serious back problems.
The person singing soprano in
your choir keeps missing the
notes, and you resort to
knowledge of prenatal
endocrinology for an
explanation.
Oh, they're a baritone.
Oddly, you have an
unfortunate research assistant
who searches for the one
green sock in a mound of a
hundred thousand red socks at
your request.
They fail.
And instead of holding this
against them, you think,
ah, that's right, they have
red-green color blindness.
And in a recent blink of a
historical eye, the majority
of Americans changed their
minds and decided that, given
the insufficiency of love in
the world, the love between
two same-sex adults should be
permitted to be consecrated
with marriage.
The long explorations in this
chapter all show the same
thing.
We can subtract responsibility
out of our view of aspects
of behavior, and this makes
the world a better place.
Conclusion
We can do lots more of the
same.
Fourteen
The Joy of Punishment
Justice Served
One
In her 1987 classic
A Distant Mirror,
historian Barbara Tuchman
famously described Europe in the
14th century as
calamitous, and in ways that
parallel the present.
Mirror or not, by anyone's
standards, the century sucked.
One source of misery was the
start of the Hundred Years' War
between France and England
in 1337, leaving destruction
in its path.
Christianity was roiled by the
papal schism, which produced
multiple competing popes.
But above all, the calamity was
the Black Plague, sweeping
through Europe beginning 1347.
Over the next few years, nearly half
the population died in bubonic
agony.
So severe was the loss that it
took London, for example, two
centuries to regain its pre-plague
population.
Things were pretty awful even
earlier in the century.
Take 1321.
The average peasant was
illiterate, parasite-riddled, and
struggling for existence.
Their life expectancy was about a
quarter of a century.
A third of infants died before
their first birthday.
Poverty was made worse by enforced
tithing of income to the church.
10-15% of people in England were
starving to death in a famine.
Moreover, everyone was still
recovering from the events of the
previous year, in which the
Shepherds' Crusade rampaged
through France rather than fulfilling
its stated goal of rampaging among
Muslims in Spain.
At least no one thought that some
outgroup was poisoning the wells.
In the summer of 1321, people
throughout France decided that some
outgroup lepers was poisoning the
wells.
The conspiracy theory soon spread to
Germany and was accepted by everyone
from peasants to royalty.
Under torture, lepers soon confessed
that, yes, they had formed a guild
sworn to poison wells, using potions made
from the likes of snakes, toads, lizards,
bats, and human excrement.
Why were the lepers supposedly
poisoning the wells?
In one Night of the Living Dead version,
people believed that the poisons caused
leprosy.
That is, were a recruitment measure.
In another interpretation,
some empathically speculated that lepers
were so embittered by the lack of
empathy with which they were treated
that this was their revenge.
But some prescient individuals,
centuries ahead of their time in
appreciating the rod of capitalism,
sensed a profit motive.
Soon, under more enhanced
interrogation, the answer emerged.
Tortured lepers passed the buck,
claiming between their shrieks of pain that
they were being paid to poison the wells
by their sidekicks, the Jews.
Perfect.
Everyone believed that Jews
couldn't get leprosy,
allowing them to safely conspire
with the lepers.
But then the Jews passed the buck further.
Despite their bloated wealth
from venal usury,
and the selling of kidnapped Christian
children for blood sacrifice,
employing that many lepers
cost them a bundle.
Soon, Jews being broken on the wheel,
proclaimed that they were just middlemen.
They were being funded by the Muslims,
specifically the king of Granada
and the sultan of Egypt,
scheming to overthrow Christendom.
Inconveniently,
the mobs couldn't get their hands
on those two.
Settling for second best,
mobs immolated lepers and Jews
in town after town in France and Germany,
killing thousands.
Having addressed what became known
historically as
the lepers' plot,
people returned to their daily struggle
for existence.
Justice had been done.
Those bleeding-heart liberals.
Reform isn't everyone's cup of tea.
Maybe you're sitting pretty in the Vatican,
and there's this uncouth German monk
going on about his 95 theses.
Or if your taste runs in the
things-have-to-get-worse-before-they-get-better
direction of the proletariat
losing its chains,
reform just undercuts revolution.
Reform especially doesn't seem like the way to go
when it accepts as a given
a system that is utterly, brutally,
indefensively nonsensical.
You can see where we're heading.
Yes, yes,
there is so much to reform
about the criminal justice system.
Prisons are criminogenic,
a training ground for revolving-door recidivism.
Implicit bias makes a mockery
of the notion of objective judges and juries.
The system offers all the justice money can buy.
All of this needs to be reformed,
and the people in the trenches trying to do it,
the Innocence Project,
candidates for district attorney intent on change from within,
lawyers helping underdogs pro bono,
are amazing.
I've now had the chance to work on around a dozen murder cases
with public defenders,
and they're inspiring.
Underpaid,
overworked,
passing up the riches of the corporate world,
losing most of their cases defending broken people
who were usually already lost
by the time they were second-trimester fetuses.
Yet,
if there's no free will,
there is no reform that can give retributive punishment
even a whiff of moral good.
Here is what criminal justice reform can look like.
In 16th century Europe,
a variety of tests were used to identify witches,
all truly awful.
One of the more benign ones was to read the suspect
the biblical account of the crucifixion of our Lord.
If they weren't moved to tears,
they were a witch.
In 1563-68,
Dutch physician Johan Weyer
tried to reform the witch justice system,
publishing a book,
De Prestigius Daemonum et Incantationibus Ac Beneficiis,
On the Illusions of the Demons
and on Spells and Poisons.
In it,
Weyer calculated that Satan
had an army of 7,405,926 devils and demons,
organized in 1,111 divisions
of 6,666 each.
So,
Weyer had bought into the system big time.
The book made three suggestions for reforming it.
First,
obviously non-witches might confess to anything,
including being witches,
just because they were being flayed.
The second,
which caused Weyer to be viewed
as one of the forefathers of psychiatry,
was that someone might appear to be a witch,
but actually just be mentally unbalanced.
The third referred to that tears test.
By all means,
use it,
urged Weyer,
but keep in mind that lacrimal glands
often atrophy in old age,
so that the tearless old woman
hearing the crucifixion story
is organically impaired from crying,
rather than a witch.
That's what it looks like
when you try to reform a system
based on sheer gibberish.
Ditto if reformist phrenologists
excluded any potential subjects
from their studies
who had gotten a bump on the head
from ice hockey,
or if reformist alchemy journals
required authors to list their funding sources,
or when reformers tried to bring more equality
to a criminal justice system.
This is trying to make the actual
meeting out of justice
more aligned with its platonic ideal,
when that very ideal
is without scientific
or moral justification.
Just to start off things off
in an understated kind of way.
Justice served, too.
Of the long line of King Louis's in France,
Louis XV was certainly underwhelming.
He was ineffectual with his few policies
and was scorned as a corrupt Sybarite
who had brought economic and military ruin to France.
The celebration of his death
by the citizenry in 1774
foretold the French Revolution 15 years later.
In 1757,
an assassin stabbed him
with what was essentially a penknife,
which,
after penetrating layers of clothing,
it was outdoors in midwinter,
caused a superficial wound.
To help out the grievously injured monarch,
the Archbishop of Paris
commanded 40 hours of prayers
for his speedy recovery.
History is unclear
as to the motives of his would-be assassin,
Robert-Francois Damien,
a household servant
dismissed from a series of jobs
for stealing from his employers.
One interpretation
is that he was deranged,
psychiatrically unwell.
Another concerned
a religious controversy at the time
where Damien was on the losing side,
which was suppressed by Louis,
and decided to take revenge.
The king particularly feared
that Damien was part of a larger conspiracy.
Although Damien didn't give up any names
while being tortured.
Motives aside,
the only pertinent thing
was that he had attempted
to kill the king.
Damien was convicted,
destined to be the last person
drawn and quartered in France.
The execution,
which took place
in a public square in Paris
on March 28, 1757,
was well documented.
Damien's feet
were first crushed
with a torture device
called the boot.
The offending hand
with which he had held the knife
was then scorched
with burning pincers.
A mixture of molten lead,
boiling oil,
burning resin,
wax,
and sulfur
was then poured
on his wounds.
He was then castrated,
and the burning mixture
applied there as well.
These actions,
along with Damien's
wailing and begging for death,
provoked cheers
from the massive crowd
that filled the square,
as well as
from the apartments above,
which had been rented out
to the wealthy
as box seats
at exorbitant prices.
But these tortures
were merely the warm-up act
for the main event,
which was
the quartering.
Each of a victim's limbs
would be tied to a horse,
and the four horses
would be led off
in opposing directions,
tearing off the person's limbs.
Damien apparently
had tougher-than-expected
connective tissue.
His limbs remained intact,
despite repeated attempts
with the horses.
Eventually,
the overseeing executioner
severed the tendons
and ligaments
in Damien's four limbs,
and the horses
were finally successful.
Damien,
reduced to a torso
and,
still breathing,
was flung onto a fire
along with his severed limbs.
When he was reduced
to ash after four hours,
the crowd dispersed,
justice having been served.
Reconciliation
and restorative justice
as band-aids
suppose trials
were abolished,
replaced by mere investigation
to figure out
who actually carried out
some act
and with what state of mind.
No prisons,
no prisoners,
no responsibility
in a moral sense,
no blame
or retribution.
This scenario
inevitably provokes
the response,
so you're saying
that violent criminals
should just run wild
with no responsibility
for their actions?
No.
A car
that through
no fault of its own
has brakes
that don't work
should be kept
off the road.
A person
with active COVID-19
through no fault
of their own
should be blocked
from attending
a crowded concert.
A leopard
that would shred you
through no fault
of its own
should be barred
from your home.
So then
what should be done
with criminals?
There have been
a few approaches
that,
while swell,
still accept
the premise
of free will,
but at least show
that really smart,
serious people
are thinking
about radical alternatives
to our current responses
to people who damage.
One possibility
is
the Truth
and Reconciliation
Commission model,
first mandated
in post-apartheid
South Africa
and since
used in numerous
countries recovering
from civil war
or a violent
dictatorship.
With South Africa
as the archetype,
architects and henchmen
of apartheid
could appear
before the commission
rather than
go to jail.
About 10%
of applicants
were granted
the opportunity
where they were
required to confess
to every detail
of their politically
motivated human rights
violations,
whom they had killed,
tortured,
and disappeared,
even the ones
whom no one knew
about
who hadn't been
pinned on them.
They would vow
to never do it again,
that is,
to not join
the white militias
that formed
a threat
to the peaceful
transition
to a free
South Africa.
Family members
of the victims
who were in attendance
essentially vowed
not to take revenge.
The killer
would then be released
rather than
imprisoned
or executed.
Mind you,
there was no requirement
for remorse.
no photo ops
where some
apartheid murderer,
anguished with
contrition,
is hugged
and forgiven
by a widow
he created.
Instead,
the approach
was pragmatic
to the frustration
of many family members,
helping the country
rebuild itself.
Most important,
it provided a parallel
to the police strategy
of getting the goods
on some
entry-level
organized crime
schnook
and offering him
immunity
in exchange
for implicating
his higher-up,
who would then
be similarly squeezed
and so on
all the way up
to implicating
the shadowy crime boss.
In this case,
immunity was being offered
to the soldiers
of apartheid
in order to implicate
the crime boss
at the top.
