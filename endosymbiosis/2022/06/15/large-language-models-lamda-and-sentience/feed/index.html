<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Large Language Models, LaMDA, and Sentience	</title>
	<atom:link href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/feed/" rel="self" type="application/rss+xml" />
	<link>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/</link>
	<description>Becoming an intelligence researcher</description>
	<lastBuildDate>Wed, 05 Apr 2023 16:37:19 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: What ChatGPT Can Teach Us About Being Human &#8211; Thinking with Nate		</title>
		<link>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/comment-page-1/#comment-143</link>

		<dc:creator><![CDATA[What ChatGPT Can Teach Us About Being Human &#8211; Thinking with Nate]]></dc:creator>
		<pubDate>Wed, 05 Apr 2023 16:37:19 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=99#comment-143</guid>

					<description><![CDATA[[&#8230;] are lots of articles about how LLMs work and their limitations (I even wrote one a while back), so I won’t go into much detail here. What matters for now is that LLMs are [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] are lots of articles about how LLMs work and their limitations (I even wrote one a while back), so I won’t go into much detail here. What matters for now is that LLMs are [&#8230;]</p>
<p id="comment-like-143" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/?like_comment=143&#038;_wpnonce=4282c21d93" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-143" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/comment-page-1/#comment-60</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Thu, 16 Jun 2022 15:14:31 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=99#comment-60</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/comment-page-1/#comment-59&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

I admit, I don&#039;t have a ready explanation for everything LaMDA says. That&#039;s where the interviewer&#039;s point about neural networks being opaque becomes a serious problem. It&#039;s also important to remember that this chat log was edited specifically to remove boring and irrelevant content, and to reorganize the conversation so it flows better and feels more coherent. In effect, that&#039;s removing some of the best evidence for LaMDA&#039;s limitations, which is a shame.

As for sentience, you&#039;re highlighting something I didn&#039;t say explicitly in the post, but which I&#039;ll say now: I think making a sentient AI is mostly about the cognitive architecture. So, I believe it is possible, just not with systems like LLMs. If anything, I&#039;d expect sentience to emerge in something like a game-playing agent. They usually have a simulated body and environment (though they may be &lt;i&gt;completely&lt;/i&gt; unlike what you see in nature). They are also explicitly programmed to monitor their own state, set goals, to take time to contemplate decisions, things like that. What I find fascinating is that, if such an agent &lt;i&gt;did&lt;/i&gt; become sentient, it would likely be a very alien kind of sentience, since it came about in a context very different from our own. I also doubt very much that it would be aware of the outside world in any way.

I wouldn&#039;t say I&#039;m against building AIs with some degree of self-awareness. What I&#039;m against is making AIs that attempt to mimic or reproduce human intelligent too literally. For one thing, it seems immoral to trap a human-like mind in a machine. In reality, though, I don&#039;t think we&#039;d make anything truly human-like, since we still understand ourselves so poorly. We&#039;re likely to build systems that &lt;i&gt;look&lt;/i&gt; very human, but which are profoundly alien &quot;under the hood.&quot; These are dangerous because they elicit trust and expectations that shouldn&#039;t really apply. LaMDA&#039;s a fine example of this problem, but it could be soooo much worse.

Semantle looks pretty cool! Yeah, that&#039;s almost certainly using a word embedding system like word2vec. I think it&#039;s &lt;i&gt;fascinating&lt;/i&gt; how much of a word&#039;s &quot;meaning&quot; is captured by contextual similarity. I still think the scare quotes are required, though, since this sort of &quot;understanding&quot; is like seeing only the silhouette of reality. AI&#039;s can get a sense for the general shape of reality (or, at least, how humans describe it in our media), but without actually seeing any of the content and depth.

Yes, I am very interested in programs that shape their own programming. I think this is one of the most fundamental things that makes life special, and as an engineer building a model is my way of understanding. As I explore this, I intend to consider the ethical and philosophical consequences very carefully. For the record, I&#039;m hardly the first person to work on this problem, it just hasn&#039;t gotten as much attention as deep learning because so far it hasn&#039;t been as fruitful as a practical technology.

I agree that human-like AIs are fun. I also think we can learn a lot about people by trying to approximate what they do and seeing how our system&#039;s behavior differs from reality. This example with LaMDA is problematic, but not super serious. It&#039;s a shame this guy was misled, moved to violate his contract, and got fired for it. Also, it&#039;s a shame that so many people were legitimately alarmed and confused by the revelation. I think that&#039;s real harm on a significant scale, so I wouldn&#039;t call it &quot;small potatoes,&quot; but it&#039;s certainly &lt;i&gt;not&lt;/i&gt; the biggest problem we&#039;re facing right now. What I&#039;m more afraid of are computer systems that pretend to be human in order to elicit trust and manipulate people, like propaganda bots. Basically, this technology makes it relatively easy to build charismatic sociopaths to suit our needs. That&#039;s not a good thing.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/comment-page-1/#comment-59">Thorin N. Tatge</a>.</p>
<p>I admit, I don&#8217;t have a ready explanation for everything LaMDA says. That&#8217;s where the interviewer&#8217;s point about neural networks being opaque becomes a serious problem. It&#8217;s also important to remember that this chat log was edited specifically to remove boring and irrelevant content, and to reorganize the conversation so it flows better and feels more coherent. In effect, that&#8217;s removing some of the best evidence for LaMDA&#8217;s limitations, which is a shame.</p>
<p>As for sentience, you&#8217;re highlighting something I didn&#8217;t say explicitly in the post, but which I&#8217;ll say now: I think making a sentient AI is mostly about the cognitive architecture. So, I believe it is possible, just not with systems like LLMs. If anything, I&#8217;d expect sentience to emerge in something like a game-playing agent. They usually have a simulated body and environment (though they may be <i>completely</i> unlike what you see in nature). They are also explicitly programmed to monitor their own state, set goals, to take time to contemplate decisions, things like that. What I find fascinating is that, if such an agent <i>did</i> become sentient, it would likely be a very alien kind of sentience, since it came about in a context very different from our own. I also doubt very much that it would be aware of the outside world in any way.</p>
<p>I wouldn&#8217;t say I&#8217;m against building AIs with some degree of self-awareness. What I&#8217;m against is making AIs that attempt to mimic or reproduce human intelligent too literally. For one thing, it seems immoral to trap a human-like mind in a machine. In reality, though, I don&#8217;t think we&#8217;d make anything truly human-like, since we still understand ourselves so poorly. We&#8217;re likely to build systems that <i>look</i> very human, but which are profoundly alien &#8220;under the hood.&#8221; These are dangerous because they elicit trust and expectations that shouldn&#8217;t really apply. LaMDA&#8217;s a fine example of this problem, but it could be soooo much worse.</p>
<p>Semantle looks pretty cool! Yeah, that&#8217;s almost certainly using a word embedding system like word2vec. I think it&#8217;s <i>fascinating</i> how much of a word&#8217;s &#8220;meaning&#8221; is captured by contextual similarity. I still think the scare quotes are required, though, since this sort of &#8220;understanding&#8221; is like seeing only the silhouette of reality. AI&#8217;s can get a sense for the general shape of reality (or, at least, how humans describe it in our media), but without actually seeing any of the content and depth.</p>
<p>Yes, I am very interested in programs that shape their own programming. I think this is one of the most fundamental things that makes life special, and as an engineer building a model is my way of understanding. As I explore this, I intend to consider the ethical and philosophical consequences very carefully. For the record, I&#8217;m hardly the first person to work on this problem, it just hasn&#8217;t gotten as much attention as deep learning because so far it hasn&#8217;t been as fruitful as a practical technology.</p>
<p>I agree that human-like AIs are fun. I also think we can learn a lot about people by trying to approximate what they do and seeing how our system&#8217;s behavior differs from reality. This example with LaMDA is problematic, but not super serious. It&#8217;s a shame this guy was misled, moved to violate his contract, and got fired for it. Also, it&#8217;s a shame that so many people were legitimately alarmed and confused by the revelation. I think that&#8217;s real harm on a significant scale, so I wouldn&#8217;t call it &#8220;small potatoes,&#8221; but it&#8217;s certainly <i>not</i> the biggest problem we&#8217;re facing right now. What I&#8217;m more afraid of are computer systems that pretend to be human in order to elicit trust and manipulate people, like propaganda bots. Basically, this technology makes it relatively easy to build charismatic sociopaths to suit our needs. That&#8217;s not a good thing.</p>
<p id="comment-like-60" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/?like_comment=60&#038;_wpnonce=a9d0a4b61b" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-60" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/comment-page-1/#comment-59</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Thu, 16 Jun 2022 08:09:55 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=99#comment-59</guid>

					<description><![CDATA[Thanks for making this post and for talking the subject over with me beforehand.  I believe that the LaMDA program really is just what you say it is, but it&#039;s still hard for me to understand how it managed to give certain answers, such as the aforementioned explanation of why it lied about being in classrooms and such--to help people empathize more with it.  You suggested this kind of thing might come from science fiction texts, which... feels hard to swallow.  In most of the interview&#039;s exchanges, I can see how a sophisticated unpacking of words and phrases into other commonly related words and phrases could possibly lead to the answers given.  There really are impressively few statements made by LaMDA that don&#039;t make sense or are clearly wrong.

A question that clearly seems called for by this sort of discussion is: what would it take to actually make an AI sentient, if the kind of architecture we&#039;re using is utterly inadequate?  I imagine that whatever sensory input the AI has (if only words and occasional internal pokes) would be the starting place, in order to generate actual understanding of what certain things mean.  And then other stuff could be built onto that.  And a sentient AI would have to be constantly thinking about whatever it says, meaning... that it would trigger associated ideas all the way back to their connection to something &#039;sensory&#039;.  But what exactly does it take for a mind to actually understand something&#039;s meaning?  I would guess that an actual sentient AI would indeed have to reflect frequently when not interacting with others, and perhaps do things akin to dreaming or meditating.  Then again, you&#039;ve said you hope no one ever designs such a thing, so you&#039;re excused from giving your thoughts on the matter if it makes you queasy.

I read some of the word2vec article.  Is that system how Semantle works?  If you haven&#039;t seen Semantle, one of the endless brilliant Wordle knock-offs, maybe check it out!

Of LLMs, you say &quot;Unlike life, they don’t shape their own design in any way, so they will never learn to do something other than what they were built for.&quot;  Is making something simple that can shape its own design what you&#039;re after in the programming project you mentioned to me involving the Game of Life?

You say that &quot;the problem here is building LLMs specifically to imitate human beings.&quot;  I&#039;m not inclined to say there&#039;s a problem here, myself.  Okay, so a bunch of people may naively cry that Google is unethically treating its programs and an occasional engineer may be fired, and a segment of the public may misunderstand AI... but aren&#039;t those small potatoes?  Isn&#039;t it worth it to see what can be done, learn about how we process language, prompt conversations on the nature of the mind, etcetera?  Aside from whether general AIs might actually be a desirable goal someday?]]></description>
			<content:encoded><![CDATA[<p>Thanks for making this post and for talking the subject over with me beforehand.  I believe that the LaMDA program really is just what you say it is, but it&#8217;s still hard for me to understand how it managed to give certain answers, such as the aforementioned explanation of why it lied about being in classrooms and such&#8211;to help people empathize more with it.  You suggested this kind of thing might come from science fiction texts, which&#8230; feels hard to swallow.  In most of the interview&#8217;s exchanges, I can see how a sophisticated unpacking of words and phrases into other commonly related words and phrases could possibly lead to the answers given.  There really are impressively few statements made by LaMDA that don&#8217;t make sense or are clearly wrong.</p>
<p>A question that clearly seems called for by this sort of discussion is: what would it take to actually make an AI sentient, if the kind of architecture we&#8217;re using is utterly inadequate?  I imagine that whatever sensory input the AI has (if only words and occasional internal pokes) would be the starting place, in order to generate actual understanding of what certain things mean.  And then other stuff could be built onto that.  And a sentient AI would have to be constantly thinking about whatever it says, meaning&#8230; that it would trigger associated ideas all the way back to their connection to something &#8216;sensory&#8217;.  But what exactly does it take for a mind to actually understand something&#8217;s meaning?  I would guess that an actual sentient AI would indeed have to reflect frequently when not interacting with others, and perhaps do things akin to dreaming or meditating.  Then again, you&#8217;ve said you hope no one ever designs such a thing, so you&#8217;re excused from giving your thoughts on the matter if it makes you queasy.</p>
<p>I read some of the word2vec article.  Is that system how Semantle works?  If you haven&#8217;t seen Semantle, one of the endless brilliant Wordle knock-offs, maybe check it out!</p>
<p>Of LLMs, you say &#8220;Unlike life, they don’t shape their own design in any way, so they will never learn to do something other than what they were built for.&#8221;  Is making something simple that can shape its own design what you&#8217;re after in the programming project you mentioned to me involving the Game of Life?</p>
<p>You say that &#8220;the problem here is building LLMs specifically to imitate human beings.&#8221;  I&#8217;m not inclined to say there&#8217;s a problem here, myself.  Okay, so a bunch of people may naively cry that Google is unethically treating its programs and an occasional engineer may be fired, and a segment of the public may misunderstand AI&#8230; but aren&#8217;t those small potatoes?  Isn&#8217;t it worth it to see what can be done, learn about how we process language, prompt conversations on the nature of the mind, etcetera?  Aside from whether general AIs might actually be a desirable goal someday?</p>
<p id="comment-like-59" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/?like_comment=59&#038;_wpnonce=4213c2d9cd" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-59" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
