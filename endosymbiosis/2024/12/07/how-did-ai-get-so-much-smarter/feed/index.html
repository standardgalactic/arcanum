<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: How did AI get so much smarter?	</title>
	<atom:link href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/feed/" rel="self" type="application/rss+xml" />
	<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/</link>
	<description>Becoming an intelligence researcher</description>
	<lastBuildDate>Sat, 04 Jan 2025 07:19:09 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-337</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Sat, 04 Jan 2025 07:19:09 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-337</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-332&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Hmm!  When we follow formal rules, then check and correct our work, are we doing anything that can&#039;t be reduced to linguistic token and pattern matching?  I&#039;m not sure either.  Certainly if we do think about stuff in the process, we&#039;ll do a better job.  But perhaps it&#039;s fair to say reasoning can be done, albeit imperfectly, through predictive methods?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;An AI talking to itself sounds like a language model, but I was thinking more like how AlphaZero played against itself billions of times... but okay, I accept that we don&#039;t have anything yet where passive thinking in order to improve the mind is an option, and that&#039;s fine. :}&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-332">Nate Gaylinn (he/him)</a>.</p>
<p>Hmm!  When we follow formal rules, then check and correct our work, are we doing anything that can&#8217;t be reduced to linguistic token and pattern matching?  I&#8217;m not sure either.  Certainly if we do think about stuff in the process, we&#8217;ll do a better job.  But perhaps it&#8217;s fair to say reasoning can be done, albeit imperfectly, through predictive methods?</p>
<p>An AI talking to itself sounds like a language model, but I was thinking more like how AlphaZero played against itself billions of times&#8230; but okay, I accept that we don&#8217;t have anything yet where passive thinking in order to improve the mind is an option, and that&#8217;s fine. :}</p>
</p>
<p id="comment-like-337" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=337&#038;_wpnonce=ecfad0332e" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-337" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-334</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Tue, 31 Dec 2024 13:19:12 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-334</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-333&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

Yeah. I&#039;ve been pondering the difference between &quot;reasoning&quot; and &quot;apparent reasoning,&quot; and it&#039;s tricky. The truth is, people&#039;s reasoning ability is also pretty fuzzy, much of the time. I think the difference is that a person can monitor and constrain their own performance. Like, if you were writing a proof, you would restrict yourself to certain arguments and review your steps to ensure each follows the previous ones logically. An AI can&#039;t do this... except it &lt;i&gt;sorta&lt;/i&gt; can? It can pretty consistently restrict itself to some domain of language, and if you ask it to critique its own work, it will find and fix contradictions by means of linguistic associations. It&#039;s not as reliable or intentional or internally driven as what a person does, but is that a difference of kind or of magnitude? It&#039;s hard to say, but I feel like our conscious awareness of attention and the train of thought are important, and absent in today&#039;s AI.

What I think would be a &quot;big leap&quot; would be designing a sort of background thought process for an AI. That&#039;s just not how it&#039;s done today. The obvious way you might do this is to have the AI &quot;talk to itself,&quot; and people are playing with this, but there&#039;s a tendency for AIs to kind of go off the rails when they ingest their own outputs. Unlike a person, which regularly pulls the train of thought back to some purpose, the AI will just lose the thread and go into meaningless loops. Again, it seems to lack awareness and control over its &quot;train of thought,&quot; if that&#039;s even the right phrase to describe it.

What you suggest about having an AI &quot;organize its thoughts&quot; when it&#039;s not interacting with a user would also be a big leap! Right now our AI models have a problem called &quot;catastrophic forgetting.&quot; If you train an AI to do one thing, then train it to do something else, it will forget the first thing. We really haven&#039;t found a way to manage and dynamically save, forget, or modify discrete &quot;memories&quot;, we just have one big tangle of neurons that somehow represents all of the AI&#039;s experience and if you want that mess to represent many things, then you need to present &lt;i&gt;all&lt;/i&gt; of those things, in the right proportions, to the AI during training. There may be other ways to fix this issue, but adopting the kind of &quot;online learning&quot; we see in human neurons would be my favorite way to do it. Unfortunately, we don&#039;t really understand how that works. For AI, we use the &quot;gradient descent&quot; trick, which is very effective, but it only works if you can break experience down into episodes with known inputs and correct outputs, so you can score the results and tweak the neural network to reflect that. It can&#039;t deal with novel experiences, and it has no conception of discrete ideas it has learned or their usefulness.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-333">Thorin N. Tatge</a>.</p>
<p>Yeah. I&#8217;ve been pondering the difference between &#8220;reasoning&#8221; and &#8220;apparent reasoning,&#8221; and it&#8217;s tricky. The truth is, people&#8217;s reasoning ability is also pretty fuzzy, much of the time. I think the difference is that a person can monitor and constrain their own performance. Like, if you were writing a proof, you would restrict yourself to certain arguments and review your steps to ensure each follows the previous ones logically. An AI can&#8217;t do this&#8230; except it <i>sorta</i> can? It can pretty consistently restrict itself to some domain of language, and if you ask it to critique its own work, it will find and fix contradictions by means of linguistic associations. It&#8217;s not as reliable or intentional or internally driven as what a person does, but is that a difference of kind or of magnitude? It&#8217;s hard to say, but I feel like our conscious awareness of attention and the train of thought are important, and absent in today&#8217;s AI.</p>
<p>What I think would be a &#8220;big leap&#8221; would be designing a sort of background thought process for an AI. That&#8217;s just not how it&#8217;s done today. The obvious way you might do this is to have the AI &#8220;talk to itself,&#8221; and people are playing with this, but there&#8217;s a tendency for AIs to kind of go off the rails when they ingest their own outputs. Unlike a person, which regularly pulls the train of thought back to some purpose, the AI will just lose the thread and go into meaningless loops. Again, it seems to lack awareness and control over its &#8220;train of thought,&#8221; if that&#8217;s even the right phrase to describe it.</p>
<p>What you suggest about having an AI &#8220;organize its thoughts&#8221; when it&#8217;s not interacting with a user would also be a big leap! Right now our AI models have a problem called &#8220;catastrophic forgetting.&#8221; If you train an AI to do one thing, then train it to do something else, it will forget the first thing. We really haven&#8217;t found a way to manage and dynamically save, forget, or modify discrete &#8220;memories&#8221;, we just have one big tangle of neurons that somehow represents all of the AI&#8217;s experience and if you want that mess to represent many things, then you need to present <i>all</i> of those things, in the right proportions, to the AI during training. There may be other ways to fix this issue, but adopting the kind of &#8220;online learning&#8221; we see in human neurons would be my favorite way to do it. Unfortunately, we don&#8217;t really understand how that works. For AI, we use the &#8220;gradient descent&#8221; trick, which is very effective, but it only works if you can break experience down into episodes with known inputs and correct outputs, so you can score the results and tweak the neural network to reflect that. It can&#8217;t deal with novel experiences, and it has no conception of discrete ideas it has learned or their usefulness.</p>
<p id="comment-like-334" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=334&#038;_wpnonce=7a74139e07" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-334" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-333</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Mon, 30 Dec 2024 22:27:27 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-333</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-332&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;It&#039;s an interesting distinction to consider--being able to describe the steps in a chain of reasoning, and actually doing reasoning.  Perhaps reasoning is a transparent enough thing that there isn&#039;t really a distinction?  Except that you point out the pattern-matching is fuzzy.  In any case, the &quot;chain of reasoning&quot; monologues you describe are happening only in response to prompts, so they aren&#039;t a case of offline thinking.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Designing a rumination program might be tough, but I imagine that it wouldn&#039;t be so much a special routine to run, as an overarching default program that tells the computer how to put its thoughts more in order, and when it gets inputs, that would interrupt the rumination, rather than the other way around.  Then again, it occurs to me that putting one&#039;s thoughts in order while inactive could also be seen as dreaming.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Would it really be a huge leap? The way you described a process gradually nudging the relationships between words that an LLM is trained on into place until the vector math works ideally sounds like the sort of thing that could perhaps be improved during downtime, and that&#039;s just regarding the least thinky of the models we&#039;ve been discussing.  The work that goes into building an artificially intelligent system... surely it doesn&#039;t just have to be one-and-done, right?  You build the brain and then it answers questions?  Doesn&#039;t it make more sense for it to improve itself when not being currently used?  Well, aside from the issue of whether it&#039;s worth paying for the processing power, as you allude to.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-332">Nate Gaylinn (he/him)</a>.</p>
<p>It&#8217;s an interesting distinction to consider&#8211;being able to describe the steps in a chain of reasoning, and actually doing reasoning.  Perhaps reasoning is a transparent enough thing that there isn&#8217;t really a distinction?  Except that you point out the pattern-matching is fuzzy.  In any case, the &#8220;chain of reasoning&#8221; monologues you describe are happening only in response to prompts, so they aren&#8217;t a case of offline thinking.</p>
<p>Designing a rumination program might be tough, but I imagine that it wouldn&#8217;t be so much a special routine to run, as an overarching default program that tells the computer how to put its thoughts more in order, and when it gets inputs, that would interrupt the rumination, rather than the other way around.  Then again, it occurs to me that putting one&#8217;s thoughts in order while inactive could also be seen as dreaming.</p>
<p>Would it really be a huge leap? The way you described a process gradually nudging the relationships between words that an LLM is trained on into place until the vector math works ideally sounds like the sort of thing that could perhaps be improved during downtime, and that&#8217;s just regarding the least thinky of the models we&#8217;ve been discussing.  The work that goes into building an artificially intelligent system&#8230; surely it doesn&#8217;t just have to be one-and-done, right?  You build the brain and then it answers questions?  Doesn&#8217;t it make more sense for it to improve itself when not being currently used?  Well, aside from the issue of whether it&#8217;s worth paying for the processing power, as you allude to.</p>
<p id="comment-like-333" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=333&#038;_wpnonce=edeb308fb9" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-333" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-332</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sun, 29 Dec 2024 13:02:27 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-332</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-331&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

I guess my main objection to the &quot;data processing&quot; model of the brain is that it feels a bit like &quot;explaining away.&quot; I&#039;m not sure what we gain by assuming the brain fundamentally turns inputs into outputs, but if it does anything else, we might miss that if we take the model too seriously.

As for your question, the short answer is &quot;no.&quot; The closest thing I know of is what OpenAI is doing right now with &quot;chain of reasoning&quot; prompts. Basically, they have the AI write its own &quot;internal monologue&quot; along with the text it sends to the user. It&#039;s unclear whether this is anything like what people do, but it does seem to improve performance on some tasks, and researchers are still figuring out why. Part of it seems to be that it literally spends more processing power on each query. I think it&#039;s misleading to call this &quot;reasoning,&quot; though. These systems can&#039;t do formal logic or follow procedures. They can write text &lt;i&gt;describing&lt;/i&gt; such things, but that&#039;s still a fuzzy pattern-matching version of the real thing.

Having an AI that &quot;thinks&quot; continuously doesn&#039;t make much sense with today&#039;s tech. For one thing, it would be &lt;i&gt;very&lt;/i&gt; expensive. It might be useful if you needed to process a continuous stream of data where context and history matter, but then you&#039;d want an AI that learns and adapts as it observes, and we don&#039;t have that yet! To put an AI in a &quot;sensory deprivation tank,&quot; you&#039;d need to give it a rumination program. You&#039;d have to literally tell it what to do with no input. That&#039;s not so far fetched. People have a very stereotyped way of ruminating which seems to be evolved (the default mode network). But instead of having evolution discover a helpful mode of rumination, we&#039;d have to design one from scratch by intuition, and that&#039;s a pretty big leap from where we are today.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-331">Thorin N. Tatge</a>.</p>
<p>I guess my main objection to the &#8220;data processing&#8221; model of the brain is that it feels a bit like &#8220;explaining away.&#8221; I&#8217;m not sure what we gain by assuming the brain fundamentally turns inputs into outputs, but if it does anything else, we might miss that if we take the model too seriously.</p>
<p>As for your question, the short answer is &#8220;no.&#8221; The closest thing I know of is what OpenAI is doing right now with &#8220;chain of reasoning&#8221; prompts. Basically, they have the AI write its own &#8220;internal monologue&#8221; along with the text it sends to the user. It&#8217;s unclear whether this is anything like what people do, but it does seem to improve performance on some tasks, and researchers are still figuring out why. Part of it seems to be that it literally spends more processing power on each query. I think it&#8217;s misleading to call this &#8220;reasoning,&#8221; though. These systems can&#8217;t do formal logic or follow procedures. They can write text <i>describing</i> such things, but that&#8217;s still a fuzzy pattern-matching version of the real thing.</p>
<p>Having an AI that &#8220;thinks&#8221; continuously doesn&#8217;t make much sense with today&#8217;s tech. For one thing, it would be <i>very</i> expensive. It might be useful if you needed to process a continuous stream of data where context and history matter, but then you&#8217;d want an AI that learns and adapts as it observes, and we don&#8217;t have that yet! To put an AI in a &#8220;sensory deprivation tank,&#8221; you&#8217;d need to give it a rumination program. You&#8217;d have to literally tell it what to do with no input. That&#8217;s not so far fetched. People have a very stereotyped way of ruminating which seems to be evolved (the default mode network). But instead of having evolution discover a helpful mode of rumination, we&#8217;d have to design one from scratch by intuition, and that&#8217;s a pretty big leap from where we are today.</p>
<p id="comment-like-332" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=332&#038;_wpnonce=b57e43a44a" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-332" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-331</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Sun, 29 Dec 2024 00:48:42 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-331</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-330&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;I think thought could be framed as a very rich sort of &#039;data processing&#039; if we were to speak broadly, if other thoughts and feelings were to be considered data.  I think further that my phrasing of thought as &quot;inputs, outputs and the processing in between&quot; is reasonable, if somewhat flip, given that a lot more can be processed than just data.  Rumination is processing, both in the mental sense and in the classical digestive sense.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Predictive language models don&#039;t do this extra stuff.  What about deep machine learning systems in general?  Do any of them do anything when they&#039;re not actually being used to talk?  Do they &#039;think&#039; while in their &#039;sensory deprivation tanks&#039;?  Do we have computers that ruminate all the time that they&#039;re turned on, whether we&#039;re using them or not?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-330">Nate Gaylinn (he/him)</a>.</p>
<p>I think thought could be framed as a very rich sort of &#8216;data processing&#8217; if we were to speak broadly, if other thoughts and feelings were to be considered data.  I think further that my phrasing of thought as &#8220;inputs, outputs and the processing in between&#8221; is reasonable, if somewhat flip, given that a lot more can be processed than just data.  Rumination is processing, both in the mental sense and in the classical digestive sense.</p>
<p>Predictive language models don&#8217;t do this extra stuff.  What about deep machine learning systems in general?  Do any of them do anything when they&#8217;re not actually being used to talk?  Do they &#8216;think&#8217; while in their &#8216;sensory deprivation tanks&#8217;?  Do we have computers that ruminate all the time that they&#8217;re turned on, whether we&#8217;re using them or not?</p>
<p id="comment-like-331" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=331&#038;_wpnonce=86de110cee" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-331" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-330</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Tue, 24 Dec 2024 13:34:36 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-330</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-329&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

These are lovely ideas.

Your point about &quot;thought&quot; is an important one, and deserving of study. Some folks argue that it&#039;s wrong to think of the mind or organisms in terms of inputs, outputs, and data processing, and that this is why AI systems based on machine learning cannot possibly recreate the intelligence of life. I think there&#039;s something to that argument. Thought is &lt;i&gt;not&lt;/i&gt; just input / output processing, there are patterns of rumination and planning that don&#039;t feel like &quot;functions&quot; at all, and seem only distantly connected with stimulus and response. Also, people are perfectly capable of thinking even when they&#039;re lying in a sensory deprivation tank. Then again, others would argue that&#039;s just processing &lt;i&gt;internal&lt;/i&gt; &quot;inputs&quot; and &quot;outputs&quot;, though I&#039;m not sure what that means, exactly. In some sense that feels like hand waving to dismiss the brain as a miracle, and argue it&#039;s &quot;just math&quot; without clear evidence.

One useful way to think about this is how an intelligent system is framed. Living creatures are continuously aware. They interact with the world, and respond in ways that further their purpose and maintain their survival. In contrast, an LLM is a mathematical function. It&#039;s only active when it&#039;s processing some query, then it is simply &quot;off&quot;. Generally, these systems have no memory, aside from re-reading the chat log before each reply. They only interact through the world via text, though they can act in the real world if we write code to realize the actions they describe textually. They have no internally generated sense of purpose or survival, though we can tell them to act like they do. Regardless of what consciousness is and whether machines can have it, I think this says a lot about what the &quot;experience&quot; of these systems is like and how it&#039;s different from our own. It&#039;s different enough that I don&#039;t want to call it the same thing, but you could also imagine systems that narrow that difference, which is... scary and fascinating. :)

Another related idea: if LLMs could have something like thought or consciousness, it would be almost exclusively textual. We often train LLMs on images and other kinds of data, too, but vastly less of it. Certainly, they don&#039;t have all the sensory modes that people do, and are currently missing other kinds of abstract thought and representation, like numeracy and logic. In other words, people can think without using words or even images, and they can mix many different kinds of thought and imagery together! LLMs cannot. What difference does that make?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-329">Thorin N. Tatge</a>.</p>
<p>These are lovely ideas.</p>
<p>Your point about &#8220;thought&#8221; is an important one, and deserving of study. Some folks argue that it&#8217;s wrong to think of the mind or organisms in terms of inputs, outputs, and data processing, and that this is why AI systems based on machine learning cannot possibly recreate the intelligence of life. I think there&#8217;s something to that argument. Thought is <i>not</i> just input / output processing, there are patterns of rumination and planning that don&#8217;t feel like &#8220;functions&#8221; at all, and seem only distantly connected with stimulus and response. Also, people are perfectly capable of thinking even when they&#8217;re lying in a sensory deprivation tank. Then again, others would argue that&#8217;s just processing <i>internal</i> &#8220;inputs&#8221; and &#8220;outputs&#8221;, though I&#8217;m not sure what that means, exactly. In some sense that feels like hand waving to dismiss the brain as a miracle, and argue it&#8217;s &#8220;just math&#8221; without clear evidence.</p>
<p>One useful way to think about this is how an intelligent system is framed. Living creatures are continuously aware. They interact with the world, and respond in ways that further their purpose and maintain their survival. In contrast, an LLM is a mathematical function. It&#8217;s only active when it&#8217;s processing some query, then it is simply &#8220;off&#8221;. Generally, these systems have no memory, aside from re-reading the chat log before each reply. They only interact through the world via text, though they can act in the real world if we write code to realize the actions they describe textually. They have no internally generated sense of purpose or survival, though we can tell them to act like they do. Regardless of what consciousness is and whether machines can have it, I think this says a lot about what the &#8220;experience&#8221; of these systems is like and how it&#8217;s different from our own. It&#8217;s different enough that I don&#8217;t want to call it the same thing, but you could also imagine systems that narrow that difference, which is&#8230; scary and fascinating. ðŸ™‚</p>
<p>Another related idea: if LLMs could have something like thought or consciousness, it would be almost exclusively textual. We often train LLMs on images and other kinds of data, too, but vastly less of it. Certainly, they don&#8217;t have all the sensory modes that people do, and are currently missing other kinds of abstract thought and representation, like numeracy and logic. In other words, people can think without using words or even images, and they can mix many different kinds of thought and imagery together! LLMs cannot. What difference does that make?</p>
<p id="comment-like-330" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=330&#038;_wpnonce=097e3f7d67" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-330" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-329</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Tue, 24 Dec 2024 12:46:57 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-329</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-328&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;I guess it makes sense to say that of all the ways there are to organize the words in a language, some ways of assigning them dimensions (by which their relationships to other words are quantized and by which they&#039;re combined into logically related ideas) work better than other ways, and that&#039;s because language is - gasp - used to describe a real world, and that world has actual things with actual characteristics in it.  For example, because there is a thing called water that makes things wet, it may make sense for &#039;wetness&#039; or &#039;moisture&#039; to be one of the dimensions in which words can be vector-added.  The situation seems reminiscent of adjusting the assignments of letters in a Cryptoquip puzzle until some of the words start to halfway make sense, and then you make more adjustments based on the first ones until a sentence that makes sense starts to grow out of the words.  Only this example is a lot more complicated and harder to understand what it actually means.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The whole concept&#039;s tenuous explicability also reminds me of an idea about the brain and mind that I have trouble expressing clearly, but which I&#039;ve alluded to before and can basically be described as the mystery of where the seat of consciousness is; or where awareness comes from amid all the actions of the brain; or what makes inputs, outputs and the processing in between them into &lt;em&gt;thoughts &lt;/em&gt;instead of just a lot of processing.  Much like LLMs trained on text with Transformers do an unexpectedly good job of sounding sensible... a human brain with lots of complex senses and internal interrelations unexpectedly produces a being that has the experience of being something.  It&#039;s easy to wave your hands and say, &quot;Well, the brain is a complex miracle, it&#039;s no surprise consciousness is in there somewhere.&quot;  But it may be harder to admit that perhaps the... lack of a core processing system in the brain; the fact that the brain is a lot like a thing that just functions based on relations... may indicate that consciousness doesn&#039;t really comprise as many ingredients as it feels like it must, after all.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-328">Nate Gaylinn (he/him)</a>.</p>
<p>I guess it makes sense to say that of all the ways there are to organize the words in a language, some ways of assigning them dimensions (by which their relationships to other words are quantized and by which they&#8217;re combined into logically related ideas) work better than other ways, and that&#8217;s because language is &#8211; gasp &#8211; used to describe a real world, and that world has actual things with actual characteristics in it.  For example, because there is a thing called water that makes things wet, it may make sense for &#8216;wetness&#8217; or &#8216;moisture&#8217; to be one of the dimensions in which words can be vector-added.  The situation seems reminiscent of adjusting the assignments of letters in a Cryptoquip puzzle until some of the words start to halfway make sense, and then you make more adjustments based on the first ones until a sentence that makes sense starts to grow out of the words.  Only this example is a lot more complicated and harder to understand what it actually means.</p>
<p>The whole concept&#8217;s tenuous explicability also reminds me of an idea about the brain and mind that I have trouble expressing clearly, but which I&#8217;ve alluded to before and can basically be described as the mystery of where the seat of consciousness is; or where awareness comes from amid all the actions of the brain; or what makes inputs, outputs and the processing in between them into <em>thoughts </em>instead of just a lot of processing.  Much like LLMs trained on text with Transformers do an unexpectedly good job of sounding sensible&#8230; a human brain with lots of complex senses and internal interrelations unexpectedly produces a being that has the experience of being something.  It&#8217;s easy to wave your hands and say, &#8220;Well, the brain is a complex miracle, it&#8217;s no surprise consciousness is in there somewhere.&#8221;  But it may be harder to admit that perhaps the&#8230; lack of a core processing system in the brain; the fact that the brain is a lot like a thing that just functions based on relations&#8230; may indicate that consciousness doesn&#8217;t really comprise as many ingredients as it feels like it must, after all.</p>
<p id="comment-like-329" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=329&#038;_wpnonce=917d57e0ee" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-329" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-328</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sat, 21 Dec 2024 13:01:25 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-328</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-327&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

Hmm, now you&#039;re getting to some of the mysterious part of Deep Learning! If DL models can &quot;embed&quot; language into a high-dimensional concept space, then what is the structure of that space? Since it&#039;s not built in by the programmers, it must be somehow inferred by the machine learning process, but how? This is where researchers must wave their hands vigorously.

To some extent, the answer is that it &quot;just does.&quot; We make a high-dimensional vector space, place words into it randomly, and then slowly move the words around until their structure allows you to predict some text with minimal error. We use gradient descent to move the words around, which means we look at how each word contributes to the error, and make an informed guess about which direction to nudge it in order to reduce the error. It seems kind of implausible that you could do this with every word in the English language simultaneously and have it work, but... it works. One theory is that having vast numbers of high-dimensional constraints actually makes the optimization problem &lt;i&gt;easier&lt;/i&gt; than if it were smaller. That does seem to be borne out by research in this space, but I&#039;ve never seen a good &lt;i&gt;explanation&lt;/i&gt;, just examples where it seems to be true.

And you make another good point: the words settled into these positions by a process we don&#039;t completely understand. Perhaps it formed a bit like a crystal, where some words became locked into place and everything else sort of... grew in around that, following a similar structure. If so, then those initial &quot;nucleation sites&quot; must be pretty important, and say a lot about the overall structure of the semantic space! If they had been different, would the semantic space have turned out differently, too? By how much? How does the data or training procedure we choose affect this process? We don&#039;t know, but I think it has big consequences for how &lt;i&gt;reliable&lt;/i&gt; these systems are, and how we understand what they do. Right now we kinda pretend like they just &quot;figure out language,&quot; but there&#039;s no one right way to understand language. There &lt;i&gt;must&lt;/i&gt; be some perspective, so what is it? We don&#039;t know.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-327">Thorin N. Tatge</a>.</p>
<p>Hmm, now you&#8217;re getting to some of the mysterious part of Deep Learning! If DL models can &#8220;embed&#8221; language into a high-dimensional concept space, then what is the structure of that space? Since it&#8217;s not built in by the programmers, it must be somehow inferred by the machine learning process, but how? This is where researchers must wave their hands vigorously.</p>
<p>To some extent, the answer is that it &#8220;just does.&#8221; We make a high-dimensional vector space, place words into it randomly, and then slowly move the words around until their structure allows you to predict some text with minimal error. We use gradient descent to move the words around, which means we look at how each word contributes to the error, and make an informed guess about which direction to nudge it in order to reduce the error. It seems kind of implausible that you could do this with every word in the English language simultaneously and have it work, but&#8230; it works. One theory is that having vast numbers of high-dimensional constraints actually makes the optimization problem <i>easier</i> than if it were smaller. That does seem to be borne out by research in this space, but I&#8217;ve never seen a good <i>explanation</i>, just examples where it seems to be true.</p>
<p>And you make another good point: the words settled into these positions by a process we don&#8217;t completely understand. Perhaps it formed a bit like a crystal, where some words became locked into place and everything else sort of&#8230; grew in around that, following a similar structure. If so, then those initial &#8220;nucleation sites&#8221; must be pretty important, and say a lot about the overall structure of the semantic space! If they had been different, would the semantic space have turned out differently, too? By how much? How does the data or training procedure we choose affect this process? We don&#8217;t know, but I think it has big consequences for how <i>reliable</i> these systems are, and how we understand what they do. Right now we kinda pretend like they just &#8220;figure out language,&#8221; but there&#8217;s no one right way to understand language. There <i>must</i> be some perspective, so what is it? We don&#8217;t know.</p>
<p id="comment-like-328" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=328&#038;_wpnonce=c7a12c2748" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-328" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-327</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Sat, 21 Dec 2024 01:21:12 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-327</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-326&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;This is interesting, because it seems to promise a glimpse into the secrets of language and our use of it--to answer questions no one was asking like &quot;What is the male equivalent in our language of &#039;ballet&#039;?&quot; or &quot;What do you get, linguistically, if you cross an elephant with a porcupine?&quot;  But what is this really telling us?  (thinks)  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;I guess, if you remove &#039;WOMAN&#039; from &#039;BALLET&#039; and add &#039;MAN&#039;, you&#039;re learning... what words are more like &#039;man&#039; than &#039;ballet&#039; is, according to certain characteristics, and less like &#039;woman&#039;... to the extent that &#039;man&#039; and &#039;woman&#039; differ from the average.  So... okay, that is reasonably interesting, but it seems to depend on what the initial however-many-dimensions are that define the word vectors.  And I get the sense those must somehow be automated, too... but how?  Where do these ten thousand dimensions come from?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-326">Nate Gaylinn (he/him)</a>.</p>
<p>This is interesting, because it seems to promise a glimpse into the secrets of language and our use of it&#8211;to answer questions no one was asking like &#8220;What is the male equivalent in our language of &#8216;ballet&#8217;?&#8221; or &#8220;What do you get, linguistically, if you cross an elephant with a porcupine?&#8221;  But what is this really telling us?  (thinks)  </p>
<p>I guess, if you remove &#8216;WOMAN&#8217; from &#8216;BALLET&#8217; and add &#8216;MAN&#8217;, you&#8217;re learning&#8230; what words are more like &#8216;man&#8217; than &#8216;ballet&#8217; is, according to certain characteristics, and less like &#8216;woman&#8217;&#8230; to the extent that &#8216;man&#8217; and &#8216;woman&#8217; differ from the average.  So&#8230; okay, that is reasonably interesting, but it seems to depend on what the initial however-many-dimensions are that define the word vectors.  And I get the sense those must somehow be automated, too&#8230; but how?  Where do these ten thousand dimensions come from?</p>
<p id="comment-like-327" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=327&#038;_wpnonce=54f69a7acf" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-327" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-326</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 18 Dec 2024 12:10:14 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267#comment-326</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-325&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

I think you understand this well. Generally, nobody manually determines the &quot;baseline equations,&quot; it&#039;s all just based on how words co-occur with each other in text. This technique does a surprisingly good job at self-organizing itself into a pretty consistent network of concepts with meaningful spatial relationships, but, as you intuited, the math only sorta works. Adding &lt;code&gt;WOMAN&lt;/code&gt; and subtracting &lt;code&gt;MAN&lt;/code&gt; tends to have a similar effect on words that are similar. So, like, for the cluster of strongly gendered nouns (&quot;king&quot;, &quot;groom&quot;, &quot;brother), you can very reliably translate to the feminine form (&quot;queen,&quot; &quot;bride,&quot; &quot;sister&quot;). It gets weirder, more sexist, and with much looser matches when you move to words that are only gender associated (&quot;nurse,&quot; &quot;architect,&quot; &quot;ballet&quot;), and if you apply it to totally ungendered words (&quot;schism,&quot; &quot;plantain,&quot; &quot;chair&quot;) you&#039;ll get a vector that represents an unrelated word, or no word at all.

This sort of &quot;word math&quot; is very crude, but it does at least let you get a peak into what the model has learned from the text. It&#039;s an old idea, though; things do get a little more complicated in Transformer models, because there is no absolute meaning of a word, it always has to be contextual. So, it&#039;s easy to see what context the model attends to, and to get a sense for how it interprets a word within a particular sentence, but much harder to probe what that word might mean across the whole range of possible texts.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/comment-page-1/#comment-325">Thorin N. Tatge</a>.</p>
<p>I think you understand this well. Generally, nobody manually determines the &#8220;baseline equations,&#8221; it&#8217;s all just based on how words co-occur with each other in text. This technique does a surprisingly good job at self-organizing itself into a pretty consistent network of concepts with meaningful spatial relationships, but, as you intuited, the math only sorta works. Adding <code>WOMAN</code> and subtracting <code>MAN</code> tends to have a similar effect on words that are similar. So, like, for the cluster of strongly gendered nouns (&#8220;king&#8221;, &#8220;groom&#8221;, &#8220;brother), you can very reliably translate to the feminine form (&#8220;queen,&#8221; &#8220;bride,&#8221; &#8220;sister&#8221;). It gets weirder, more sexist, and with much looser matches when you move to words that are only gender associated (&#8220;nurse,&#8221; &#8220;architect,&#8221; &#8220;ballet&#8221;), and if you apply it to totally ungendered words (&#8220;schism,&#8221; &#8220;plantain,&#8221; &#8220;chair&#8221;) you&#8217;ll get a vector that represents an unrelated word, or no word at all.</p>
<p>This sort of &#8220;word math&#8221; is very crude, but it does at least let you get a peak into what the model has learned from the text. It&#8217;s an old idea, though; things do get a little more complicated in Transformer models, because there is no absolute meaning of a word, it always has to be contextual. So, it&#8217;s easy to see what context the model attends to, and to get a sense for how it interprets a word within a particular sentence, but much harder to probe what that word might mean across the whole range of possible texts.</p>
<p id="comment-like-326" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/?like_comment=326&#038;_wpnonce=ebd3780052" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-326" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
