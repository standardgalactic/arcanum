<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: Status Update: End of Semester 3	</title>
	<atom:link href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/feed/" rel="self" type="application/rss+xml" />
	<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/</link>
	<description>Becoming an intelligence researcher</description>
	<lastBuildDate>Tue, 17 Dec 2024 20:57:23 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-324</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Tue, 17 Dec 2024 20:57:23 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=263#comment-324</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-323&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

Your memory is correct! In this case, the main difference was not in the architecture of the AI system, but in its training process. They trained AlphaGo by replaying the games of experts, so it learned to imitate those players. With AlphaGoZero, they trained it just by having it play against itself. It rediscovered some classic Go moves and strategies, but also invented its own repertoire, by trial and error. But it&#039;s unclear whether losing the human bias made it a better Go player generally, or just one that&#039;s weirdly different from human players. AlphaGoZero generally does beat AlphaGo (which is playing like a typical human player, using traditional strategy), but human players who have studied how AlphaGoZero plays do much better.

What made AlphaGo revolutionary was that they figured out how to offload some of the key operations from traditional tree search algorithms to a neural network that could be trained by example. In some sense, that&#039;s another revolution by simplification / removing pieces, but it&#039;s also a little different. Successfully training AlphaGoZero without &lt;i&gt;any&lt;/i&gt; prior examples of how to play was pretty surprising at the time, and it has definitely changed how folks think about this kind of problem, and about how AI works generally.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-323">Thorin N. Tatge</a>.</p>
<p>Your memory is correct! In this case, the main difference was not in the architecture of the AI system, but in its training process. They trained AlphaGo by replaying the games of experts, so it learned to imitate those players. With AlphaGoZero, they trained it just by having it play against itself. It rediscovered some classic Go moves and strategies, but also invented its own repertoire, by trial and error. But it&#8217;s unclear whether losing the human bias made it a better Go player generally, or just one that&#8217;s weirdly different from human players. AlphaGoZero generally does beat AlphaGo (which is playing like a typical human player, using traditional strategy), but human players who have studied how AlphaGoZero plays do much better.</p>
<p>What made AlphaGo revolutionary was that they figured out how to offload some of the key operations from traditional tree search algorithms to a neural network that could be trained by example. In some sense, that&#8217;s another revolution by simplification / removing pieces, but it&#8217;s also a little different. Successfully training AlphaGoZero without <i>any</i> prior examples of how to play was pretty surprising at the time, and it has definitely changed how folks think about this kind of problem, and about how AI works generally.</p>
<p id="comment-like-324" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/?like_comment=324&#038;_wpnonce=e01478d8c3" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-324" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-323</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Tue, 17 Dec 2024 20:00:58 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=263#comment-323</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-322&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Ah!  Well, does that mean there was a separate AI research insight with a similar form, involving some piece of the system that turned out to be holding the rest back?  Because I remember reading that the &quot;Zero&quot; in AlphaZero was about how it got better results by starting from scratch, instead of being given introductory knowledge in Go or chess.  Maybe that insight wasn&#039;t as fundamental or surprising, though?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-322">Nate Gaylinn (he/him)</a>.</p>
<p>Ah!  Well, does that mean there was a separate AI research insight with a similar form, involving some piece of the system that turned out to be holding the rest back?  Because I remember reading that the &#8220;Zero&#8221; in AlphaZero was about how it got better results by starting from scratch, instead of being given introductory knowledge in Go or chess.  Maybe that insight wasn&#8217;t as fundamental or surprising, though?</p>
<p id="comment-like-323" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/?like_comment=323&#038;_wpnonce=89447d28f2" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-323" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-322</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Tue, 17 Dec 2024 13:03:50 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=263#comment-322</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-321&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

For the record, the AlphaGo family does not use Transformers. They&#039;re based on a different branch of machine learning that&#039;s all about agents that assess situations, make decisions, and act (reinforcement learning). Specifically, they built Deep Learning models that look at a board and estimate two things: &quot;who is winning now?&quot; and &quot;what move(s) would be good to make?&quot; The rest of the program is a custom-built search over the game state tree. So, it&#039;s not &quot;transforming&quot; any data and not using the concept space from human language to do anything. However, we should probably expect future models that are hybrids of these techniques! That could be a Go bot that generates plausible explanations for its moves, or a bot that can play more complex games that involve human knowledge or interacting with the game via text or images instead of moving stones. And, of course, &quot;games&quot; could mean whatever task you want an AI to do, if you can describe that task in formal, game-like terms.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-321">Thorin N. Tatge</a>.</p>
<p>For the record, the AlphaGo family does not use Transformers. They&#8217;re based on a different branch of machine learning that&#8217;s all about agents that assess situations, make decisions, and act (reinforcement learning). Specifically, they built Deep Learning models that look at a board and estimate two things: &#8220;who is winning now?&#8221; and &#8220;what move(s) would be good to make?&#8221; The rest of the program is a custom-built search over the game state tree. So, it&#8217;s not &#8220;transforming&#8221; any data and not using the concept space from human language to do anything. However, we should probably expect future models that are hybrids of these techniques! That could be a Go bot that generates plausible explanations for its moves, or a bot that can play more complex games that involve human knowledge or interacting with the game via text or images instead of moving stones. And, of course, &#8220;games&#8221; could mean whatever task you want an AI to do, if you can describe that task in formal, game-like terms.</p>
<p id="comment-like-322" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/?like_comment=322&#038;_wpnonce=8ea276f40c" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-322" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-321</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Tue, 17 Dec 2024 09:13:16 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=263#comment-321</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-319&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;That analogy of a bicycle wheel reaching the supermarket on its own is really illustrative!  I tried to think of what that example was reminding me of, since you don&#039;t come across phenomena like that too often.  I thought of how AlphaGo Zero turned out to be better at playing Go than the version where they gave it basic opening theory and rules.  But that&#039;s probably the result of Transformers, too!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;What examples can I muster up of a part outperforming a whole?&lt;br&gt;* I prefer pumpkin pudding, i.e. the filling of pumpkin pie, without the crust.&lt;br&gt;* The turn-based mech warfare puzzle game Into the Breach was a bulky, complicated world-management game before the designers just took the minigame scenarios and made them the whole game.&lt;br&gt;* One of my characters prefers straight vermouth to martinis or negronis with vermouth in them.&lt;br&gt;* Some hobby games, like party games with challenges on cards, work better if you just use the cards instead of using the framing device.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;But these kinds of examples are clearly not on the same scale of complexity.  It does sound like &quot;Nothing But Attention&quot; was a bit of a miracle worth having a class about.  If there&#039;s a lesson to the layperson, it would seem to be that thinking is more cloudy and less step-by-step methodological than researchers were thinking about it!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-319">Nate Gaylinn (he/him)</a>.</p>
<p>That analogy of a bicycle wheel reaching the supermarket on its own is really illustrative!  I tried to think of what that example was reminding me of, since you don&#8217;t come across phenomena like that too often.  I thought of how AlphaGo Zero turned out to be better at playing Go than the version where they gave it basic opening theory and rules.  But that&#8217;s probably the result of Transformers, too!</p>
<p>What examples can I muster up of a part outperforming a whole?<br />* I prefer pumpkin pudding, i.e. the filling of pumpkin pie, without the crust.<br />* The turn-based mech warfare puzzle game Into the Breach was a bulky, complicated world-management game before the designers just took the minigame scenarios and made them the whole game.<br />* One of my characters prefers straight vermouth to martinis or negronis with vermouth in them.<br />* Some hobby games, like party games with challenges on cards, work better if you just use the cards instead of using the framing device.</p>
<p>But these kinds of examples are clearly not on the same scale of complexity.  It does sound like &#8220;Nothing But Attention&#8221; was a bit of a miracle worth having a class about.  If there&#8217;s a lesson to the layperson, it would seem to be that thinking is more cloudy and less step-by-step methodological than researchers were thinking about it!</p>
<p id="comment-like-321" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/?like_comment=321&#038;_wpnonce=e010e6f6f3" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-321" class="comment-like-feedback">Liked by <a href="#" class="view-likers" data-like-count="1">1 person</a></span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-319</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sun, 15 Dec 2024 14:55:07 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=263#comment-319</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-317&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

Thanks, Thorin!

Yeah, everything I&#039;ve built so far and (as far as I know) every computer simulation of evolution ever built has &quot;genomes.&quot; Basically, to evolve the design for some thing, you represent the thing as data, then you repeatedly tweak the data until the corresponding thing has the properties you want. Evolving something that &lt;i&gt;doesn&#039;t&lt;/i&gt; have some explicit data object that represents it is new and strange! Perhaps impossible, though we&#039;ll see about that.  :)

Yes, deep learning research has been a lot of throwing stuff at the wall! Some of it has stuck on real good, but, honestly, we still have no idea what we&#039;re doing. For instance: Transformers. They weren&#039;t an incremental improvement on what was working. No, they took a piece of what was working, used it in a way that nobody thought to try, and it somehow worked even better than before. It&#039;s like they took a wheel off a bicycle and discovered that the wheel alone could get to the grocery store faster than the bike. It was surprising that it worked, it violated the normal rules of how we build things, and it makes you wonder if we properly understood what the earlier models were doing in the first place!

But, yeah, talking about these surprises and coming up with new ways to explain them is probably the best way to improve our understanding.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-317">Thorin N. Tatge</a>.</p>
<p>Thanks, Thorin!</p>
<p>Yeah, everything I&#8217;ve built so far and (as far as I know) every computer simulation of evolution ever built has &#8220;genomes.&#8221; Basically, to evolve the design for some thing, you represent the thing as data, then you repeatedly tweak the data until the corresponding thing has the properties you want. Evolving something that <i>doesn&#8217;t</i> have some explicit data object that represents it is new and strange! Perhaps impossible, though we&#8217;ll see about that.  🙂</p>
<p>Yes, deep learning research has been a lot of throwing stuff at the wall! Some of it has stuck on real good, but, honestly, we still have no idea what we&#8217;re doing. For instance: Transformers. They weren&#8217;t an incremental improvement on what was working. No, they took a piece of what was working, used it in a way that nobody thought to try, and it somehow worked even better than before. It&#8217;s like they took a wheel off a bicycle and discovered that the wheel alone could get to the grocery store faster than the bike. It was surprising that it worked, it violated the normal rules of how we build things, and it makes you wonder if we properly understood what the earlier models were doing in the first place!</p>
<p>But, yeah, talking about these surprises and coming up with new ways to explain them is probably the best way to improve our understanding.</p>
<p id="comment-like-319" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/?like_comment=319&#038;_wpnonce=ebd22ac34c" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-319" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-317</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Sat, 14 Dec 2024 20:05:31 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=263#comment-317</guid>

					<description><![CDATA[&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Congrats on Semester 3, Nate!!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;So, you told me about your intention to simulate endosymbiotic relationship evolving in some sort of simple &#039;game&#039; mechanism, and we brainstormed what form that might take. Later on, I think you revealed it wound up being totally different from what you discussed. So, I would definitely be interested in hearing or seeing what it wound up being! What the toy problem was you solved, what you hope to solve next… if you&#039;re excited to evolve a population without genomes, does that mean the symbiotic system -did- have genomes?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Anyway, the sneak peek was intriguing, if only a glance at something. Vibes of trying to live in a stomach full of churning acids and juices.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Counter-intuitively GOOD results from Deep Learning, eh? I guess if thinkers and researchers throw six dozen things at the wall and three of them stick, it makes sense for bunches of students to study the things that stuck! Figure out if they have anything in common, and if they closely resemble brain intelligence, a.k.a. what stuck in real life. Or heck, maybe gut intelligence! I think an issue for many of us is having difficulty with how counterintuitively effective our brains are just being what they are… in that, in some ways, for example, our thoughts and memories may be simpler than it feels like they should have to be. So, this notion of examining why a system is oddly effective as creating intelligence seems to resonate with me.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>Congrats on Semester 3, Nate!!</p>
<p>So, you told me about your intention to simulate endosymbiotic relationship evolving in some sort of simple &#8216;game&#8217; mechanism, and we brainstormed what form that might take. Later on, I think you revealed it wound up being totally different from what you discussed. So, I would definitely be interested in hearing or seeing what it wound up being! What the toy problem was you solved, what you hope to solve next… if you&#8217;re excited to evolve a population without genomes, does that mean the symbiotic system -did- have genomes?</p>
<p>Anyway, the sneak peek was intriguing, if only a glance at something. Vibes of trying to live in a stomach full of churning acids and juices.</p>
<p>Counter-intuitively GOOD results from Deep Learning, eh? I guess if thinkers and researchers throw six dozen things at the wall and three of them stick, it makes sense for bunches of students to study the things that stuck! Figure out if they have anything in common, and if they closely resemble brain intelligence, a.k.a. what stuck in real life. Or heck, maybe gut intelligence! I think an issue for many of us is having difficulty with how counterintuitively effective our brains are just being what they are… in that, in some ways, for example, our thoughts and memories may be simpler than it feels like they should have to be. So, this notion of examining why a system is oddly effective as creating intelligence seems to resonate with me.</p>
<p id="comment-like-317" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/?like_comment=317&#038;_wpnonce=9a93e68863" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-317" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: rgritter		</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/comment-page-1/#comment-315</link>

		<dc:creator><![CDATA[rgritter]]></dc:creator>
		<pubDate>Sat, 07 Dec 2024 22:07:41 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=263#comment-315</guid>

					<description><![CDATA[&lt;!-- wp:paragraph --&gt;
&lt;p&gt;I understand almost nothing of your studies, but I read them all. So proud of you. Have a happy birthday!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Aunt Renee&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;]]></description>
			<content:encoded><![CDATA[<p>I understand almost nothing of your studies, but I read them all. So proud of you. Have a happy birthday!</p>
<p>Aunt Renee</p>
</p>
<p id="comment-like-315" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2024/12/07/status-update-end-of-semester-3/?like_comment=315&#038;_wpnonce=5b63786b79" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-315" class="comment-like-feedback">Liked by <a href="#" class="view-likers" data-like-count="1">1 person</a></span></p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
