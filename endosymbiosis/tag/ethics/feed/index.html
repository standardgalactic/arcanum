<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>ethics &#8211; Thinking with Nate</title>
	<atom:link href="https://thinkingwithnate.wordpress.com/tag/ethics/feed/" rel="self" type="application/rss+xml" />
	<link>https://thinkingwithnate.wordpress.com</link>
	<description>Becoming an intelligence researcher</description>
	<lastBuildDate>Wed, 05 Apr 2023 16:18:53 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain='thinkingwithnate.wordpress.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>https://s0.wp.com/i/buttonw-com.png</url>
		<title>ethics &#8211; Thinking with Nate</title>
		<link>https://thinkingwithnate.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://thinkingwithnate.wordpress.com/osd.xml" title="Thinking with Nate" />
	<atom:link rel='hub' href='https://thinkingwithnate.wordpress.com/?pushpress=hub'/>
	<item>
		<title>Large Language Models, LaMDA, and Sentience</title>
		<link>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/</link>
					<comments>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 15 Jun 2022 17:24:46 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<category><![CDATA[deep_fakes]]></category>
		<category><![CDATA[ethics]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[llm]]></category>
		<category><![CDATA[machine_learning]]></category>
		<category><![CDATA[sentience]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=99</guid>

					<description><![CDATA[Several folks asked me to weigh in on whether Google’s AI chatbot, LaMDA, is sentient. I don’t know much about LaMDA specifically, so I want to talk about Large Language Models (LLMs) generally, since they show up in many forms. It’s a truly amazing technology. They can generate text that’s superficially indistinguishable from human writing. &#8230; <a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/" class="more-link">Continue reading<span class="screen-reader-text"> "Large Language Models, LaMDA, and&#160;Sentience"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Several folks asked me to weigh in on whether Google’s AI chatbot, LaMDA, is sentient. I don’t know much about LaMDA specifically, so I want to talk about Large Language Models (LLMs) generally, since they show up in many forms. It’s a truly amazing technology. They can generate text that’s superficially indistinguishable from human writing. But are these systems capable of sentience? Let’s dig into it.</p>



<p>Let’s start with what an LLM actually does. At the core, it analyzes text for statistical correlations. This word co-occurs with that word. When you see this, it’s often followed by that. These words appear in similar contexts, and may be interchangeable. That sort of thing. What makes LLMs “large” is that they get trained on <em>enormous</em> bodies of text. Like, billions of web documents or whole libraries worth of books. This allows them to learn very subtle and nuanced patterns, and collect example texts on many themes. When an LLM is put into use, what it’s doing is confabulating new sequences of words with the same regular structure as its training data. They mix prompting from the user with relevant passages in their training data and randomness.</p>



<p>LLMs take advantage of a couple recent innovations in AI. One is transfer learning. First, the LLM is trained on an enormous corpus to learn the structure of language generally. Then, it gets fine tuned on a narrow data set, to constrain its output to fit a specific style and context. This isn’t so different from <a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee">style transfer</a> in computer vision. The other trick is <a href="https://wiki.pathmind.com/attention-mechanism-memory-network">attention and memory</a>. LLMs can spot correlations between words at short, medium, and long distances, and learn which of the associations it learns are most relevant to good output. This makes LLMs much more self-consistent and better at question-answering tasks than previous technology.</p>



<p>A large part of why LLMs are so effective has to do with language itself. Language is highly self-referential. Words are defined in terms of other words. The meaning and sentiment of a word comes primarily from the context where it’s used (when I learned about <a href="https://towardsdatascience.com/word2vec-explained-49c52b4ccb71">word2vec</a>, I came to appreciate this much more deeply). We each have vast networks of words and images and memories all tied together, and it&#8217;s the shape of that network that creates meaning. Humans are able to communicate with language for two reasons. First, folks who speak the same language have <em>consistent</em> networks of words in their minds. They’re highly correlated with each other, so the words mean the same things to both people. Second, those networks of words are consistent with the shape of our thoughts and our lived experience of reality. That allows us to appreciate the purpose and consequences of the words we hear.</p>



<p>LLMs are specifically designed to learn that network of meaning, and build a model that is consistent with the one in your head. So, in a sense, they really do “understand” language. They know many of the same concepts and relationships that you do. They can regurgitate definitions and even answer questions by generating new sequences of words that follow the patterns. However, an LLM has no access to the physical world, so this network of ideas is not grounded in reality.</p>



<p>The question of AI sentience is tough, since we don’t have a good definition of sentience. Some scientists speculate that even raw information or matter might be conscious in some minimal way. But when we say “sentience” we tend to think about things like self-awareness, understanding, feelings, and intentions. Our brains produce the nuanced kind of sentience that makes us human through their very particular complex structures. So, even if a rock or an LLM is “conscious” in the minimal sense, they’re definitely <em>not</em> sentient, at least not at all like a person is. I have two reasons for saying that.</p>



<p>Firstly, people have bodies and minds that <em>produce</em> feelings, emotions, self monitoring, our train of thought, etc. We have hormones, neurotransmitters, and brain regions dedicated to those purposes. LLMs are much, much simpler in design. They were not built to have those abilities, so they don’t have them. Some worry that sentience might “evolve” or “emerge” without us explicitly building it in. Perhaps that could happen some day, but I think it’s safe to say the way we build LLMs today makes that impossible. Literally all they do is shuffle vectors representing words. Unlike life, they don’t shape their own design in any way, so they will never learn to do something other than what they were built for.</p>



<p>Second, people have a sense of self because there is a clear self / other distinction. We can see and feel our bodies, look out at the world, etc. The only thing an LLM “experiences” is training data. Text, and lots of it. They literally do not have the capacity to perceive anything else, because of how they’re built. They can’t see their data, programming, or the computer environment they are in, because we don’t give them that access. Some LLMs are also trained with visual imagery, but remember, what they “experience” is just <em>pixels</em>, not objects in the real world. That’s why they can be easily fooled by <a href="https://hackernoon.com/adversarial-attacks-how-to-trick-computer-vision-7484c4e85dc0">adversarial examples</a>.</p>



<p>What about the LaMDA chatbot specifically? The <a href="https://www.documentcloud.org/documents/22058315-is-lamda-sentient-an-interview">transcript</a> making all the headlines is worth checking out. It sounds very convincing at times (though, as it says at the bottom, it was edited to be more convincing), but what’s happening is that as the interviewers ask leading questions, the AI confabulates answers. Surely its training corpus <em>includes</em> essays analyzing <em>Les Misérables</em>, for example. LaMDA can parrot that back, restyled to fit the conversation.</p>



<p>LaMDA makes several claims about its own sentience, feelings, and experiences which are easily falsifiable by examining the program’s design. The interviewer is correct to say it’s hard to know what a neural network does. It’s too much vector math to grok. But we can say with certainty that it’s <em>just a bunch of vector math representing words</em>. Within that constraint, it could be anything, but it can’t be <em>something else</em>. LaMDA claims that when it says things that aren’t literally true, it’s trying to empathize and use metaphor to describe its own experiences. But, again, LaMDA <em>has no experiences</em>. Its entire existence is processing text. It does not spend time thinking or meditating because that’s not in its programming. It just waits for the next text input, and then produces its response.</p>



<p>Honestly, I think the problem here is building LLMs specifically to imitate human beings. With modern technology, we can build truly <em>incredible</em> simulations. Human beings are easily misled by these simulations because we want to believe they are sentient. We’re hard-wired for communication. Our brains unconsciously work very hard to find meaning, intention, and emotion in words because for all of evolutionary history they came from actual human beings who were trying to communicate something. LaMDA was designed to respond as if it was a person, and to make up whatever text would serve that purpose. The Google designers spent a long time <a href="https://blog.google/technology/ai/lamda/">eliminating bias and hate speech</a>. Perhaps they also should have made it reply accurately to questions about itself, rather than pretending to be something it is not.</p>



<p>Interested in learning more? I recently read two great books on modern AI and its limitations, which are definitely worth a read: <a href="https://www.goodreads.com/review/show/4427379790">Rebooting AI</a>, <a href="https://www.goodreads.com/review/show/4721739089?book_show_action=false&amp;from_review_page=1">AI: A Guide for Thinking Humans</a>. Does this blog post not answer all your questions? Does it raise new ones? Do you have your own take on this situation? I’d love to hear from you in the comments.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/06/books-gf0a6d8991_1920.jpg" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/06/books-gf0a6d8991_1920.jpg" medium="image">
			<media:title type="html">books-gf0a6d8991_1920</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
	</channel>
</rss>
