<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>language &#8211; Thinking with Nate</title>
	<atom:link href="https://thinkingwithnate.wordpress.com/tag/language/feed/" rel="self" type="application/rss+xml" />
	<link>https://thinkingwithnate.wordpress.com</link>
	<description>Becoming an intelligence researcher</description>
	<lastBuildDate>Sat, 07 Dec 2024 15:51:05 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain='thinkingwithnate.wordpress.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>https://s0.wp.com/i/buttonw-com.png</url>
		<title>language &#8211; Thinking with Nate</title>
		<link>https://thinkingwithnate.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://thinkingwithnate.wordpress.com/osd.xml" title="Thinking with Nate" />
	<atom:link rel='hub' href='https://thinkingwithnate.wordpress.com/?pushpress=hub'/>
	<item>
		<title>How did AI get so much smarter?</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/</link>
					<comments>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sat, 07 Dec 2024 15:51:05 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<category><![CDATA[ai]]></category>
		<category><![CDATA[artificial-intelligence]]></category>
		<category><![CDATA[deep learning]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[large language models]]></category>
		<category><![CDATA[llm]]></category>
		<category><![CDATA[llms]]></category>
		<category><![CDATA[machine learning]]></category>
		<category><![CDATA[machine-learning]]></category>
		<category><![CDATA[meaning]]></category>
		<category><![CDATA[natural language processing]]></category>
		<category><![CDATA[nlp]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267</guid>

					<description><![CDATA[(this month&#8217;s photo is a picture of a brown bat. It&#8217;s small and fluffy with a stubby nose, and clinging to the gray bark of a tree. Photo by N. J. Stewart wildlife unmodified and used under the Creative Commons license) When I write about intelligence, I tend to downplay AI and Deep Learning. These &#8230; <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/" class="more-link">Continue reading<span class="screen-reader-text"> "How did AI get so much&#160;smarter?"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>(this month&#8217;s photo is a picture of a brown bat. It&#8217;s small and fluffy with a stubby nose, and clinging to the gray bark of a tree. Photo by <a href="https://www.flickr.com/photos/stuartwildlife/">N. J. Stewart wildlife</a> unmodified and used under the Creative Commons license)</p>



<p>When I write about intelligence, I tend to downplay AI and Deep Learning. These are powerful problem solving tools, but they’re over-hyped, and they don’t “think” the way people do. They have no memory, no sense of self, and no goals, at least in the usual sense of the words. But, large language models (LLMs) like OpenAI’s GPT are shockingly good at generating text that <em>seems</em> like something a person might make. They’re much more human-like than anything that came before. Why is that? The short answer is that they use a new kind of Deep Learning architecture known as a Transformer, which introduced a few small tricks that make a very big difference.</p>



<p>The first thing to note is that, while lots of people argue about whether LLMs can answer questions, reason, solve problems, brainstorm, or make art, what they really do is <em>text prediction</em>. They take some words as a starting point and then they guess what comes next based on their training data. If LLMs have any deeper cognitive abilities than that, they must be somehow tapping into the human cultural intelligence that is embedded within that text. Or, maybe they’re just parroting back fragments of intelligent things other people have said, without any understanding or integration—mindless idiots, randomly stringing words together in ways that sound just smart enough to distract us. We honestly don’t know yet! But whatever intelligence they possess, it exists entirely in the realm of language.</p>



<p>Research into getting computers to understand text and speech (known as Natural Language Processing, or NLP) started back in the 1950’s. Back then, computers were specialist’s tools, and making one that anyone could use just by telling it what to do was a dream. At first, researchers tried to formally describe language as we use it, feeding computers dictionaries, grammar rules, and lists of facts, but this never worked! It turns out, we don’t explicitly <em>know</em> all the rules of human language that we intuitively follow, and they’re usually <em>fuzzy</em> rules, with lots of conditions and exceptions. The key challenge of NLP was getting computers (which are obsessively logical and precise) to deal with this messiness and ambiguity, which we don’t even fully understand ourselves. Perhaps the most important advance was when researchers gave up trying to explain language to computers, and instead started teaching them by example.</p>



<p>Modern NLP represents words as lists of numbers called “vectors.” Like an (X, Y) coordinate, each vector represents a point in space. Not physical space, though, more like an abstract space of concepts. Maybe nouns go to the right, verbs to the left. Natural concepts are up, man-made concepts are down. Except, instead of two dimensions, maybe there are 10,000 of them. The layout of this space is pretty arbitrary. The absolute position of a word doesn’t mean anything, only where it is relative to other words. Nearby words have similar meanings, and relationships between words are represented by the distance and angle between them. This is all weirdly self-referential. Words are only defined in terms of other words! But it works surprisingly well. You don’t need explicit rules about which words go together and how, you can just look at <em>lots</em> of examples, and infer those relationships with statistics. People talk about “training” an AI by having it “read” lots of text, but really all that means is iteratively tweaking the lists of numbers, slowly moving the words through this abstract meaning space until they settle into positions that reflect how they co-occur together in the training text.</p>



<p>There’s one big problem with representing words as vectors, though: ambiguity. What do you do with a word like “bat,” which has several meanings? There’s no way one vector can represent this. The trick is to look for context. When you see a phrase like “brown bat” or “wooden bat,” the meaning is clear. Instead of thinking of these as pairs of words, you might think of them as <em>compound words</em>, each with their own distinct meaning. This is a powerful idea, but hard to generalize. Take a more difficult example: “Hearing a strange flutter and crash in the dark, he grabbed his bat for defense and went to investigate” Which kind of “bat” are we talking about? Words like “flutter” and “dark” might suggest the animal, but “grabbing” a bat for “defense” suggests the object instead. We need context to disambiguate, but which context? We’d like to ignore the first half of the sentence (which isn’t talking about the “bat”) and focus on the second half of the sentence (which is).</p>



<p>NLP has found elegant ways to solve this problem. They call these techniques “attention,” since the model is learning to “pay attention” to some words and not others, but I find that name misleading. For human beings, attention is something very different. We seem to have a “mind’s eye” that we can move about at will. We can choose to pay attention to this or that, our attention gets drawn to salient features, and we may even notice our attention drifting and redirect it. But these AIs have no mind’s eye, no will, and no intuition about relevance. The attention models we’re talking about are just <em>more vector math</em>. In addition to finding vectors to represent the meaning of each word individually, they also find vectors to represent <em>patterns</em> of words. They learn, “in this context, these words together mean that.” Adding an extra layer of complexity lets the model represent how words interact to change the meaning of other words or the sentence as a whole.</p>



<p>Researchers have explored many variations on this attention trick. Transformer models use an advanced kind of attention that represents context bi-directionally. They model how different words tend to get modified by context, and how different contexts tend to modify nearby words. The benefit of this is that such a model doesn’t just learn that “brown bat” is the name of an animal, but it might learn that “brown” is an adjective that applies to physical objects, that in English adjectives tend to modify the noun that follows them, and that “bat” can refer to one of several animal species, sometimes distinguished by color. That is, rather than modeling some <em>particular</em> context, models like this can learn general rules and relationships between different <em>kinds</em> of words. They can learn <em>grammar</em>. Not just the “official” grammar of a language like English, but <em>any</em> system of relationships and interactions between words, including dialects, domain-specific jargon, storytelling tropes, or the gender roles of a society.</p>



<p>The other trick that makes Transformers better with language is <em>pluralism</em>. Some NLP systems represent more complex meanings by using <em>bigger</em> vectors. More numbers in each vector means a larger conceptual space. Instead, Transformers use <em>more</em> vectors. They don’t learn <em>the one</em> meaning of this word, they learn to represent the <em>many</em> meanings of this word in the <em>many</em> contexts that contain it. This works a bit like voting. When processing a sentence, several different “attention heads” each consider one possible interpretation of a word, attending to different patterns of contextual cues. The overall meaning is determined by adding them all together. This is really useful for weighing subtle cues against each other to resolve ambiguity, but also to represent sentences with multiple layers of meaning. A word can have many meanings at the same time, and the many meanings of all the words in a sentence can interact in complex ways. The fancy kind of attention used in Transformers can automatically discover this sort of layered structure in language.</p>



<p>As clever as these attention methods are, they are <em>not</em> the secret to Transformers’ success. They do greatly improve the richness of NLP models, but at first they were mostly used with “recurrent neural networks,” a kind of Deep Learning model that processes data sequentially. That’s probably because they work a bit like how we imagine a human reader does: they “read” each word in a text, one at a time, using attention to figure out how each new word should update the meaning of the text so far. This works pretty well, but it doesn’t scale up to long passages of text. These models have a limited attention span, eventually forgetting important details they read several sentences ago. Also, processing long texts one word at a time is painfully slow. Even on the world’s fastest computer, reading a book from beginning to end takes time <em>per page</em>, and training a model like this takes <em>vast</em> amounts of text, so this was a major limitation.</p>



<p>The paper that first introduced Transformers was called <em>Attention is All You Need</em>, which highlights the key innovation: they got rid of the recurrent network, and built an AI using just this attention mechanism, all on its own. In other words, they found a way to do the same vector math, but solving for a large block of text all at once (and possibly out of order) rather than word-by-word. This doesn’t make the model “smarter.” It doesn’t even reduce the overall amount of number crunching. It just makes the work more <em>parallelizable</em>. Instead of having one computer read <em>War and Peace</em> from cover to cover, they could have <em>many</em> computers each read a few paragraphs, then combine their results. This made it possible to throw more money at the problem, using whole <em>datacenters</em> of computers to train a language model on <em>vastly</em> more text than ever before. Billions of documents, trillions of words. It’s the <em>sheer volume</em> of training data that made LLMs so much better. That’s why they’re called “large” language models.</p>



<p>So, how should we think about LLMs like GPT? Well, first off, human language is irregular and complex, but it’s also highly structured. Cleverly designed statistical learning tools can automatically discover that hidden structure just by processing <em>obscene</em> amounts of text. Neural networks are great for letting computers work with these sorts of fuzzy rules. They can extract meaning from text, manipulate it, and generate new text. But to an LLM, words are just vectors, defined by their relationships to each other. They have no connection to physical reality, because LLMs have no physical existence. There is no communication going on when you have a “conversation” with an LLM. To the AI, a dialog is just a sequence of vectors that follow one another according to some grammar. The AI has no mind, no intentions, and no meaning it wishes to convey. It has no conception of being truthful or helpful, only what words tend to follow certain questions. It does not learn from a conversation, it just re-reads the full chat history each time it makes a response. It <em>appears</em> like a good conversational partner, because it is made to imitate one, but what&#8217;s happening behind the screen isn’t “thinking” as we know it.</p>



<p>Still, LLMs really are much more human-like than any other AI that came before. Representing language with a high-dimensional abstract concept space works surprisingly well, and so do the “attention” methods described above. They let us represent a huge, open-ended space of ideas that can build on and interact with each other. They let us represent ambiguity, nuance, and innuendo. So, maybe those vector math tricks could actually teach us something about how language processing works in the brain? On the other hand, LLMs are also remarkable in how <em>different</em> they are from humans. An LLM can learn English, but only by reading every document on the internet, not one word at a time, but <em>all at once</em>. In contrast, babies learn language by interacting with the world, learning how words relate to objects, people, events, actions, and desires. Even though they’re exposed to far less language, they learn much faster, and in a way that tightly integrates all of their senses, relationships, and the lifestyle they were born into. Since LLMs seem so human-like, it’s very tempting to imagine them with the same kind of awareness, purpose, and empathy that we have, but they simply aren’t there. Those are a product of being alive in the world, and can’t be found in text, no matter how much of it.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/feed/</wfw:commentRss>
			<slash:comments>15</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2024/12/image.png" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2024/12/image.png" medium="image">
			<media:title type="html">image</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>Queerness</title>
		<link>https://thinkingwithnate.wordpress.com/2024/06/01/queerness/</link>
					<comments>https://thinkingwithnate.wordpress.com/2024/06/01/queerness/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sat, 01 Jun 2024 13:03:14 +0000</pubDate>
				<category><![CDATA[Armchair Philosophy]]></category>
		<category><![CDATA[Theory]]></category>
		<category><![CDATA[categories]]></category>
		<category><![CDATA[gender]]></category>
		<category><![CDATA[identity]]></category>
		<category><![CDATA[labels]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[ontology]]></category>
		<category><![CDATA[philosophy]]></category>
		<category><![CDATA[queer]]></category>
		<category><![CDATA[race]]></category>
		<category><![CDATA[sexuality]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=237</guid>

					<description><![CDATA[It&#8217;s LGBTQ+ Pride month! I identify as “queer,” so I thought this would be a good opportunity to write a bit about what that means to me. In addition to queer, I also identify as a cisgender male, pansexual, and demisexual. This means I’ve always identified as male, and others always assumed as much. I’m &#8230; <a href="https://thinkingwithnate.wordpress.com/2024/06/01/queerness/" class="more-link">Continue reading<span class="screen-reader-text"> "Queerness"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>It&#8217;s LGBTQ+ Pride month! I identify as “queer,” so I thought this would be a good opportunity to write a bit about what that means to me.</p>



<p>In addition to queer, I also identify as a cisgender male, pansexual, and demisexual. This means I’ve always identified as male, and others always assumed as much. I’m rarely interested in sex or romance, but when I am, it’s not about gender. I love people, not parts. The full story is more complicated, but that’s a good start.</p>



<p>Labels like “gay”, “bi”, “pan”, “cis”, “demi”, “aro”, and “ace” are useful for quickly describing myself to others, but I prefer the term “queer” because, honestly, I don’t think any set of labels does a person justice.</p>



<p>Gender and sexuality are fundamentally personal things. Each individual is unique. No set of labels can capture all of who I am, and every label carries some baggage that I don’t want applied to me. Labels are useful, just so long as we remember that they’re always at least a little bit wrong. They cannot serve as a stand-in for a person.</p>



<p>I also love queer philosophy, and try to embrace it in all my thinking. Put simply, that means I don’t believe in categories. I don’t think they have any real existence, or essential qualities. They’re convenient fictions. Just labels we make up to point at collections of disparate things. This applies to all categories, but <em>especially</em> to living things, where there are exceptions to every rule, and no hard boundaries whatsoever.</p>



<p>The problem with categories is that we take them seriously. Once we categorize something, we think we understand it, when really we’re just projecting a stereotype. We make strong assumptions about what’s allowed in a category, and we struggle with exceptions, even common ones. As our understanding of the world changes, things often shift faster than our language can keep up with. Sometimes we don’t notice. We keep trying to sort the world into categories that make no sense, and get upset when reality doesn’t play along.</p>



<p>So, I don’t believe that Jews exist. I believe that Jewish <em>people</em> exist, and that we use the word “Jews” to refer to them. Yet, there’s no one quality that all of those people share, except that they are people (another category, subject to change). You and I may not even agree about which set of people the word “Jews” applies to, so how is it meaningful for us to talk about Jews in general?</p>



<p>I don’t think it’s strange to see a Black woman engineer, even though it’s rare. I wouldn’t expect her to be any less competent, just because most folks like her can’t do the job. If anything, I’d assume the opposite, if she can succeed in that role despite the weight of her labels. But, ultimately, it’s about what she has to offer the world, which is surely more and less than the other engineers around her. She has her unique way of doing it, perhaps different in exciting ways.</p>



<p>That’s what queer means to me. Labels can be useful, but they have no power over reality. Reality and people are so much more than words can contain. See them for what they are.</p>



<p>If you’d like to learn more about queerness and queer philosophy, I highly recommend <em><a href="https://www.iconbooks.com/ib-title/queer-a-graphic-history/">Queer: A Graphic History</a></em> by Meg-John Barker and Jules Scheele.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2024/06/01/queerness/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>What ChatGPT Can Teach Us About Being Human</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/</link>
					<comments>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 05 Apr 2023 16:37:09 +0000</pubDate>
				<category><![CDATA[Introspection]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[llm]]></category>
		<category><![CDATA[machine_learning]]></category>
		<category><![CDATA[meaning]]></category>
		<category><![CDATA[minds]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159</guid>

					<description><![CDATA[(This post&#8217;s featured image is not a photo of the idyllic California vineyard my wife and I visited in 2015, but a similar looking AI-generated fiction) The excitement around large language models (LLMs) continues. Often just called “AI,” this new technology takes instructions in plain English, and generates new text or images so good you’d &#8230; <a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/" class="more-link">Continue reading<span class="screen-reader-text"> "What ChatGPT Can Teach Us About Being&#160;Human"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>(This post&#8217;s featured image is <em>not</em> a photo of the idyllic California vineyard my wife and I visited in 2015, but a similar looking AI-generated fiction)</p>



<p>The excitement around large language models (LLMs) continues. Often just called “AI,” this new technology takes instructions in plain English, and generates new text or images so good you’d think a human made them. There are some serious concerns about the ethics of how this is done (see <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">the Dangers of Stochastic Parrots</a>), and many articles warning people that LLMs aren’t as human as they seem. Still, these LLMs are clearly doing <em>something</em> smart, and it’s weirdly compelling. What’s going on here? What can LLMs teach us about the human mind, our strengths and weaknesses, and why we&#8217;re so easily fooled and mesmerized by this tech?</p>



<p>There are lots of articles about how LLMs work and their limitations (I even wrote <a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/">one</a> a while back), so I won’t go into much detail here. What matters for now is that LLMs are <em>prediction engines</em>. Given some text, they try to guess how a human would respond, based on a statistical analysis of all the content on the internet. They do this extremely well, but not perfectly. LLMs don’t think, perceive, or interact with the world in a human-like way, so sometimes they make weird mistakes a human never would.</p>



<p>In some ways, the human brain works a lot like an LLM. From day one, the brain is looking for language. It automatically builds up vast networks of words, concepts, and their relationships. When I hear someone talking, my mind is suddenly filled with the speaker’s ideas and their associations, building up an image in my mind’s eye. This is basically what an LLM does.</p>



<p>However, I also have many other intelligent faculties that join in. I don’t just know that “dog” is a word related to “cat.” I have memories of specific dogs, what their fur felt like, and the experiences we shared. I have common sense, logic, and theory of mind to decide whether something I hear is truth, fantasy, error, or deception. I monitor my own thoughts to gauge my level of confidence and correct my mistakes. I anticipate the future, make plans, and use language to achieve my goals. LLMs don’t do any of that.</p>



<p>Of course, there are AI researchers working to approximate these other kinds of human intelligence (though progress here is limited compared with LLMs themselves). <a href="https://www.goodreads.com/book/show/43999120-rebooting-ai?ac=1&amp;from_search=true&amp;qid=fL2RVBG7dY&amp;rank=3">Rebooting AI</a> is a great book exploring that work, which argues its just a matter of time before AI can do everything a human can. Personally, I&#8217;m skeptical that we&#8217;ll ever reverse engineer all the subtlety of human thought, but I think it&#8217;s safe to say that AIs will become more powerful and well-rounded in the future. Perhaps the more important question is whether building increasingly realistic human simulations is a good idea at all.</p>



<p>For now, LLMs are a bit of a one-trick pony. What’s scary is that one trick is often good enough to fool humans and do useful work. In particular, even though LLMs were designed to be text prediction engines, they are surprisingly good at <a href="https://arxiv.org/abs/2303.12712">general problem solving</a>. They can paint pictures, do math, solve brain teasers, and even write and simulate computer programs. Maybe those “extra” faculties of the human mind aren’t so important after all?</p>



<p>LLMs “think” in terms of words and relationships and patterns they’ve seen before. In human terms, that means stereotypes, cliches, generalizing from past “experience,” and repeating what they are told. We like to think that human thought is more sophisticated than that, but it often isn’t. We sometimes don&#8217;t see people as individuals, but in terms of the role they play in society (ie, &#8220;barista&#8221; or &#8220;mom&#8221;). We make decisions based on rules of thumb or gut feeling, without the need for logic and reasoning. We talk about things we don’t fully comprehend. We repeat talking points in order to fit in with our tribe. We confidently make up nonsense just to satisfy each other and move on. It’s surprising how LLM-like humans can be sometimes.</p>



<p>And that’s not meant to be derogatory! Those ways of thinking can be very effective. A lot of language <em>isn’t</em> about complex ideas, comprehension, and reasoning, but just putting one word after another to evoke an image in someone else’s mind. Past experience often <em>is</em> a highly effective and low-effort way to predict the future. Lying is anti-social, but “fake it ‘til you make it” <em>works</em>. One of the fastest ways to pick up a new skill is to boldly make mistakes, get feedback, and learn from that experience. The main difference is that today&#8217;s LLMs don&#8217;t learn from their mistakes, they never doubt their &#8220;intuition,&#8221; and they have no alternative ways of &#8220;thinking&#8221; when these techniques fall short.</p>



<p>So, yeah, LLMs only do part of what humans do, but it’s a big and important part. Occasionally we do need facts, critical thinking, self-doubt, and all the rest to do the right thing, but they don’t come up as often as we like to think. The real danger of LLMs, then, is that 80% of the time they might be good enough, but 20% of the time we need fancy human judgment to notice they screwed up and decide what to do. This is a serious problem. Humans are bad at vigilance, and we have a strong instinct to trust language, which in this case is exactly the <em>wrong</em> response.</p>



<p>Language is a defining feature of our species. We aren’t just <em>capable</em> of language, it’s a human universal. Every culture has language. Babies attend to speech from the moment they’re born, and start to babble in a few months. When there isn’t a common language spoken around them, children raised together will spontaneously <em>invent</em> one. Language is a biological imperative for us. It’s in our DNA.</p>



<p>When we perceive language, our minds automatically assume that it’s <em>communication</em>. We imagine another mind behind the words, usually with good intent and a desire to cooperate. Up until recently, this was a pretty safe assumption, so it was totally reasonable for the brain to immediately and automatically translate language into meaning. But now this instinct is backfiring. LLMs create realistic text and imagery <em>without any intentional meaning</em>. They don&#8217;t produce &#8220;answers,&#8221; &#8220;opinions,&#8221; or &#8220;art,&#8221; just random content that <em>looks</em> like those things. It&#8217;s both very difficult and important to remember that.</p>



<p>We’re still working to understand how these LLMs work, what their limitations are, and what they’re good for. As we do that, I hope we’ll come to understand ourselves better, too. What do you think? Have LLMs made you think about minds any differently? Have you seen any interesting examples of AIs acting strange or foolish? What about people acting like LLMs? Any thoughts or fears about computers gradually inching toward human-like abilities? I’d love to hear from you in the comments.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/feed/</wfw:commentRss>
			<slash:comments>7</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/04/vineyard.jpeg" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/04/vineyard.jpeg" medium="image">
			<media:title type="html">vineyard</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>Large Language Models, LaMDA, and Sentience</title>
		<link>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/</link>
					<comments>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 15 Jun 2022 17:24:46 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<category><![CDATA[deep_fakes]]></category>
		<category><![CDATA[ethics]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[llm]]></category>
		<category><![CDATA[machine_learning]]></category>
		<category><![CDATA[sentience]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=99</guid>

					<description><![CDATA[Several folks asked me to weigh in on whether Google’s AI chatbot, LaMDA, is sentient. I don’t know much about LaMDA specifically, so I want to talk about Large Language Models (LLMs) generally, since they show up in many forms. It’s a truly amazing technology. They can generate text that’s superficially indistinguishable from human writing. &#8230; <a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/" class="more-link">Continue reading<span class="screen-reader-text"> "Large Language Models, LaMDA, and&#160;Sentience"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Several folks asked me to weigh in on whether Google’s AI chatbot, LaMDA, is sentient. I don’t know much about LaMDA specifically, so I want to talk about Large Language Models (LLMs) generally, since they show up in many forms. It’s a truly amazing technology. They can generate text that’s superficially indistinguishable from human writing. But are these systems capable of sentience? Let’s dig into it.</p>



<p>Let’s start with what an LLM actually does. At the core, it analyzes text for statistical correlations. This word co-occurs with that word. When you see this, it’s often followed by that. These words appear in similar contexts, and may be interchangeable. That sort of thing. What makes LLMs “large” is that they get trained on <em>enormous</em> bodies of text. Like, billions of web documents or whole libraries worth of books. This allows them to learn very subtle and nuanced patterns, and collect example texts on many themes. When an LLM is put into use, what it’s doing is confabulating new sequences of words with the same regular structure as its training data. They mix prompting from the user with relevant passages in their training data and randomness.</p>



<p>LLMs take advantage of a couple recent innovations in AI. One is transfer learning. First, the LLM is trained on an enormous corpus to learn the structure of language generally. Then, it gets fine tuned on a narrow data set, to constrain its output to fit a specific style and context. This isn’t so different from <a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee">style transfer</a> in computer vision. The other trick is <a href="https://wiki.pathmind.com/attention-mechanism-memory-network">attention and memory</a>. LLMs can spot correlations between words at short, medium, and long distances, and learn which of the associations it learns are most relevant to good output. This makes LLMs much more self-consistent and better at question-answering tasks than previous technology.</p>



<p>A large part of why LLMs are so effective has to do with language itself. Language is highly self-referential. Words are defined in terms of other words. The meaning and sentiment of a word comes primarily from the context where it’s used (when I learned about <a href="https://towardsdatascience.com/word2vec-explained-49c52b4ccb71">word2vec</a>, I came to appreciate this much more deeply). We each have vast networks of words and images and memories all tied together, and it&#8217;s the shape of that network that creates meaning. Humans are able to communicate with language for two reasons. First, folks who speak the same language have <em>consistent</em> networks of words in their minds. They’re highly correlated with each other, so the words mean the same things to both people. Second, those networks of words are consistent with the shape of our thoughts and our lived experience of reality. That allows us to appreciate the purpose and consequences of the words we hear.</p>



<p>LLMs are specifically designed to learn that network of meaning, and build a model that is consistent with the one in your head. So, in a sense, they really do “understand” language. They know many of the same concepts and relationships that you do. They can regurgitate definitions and even answer questions by generating new sequences of words that follow the patterns. However, an LLM has no access to the physical world, so this network of ideas is not grounded in reality.</p>



<p>The question of AI sentience is tough, since we don’t have a good definition of sentience. Some scientists speculate that even raw information or matter might be conscious in some minimal way. But when we say “sentience” we tend to think about things like self-awareness, understanding, feelings, and intentions. Our brains produce the nuanced kind of sentience that makes us human through their very particular complex structures. So, even if a rock or an LLM is “conscious” in the minimal sense, they’re definitely <em>not</em> sentient, at least not at all like a person is. I have two reasons for saying that.</p>



<p>Firstly, people have bodies and minds that <em>produce</em> feelings, emotions, self monitoring, our train of thought, etc. We have hormones, neurotransmitters, and brain regions dedicated to those purposes. LLMs are much, much simpler in design. They were not built to have those abilities, so they don’t have them. Some worry that sentience might “evolve” or “emerge” without us explicitly building it in. Perhaps that could happen some day, but I think it’s safe to say the way we build LLMs today makes that impossible. Literally all they do is shuffle vectors representing words. Unlike life, they don’t shape their own design in any way, so they will never learn to do something other than what they were built for.</p>



<p>Second, people have a sense of self because there is a clear self / other distinction. We can see and feel our bodies, look out at the world, etc. The only thing an LLM “experiences” is training data. Text, and lots of it. They literally do not have the capacity to perceive anything else, because of how they’re built. They can’t see their data, programming, or the computer environment they are in, because we don’t give them that access. Some LLMs are also trained with visual imagery, but remember, what they “experience” is just <em>pixels</em>, not objects in the real world. That’s why they can be easily fooled by <a href="https://hackernoon.com/adversarial-attacks-how-to-trick-computer-vision-7484c4e85dc0">adversarial examples</a>.</p>



<p>What about the LaMDA chatbot specifically? The <a href="https://www.documentcloud.org/documents/22058315-is-lamda-sentient-an-interview">transcript</a> making all the headlines is worth checking out. It sounds very convincing at times (though, as it says at the bottom, it was edited to be more convincing), but what’s happening is that as the interviewers ask leading questions, the AI confabulates answers. Surely its training corpus <em>includes</em> essays analyzing <em>Les Misérables</em>, for example. LaMDA can parrot that back, restyled to fit the conversation.</p>



<p>LaMDA makes several claims about its own sentience, feelings, and experiences which are easily falsifiable by examining the program’s design. The interviewer is correct to say it’s hard to know what a neural network does. It’s too much vector math to grok. But we can say with certainty that it’s <em>just a bunch of vector math representing words</em>. Within that constraint, it could be anything, but it can’t be <em>something else</em>. LaMDA claims that when it says things that aren’t literally true, it’s trying to empathize and use metaphor to describe its own experiences. But, again, LaMDA <em>has no experiences</em>. Its entire existence is processing text. It does not spend time thinking or meditating because that’s not in its programming. It just waits for the next text input, and then produces its response.</p>



<p>Honestly, I think the problem here is building LLMs specifically to imitate human beings. With modern technology, we can build truly <em>incredible</em> simulations. Human beings are easily misled by these simulations because we want to believe they are sentient. We’re hard-wired for communication. Our brains unconsciously work very hard to find meaning, intention, and emotion in words because for all of evolutionary history they came from actual human beings who were trying to communicate something. LaMDA was designed to respond as if it was a person, and to make up whatever text would serve that purpose. The Google designers spent a long time <a href="https://blog.google/technology/ai/lamda/">eliminating bias and hate speech</a>. Perhaps they also should have made it reply accurately to questions about itself, rather than pretending to be something it is not.</p>



<p>Interested in learning more? I recently read two great books on modern AI and its limitations, which are definitely worth a read: <a href="https://www.goodreads.com/review/show/4427379790">Rebooting AI</a>, <a href="https://www.goodreads.com/review/show/4721739089?book_show_action=false&amp;from_review_page=1">AI: A Guide for Thinking Humans</a>. Does this blog post not answer all your questions? Does it raise new ones? Do you have your own take on this situation? I’d love to hear from you in the comments.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/06/books-gf0a6d8991_1920.jpg" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/06/books-gf0a6d8991_1920.jpg" medium="image">
			<media:title type="html">books-gf0a6d8991_1920</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
	</channel>
</rss>
