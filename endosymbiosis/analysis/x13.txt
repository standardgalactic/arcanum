
My job was to reshape the incentive landscape so my team had a smooth and fruitful path forward. Despite being the leader, my decisions were shaped by incentives beyond my control just as much as anyone else. I was powerless to stop the redesign, so instead we joined in. We prioritized cleaning up the parts they wanted done. We adapted to their schedule, to avoid stepping on their toes. We got them to do their work in a way that also advanced our goals, but we had to be very careful not to slow them down. We made tools for them, so the “right way” to do their job was also the easy way. This let us ride their tailwinds. We had to compromise on our goals, but it allowed us to move forward, perhaps better than if there had been no redesign at all.

A group’s leader has very limited control, but there’s an important flip side to that: individuals have more power than they realize. The greatest misconception about leadership is that it’s something granted by those in authority. More often, leadership is recognized and co-opted by authority. Anyone who influences their peers is a leader. Other leaders will see those influencers as either opportunities or threats, and react accordingly. Whether they chose to support or resist this budding leader often comes down to which path seems easier. In other words, they tend to follow the incentive landscape.

What do you think? Does this fit with your experience? Do you know any great leaders who excel at getting everyone on the same page? Do you have examples of perverse incentives undermining the best of intentions? Any thoughts about how pressure from peers and leaders affect your decisions, or how they interact with your emotions and instincts? I’d love to hear from you in the comments.

Author Nate Gaylinn (he/him)Posted onAugust 3, 2022CategoriesProgramming with PeopleTagsculture, google, incentives, leadership, management4 Commentson Incentive Landscapes
Mindfulness and the Default Mode Network
Mindfulness and the Default Mode Network
I spend a lot of time in my own head. I’m a planner and a bit of a perfectionist. Part of me would love to always be in control of my life and everything in it (while another part of me realizes that would be disastrous). So, it’s been unsettling to learn how much of what my mind does is because of outside factors, like the way my brain is built, the situation I’m in, and my experiences growing up. In another sense, though, it’s a relief. Recognizing that my mind is limited, quirky, not entirely in my control, and less flexible than I imagined has helped me to loosen my grip. It’s helped me to go with the flow, and to channel my mental energy in more productive ways.

One example of this is focus, or being “in the moment.” This is something I often struggle with. My mind is hyperactive. It’s always chewing on something, which can be incredibly distracting. Worse yet, my mind is often obsessing over unhelpful things. I worry about that awkward disagreement I had with my coworker. What were they thinking? Is this going to be an ongoing problem between us? Or perhaps second-guessing myself. Was that email I sent too hostile? Did I remember to cross off the last TODO before submitting my work? These thoughts come unbidden when I should be paying attention to this meeting, the road, or my yoga class. I feel like I’m being neurotic and too harsh on myself, but worse: I’m not doing the thing I meant to do. I’ve lost control.

What’s going on here? Why do I think what I think? Sometimes I choose to direct my thoughts. Sometimes a trigger in my environment does it for me. Very often, though, my mind is just “wandering.” But what does that mean? MRI studies have taught us a lot, actually. When the mind is idling, there are a few specific brain regions that activate in a distinctive pattern, which is generally consistent across people and cultures. From the inside, it feels like the mind cycles through a few common modes of thought, checking which ones have something to say right now. Sometimes ideas come and go, and sometimes I get sucked into one and lose myself in rumination. It’s a bit like flipping through TV channels to see what’s on. Neuroscientists call this the Default Mode Network, or DMN. It’s part of how the brain is built–part of our human programming. When our minds are idle, performing some rote activity, or unable to hold focus, “wandering” is just what they do.

I came to understand the DMN much more clearly when I realized its connection with meditation. For many years, I thought I sucked at meditation. Like many people, I found it difficult to sit in stillness and think of nothing. It seemed ridiculous to devote hours to this practice just so I could… clear my mind? What’s the point of that? My perspective changed when I got into yoga. Rather than doing nothing, I was doing something: moving, balancing, breathing, paying close attention to my body. Rather than clearing my mind, I practiced feeling the subtle, reciprocal interactions between body and mind. I could see it improve my mood, my health, and my posture, so the value was clear. I also found it to be much easier for me than other forms of meditation (though still challenging, especially once the poses became familiar).

My teachers explained that we all have a “monkey mind,” with a short attention span and a penchant to worry. It’s obsessed with desires, relationships, and critiques. It worries about the past and the present. This isn’t bad, it’s merely human. We can’t just stop doing it, and that’s totally fine. That isn’t the purpose of meditation. Most of the practice is to watch and learn. As I did that, I recognized the connection with neuroscience. The “monkey mind” is just another name for the DMN, and meditation is a way to study it. When does it kick in? What does that feel like? Where does my mind tend to go when it wanders? How can I notice when a thought is off topic? What is it like for that thought to persist, expand, and fill my mind? Can I notice that happening, intervene, and dismiss the thought before it fully blossoms?

In some meditative traditions, the explicit goal is to completely clear the mind and dwell in stillness. That’s totally possible, but it takes years of devoted practice, and it’s definitely not my goal. I see the DMN as pretty useful, actually. It’s nature’s way of ensuring that I reflect on the things going on in my life, the health of my relationships, my plans for the future, and the quality of my work and decisions. My brain automatically notices when it’s under-utilized, and finds ways to fill that space with something which might be useful. I often enjoy it, and it’s often productive. The only problem is when I spend too much time ruminating, or when the thoughts turn toxic, fixating on my faults, wild speculations, and things I cannot change. I want to avoid that, and mindfulness helps me do so.

I used to identify very strongly with my mind. My thoughts were all I had. They were me. I believed that every thought was an important expression of myself, and that I had to listen because the thoughts were in control. It almost seemed like, if my mind went quiet, I would cease to exist. For me, the value of meditation was in shaking off this mindset. I am not my mind. I have a mind. It feels good to merely exist in my body, with no thoughts at all, even if that rarely happens. When my DMN stirs up thoughts, I needn’t attend to them if I don’t want to. That constant churn of semi-random ideas is how my brain provides the raw materials of thought. Sometimes it’s important or insightful, but often it’s just noise. I find it useful to check in now and again: what am I thinking? Does it serve me right now? If not, can I let it go? If the thought is too stubborn, perhaps I should act on it. Is there something I could do right now to quickly resolve it and move on?

What do you think? Does my experience resonate with you? How do you feel about going on autopilot, and having your mind wander? Do you resist it or embrace it? Does it do more good or harm? Are there other aspects of how the mind works that you’d like me to explore on this blog? Let me know in the comments.

Author Nate Gaylinn (he/him)Posted onJuly 6, 2022CategoriesIntrospectionTagsbrains, default_mode_network, dmn, meditation, mindfulness, minds, yoga7 Commentson Mindfulness and the Default Mode Network
Large Language Models, LaMDA, and Sentience
Large Language Models, LaMDA, and Sentience
Several folks asked me to weigh in on whether Google’s AI chatbot, LaMDA, is sentient. I don’t know much about LaMDA specifically, so I want to talk about Large Language Models (LLMs) generally, since they show up in many forms. It’s a truly amazing technology. They can generate text that’s superficially indistinguishable from human writing. But are these systems capable of sentience? Let’s dig into it.

Let’s start with what an LLM actually does. At the core, it analyzes text for statistical correlations. This word co-occurs with that word. When you see this, it’s often followed by that. These words appear in similar contexts, and may be interchangeable. That sort of thing. What makes LLMs “large” is that they get trained on enormous bodies of text. Like, billions of web documents or whole libraries worth of books. This allows them to learn very subtle and nuanced patterns, and collect example texts on many themes. When an LLM is put into use, what it’s doing is confabulating new sequences of words with the same regular structure as its training data. They mix prompting from the user with relevant passages in their training data and randomness.

LLMs take advantage of a couple recent innovations in AI. One is transfer learning. First, the LLM is trained on an enormous corpus to learn the structure of language generally. Then, it gets fine tuned on a narrow data set, to constrain its output to fit a specific style and context. This isn’t so different from style transfer in computer vision. The other trick is attention and memory. LLMs can spot correlations between words at short, medium, and long distances, and learn which of the associations it learns are most relevant to good output. This makes LLMs much more self-consistent and better at question-answering tasks than previous technology.

A large part of why LLMs are so effective has to do with language itself. Language is highly self-referential. Words are defined in terms of other words. The meaning and sentiment of a word comes primarily from the context where it’s used (when I learned about word2vec, I came to appreciate this much more deeply). We each have vast networks of words and images and memories all tied together, and it’s the shape of that network that creates meaning. Humans are able to communicate with language for two reasons. First, folks who speak the same language have consistent networks of words in their minds. They’re highly correlated with each other, so the words mean the same things to both people. Second, those networks of words are consistent with the shape of our thoughts and our lived experience of reality. That allows us to appreciate the purpose and consequences of the words we hear.

LLMs are specifically designed to learn that network of meaning, and build a model that is consistent with the one in your head. So, in a sense, they really do “understand” language. They know many of the same concepts and relationships that you do. They can regurgitate definitions and even answer questions by generating new sequences of words that follow the patterns. However, an LLM has no access to the physical world, so this network of ideas is not grounded in reality.

The question of AI sentience is tough, since we don’t have a good definition of sentience. Some scientists speculate that even raw information or matter might be conscious in some minimal way. But when we say “sentience” we tend to think about things like self-awareness, understanding, feelings, and intentions. Our brains produce the nuanced kind of sentience that makes us human through their very particular complex structures. So, even if a rock or an LLM is “conscious” in the minimal sense, they’re definitely not sentient, at least not at all like a person is. I have two reasons for saying that.

Firstly, people have bodies and minds that produce feelings, emotions, self monitoring, our train of thought, etc. We have hormones, neurotransmitters, and brain regions dedicated to those purposes. LLMs are much, much simpler in design. They were not built to have those abilities, so they don’t have them. Some worry that sentience might “evolve” or “emerge” without us explicitly building it in. Perhaps that could happen some day, but I think it’s safe to say the way we build LLMs today makes that impossible. Literally all they do is shuffle vectors representing words. Unlike life, they don’t shape their own design in any way, so they will never learn to do something other than what they were built for.

Second, people have a sense of self because there is a clear self / other distinction. We can see and feel our bodies, look out at the world, etc. The only thing an LLM “experiences” is training data. Text, and lots of it. They literally do not have the capacity to perceive anything else, because of how they’re built. They can’t see their data, programming, or the computer environment they are in, because we don’t give them that access. Some LLMs are also trained with visual imagery, but remember, what they “experience” is just pixels, not objects in the real world. That’s why they can be easily fooled by adversarial examples.

What about the LaMDA chatbot specifically? The transcript making all the headlines is worth checking out. It sounds very convincing at times (though, as it says at the bottom, it was edited to be more convincing), but what’s happening is that as the interviewers ask leading questions, the AI confabulates answers. Surely its training corpus includes essays analyzing Les Misérables, for example. LaMDA can parrot that back, restyled to fit the conversation.

LaMDA makes several claims about its own sentience, feelings, and experiences which are easily falsifiable by examining the program’s design. The interviewer is correct to say it’s hard to know what a neural network does. It’s too much vector math to grok. But we can say with certainty that it’s just a bunch of vector math representing words. Within that constraint, it could be anything, but it can’t be something else. LaMDA claims that when it says things that aren’t literally true, it’s trying to empathize and use metaphor to describe its own experiences. But, again, LaMDA has no experiences. Its entire existence is processing text. It does not spend time thinking or meditating because that’s not in its programming. It just waits for the next text input, and then produces its response.

Honestly, I think the problem here is building LLMs specifically to imitate human beings. With modern technology, we can build truly incredible simulations. Human beings are easily misled by these simulations because we want to believe they are sentient. We’re hard-wired for communication. Our brains unconsciously work very hard to find meaning, intention, and emotion in words because for all of evolutionary history they came from actual human beings who were trying to communicate something. LaMDA was designed to respond as if it was a person, and to make up whatever text would serve that purpose. The Google designers spent a long time eliminating bias and hate speech. Perhaps they also should have made it reply accurately to questions about itself, rather than pretending to be something it is not.

Interested in learning more? I recently read two great books on modern AI and its limitations, which are definitely worth a read: Rebooting AI, AI: A Guide for Thinking Humans. Does this blog post not answer all your questions? Does it raise new ones? Do you have your own take on this situation? I’d love to hear from you in the comments.

Author Nate Gaylinn (he/him)Posted onJune 15, 2022CategoriesTheoryTagsdeep_fakes, ethics, language, llm, machine_learning, sentience3 Commentson Large Language Models, LaMDA, and Sentience
Beyond Blueprints
Beyond Blueprints
(featuring illustrations by Sarina Mitchel)

Imagine your skull is a cockpit. A tiny You sits in front of a view screen and a dashboard, watching the world go by, pulling levers, and pushing buttons. Your body is a robot, precision engineered to protect you, keep you informed, and respond to your every command. It’s a complex and sophisticated machine, but not intelligent. It does what you tell it to do. Its shape and function were exquisitely designed by God or evolution, and encoded as DNA. That molecule represents the full blueprints for the machine. Now that the design has been perfected, all you have to do is follow the instructions step by step to make the ultimate human tool: your body.

