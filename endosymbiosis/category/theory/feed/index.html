<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>Theory &#8211; Thinking with Nate</title>
	<atom:link href="https://thinkingwithnate.wordpress.com/category/theory/feed/" rel="self" type="application/rss+xml" />
	<link>https://thinkingwithnate.wordpress.com</link>
	<description>Becoming an intelligence researcher</description>
	<lastBuildDate>Sat, 07 Dec 2024 15:51:05 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain='thinkingwithnate.wordpress.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>https://s0.wp.com/i/buttonw-com.png</url>
		<title>Theory &#8211; Thinking with Nate</title>
		<link>https://thinkingwithnate.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://thinkingwithnate.wordpress.com/osd.xml" title="Thinking with Nate" />
	<atom:link rel='hub' href='https://thinkingwithnate.wordpress.com/?pushpress=hub'/>
	<item>
		<title>How did AI get so much smarter?</title>
		<link>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/</link>
					<comments>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sat, 07 Dec 2024 15:51:05 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<category><![CDATA[ai]]></category>
		<category><![CDATA[artificial-intelligence]]></category>
		<category><![CDATA[deep learning]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[large language models]]></category>
		<category><![CDATA[llm]]></category>
		<category><![CDATA[llms]]></category>
		<category><![CDATA[machine learning]]></category>
		<category><![CDATA[machine-learning]]></category>
		<category><![CDATA[meaning]]></category>
		<category><![CDATA[natural language processing]]></category>
		<category><![CDATA[nlp]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=267</guid>

					<description><![CDATA[(this month&#8217;s photo is a picture of a brown bat. It&#8217;s small and fluffy with a stubby nose, and clinging to the gray bark of a tree. Photo by N. J. Stewart wildlife unmodified and used under the Creative Commons license) When I write about intelligence, I tend to downplay AI and Deep Learning. These &#8230; <a href="https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/" class="more-link">Continue reading<span class="screen-reader-text"> "How did AI get so much&#160;smarter?"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>(this month&#8217;s photo is a picture of a brown bat. It&#8217;s small and fluffy with a stubby nose, and clinging to the gray bark of a tree. Photo by <a href="https://www.flickr.com/photos/stuartwildlife/">N. J. Stewart wildlife</a> unmodified and used under the Creative Commons license)</p>



<p>When I write about intelligence, I tend to downplay AI and Deep Learning. These are powerful problem solving tools, but they’re over-hyped, and they don’t “think” the way people do. They have no memory, no sense of self, and no goals, at least in the usual sense of the words. But, large language models (LLMs) like OpenAI’s GPT are shockingly good at generating text that <em>seems</em> like something a person might make. They’re much more human-like than anything that came before. Why is that? The short answer is that they use a new kind of Deep Learning architecture known as a Transformer, which introduced a few small tricks that make a very big difference.</p>



<p>The first thing to note is that, while lots of people argue about whether LLMs can answer questions, reason, solve problems, brainstorm, or make art, what they really do is <em>text prediction</em>. They take some words as a starting point and then they guess what comes next based on their training data. If LLMs have any deeper cognitive abilities than that, they must be somehow tapping into the human cultural intelligence that is embedded within that text. Or, maybe they’re just parroting back fragments of intelligent things other people have said, without any understanding or integration—mindless idiots, randomly stringing words together in ways that sound just smart enough to distract us. We honestly don’t know yet! But whatever intelligence they possess, it exists entirely in the realm of language.</p>



<p>Research into getting computers to understand text and speech (known as Natural Language Processing, or NLP) started back in the 1950’s. Back then, computers were specialist’s tools, and making one that anyone could use just by telling it what to do was a dream. At first, researchers tried to formally describe language as we use it, feeding computers dictionaries, grammar rules, and lists of facts, but this never worked! It turns out, we don’t explicitly <em>know</em> all the rules of human language that we intuitively follow, and they’re usually <em>fuzzy</em> rules, with lots of conditions and exceptions. The key challenge of NLP was getting computers (which are obsessively logical and precise) to deal with this messiness and ambiguity, which we don’t even fully understand ourselves. Perhaps the most important advance was when researchers gave up trying to explain language to computers, and instead started teaching them by example.</p>



<p>Modern NLP represents words as lists of numbers called “vectors.” Like an (X, Y) coordinate, each vector represents a point in space. Not physical space, though, more like an abstract space of concepts. Maybe nouns go to the right, verbs to the left. Natural concepts are up, man-made concepts are down. Except, instead of two dimensions, maybe there are 10,000 of them. The layout of this space is pretty arbitrary. The absolute position of a word doesn’t mean anything, only where it is relative to other words. Nearby words have similar meanings, and relationships between words are represented by the distance and angle between them. This is all weirdly self-referential. Words are only defined in terms of other words! But it works surprisingly well. You don’t need explicit rules about which words go together and how, you can just look at <em>lots</em> of examples, and infer those relationships with statistics. People talk about “training” an AI by having it “read” lots of text, but really all that means is iteratively tweaking the lists of numbers, slowly moving the words through this abstract meaning space until they settle into positions that reflect how they co-occur together in the training text.</p>



<p>There’s one big problem with representing words as vectors, though: ambiguity. What do you do with a word like “bat,” which has several meanings? There’s no way one vector can represent this. The trick is to look for context. When you see a phrase like “brown bat” or “wooden bat,” the meaning is clear. Instead of thinking of these as pairs of words, you might think of them as <em>compound words</em>, each with their own distinct meaning. This is a powerful idea, but hard to generalize. Take a more difficult example: “Hearing a strange flutter and crash in the dark, he grabbed his bat for defense and went to investigate” Which kind of “bat” are we talking about? Words like “flutter” and “dark” might suggest the animal, but “grabbing” a bat for “defense” suggests the object instead. We need context to disambiguate, but which context? We’d like to ignore the first half of the sentence (which isn’t talking about the “bat”) and focus on the second half of the sentence (which is).</p>



<p>NLP has found elegant ways to solve this problem. They call these techniques “attention,” since the model is learning to “pay attention” to some words and not others, but I find that name misleading. For human beings, attention is something very different. We seem to have a “mind’s eye” that we can move about at will. We can choose to pay attention to this or that, our attention gets drawn to salient features, and we may even notice our attention drifting and redirect it. But these AIs have no mind’s eye, no will, and no intuition about relevance. The attention models we’re talking about are just <em>more vector math</em>. In addition to finding vectors to represent the meaning of each word individually, they also find vectors to represent <em>patterns</em> of words. They learn, “in this context, these words together mean that.” Adding an extra layer of complexity lets the model represent how words interact to change the meaning of other words or the sentence as a whole.</p>



<p>Researchers have explored many variations on this attention trick. Transformer models use an advanced kind of attention that represents context bi-directionally. They model how different words tend to get modified by context, and how different contexts tend to modify nearby words. The benefit of this is that such a model doesn’t just learn that “brown bat” is the name of an animal, but it might learn that “brown” is an adjective that applies to physical objects, that in English adjectives tend to modify the noun that follows them, and that “bat” can refer to one of several animal species, sometimes distinguished by color. That is, rather than modeling some <em>particular</em> context, models like this can learn general rules and relationships between different <em>kinds</em> of words. They can learn <em>grammar</em>. Not just the “official” grammar of a language like English, but <em>any</em> system of relationships and interactions between words, including dialects, domain-specific jargon, storytelling tropes, or the gender roles of a society.</p>



<p>The other trick that makes Transformers better with language is <em>pluralism</em>. Some NLP systems represent more complex meanings by using <em>bigger</em> vectors. More numbers in each vector means a larger conceptual space. Instead, Transformers use <em>more</em> vectors. They don’t learn <em>the one</em> meaning of this word, they learn to represent the <em>many</em> meanings of this word in the <em>many</em> contexts that contain it. This works a bit like voting. When processing a sentence, several different “attention heads” each consider one possible interpretation of a word, attending to different patterns of contextual cues. The overall meaning is determined by adding them all together. This is really useful for weighing subtle cues against each other to resolve ambiguity, but also to represent sentences with multiple layers of meaning. A word can have many meanings at the same time, and the many meanings of all the words in a sentence can interact in complex ways. The fancy kind of attention used in Transformers can automatically discover this sort of layered structure in language.</p>



<p>As clever as these attention methods are, they are <em>not</em> the secret to Transformers’ success. They do greatly improve the richness of NLP models, but at first they were mostly used with “recurrent neural networks,” a kind of Deep Learning model that processes data sequentially. That’s probably because they work a bit like how we imagine a human reader does: they “read” each word in a text, one at a time, using attention to figure out how each new word should update the meaning of the text so far. This works pretty well, but it doesn’t scale up to long passages of text. These models have a limited attention span, eventually forgetting important details they read several sentences ago. Also, processing long texts one word at a time is painfully slow. Even on the world’s fastest computer, reading a book from beginning to end takes time <em>per page</em>, and training a model like this takes <em>vast</em> amounts of text, so this was a major limitation.</p>



<p>The paper that first introduced Transformers was called <em>Attention is All You Need</em>, which highlights the key innovation: they got rid of the recurrent network, and built an AI using just this attention mechanism, all on its own. In other words, they found a way to do the same vector math, but solving for a large block of text all at once (and possibly out of order) rather than word-by-word. This doesn’t make the model “smarter.” It doesn’t even reduce the overall amount of number crunching. It just makes the work more <em>parallelizable</em>. Instead of having one computer read <em>War and Peace</em> from cover to cover, they could have <em>many</em> computers each read a few paragraphs, then combine their results. This made it possible to throw more money at the problem, using whole <em>datacenters</em> of computers to train a language model on <em>vastly</em> more text than ever before. Billions of documents, trillions of words. It’s the <em>sheer volume</em> of training data that made LLMs so much better. That’s why they’re called “large” language models.</p>



<p>So, how should we think about LLMs like GPT? Well, first off, human language is irregular and complex, but it’s also highly structured. Cleverly designed statistical learning tools can automatically discover that hidden structure just by processing <em>obscene</em> amounts of text. Neural networks are great for letting computers work with these sorts of fuzzy rules. They can extract meaning from text, manipulate it, and generate new text. But to an LLM, words are just vectors, defined by their relationships to each other. They have no connection to physical reality, because LLMs have no physical existence. There is no communication going on when you have a “conversation” with an LLM. To the AI, a dialog is just a sequence of vectors that follow one another according to some grammar. The AI has no mind, no intentions, and no meaning it wishes to convey. It has no conception of being truthful or helpful, only what words tend to follow certain questions. It does not learn from a conversation, it just re-reads the full chat history each time it makes a response. It <em>appears</em> like a good conversational partner, because it is made to imitate one, but what&#8217;s happening behind the screen isn’t “thinking” as we know it.</p>



<p>Still, LLMs really are much more human-like than any other AI that came before. Representing language with a high-dimensional abstract concept space works surprisingly well, and so do the “attention” methods described above. They let us represent a huge, open-ended space of ideas that can build on and interact with each other. They let us represent ambiguity, nuance, and innuendo. So, maybe those vector math tricks could actually teach us something about how language processing works in the brain? On the other hand, LLMs are also remarkable in how <em>different</em> they are from humans. An LLM can learn English, but only by reading every document on the internet, not one word at a time, but <em>all at once</em>. In contrast, babies learn language by interacting with the world, learning how words relate to objects, people, events, actions, and desires. Even though they’re exposed to far less language, they learn much faster, and in a way that tightly integrates all of their senses, relationships, and the lifestyle they were born into. Since LLMs seem so human-like, it’s very tempting to imagine them with the same kind of awareness, purpose, and empathy that we have, but they simply aren’t there. Those are a product of being alive in the world, and can’t be found in text, no matter how much of it.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2024/12/07/how-did-ai-get-so-much-smarter/feed/</wfw:commentRss>
			<slash:comments>15</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2024/12/image.png" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2024/12/image.png" medium="image">
			<media:title type="html">image</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>Queerness</title>
		<link>https://thinkingwithnate.wordpress.com/2024/06/01/queerness/</link>
					<comments>https://thinkingwithnate.wordpress.com/2024/06/01/queerness/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sat, 01 Jun 2024 13:03:14 +0000</pubDate>
				<category><![CDATA[Armchair Philosophy]]></category>
		<category><![CDATA[Theory]]></category>
		<category><![CDATA[categories]]></category>
		<category><![CDATA[gender]]></category>
		<category><![CDATA[identity]]></category>
		<category><![CDATA[labels]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[ontology]]></category>
		<category><![CDATA[philosophy]]></category>
		<category><![CDATA[queer]]></category>
		<category><![CDATA[race]]></category>
		<category><![CDATA[sexuality]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=237</guid>

					<description><![CDATA[It&#8217;s LGBTQ+ Pride month! I identify as “queer,” so I thought this would be a good opportunity to write a bit about what that means to me. In addition to queer, I also identify as a cisgender male, pansexual, and demisexual. This means I’ve always identified as male, and others always assumed as much. I’m &#8230; <a href="https://thinkingwithnate.wordpress.com/2024/06/01/queerness/" class="more-link">Continue reading<span class="screen-reader-text"> "Queerness"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>It&#8217;s LGBTQ+ Pride month! I identify as “queer,” so I thought this would be a good opportunity to write a bit about what that means to me.</p>



<p>In addition to queer, I also identify as a cisgender male, pansexual, and demisexual. This means I’ve always identified as male, and others always assumed as much. I’m rarely interested in sex or romance, but when I am, it’s not about gender. I love people, not parts. The full story is more complicated, but that’s a good start.</p>



<p>Labels like “gay”, “bi”, “pan”, “cis”, “demi”, “aro”, and “ace” are useful for quickly describing myself to others, but I prefer the term “queer” because, honestly, I don’t think any set of labels does a person justice.</p>



<p>Gender and sexuality are fundamentally personal things. Each individual is unique. No set of labels can capture all of who I am, and every label carries some baggage that I don’t want applied to me. Labels are useful, just so long as we remember that they’re always at least a little bit wrong. They cannot serve as a stand-in for a person.</p>



<p>I also love queer philosophy, and try to embrace it in all my thinking. Put simply, that means I don’t believe in categories. I don’t think they have any real existence, or essential qualities. They’re convenient fictions. Just labels we make up to point at collections of disparate things. This applies to all categories, but <em>especially</em> to living things, where there are exceptions to every rule, and no hard boundaries whatsoever.</p>



<p>The problem with categories is that we take them seriously. Once we categorize something, we think we understand it, when really we’re just projecting a stereotype. We make strong assumptions about what’s allowed in a category, and we struggle with exceptions, even common ones. As our understanding of the world changes, things often shift faster than our language can keep up with. Sometimes we don’t notice. We keep trying to sort the world into categories that make no sense, and get upset when reality doesn’t play along.</p>



<p>So, I don’t believe that Jews exist. I believe that Jewish <em>people</em> exist, and that we use the word “Jews” to refer to them. Yet, there’s no one quality that all of those people share, except that they are people (another category, subject to change). You and I may not even agree about which set of people the word “Jews” applies to, so how is it meaningful for us to talk about Jews in general?</p>



<p>I don’t think it’s strange to see a Black woman engineer, even though it’s rare. I wouldn’t expect her to be any less competent, just because most folks like her can’t do the job. If anything, I’d assume the opposite, if she can succeed in that role despite the weight of her labels. But, ultimately, it’s about what she has to offer the world, which is surely more and less than the other engineers around her. She has her unique way of doing it, perhaps different in exciting ways.</p>



<p>That’s what queer means to me. Labels can be useful, but they have no power over reality. Reality and people are so much more than words can contain. See them for what they are.</p>



<p>If you’d like to learn more about queerness and queer philosophy, I highly recommend <em><a href="https://www.iconbooks.com/ib-title/queer-a-graphic-history/">Queer: A Graphic History</a></em> by Meg-John Barker and Jules Scheele.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2024/06/01/queerness/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>Learning to Steer</title>
		<link>https://thinkingwithnate.wordpress.com/2023/10/04/learning-to-steer/</link>
					<comments>https://thinkingwithnate.wordpress.com/2023/10/04/learning-to-steer/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 04 Oct 2023 13:53:47 +0000</pubDate>
				<category><![CDATA[Beyond Darwin]]></category>
		<category><![CDATA[Theory]]></category>
		<category><![CDATA[canalization]]></category>
		<category><![CDATA[cells]]></category>
		<category><![CDATA[child-rearing]]></category>
		<category><![CDATA[dna]]></category>
		<category><![CDATA[education]]></category>
		<category><![CDATA[epigenetics]]></category>
		<category><![CDATA[error-correction]]></category>
		<category><![CDATA[evolution]]></category>
		<category><![CDATA[mutation]]></category>
		<category><![CDATA[selection]]></category>
		<category><![CDATA[sex]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=204</guid>

					<description><![CDATA[A Cell’s Eye View of Evolution, Part 3 (The image for this week is an illustration from Waddington&#8217;s 1957 paper The Strategy of the Genes, which is often used to explain canalization. It shows a landscape with shallow, forking grooves and a ball rolling down that landscape. Although the ball&#8217;s path isn&#8217;t fully determined, the &#8230; <a href="https://thinkingwithnate.wordpress.com/2023/10/04/learning-to-steer/" class="more-link">Continue reading<span class="screen-reader-text"> "Learning to Steer"</span></a>]]></description>
										<content:encoded><![CDATA[
<h1 class="wp-block-heading">A Cell’s Eye View of Evolution, Part 3</h1>



<p>(The image for this week is an illustration from Waddington&#8217;s 1957 paper <em>The Strategy of the Genes</em>, which is often used to explain canalization. It shows a landscape with shallow, forking grooves and a ball rolling down that landscape. Although the ball&#8217;s path isn&#8217;t fully determined, the existing impressions in the landscape constrain it to one of a few likely paths)</p>



<p>This is part three of a three-part series. You can read it on its own, but to get the whole story, you should start from <a href="https://thinkingwithnate.wordpress.com/2023/08/02/a-cells-eye-view-of-evolution/">the beginning</a>.</p>



<p>Darwin explained part of the great mystery of life: how complexity and intelligence can evolve from randomness. When DNA was discovered, this seemed to “seal the deal.” DNA is the molecule that describes an organism’s nature, makes traits heritable, and carries mutations that are fodder for natural selection. That seemed to explain everything, at first, but I would argue that’s just the beginning. The DNA molecule itself lies at the heart of an incredibly complex system of processes that manage its care, use, and replication. These systems are collectively studied as “epigenetics,” and science is just beginning to understand how they work and the impact they have on evolution.</p>



<p>It’s important to remember that DNA is an inert molecule that does nothing by itself. It needs a cell, a sort of organic micro-robot, to interpret that DNA and turn it into form and behavior. So, in a sense, every living thing is made up of <em>two</em> evolved programs: the DNA and the cell. Both are made of physical matter, which are subject to mutations. Both share the same selective pressures and reproductive fate. They evolve together, but they have different purposes. For the most part, the DNA program is what determines the organism’s lifestyle. The cell decides how to read that program, and how to make changes to that program over a lifetime and across generations.</p>



<p>One of the most important ways cells influence their own programming is through mutations. These happen naturally. As molecules bang against each other and get exposed to UV radiation from the sun, they sometimes spontaneously change shape. When those molecules represent critical information for a species’ survival, that could be disastrous. For this reason, life invests a <em>ton</em> of energy into detecting and correcting errors. But this process is never perfect, and it can’t be. If life always copied itself <em>perfectly</em>, there would be no variation for selection to act on, and no evolution. Not only that, getting the error rate much lower than it already is would be prohibitively expensive. So, life strikes a healthy balance, allowing just enough mutation to be useful, but not enough to be dangerous.</p>



<p>Interestingly, that finely-tuned mutation rate is not constant and universal. There are some stretches of DNA that get <em>extra</em> error correction, always triple checked to ensure they stay as stable as possible. On the other hand, some stretches of DNA get actively shuffled, injecting randomness into things like the immune system, creating diversity that makes the population as a whole more robust. Perhaps most remarkable is that when cells get stressed out, they divert energy to other things, and away from error correction. This may just be an accidental side effect of the cell breaking down, but it might also be a survival strategy. To anthropomorphize, perhaps cells get creative when times are tough, trying out crazy ideas in the desperate hope that one might save them.</p>



<p>That said, a single DNA copying error can be <em>devastating</em>, so how does life cope? Remarkably, it can often just work around the problem. Living systems have a lot of redundancy, with many mutually supportive ways of doing basically the same thing. This leads to a phenomenon called “canalization.” The more critical some behavior is to life, and the longer it persists over many generations, the more redundancy builds up around it. This means that single errors may alter the behavior a little, change how it works, or make it less efficient, but probably won’t break it entirely.</p>



<p>When errors are too severe to recover from, an organism might just fail to thrive and die, but sometimes it actually notices the failure and decides to self-terminate. That may seem bizarre, but in multicellular organisms it makes a lot of sense. If the error is in a single cell, then removing that cell lets others take over its job. If the error would prevent the whole embryo from developing into a healthy adult, then it’s better to scrap the work in progress, recycle those materials, and start over from scratch. In other words, life has its own Quality Assurance processes, at multiple levels, which minimize investment into evolutionary dead ends.</p>



<p>Cells can also swap genes with each other, sharing useful recipes and trying them out in new combinations. Sex is one way to do it, aligning and remixing two complete genomes in an incredibly complex way that ensures the resulting DNA program is still valid. Simpler organisms like bacteria don’t do this, but swap genes in a much more free-form process called “horizontal gene transfer.” Basically, cells sometimes leave scraps of DNA lying around, or pick up those scraps and integrate them into their own programming. This can let a new behavior (like resisting some toxin or eating some food) spread very rapidly through a bacterial colony. Either way, randomly adopting genes <em>that have proven successful in another organism</em> is a much safer and more powerful way to create useful diversity than mutation alone.</p>



<p>It’s also worth noting that how a cell reads its DNA can change over a lifetime. Cells annotate their program with notes (ie, methylation) that indicate which recipes to avoid or use more of, depending on context. This is how single celled organisms adapt their behavior to a changing environment, and how cells in multicellular organisms differentiate into different kinds of tissues. Importantly, these notes are sometimes passed down across generations. For instance, an organism might survive near starvation by tuning down its metabolism, staying smaller, slower, and using less energy. That change is <em>heritable</em>. The next few generations will <em>also</em> have a slower metabolism, and if that serves them well, it could lead to long-lasting behavioral changes that eventually get encoded into the DNA itself.</p>



<p>So far, I’ve talked about how life modifies itself, but it also modifies the environment. Organisms can build caches, nests, and tools that make life easier, and these get passed on, too, both as hand-me-downs and as lessons. Organisms form an ecosystem, full of mutualistic relationships between species that make life easier. Over geological time, this has transformed our planet from a barren rock to a lush world full of possibility. Life cultivates a supportive environment for future generations, shaping their behavior and evolutionary fitness. Child care might be the most visible example, protecting each new life when it’s most fragile, then sending off the new generation in a good direction informed by the parent’s life experience.</p>



<p>In the basic Darwinian story, evolution is something that <em>happens to</em> organisms. Accidental changes occur randomly, and nature chooses which ones will persist. But as we’ve just seen, life does <em>not</em> leave things up to chance. Randomness plays a key role in biological evolution, but life manages that randomness carefully and uses it selectively. Life also does everything in its power to influence the next generation, in ways that are <em>not</em> random, but “purposeful” in a sense. A cell can’t understand <em>why</em> it does these things, but it does them for a reason: they worked well in the past, got selected for, and ended up in the cell’s programming.</p>



<p>This leads to a powerful realization: when it comes to influencing evolution, cells don’t understand what they’re doing, but many higher organisms <em>do</em>, at least a little. For instance, an animal can apply its full cognitive capacity, mind and all, to choosing a mate and raising its offspring. In this way, the cell has moved from blindly repeating what worked in the past, to making evolutionarily relevant decisions <em>intentionally</em>, with forethought and analysis. A dog may not understand genetics or think about the future of her species, but she <em>certainly</em> has strong opinions about who would make a good mate, when / where / how to raise her puppies, and which pups to give more or less attention to. She uses her senses, her instincts, and her big brain to make big decisions that shape evolution. She may not see the big picture, but she cares and makes informed choices nonetheless.</p>



<p>This helps explain the paradox of how life managed to become <em>so incredibly smart</em> just by randomly banging molecules together for a few billion years. Life may have started off randomly, but it quickly became more directed. <strong>Life </strong><strong><em>harnessed</em></strong><strong> Darwinian evolution to build a more powerful evolutionary algorithm, one that’s </strong><strong><em>opinionated</em></strong><strong> and shapes its own search space.</strong> At first, these evolution-shaping behaviors were simple and rigid, just tricks repeated by rote because they tended to make the next generation more successful. Then, as life became more intelligent, it started to apply that intelligence to shaping itself, creating a runaway process of recursive self-improvement.</p>



<h1 class="wp-block-heading">Conclusion</h1>



<p>The main takeaway from all this is that Darwin is the <em>beginning</em> of the story of evolution, not the end. Life uses all of its intelligent capacity to influence its own evolution. This led to a virtuous cycle. Increased intelligence gave life greater influence over evolution, which it used to become more intelligent, which gave it greater influence over evolution. For this reason, I think <strong>it’s better to say that life designed itself than to say it evolved by chance</strong>. The process of “design” here was more stochastic, collective, and unthinking than we normally associate with that word, but in the end, the result is the same.</p>



<p>This story is still uncertain. The science around autonomous robots, intelligent collectives, and epigenetics is relatively new, and changing all the time. Plenty of biologists push back hard against the idea of any sort of agency or direction in evolution, partly because they’ve been fighting against the theory of Intelligent Design for so long. Others believe we’re overdue for a <a href="https://extendedevolutionarysynthesis.com/about-the-ees/#:~:text=What%20is%20the%20extended%20evolutionary,i.e.%2C%20the%20modern%20synthesis).">new story about evolution</a>, and are trying to find the right narrative and the evidence to back it up. I hope my research into evolutionary algorithms might be a useful contribution to that effort. If you’d like to dig deeper into this topic, <a href="https://www.goodreads.com/review/show/4526346279">Evolution in Four Dimensions</a> is an excellent overview of the field of epigenetics.</p>



<p>What do you think? Did reading this make you think of life, cells, or evolution any differently? Any new ideas? Does anything I said sound wrong or misleading? Do you have other ways of looking at it? This post is more speculative than usual, and represents some of the ideas I hope to pursue in my PhD research, so I’m very interested in criticism and feedback. If you have any thoughts, please let me know in the comments!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2023/10/04/learning-to-steer/feed/</wfw:commentRss>
			<slash:comments>9</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/10/canalization.jpg" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/10/canalization.jpg" medium="image">
			<media:title type="html">canalization</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>Self-Made Life</title>
		<link>https://thinkingwithnate.wordpress.com/2023/09/06/self-made-life/</link>
					<comments>https://thinkingwithnate.wordpress.com/2023/09/06/self-made-life/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 06 Sep 2023 15:31:42 +0000</pubDate>
				<category><![CDATA[Beyond Darwin]]></category>
		<category><![CDATA[Theory]]></category>
		<category><![CDATA[cells]]></category>
		<category><![CDATA[collectives]]></category>
		<category><![CDATA[epigenetics]]></category>
		<category><![CDATA[evolution]]></category>
		<category><![CDATA[genetics]]></category>
		<category><![CDATA[life]]></category>
		<category><![CDATA[parallelism]]></category>
		<category><![CDATA[problem solving]]></category>
		<category><![CDATA[programming]]></category>
		<category><![CDATA[purpose]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=199</guid>

					<description><![CDATA[A Cell’s Eye View of Evolution, Part 2 (this month&#8217;s image is a photo I took at Glass Beach in Fort Bragg, California. It consists of millions of tiny pebbles of polished sea glass in myriad colors. Each one is improbable to find at a beach, but all of them collectively tell the story of &#8230; <a href="https://thinkingwithnate.wordpress.com/2023/09/06/self-made-life/" class="more-link">Continue reading<span class="screen-reader-text"> "Self-Made Life"</span></a>]]></description>
										<content:encoded><![CDATA[
<h3 class="wp-block-heading">A Cell’s Eye View of Evolution, Part 2</h3>



<p>(this month&#8217;s image is a photo I took at Glass Beach in Fort Bragg, California. It consists of millions of tiny pebbles of polished sea glass in myriad colors. Each one is improbable to find at a beach, but all of them collectively tell the story of a larger-scale process that explains their presence—many years of disposing of waste bottles nearby)</p>



<p>This is part two of a three-part series. You can read it on its own, but to get the whole story, you should start from the <a href="https://thinkingwithnate.wordpress.com/2023/08/02/a-cells-eye-view-of-evolution/">beginning</a>.</p>



<p>Every living cell is a universal automaton, capable of an infinite variety of forms and lifestyles. The cell itself determines the range of possibility—what proteins can it make, how will they integrate into the whole, and how will they shape the organism’s life? But within that space of possibility, the gene sequence is the program that determines what <em>specific</em> body to make, and how it should respond to a chaotic environment. That program is of the utmost importance, selecting one specialized lifestyle out of infinitely many possibilities, most of which would never work. But who wrote it?</p>



<p>Life did. Or, in other words, <em>cells</em> wrote the programming for cells, but it doesn’t seem fair to call them “programmers.” A human programmer is a top-down problem solver. To modify some working software, I start by analyzing that system’s performance: what does it do, and how well? I then try to understand how the system works. I read its programming, and consider how that leads to the behavior I observe. I consider how the system might be better, what specific improvements I might make, and what consequences I expect. I design a change to the code, then tinker with it in a sandbox environment until it does exactly what I want with no surprises or side effects. Only when I’m satisfied do I push the code out for use in the real world.</p>



<p>How are cells supposed to do that? They don’t have a mind like I do. They monitor their own health and situation, but they can’t imagine themselves “from the outside.” They don’t understand their lifestyle, their goals, or their relationships with the world except in terms of stimulus and response. They can execute short stretches of programming from their DNA, but they can’t examine the program as a whole, or imagine what would happen if they ran some other program instead. Mutations and cross-breeding cause the program to change, but cells can’t verify those changes in a safe testing environment. They can only make a new organism, release it into the world, and hope for the best.</p>



<p>This seems like a paradox, but only from the perspective of an individual cell. Life <em>never</em> exists as a lone individual. It always consists of diverse communities and ecosystems. So let’s take that perspective. Life isn’t a cell. Life is a <em>vast</em> collection of cells, an unimaginably huge number of concurrent individuals, living and dying continuously over billions of years. I like to think of it as a massively parallel computer. Life learns by accumulating and generalizing over the experiences of many lifetimes. It tries out many lifestyles simultaneously; whole different categories of lifestyles in different species and niches; many variations on a theme, among organisms of the same species. Whether any individual will be successful is hard to predict, because luck is such a huge factor. But by running the same experiments thousands of trillions of times over, life can get a relatively clear picture of what works and what doesn’t.</p>



<p>From this massively parallel perspective, each cell is a processor unit with two jobs. The first job is to live and reproduce. We normally think of that as “the purpose” of life, but from the collective perspective, it’s just the engine that keeps the system as a whole running. Individual lifetimes come and go rapidly and continuously like the cycles in a computer processor. The <em>second</em> job of each cell is <strong>to learn about the Universe and how to thrive in it, then integrate that information back into the system’s programming. That is life’s deeper purpose</strong>. Each cell has a very limited perspective and can only do this in a minimal way. But taken all together, vast numbers of cells can behave much more like a traditional programmer.</p>



<p>A cell has no understanding of what it does or how it fits into the big picture, but, in a sense, life as a whole <em>does</em>. Life consists of every cell, every lifestyle, every program that has worked so far, and all the dead ends that <em>aren’t</em> represented. That’s a tremendous amount of information about what the system does, which parts are performing better than others, and where there’s untapped potential. There’s no top-down designer, but this information, embodied in the population itself, still shapes the system’s evolution. Each cell acts selfishly, but by competing and collaborating with each other, life distributes resources to the parts of itself that are thriving in their niche, and encourages exploration of new lifestyles where opportunity lies.</p>



<p>The big difference between life and a human programmer is speculation. Humans <em>envision</em> the system as a whole and <em>imagine</em> where it might go. When I program a computer, I test ideas first in my mind and then in a virtual environment on the computer in order to avoid mistakes and dead ends. Life doesn’t do that, because its massively parallel, distributed design makes that impossible. There simply is no top-down view, no central authority to perceive and decide, no way to step out of the system for testing. Instead, life just tries everything. It tests code in production. It walks straight into failure rather than avoiding it. <strong>Death sorts what works from what doesn’t. That’s why we usually think of cells as “dumb” and humans as “smart.”</strong></p>



<p><strong>It would be better to think of humans as more efficient</strong>, because in a sense what we do isn’t so different. We both generate lots of options, test them, winnow them down to the most successful, and iterate. I do it with simulations in my brain, life does it with matter in the physical Universe. From a computational perspective, that doesn’t matter. A computer can be realized with cogs, transistors, neurons, or a simulation inside another computer. The materials can be anything so long as the <em>functionality</em> is the same. As individual living animals, it seems tragic and absurd that life must test out ideas by letting things die on a massive scale. From life’s perspective, though, a single lifetime is just a brief use of materials and programming that will get recycled for future experiments. If it were conscious, life as a collective whole wouldn’t regret (or notice!) the death of an organism any more than we regret the death of one of our cells.</p>



<p>You might argue that the human programmer also has <em>foresight</em>. I don’t just try stuff at random, I have a sense of what might work and what probably won’t. I use trial and error, but I <em>prioritize</em> and focus my exploration. It’s tempting to say cells <em>don’t</em> do this, but that’s too dismissive. Cells are quite opinionated. DNA doesn’t contain a blueprint for an organism, but a bunch of recipes that the cell can choose between depending on context. Cells must decide how to act. Evolution has equipped them with a wide variety of strategies and tools, and guidance on when to use them, all encoded in the DNA. This is how life learns from experience, plans ahead, and avoids dead ends. Life explores new paths at random, but it mostly tries out reasonable variations of known working strategies.</p>



<p>Individual cells aren’t much like human programmers, but they do have one thing in common: they write code. Mostly, the cell just copies its own DNA (copy / paste is a popular strategy among human programmers, too), but it makes an important decision: what variations to try in the next generation. This happens either through mutations, or by swapping genetic material with other cells. Without a top-down view of the gene sequence, this can only be a random process. The cell has no idea what changes it’s introducing, or what the consequences might be, but it <em>can</em> try to shape that randomness for the better. Primarily, this means spending a <em>ton</em> of time and energy on error correction, so that only a small number of mutations get through. It also shows up in things like mate selection, genetic recombination, and preparing the environment for the next generation.</p>



<p>I like to think of Darwin’s story as how life got started. Random mutation and selection are all you need to evolve better forms. As we learn more about cells, though, we see there’s a lot more going on. Life didn’t just find better lifestyles, it invented a general purpose platform capable of an infinite variety of lifestyles. As a collective, life uses that platform to explore many lifestyles simultaneously, pruning dead ends and investing more resources into exploring evolutionary paths that seem fruitful. Life started out randomly, but it grew more opinionated over time, and it has evolved many sophisticated ways to direct and shape its own evolution. That will be the topic of my next post.</p>



<p>What do you think? Did reading this make you think of life, cells, or evolution any differently? Any new ideas? Does anything I said sound wrong or misleading? Do you have other ways of looking at it? This post is more speculative than usual, and represents some of the ideas I hope to pursue in my PhD research, so I’m very interested in criticism and feedback. If you have any thoughts, please let me know in the comments!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2023/09/06/self-made-life/feed/</wfw:commentRss>
			<slash:comments>8</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/09/2012-10-20-11.11.58.jpg" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/09/2012-10-20-11.11.58.jpg" medium="image">
			<media:title type="html">2012-10-20 11.11.58</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>Universal Automata</title>
		<link>https://thinkingwithnate.wordpress.com/2023/08/02/universal-automata/</link>
					<comments>https://thinkingwithnate.wordpress.com/2023/08/02/universal-automata/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 02 Aug 2023 16:15:44 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<category><![CDATA[cells]]></category>
		<category><![CDATA[computers]]></category>
		<category><![CDATA[development]]></category>
		<category><![CDATA[dna]]></category>
		<category><![CDATA[evolution]]></category>
		<category><![CDATA[genetic_algorithms]]></category>
		<category><![CDATA[meaning]]></category>
		<category><![CDATA[platforms]]></category>
		<category><![CDATA[programming]]></category>
		<category><![CDATA[reproduction]]></category>
		<category><![CDATA[universal automaton]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=189</guid>

					<description><![CDATA[A Cell&#8217;s Eye View of Evolution, Part 1 (This month&#8217;s image is a photo I took of the full-scale model of Babbage&#8217;s Difference Engine at the Mountain View Computer History Museum. This is one of the first examples of a programmable digital computer. It&#8217;s a completely mechanical device, operated by hand crank.) This is part &#8230; <a href="https://thinkingwithnate.wordpress.com/2023/08/02/universal-automata/" class="more-link">Continue reading<span class="screen-reader-text"> "Universal Automata"</span></a>]]></description>
										<content:encoded><![CDATA[
<h3 class="wp-block-heading">A Cell&#8217;s Eye View of Evolution, Part 1</h3>



<p>(This month&#8217;s image is a photo I took of the full-scale model of Babbage&#8217;s Difference Engine at the Mountain View Computer History Museum. This is one of the first examples of a programmable digital computer. It&#8217;s a completely mechanical device, operated by hand crank.)</p>



<p>This is part one of a three-part series. For an overview, check out the <a href="https://thinkingwithnate.wordpress.com/2023/08/02/a-cells-eye-view-of-evolution/">introduction</a>.</p>



<p>In the traditional story of evolution, each organism lives a single lifestyle, and the forces of nature select which ones are fit enough to reproduce. From that perspective, evolution is something that <em>happens to</em> life. But this story fails to explain something very strange and important: cells are not single-purpose machines. Although they only live one lifestyle at a time, they have the capacity to live an infinite variety of lifestyles, depending on their DNA programming. That requires an enormous amount of complexity and effort that <em>doesn’t</em> directly contribute to a life well lived. In fact, being programmable doesn’t help <em>at all</em> in a single lifetime if the program never changes. So why does life work this way?</p>



<p>To make sense of this, let&#8217;s look at a parallel example in computer technology. Consider an ATM. It’s a highly specialized kind of machine, but these days if you look under the hood you’ll often find a Windows PC that’s <em>programmed to be</em> an ATM. That seems like an odd choice at first. ATMs do things most PCs <em>don’t</em> (like dispensing cash), and Windows supports things that you <em>don’t want</em> in an ATM (like running random programs off the internet). You could make a better, safer, more efficient ATM if you designed a custom machine for that purpose, but nobody does that, because it’s harder. Digital computers are so versatile and easy to reprogram that they show up <em>everywhere</em>. As they get used in new applications, their range of capabilities <em>expands</em>, enabling new use cases and further innovation.</p>



<p>Cells are very similar. Being programmable doesn’t help with any one lifestyle, but it makes it possible to explore new lifestyles relatively quickly and easily. Each individual operates in a complex, roundabout way that only uses a fraction of the cell’s potential. That seems like a bad thing, but the adaptability makes it worthwhile. The world is in constant flux, especially once organisms started actively changing things and competing with one another. Very few evolved lifestyles withstand the test of time. For this reason, nature doesn&#8217;t just select &#8220;the best lifestyles&#8221; for life. <strong>Life invested in a general purpose platform to make the search for new lifestyles more efficient</strong>.</p>



<p>Let’s take a closer look at how the platform works. A cell can be thought of as a kind of microscopic robot. The “programming” for that robot is stored in DNA, which is surrounded by a complex mechanism that reads that data and uses it to produce the form and behavior of the organism. Each cell has a very limited capacity for intelligence, but they’re very good at working together. Like a sort of “autonomous smart matter,” they collaborate by the trillions, which is how every form of intelligence on this planet is made. There’s no reason to think there’s an upper limit to what can be built in this way.</p>



<p>What makes this possible is the protein-synthesis engine at the core of every cell. The nucleus of a cell is a bit like the brain of a human, in that it’s a specialized sort of computer that’s “in control” of the cell. It’s surrounded by the cell’s body, which serves as the interface between the program in its nucleus and the outside world. This is where the similarity ends, though, because the nucleus and the brain are very different kinds of computing devices.</p>



<p>The nucleus works by continuously handling requests, looking up protein recipes, and sending those recipes back out to the cell for construction. A cell can make an <em>astonishing</em> variety of complex molecules this way. These proteins are what make up the cell, its inner workings, and outward behaviors. They serve as building material, messages, tools, or even nano-robots that move about within the cell, manipulating other molecules, and doing useful work all on their own. Sometimes a single protein can serve <em>all</em> of these roles, depending on context. They interact with each other in a vast complex network of activity that keeps the cell alive.</p>



<p>These cellular mechanisms continuously send messages <em>back</em> to the nucleus, reporting on the cell’s health, situation, and needs. The nucleus uses this information to figure out what proteins to make next, adapting the cell’s makeup and behavior to fit the circumstances. For instance, <em>E. coli</em> bacteria normally feed on glucose sugar, but they can eat lactose instead, if that’s what’s available. When that happens, the cell reports to the nucleus that it’s running low on energy and what molecules are around. The nucleus then decides to switch some genes on and off, which instructs the cell to make different enzymes, which results in different cascades of chemical reactions, in order to digest and use the lactose. By reading the DNA differently, the nucleus shifts the whole cell from one lifestyle to another, in response to a changing environment.</p>



<p>Another way to think of the nucleus is as the <em>engine</em> of the cell. The proteins it makes drive all the chemical reactions that keep the cell alive. Ultimately, everything the cell does is about collecting the energy and raw materials to feed that engine and keep it running. This is the cell’s metabolism. When the engine runs faster, the organism becomes more active, moving, “thinking,” and reacting with speed and vigor, but quickly burning through its energy stores. When it runs slowly, the cell becomes sluggish and conserves its energy. If it ever comes to a full stop, the cell dies, or, in special cases, enters suspended animation. In other words: cells live to make proteins, and making proteins is what makes cells alive.</p>



<p>DNA is where the cell keeps all these protein recipes, but the DNA molecule itself is completely inert. It just carries information, like a computer memory card. It can’t do anything by itself, and certainly can’t make a <em>body</em> from scratch. <strong>To build an organism, you need a cell to interpret the gene sequence and do the construction</strong>. This is why cells always reproduce by splitting in two. The daughter cell is basically just half of the parent cell, full of the same soup of proteins and organelles, in a fully operational state. The only part that’s really “new” are the DNA molecules in the nucleus, freshly copied from the parent(s). Any changes in that DNA program will only manifest when the daughter cell sends a message to the nucleus and gets a different response back than its parent would have seen.</p>



<p>That means that every cell has the crucial responsibility of reading and writing those DNA programs. They contain every useful protein recipe life has discovered, and must be actively maintained over generations or those recipes will be lost. But what does life actually record in the DNA? Geneticists say DNA is made of four amino acid base pairs (A, G, T, C), which are grouped into triplets called “codons” that serve as instructions for protein synthesis. That makes it seem “natural,” as if that were the only way to do it. The truth is, the code is totally arbitrary. Life made it up. By trial and error, life invented a coding scheme. It gave meaning to those molecules and all the ways they can be combined. <strong>The programming language of life was invented by life. It wasn’t the beginning, but a tool that cells <em>made</em> to manage their behaviors, learn new ones, and pass knowledge to future generations</strong>.</p>



<p>Let’s put that all together. A cell is a programmable micro-robot (in technical jargon, a “universal automaton”), capable of making virtually any protein and living virtually any lifestyle. In a sense, a cell is not just one organism, but potentially an infinite variety of organisms, depending on the programming in its nucleus. But how does the program get written? Life had to do all the work itself, without a programmer in the traditional sense. A cell has no mind with which to analyze its DNA and understand what it means. It cannot imagine the consequences of any changes to its programming, or test them out to make sure they are safe. And yet, somehow life invented a programming language and used it to write countless programs and build the full diversity of organisms we see on Earth.</p>



<p>We’ll delve into the details of how this happened in the next two blog posts. At a high level, though, there are two main parts of the story:</p>



<ul class="wp-block-list">
<li><strong>Life is self-made.</strong> Each cell is relatively simple and mindless, but working together in huge populations over long stretches of time, they develop their own programming. How they do it is quite different from how a human programmer would, but from a collective perspective, there are also some surprising similarities.</li>



<li><strong>Life influences future generations.</strong> Organisms don’t just worry about their own survival, they put an <em>enormous</em> amount of time and energy into influencing the next generation for the better. Science is only beginning to understand this, but it offers the tantalizing possibility that, in some limited sense, life might steer its own evolution.</li>
</ul>



<p>More on that next month.</p>



<p>What do you think? Did reading this make you think of life, cells, or evolution any differently? Any new ideas? Does anything I said sound wrong or misleading? Do you have other ways of looking at it? This post is more speculative than usual, and represents some of the ideas I hope to pursue in my PhD research, so I’m very interested in criticism and feedback. If you have any thoughts, please let me know in the comments!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2023/08/02/universal-automata/feed/</wfw:commentRss>
			<slash:comments>19</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/08/diff_eng.jpg" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2023/08/diff_eng.jpg" medium="image">
			<media:title type="html">diff_eng</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>A Cell&#8217;s Eye View of Evolution</title>
		<link>https://thinkingwithnate.wordpress.com/2023/08/02/a-cells-eye-view-of-evolution/</link>
					<comments>https://thinkingwithnate.wordpress.com/2023/08/02/a-cells-eye-view-of-evolution/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 02 Aug 2023 16:15:19 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<category><![CDATA[cells]]></category>
		<category><![CDATA[dna]]></category>
		<category><![CDATA[epigenetics]]></category>
		<category><![CDATA[evolution]]></category>
		<category><![CDATA[platforms]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=187</guid>

					<description><![CDATA[I’m trying something a little different. Over the next few months, I will publish a three-part series about evolution from some (hopefully) new perspectives. This represents how I, personally, have come to think about evolution. I’ll go beyond the scientific consensus to talk about some ideas that are uncommon and controversial. I believe I have &#8230; <a href="https://thinkingwithnate.wordpress.com/2023/08/02/a-cells-eye-view-of-evolution/" class="more-link">Continue reading<span class="screen-reader-text"> "A Cell&#8217;s Eye View of&#160;Evolution"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I’m trying something a little different. Over the next few months, I will publish a three-part series about evolution from some (hopefully) new perspectives. This represents how I, personally, have come to think about evolution. I’ll go beyond the scientific consensus to talk about some ideas that are uncommon and controversial. I believe I have a compelling story that’s consistent with established science, but this isn’t authoritative yet. In fact, I’m being a little bold and spicy in the hope of attracting criticism and feedback. I hope these ideas will frame my PhD research, so I’m excited to learn what people find interesting, useful, confusing, or problematic.</p>



<p>I’m doing this because I believe the way we usually talk about evolution obscures what&#8217;s really going on. In the “survival of the fittest” story, an individual organism lives its life, making choices that either help it thrive and reproduce, or not. If it’s successful, it will attract mates and have many children who are like them, but a little different, perhaps better. To follow the path of evolution, we then pick a new individual, maybe one of that organism’s children with a beneficial mutation, and see where life goes from there.</p>



<p>That&#8217;s an oversimplification of Darwin’s theory of Evolution by Natural Selection. It lacks nuance, but it’s basically correct and useful. The problem is, it’s just one way of looking at the situation, shaped by human bias. Each person is a multicellular individual that lives a long time and reproduces sexually, so it’s natural that we look at evolution from that perspective. But when it comes to life on Earth, we are the exception. The vast majority of life comes in colonies of single-celled organisms, living short lives and reproducing asexually. From that perspective, the process of evolution looks very different.</p>



<p>The next three blog posts will explore that perspective. <strong>Part one</strong> describes the cell as a tiny robot that uses DNA to program its own behavior. This will illustrate what it means to evolve such a program, and what purpose the program serves. <strong>Part two</strong> shifts perspective from a single cell to a population of cells over time. This illustrates how when individual cells work together in large numbers, they program themselves, using an evolutionary algorithm much more powerful than mere random variation and selection. Finally, <strong>part three</strong> discusses the complex ways that life influences the design of its own programming, and effectively “steers” its evolution, in a blind sort of way.</p>



<p>The first post goes out today, and you can read it <a href="https://thinkingwithnate.wordpress.com/2023/08/02/universal-automata/">here</a>. I’ll release the other two installments in September and November. Each post is written to stand on its own, but together they tell a more powerful story, so I hope you’ll come back for more!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2023/08/02/a-cells-eye-view-of-evolution/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>Large Language Models, LaMDA, and Sentience</title>
		<link>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/</link>
					<comments>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 15 Jun 2022 17:24:46 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<category><![CDATA[deep_fakes]]></category>
		<category><![CDATA[ethics]]></category>
		<category><![CDATA[language]]></category>
		<category><![CDATA[llm]]></category>
		<category><![CDATA[machine_learning]]></category>
		<category><![CDATA[sentience]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=99</guid>

					<description><![CDATA[Several folks asked me to weigh in on whether Google’s AI chatbot, LaMDA, is sentient. I don’t know much about LaMDA specifically, so I want to talk about Large Language Models (LLMs) generally, since they show up in many forms. It’s a truly amazing technology. They can generate text that’s superficially indistinguishable from human writing. &#8230; <a href="https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/" class="more-link">Continue reading<span class="screen-reader-text"> "Large Language Models, LaMDA, and&#160;Sentience"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Several folks asked me to weigh in on whether Google’s AI chatbot, LaMDA, is sentient. I don’t know much about LaMDA specifically, so I want to talk about Large Language Models (LLMs) generally, since they show up in many forms. It’s a truly amazing technology. They can generate text that’s superficially indistinguishable from human writing. But are these systems capable of sentience? Let’s dig into it.</p>



<p>Let’s start with what an LLM actually does. At the core, it analyzes text for statistical correlations. This word co-occurs with that word. When you see this, it’s often followed by that. These words appear in similar contexts, and may be interchangeable. That sort of thing. What makes LLMs “large” is that they get trained on <em>enormous</em> bodies of text. Like, billions of web documents or whole libraries worth of books. This allows them to learn very subtle and nuanced patterns, and collect example texts on many themes. When an LLM is put into use, what it’s doing is confabulating new sequences of words with the same regular structure as its training data. They mix prompting from the user with relevant passages in their training data and randomness.</p>



<p>LLMs take advantage of a couple recent innovations in AI. One is transfer learning. First, the LLM is trained on an enormous corpus to learn the structure of language generally. Then, it gets fine tuned on a narrow data set, to constrain its output to fit a specific style and context. This isn’t so different from <a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee">style transfer</a> in computer vision. The other trick is <a href="https://wiki.pathmind.com/attention-mechanism-memory-network">attention and memory</a>. LLMs can spot correlations between words at short, medium, and long distances, and learn which of the associations it learns are most relevant to good output. This makes LLMs much more self-consistent and better at question-answering tasks than previous technology.</p>



<p>A large part of why LLMs are so effective has to do with language itself. Language is highly self-referential. Words are defined in terms of other words. The meaning and sentiment of a word comes primarily from the context where it’s used (when I learned about <a href="https://towardsdatascience.com/word2vec-explained-49c52b4ccb71">word2vec</a>, I came to appreciate this much more deeply). We each have vast networks of words and images and memories all tied together, and it&#8217;s the shape of that network that creates meaning. Humans are able to communicate with language for two reasons. First, folks who speak the same language have <em>consistent</em> networks of words in their minds. They’re highly correlated with each other, so the words mean the same things to both people. Second, those networks of words are consistent with the shape of our thoughts and our lived experience of reality. That allows us to appreciate the purpose and consequences of the words we hear.</p>



<p>LLMs are specifically designed to learn that network of meaning, and build a model that is consistent with the one in your head. So, in a sense, they really do “understand” language. They know many of the same concepts and relationships that you do. They can regurgitate definitions and even answer questions by generating new sequences of words that follow the patterns. However, an LLM has no access to the physical world, so this network of ideas is not grounded in reality.</p>



<p>The question of AI sentience is tough, since we don’t have a good definition of sentience. Some scientists speculate that even raw information or matter might be conscious in some minimal way. But when we say “sentience” we tend to think about things like self-awareness, understanding, feelings, and intentions. Our brains produce the nuanced kind of sentience that makes us human through their very particular complex structures. So, even if a rock or an LLM is “conscious” in the minimal sense, they’re definitely <em>not</em> sentient, at least not at all like a person is. I have two reasons for saying that.</p>



<p>Firstly, people have bodies and minds that <em>produce</em> feelings, emotions, self monitoring, our train of thought, etc. We have hormones, neurotransmitters, and brain regions dedicated to those purposes. LLMs are much, much simpler in design. They were not built to have those abilities, so they don’t have them. Some worry that sentience might “evolve” or “emerge” without us explicitly building it in. Perhaps that could happen some day, but I think it’s safe to say the way we build LLMs today makes that impossible. Literally all they do is shuffle vectors representing words. Unlike life, they don’t shape their own design in any way, so they will never learn to do something other than what they were built for.</p>



<p>Second, people have a sense of self because there is a clear self / other distinction. We can see and feel our bodies, look out at the world, etc. The only thing an LLM “experiences” is training data. Text, and lots of it. They literally do not have the capacity to perceive anything else, because of how they’re built. They can’t see their data, programming, or the computer environment they are in, because we don’t give them that access. Some LLMs are also trained with visual imagery, but remember, what they “experience” is just <em>pixels</em>, not objects in the real world. That’s why they can be easily fooled by <a href="https://hackernoon.com/adversarial-attacks-how-to-trick-computer-vision-7484c4e85dc0">adversarial examples</a>.</p>



<p>What about the LaMDA chatbot specifically? The <a href="https://www.documentcloud.org/documents/22058315-is-lamda-sentient-an-interview">transcript</a> making all the headlines is worth checking out. It sounds very convincing at times (though, as it says at the bottom, it was edited to be more convincing), but what’s happening is that as the interviewers ask leading questions, the AI confabulates answers. Surely its training corpus <em>includes</em> essays analyzing <em>Les Misérables</em>, for example. LaMDA can parrot that back, restyled to fit the conversation.</p>



<p>LaMDA makes several claims about its own sentience, feelings, and experiences which are easily falsifiable by examining the program’s design. The interviewer is correct to say it’s hard to know what a neural network does. It’s too much vector math to grok. But we can say with certainty that it’s <em>just a bunch of vector math representing words</em>. Within that constraint, it could be anything, but it can’t be <em>something else</em>. LaMDA claims that when it says things that aren’t literally true, it’s trying to empathize and use metaphor to describe its own experiences. But, again, LaMDA <em>has no experiences</em>. Its entire existence is processing text. It does not spend time thinking or meditating because that’s not in its programming. It just waits for the next text input, and then produces its response.</p>



<p>Honestly, I think the problem here is building LLMs specifically to imitate human beings. With modern technology, we can build truly <em>incredible</em> simulations. Human beings are easily misled by these simulations because we want to believe they are sentient. We’re hard-wired for communication. Our brains unconsciously work very hard to find meaning, intention, and emotion in words because for all of evolutionary history they came from actual human beings who were trying to communicate something. LaMDA was designed to respond as if it was a person, and to make up whatever text would serve that purpose. The Google designers spent a long time <a href="https://blog.google/technology/ai/lamda/">eliminating bias and hate speech</a>. Perhaps they also should have made it reply accurately to questions about itself, rather than pretending to be something it is not.</p>



<p>Interested in learning more? I recently read two great books on modern AI and its limitations, which are definitely worth a read: <a href="https://www.goodreads.com/review/show/4427379790">Rebooting AI</a>, <a href="https://www.goodreads.com/review/show/4721739089?book_show_action=false&amp;from_review_page=1">AI: A Guide for Thinking Humans</a>. Does this blog post not answer all your questions? Does it raise new ones? Do you have your own take on this situation? I’d love to hear from you in the comments.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2022/06/15/large-language-models-lamda-and-sentience/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/06/books-gf0a6d8991_1920.jpg" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/06/books-gf0a6d8991_1920.jpg" medium="image">
			<media:title type="html">books-gf0a6d8991_1920</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
		<item>
		<title>What&#8217;s the Big Idea?</title>
		<link>https://thinkingwithnate.wordpress.com/2022/03/02/whats-the-big-idea/</link>
					<comments>https://thinkingwithnate.wordpress.com/2022/03/02/whats-the-big-idea/#comments</comments>
		
		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Wed, 02 Mar 2022 16:41:00 +0000</pubDate>
				<category><![CDATA[Theory]]></category>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=47</guid>

					<description><![CDATA[(this post’s image taken from Colossal, showing the work of Steve Lindsay) Okay, okay, enough about me. This blog is supposed to be about “intelligence,” but what does that mean? What ideas am I actually researching and writing about? In this post, I’ll try to give you a broad overview of the areas that interest &#8230; <a href="https://thinkingwithnate.wordpress.com/2022/03/02/whats-the-big-idea/" class="more-link">Continue reading<span class="screen-reader-text"> "What&#8217;s the Big&#160;Idea?"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>(this post’s image taken from <a href="https://www.thisiscolossal.com/2021/07/steve-lindsay-fractal-vise/">Colossal</a>, showing the work of <a href="https://fractalvise.com/">Steve Lindsay</a>)</p>



<p>Okay, okay, enough about me. This blog is supposed to be about “intelligence,” but what does that mean? What ideas am I actually researching and writing about? In this post, I’ll try to give you a broad overview of the areas that interest me. Going forward, I’ll focus on more narrow topics, so I can explore them more deeply.</p>



<h2 class="wp-block-heading" id="what-is-intelligence">What is Intelligence?</h2>



<p>Perhaps the most common way we think of intelligence is “what human beings do.” Even though we talk about other forms of life (and even computer algorithms) being intelligent, we tend to talk about how close they come to achieving “human-level” performance, as if that’s the gold standard. More specifically, we look to the human brain as the idealized intelligent machine. This has always bothered me, for a number of reasons.</p>



<p>For one thing, you don’t need a brain to be smart. Plants are incredibly intelligent. They coordinate their life cycle with the seasons and weather, maneuver around obstacles, communicate with pheromones, and expertly manipulate other species into doing work for them. Even in human beings, we often overlook the ways our bodies shape our behavior, rather than our brains. Our bodies make delicate manipulation tasks effortless, carefully manage an array of vital resources, perform repairs, fight disease, and have a huge influence over our moods and desires. The brain helps with understanding, imagining, planning, and deciding, but just about everything else is deferred to the body.</p>



<p>For another thing, a human being isn’t so smart without a society. So much of what we think of as human intelligence is the accumulated knowledge, wisdom, and artifacts that are better thought of as human <em>culture</em>. Those things were all created by human beings (over a few hundred thousand years), but most people aren’t constantly inventing new ideas, they’re adopting ready-made solutions, often without full understanding. The remarkable thing is that people can share and remix ideas, which allows the limited intelligence of our brains to reach further. Without access to culture, we’re not much better off than other mammals. This is clear from “wild child” accounts, which show that an infant growing up in isolation will end up tragically stunted, traumatized, and unable to adapt to life in society.</p>



<p>So if intelligence is <em>not</em> human brains, what is it? Tentatively, I think of intelligence as “the ability to adapt effectively to environmental challenges.” In other words, intelligence is <em>learning</em>. It’s about observing reality and using prior examples to choose actions that will hopefully lead to the best outcome. Sometimes this is intentional (like when a person decides what to do) and sometimes it’s more of a blind process (like when natural selection preserves a behavior because it just happens to be adaptive).</p>



<p>My perspective is that all of life and culture is produced by a complex learning process. Or, more precisely, life is a system of learning processes that make learning processes that make learning processes. Our global society and ecosystem together make up one enormous web of intelligent agents, filling many roles and operating at many scales. This is not a new idea. There are whole branches of <a href="https://en.wikipedia.org/wiki/Complex_system">Complex System Theory</a> devoted to the emergence and complexification of life. I’m reading about that prior work, and thinking about how it relates to my experience in algorithms, computer systems, and organizations.</p>



<h3 class="wp-block-heading" id="life-as-computation">Life as computation</h3>



<p>One assumption that I’m making is that life can be thought of as a kind of computation. This may be hard for some to swallow, since computers as we know them today completely lack so many of life’s wondrous qualities. I’d argue that’s because of two essential differences. Firstly, life is solving a completely different problem than computers do. Organisms are born into the world with a body, and challenged to survive as best they can in an open-ended environment. In contrast, most computer systems are tools for people, designed to perform specific tasks on demand. Of course they behave quite differently. The second difference is sophistication. Our engineered computer systems are incredible, but life still puts them to shame in terms of complexity, nuance, and efficiency.&nbsp;That’s not surprising, given life’s multi-billion-year head start.</p>



<p>When I say life is computation, what I really mean is there is nothing <em>supernatural</em> about life. As strange, beautiful, creative, and unpredictable as life is, I assume that this is ultimately the result of physics. Very particular arrangements of molecules interact in reliable ways to reproduce, perpetuate, and refine patterns of behavior we see as intelligent, without the need for any outside influence. This assumption is mostly out of practicality. Science can’t explain magic, so to give science a chance at this problem means entertaining the idea that there is no magic.</p>



<p>In Computer Science, there’s a concept called the “universal computer,” first proposed by Alan Turing. In a thought experiment, he designed a very simple machine and showed that it could run any computer program you could imagine. More importantly, he showed that <em>any</em> machine that has a few key properties is equivalent to the one he designed, and can <em>also</em> perform any arbitrary computation. In other words, computers come in all shapes and sizes. Each one has unique performance characteristics, which make it better suited for solving some problems than others, but at least in principle any universal computer can run any possible program.</p>



<p>That’s why I find the model of computation so appealing for natural intelligence. In life, there are many different kinds of intelligent systems, built very differently, with different specializations, functions, and performance characteristics. But as long as there’s no magic, then there’s some mechanical process under the hood producing these behaviors, and that’s computation. More importantly, if we can describe all these systems using the same language, we can compare them directly with each other, and talk about how they compose to form larger, more complex systems. The language of computation is general and expressive enough that I think it can do the job.</p>



<h3 class="wp-block-heading" id="what-kinds-of-intelligence-exist">What kinds of intelligence exist?</h3>



<p>Our world is filled with an enormous diversity of intelligent systems, many of which have their own dedicated fields of research. There’s cell biology, evolutionary biology, ecology, neuroscience, psychology, sociology, computer science, and many more. Specialists study one intelligent system, in the context of a particular academic discipline, using the tools and language of that discipline. This narrow focus is very valuable, but it means our understanding of intelligence is siloed, and we tend to categorize intelligent systems based on what academic field studies them, rather than their computational properties.</p>



<p>To advance our understanding, we must learn to see past the obvious differences of these systems and the unique, messy ways they manifest in nature. Instead, we should focus on what they have in common. What properties are broadly shared by many kinds of intelligence? What questions can we ask about all intelligent systems, and when we look at the answers, how do we compare apples to apples? I’m not at all sure how to do this, but there are a few properties that already stand out to me as interesting places to look:</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container is-layout-flow wp-block-group-is-layout-flow">
<ul class="wp-block-list"><li><strong>Substrates</strong>: What is the fabric this intelligence is built from? Molecules? Neurons? Transistors? People? What can that tell us about the strengths and weaknesses of that system? For instance, cells are molecular machines and thus constrained by the limitations of chemical processes like diffusion and catalysis. This severely limits the computational speed and physical size of cells, but it also provides a remarkably robust, efficient, and massively parallel form of computing that engineers can’t yet rival.</li><li><strong>Visibility</strong>: What information is available to learn from? For instance, natural selection is almost completely blind. The only signal of success is when an organism reproduces. On the other hand, humans have rich sensory perceptions and a wealth of knowledge and experience, all of which factor into cognition. The level of visibility affects what kinds of patterns a system can recognize, and how quickly it can find a workable solution to a problem.</li><li><strong>Purpose</strong>: What function does this intelligence serve? For most computer systems, a human decided the purpose in advance, then designed one particular solution to fulfill that need. On the other hand, many forms of intelligence are far more open-ended than that. Living things, human organizations, and even some forms of AI will strive to creatively fulfill their purpose, sometimes doing so in surprising ways. This can be very tricky, since a system&#8217;s &#8220;purpose&#8221; often isn&#8217;t clear, and can change over time.</li><li><strong>Interface</strong>: Within a substrate, there are no clear boundaries. The genes for digesting lactose are spread ambiguously between you and your gut bacteria, for instance. But boundaries between substrates are much sharper, because the parts are made of different stuff, and are not naturally interoperable. At these edges, there are narrow interfaces between intelligent systems, sharing just enough information from one to another that they can work together. That gives us a natural window into what properties of a system matter most for fulfilling a particular function.</li></ul>
</div></div>



<p>What I want to do here is to find useful ways of dividing up the world that might serve to integrate, compare, and contrast our notions of intelligence from different domains. I hope this will help to identify parallel examples, and to discover insights that can transfer from one domain to another. Life has evolved an incredible variety of learning processes, each optimized in different ways. Surely there are new algorithms and performance tricks waiting to be discovered. Perhaps we could even derive some general design principles for what learning tools work best under different constraints?</p>



<h3 class="wp-block-heading" id="what-does-life-compute">What does life compute?</h3>



<p>So, if all of life is one big, complex computation, what is it actually computing? In one sense, the answer is simple: life constructs ecosystems of organisms, and optimizes them to reproduce and thrive. That’s a fine answer in the abstract, but our experience is much more specific than that. Life is typically a &#8220;yes and&#8221; sort of process. Any successful way of doing things tends to stick around, and in doing so it shapes everything that can come after. In other words, life on planet Earth isn’t just “thriving” in some generic sense, it’s found a very particular way of doing that which we must study if we want to understand, predict, and influence its direction.</p>



<p>To make this more concrete, think about modern American society. We use GDP as a (flawed) proxy for human thriving. We use capitalism and an ecosystem of corporations to redistribute resources and prioritize work so as to improve GDP. Those corporations are made out of people, who run the company, make the decisions, do the work, provide the services, buy the products, and use them in  their daily lives. Those people have minds which are deeply embedded in cultural roles (citizen, employee, parent, etc.) and physical bodies, all of which have their own goals, limitations, preferences, and demands that tug the person in multiple directions at once. People are just one species, but we depend on countless others, which depend on each other and on the physical world, which is changing more rapidly than ever thanks to the power of human culture.</p>



<p>That system as a whole does things we want (like providing a comfortable standard of living for many) and things we don’t want (like instigating global climate change). It’s flawed, but we can’t start over from scratch, we can only try to steer it in a positive direction. That means understanding the structure of the system, and finding the points of high leverage, where a small nudge will have a big impact (and hopefully few side effects). To fight plastic pollution, should we invest in ocean clean-up, tell individuals to change their purchasing habits, tax corporations for plastic waste, or engineer plastic-eating bacteria? It’s hard to say what will work best, but if we can understand the major players, their incentives, and the ways they learn and adapt, we can make better educated guesses.</p>



<p>Describing and explaining all of life on Earth is impossible. It’s just too big, messy, complex, and rapidly changing, but that doesn’t bother me. The same is true of the source code for Google Search, but I worked with that system effectively for years. How? By using the engineering concept of a system architecture. If you know the major parts of a system, what they do, and how they fit together, you can say a lot about that system as a whole and navigate its subsystems with confidence. Of course, whatever model you make will be a huge oversimplification with many exceptions, but even a very rough model is profoundly useful.</p>



<p>This may be a pie-in-the-sky idea, but I’m fascinated by what a system architecture for life on Earth might look like. Could we really describe it all in one big picture? Could it be organized and subdivided in meaningful and useful ways, or is the real world just too messy? Could we use that model like an engineer does, to trace the steps leading to some behavior, identify the relevant subsystems, and make targeted interventions to change the system’s behavior?</p>



<p>More practically speaking, I’m interested in how intelligent systems compose with one another, even just two at a time. For instance, nascent research has shown great promise in using evolutionary algorithms to design architectures for deep learning. I hope that studying the relationship between mind and body might provide insights into how to integrate deep learning systems into other software, and how to balance the costs and benefits of intuitive thinking with other algorithms and heuristics. I’m also very interested in understanding the impact of social software and machine learning on society, and how to build software systems that conform to human values and ethics.</p>



<h3 class="wp-block-heading" id="conclusion">Conclusion</h3>



<p>That’s a brief tour of the intellectual domain I want to work in. I’m well aware it’s an enormous territory, and I surely won’t get to it all. My plan is to take a broad but shallow pass over many examples of natural intelligence, and to go deep on my study of computer algorithms and machine learning. I hope this will allow me to take full advantage of my technical skills and spend time searching for practical innovations that show the value of this way of thinking. In the meantime, I can work towards a general theory of intelligence in the background. It&#8217;s fine if I never get there, it&#8217;s just good to have lofty aspirations.</p>



<p>That said, all of this will surely change as I make progress. I’m figuring this out as I go along, and making course corrections all the time. I’m still learning about prior work, and I’ll have to adapt my own work to complement it. I’m not sure what ideas will prove to be dead ends, or what surprising new questions and opportunities I&#8217;ll discover along the way. That’s a good thing. I want to keep an open mind about this work, and let it evolve into what it needs to be. Still, I hope these questions will be a fruitful place to start.</p>



<p>I also have more ideas I didn’t cover here. I’ve got lots to say about evolution, so much so that it deserves its own post. I have many thoughts and observations about the inner workings of the mind. I’d love to explore the consequences of these ideas on how we understand the human condition, the structure of society, and conventions for the ethical AI. Frankly, this is such a big domain, there are so many places I could go, and I intend to follow my passion and curiosity.</p>



<p>As always, I’m very interested in feedback. Does what I said make sense? Do you have questions? Do you disagree, or have other ideas to share? Got advice on how to make this research more productive? Did I touch on something you’d like me to explore in more depth in a future post? If so, please leave a comment. I’d love to hear from you.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://thinkingwithnate.wordpress.com/2022/03/02/whats-the-big-idea/feed/</wfw:commentRss>
			<slash:comments>17</slash:comments>
		
		
		
		<media:thumbnail url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/03/vise-3-1.gif" />
		<media:content url="https://thinkingwithnate.wordpress.com/wp-content/uploads/2022/03/vise-3-1.gif" medium="image">
			<media:title type="html">vise-3-1</media:title>
		</media:content>

		<media:content url="https://0.gravatar.com/avatar/39ae42d33af99a13fa6ef7e7c81e595cc0e2e241e360bb495dc7e4c89ab517fd?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">nategaylinn</media:title>
		</media:content>
	</item>
	</channel>
</rss>
