<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: What ChatGPT Can Teach Us About Being Human	</title>
	<atom:link href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/feed/" rel="self" type="application/rss+xml" />
	<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/</link>
	<description>Becoming an intelligence researcher</description>
	<lastBuildDate>Thu, 20 Apr 2023 15:19:41 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-161</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Thu, 20 Apr 2023 15:19:41 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159#comment-161</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-160&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

One thing to keep in mind is that any &quot;knowledge&quot; gained in a conversation takes the form of... the text of the conversation itself. On each turn, the LLM is given the conversation so far and asked to add something to the end. So, its responses &lt;i&gt;are&lt;/i&gt; conditioned on what came before, but there&#039;s no semantic model of the information. That might explain the Brazil / Argentina example you tried.

The ratatouille example is a fun one! A nice way to think about that is in terms of vector embeddings of words (a la word2vec). The token &quot;spinach&quot; has a similar usage profile to traditional ratatouille ingredients like tomato, eggplant, and basil. The LLM doesn&#039;t know that&#039;s because these ingredients have complementary flavors, it just knows they co-occur in recipe documents. So, the idea of a ratatouille recipe with spinach is plausible, and may even appear in its source data. The token &quot;banana,&quot; on the other hand, is used so differently that putting it in ratatouille seems implausible, so the LLM behaves very differently.

When I brought up this self-correction issue, I was thinking of an example from this delightful &lt;a href=&quot;https://tech.lgbt/@freelanceastro@octodon.social/110115017528594711&quot; rel=&quot;nofollow ugc&quot;&gt;Mastodon post&lt;/a&gt;. Adam very easily got ChatGPT to agree that 3+4=8. That&#039;s interesting, because surely the correct answer is much more common in the data set. I&#039;m just speculating here, but perhaps the tokens for individual numerals are hard for an LLM to work with because they are so meaningless and interchangeable. The &quot;spinach&quot; / &quot;banana&quot; trick doesn&#039;t apply here.

I agree, whether LLMs can &quot;answer&quot; questions and how well is kinda ambiguous. I mostly bring it up because it&#039;s tempting to apply that word here, yet in this context it probably doesn&#039;t mean &lt;i&gt;everything&lt;/i&gt; that people usually expect that word to mean. We have to be careful not to anthropomorphize too much, and we may need to update our definitions / come up with new words to say precisely what we mean.

Great example of &quot;how to think&quot; knowledge! That is precisely what I had in mind. Another example might be logical categories. For example, &quot;a robin is a kind of bird&quot; and &quot;birds can fly,&quot; implies &quot;robins can fly.&quot; An LLM can generalize those kinds of relationships (to some extent, not perfectly) and even learn exceptions like &quot;a penguin is a kind of bird that &lt;i&gt;doesn&#039;t&lt;/i&gt; fly&quot; (again, imperfectly). I read somewhere recently that it seems this sort of logical reasoning is &lt;i&gt;not&lt;/i&gt; automatic for people, but must be learned culturally. So perhaps its another powerful cognitive tool that&#039;s passed on implicitly through language use.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-160">Thorin N. Tatge</a>.</p>
<p>One thing to keep in mind is that any &#8220;knowledge&#8221; gained in a conversation takes the form of&#8230; the text of the conversation itself. On each turn, the LLM is given the conversation so far and asked to add something to the end. So, its responses <i>are</i> conditioned on what came before, but there&#8217;s no semantic model of the information. That might explain the Brazil / Argentina example you tried.</p>
<p>The ratatouille example is a fun one! A nice way to think about that is in terms of vector embeddings of words (a la word2vec). The token &#8220;spinach&#8221; has a similar usage profile to traditional ratatouille ingredients like tomato, eggplant, and basil. The LLM doesn&#8217;t know that&#8217;s because these ingredients have complementary flavors, it just knows they co-occur in recipe documents. So, the idea of a ratatouille recipe with spinach is plausible, and may even appear in its source data. The token &#8220;banana,&#8221; on the other hand, is used so differently that putting it in ratatouille seems implausible, so the LLM behaves very differently.</p>
<p>When I brought up this self-correction issue, I was thinking of an example from this delightful <a href="https://tech.lgbt/@freelanceastro@octodon.social/110115017528594711" rel="nofollow ugc">Mastodon post</a>. Adam very easily got ChatGPT to agree that 3+4=8. That&#8217;s interesting, because surely the correct answer is much more common in the data set. I&#8217;m just speculating here, but perhaps the tokens for individual numerals are hard for an LLM to work with because they are so meaningless and interchangeable. The &#8220;spinach&#8221; / &#8220;banana&#8221; trick doesn&#8217;t apply here.</p>
<p>I agree, whether LLMs can &#8220;answer&#8221; questions and how well is kinda ambiguous. I mostly bring it up because it&#8217;s tempting to apply that word here, yet in this context it probably doesn&#8217;t mean <i>everything</i> that people usually expect that word to mean. We have to be careful not to anthropomorphize too much, and we may need to update our definitions / come up with new words to say precisely what we mean.</p>
<p>Great example of &#8220;how to think&#8221; knowledge! That is precisely what I had in mind. Another example might be logical categories. For example, &#8220;a robin is a kind of bird&#8221; and &#8220;birds can fly,&#8221; implies &#8220;robins can fly.&#8221; An LLM can generalize those kinds of relationships (to some extent, not perfectly) and even learn exceptions like &#8220;a penguin is a kind of bird that <i>doesn&#8217;t</i> fly&#8221; (again, imperfectly). I read somewhere recently that it seems this sort of logical reasoning is <i>not</i> automatic for people, but must be learned culturally. So perhaps its another powerful cognitive tool that&#8217;s passed on implicitly through language use.</p>
<p id="comment-like-161" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/?like_comment=161&#038;_wpnonce=f3508e2652" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-161" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-160</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Thu, 20 Apr 2023 09:00:47 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159#comment-160</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-159&quot;&gt;Nate Gaylinn (he/him)&lt;/a&gt;.

Yes, I was amused to see some examples of ChatGPT being asked to use a particular voice.  I may experiment with that at some point!

I recently learned a technical word for the sort of mostly meaningless &quot;mouth noise&quot; you mention--&quot;phatic.&quot;  Of course, one can also talk about pleasantries and formalities.

I did hear about Tay, without remembering which bot it was, so I assumed that each one learned new information from its conversations with the public.  It&#039;s good to know that&#039;s not generally the case these days.

Several times, I corrected GPT and it revised its answer to fix the error I pointed out.  I seem to recall it drawing on that new &quot;knowledge&quot; later in the conversation, too, though now I want to test that.  I imagine in most cases it mindlessly lets the user&#039;s corrections override whatever &quot;knowledge&quot; it has, rather than trying to verify what it&#039;s told.  But that&#039;s not the same as just nodding and agreeing and then ignoring a correction entirely.

Okay, I went again and had a brief test conversation!  I just lied to ChatGPT, telling it that Brazil and Argentina do not share a land border.  Oddly, it was able to adapt its list of countries that border Brazil to no longer include Argentina, but it couldn&#039;t adapt its list of countries that border Argentina not to include Brazil, even though I tried several times.  I then told it that spinach is an essential ingredient in ratatouille, and it caviled, saying that it is sometimes used as an ingredient but not admitting it was essential.  I then told it that bananas are an essential ingredient in ratatouille, and it held its ground even when I reiterated my lie three times, saying that it respectfully disagreed but that they are not typically included in the dish.  Way to go, bot!  So, based on that brief experiment, it may not learn from a single user very well, but it doesn&#039;t just automatically accept lies.  Based on previous longer conversations, I think if I were to continually lie about the same thing over and over, though, I might wear it down.

Your example about the broken bathroom is illustrative, but that&#039;s an exceptional case.  A lot of the time, people can answer questions perfectly well without understanding why the person who asked the question asked it.  So, I would say the fact that LLMs don&#039;t understand the intention behind a question doesn&#039;t mean they&#039;re not giving answers, just that they aren&#039;t necessarily giving good answers.  Incidentally, at one point I asked ChatGPT why it thought I&#039;d been asking the questions I had, and it came up with a very plausible (and sort-of true) speculation about my having wanting to explore certain themes.  So it seems it has some ability, if not to *understand* a user&#039;s intentions, at least to produce good *guesses* regarding them.

I think I would like to hear more about the &quot;how to think&quot; information that our use of language gives us access to.  I guess one easy example is that if a thing has a name, we don&#039;t just learn what the name for it is; we learn that it&#039;s worth giving that thing a name and thinking of it as a specific kind of thing.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-159">Nate Gaylinn (he/him)</a>.</p>
<p>Yes, I was amused to see some examples of ChatGPT being asked to use a particular voice.  I may experiment with that at some point!</p>
<p>I recently learned a technical word for the sort of mostly meaningless &#8220;mouth noise&#8221; you mention&#8211;&#8220;phatic.&#8221;  Of course, one can also talk about pleasantries and formalities.</p>
<p>I did hear about Tay, without remembering which bot it was, so I assumed that each one learned new information from its conversations with the public.  It&#8217;s good to know that&#8217;s not generally the case these days.</p>
<p>Several times, I corrected GPT and it revised its answer to fix the error I pointed out.  I seem to recall it drawing on that new &#8220;knowledge&#8221; later in the conversation, too, though now I want to test that.  I imagine in most cases it mindlessly lets the user&#8217;s corrections override whatever &#8220;knowledge&#8221; it has, rather than trying to verify what it&#8217;s told.  But that&#8217;s not the same as just nodding and agreeing and then ignoring a correction entirely.</p>
<p>Okay, I went again and had a brief test conversation!  I just lied to ChatGPT, telling it that Brazil and Argentina do not share a land border.  Oddly, it was able to adapt its list of countries that border Brazil to no longer include Argentina, but it couldn&#8217;t adapt its list of countries that border Argentina not to include Brazil, even though I tried several times.  I then told it that spinach is an essential ingredient in ratatouille, and it caviled, saying that it is sometimes used as an ingredient but not admitting it was essential.  I then told it that bananas are an essential ingredient in ratatouille, and it held its ground even when I reiterated my lie three times, saying that it respectfully disagreed but that they are not typically included in the dish.  Way to go, bot!  So, based on that brief experiment, it may not learn from a single user very well, but it doesn&#8217;t just automatically accept lies.  Based on previous longer conversations, I think if I were to continually lie about the same thing over and over, though, I might wear it down.</p>
<p>Your example about the broken bathroom is illustrative, but that&#8217;s an exceptional case.  A lot of the time, people can answer questions perfectly well without understanding why the person who asked the question asked it.  So, I would say the fact that LLMs don&#8217;t understand the intention behind a question doesn&#8217;t mean they&#8217;re not giving answers, just that they aren&#8217;t necessarily giving good answers.  Incidentally, at one point I asked ChatGPT why it thought I&#8217;d been asking the questions I had, and it came up with a very plausible (and sort-of true) speculation about my having wanting to explore certain themes.  So it seems it has some ability, if not to *understand* a user&#8217;s intentions, at least to produce good *guesses* regarding them.</p>
<p>I think I would like to hear more about the &#8220;how to think&#8221; information that our use of language gives us access to.  I guess one easy example is that if a thing has a name, we don&#8217;t just learn what the name for it is; we learn that it&#8217;s worth giving that thing a name and thinking of it as a specific kind of thing.</p>
<p id="comment-like-160" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/?like_comment=160&#038;_wpnonce=a9d4037105" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-160" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-159</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Tue, 18 Apr 2023 17:05:16 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159#comment-159</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-158&quot;&gt;Thorin N. Tatge&lt;/a&gt;.

I love how you describe it as &quot;talking to the whole world [...] polished off with a veneer [...] of politeness.&quot; That captures the situation really nicely, I think! It is like talking to the whole world, &#039;cuz LLMs draw from everything on the internet. And there is an important last step where they filter that response to give it a certain voice. There&#039;s a default voice, which is the average of the internet and sounds kinda authoritative, but you could give it &lt;i&gt;any&lt;/i&gt; voice you want, and that choice has a huge effect on how we perceive it.

Interesting question about how prediction plays into human language. I wonder. Certainly, we&#039;re always predicting what someone will say, which we notice most when they says something &lt;i&gt;else&lt;/i&gt;. It&#039;s jarring, grabs your attention, and often leads to follow-up questions. But in many conversations, that never happens. Sometimes we just make predictably common mouth noises at each other (&quot;Hey, how are you?&quot; &quot;Oh, fine, fine. Did you have a good weekend?&quot;) and the only meaning conveyed is: nothing is out of the ordinary.

Learning with LLMs is a tricky subject! There are at least three kinds of learning going on:

Training the foundation model with &quot;all of the internet&quot;
Fine-tuning that model with some subset of that data and / or a frame prompt
Maintaining context over the course of a conversation&lt;/i&gt;

What these models typically &lt;i&gt;don&#039;t&lt;/i&gt; do is propagate knowledge back through those steps. It may learn over the course of a conversation with you, but that &quot;knowledge&quot; is usually thrown away rather than integrated back into the earlier learning processes. That&#039;s probably a good thing, since it prevents trolls from corrupting the system, like &lt;a href=&quot;https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist&quot; rel=&quot;nofollow ugc&quot;&gt;Microsoft&#039;s Tay&lt;/a&gt;. That may not  be universally true, though, and LLM developers could certainly do it differently in the future.

One fun fact about GPT &quot;learning from its mistakes&quot;: when you correct GPT, it will usually politely agree with whatever you say. It doesn&#039;t seem to matter much whether GPT&#039;s first response was right or wrong, or what evidence is available. To some extent, the appearance of learning is just theater to smooth over the conversation.

When it comes to &quot;answers, opinions, and art&quot; it&#039;s really all about intent for me. Visual artists use images as a medium of expression. An AI has nothing to express except what the users asks it to or perhaps whatever themes are common on the internet. Real artists have truly new ideas that help society make sense of complex problems that matter in our daily lives. They cultivate skills and styles that fit what they want to say and how they want to say it. AI &quot;art&quot; is always missing one or both of these aspects, depending on the intent of whoever is using the tool. If we think art is important, then I think we have to make a distinction.

&quot;Answers&quot; are more complicated. In my mind, a good answer resolves the speaker&#039;s information need, which means understanding their intentions and interactions with the world. For instance, if someone asks &quot;where&#039;s the bathroom?&quot; I might answer &quot;down the hall and to the left,&quot; but a better answer might be &quot;Oh, our bathroom is out of service. You might try the Starbucks down the block.&quot; LLMs can generate likely answers, but have no conception of whether or not their answers are helpful or correct. I would argue those are important qualities of an &quot;answer.&quot;

Seeing what LLMs can do has definitely changed how I think about the language faculty and what sets people apart from other animals. I still think the primary value of language is being able to tap into a form of collective, social intelligence. What I&#039;m starting to appreciate more is just how &lt;i&gt;jam packed&lt;/i&gt; language is with implicit information about how to break the world into concepts, manipulate those concepts, and apply them to problem solving. I believe other animals have very similar mechanisms in their brain, but only operating on the their own ideas. With humans, we run vast amounts of external language data through those networks and train them up in ways that make them &lt;i&gt;vastly&lt;/i&gt; more effective. So, when a human taps into their local culture, they&#039;re not just getting facts and stories and art, they&#039;re getting a rich model of &quot;how to think&quot; that&#039;s a huge improvement on our biological starting point. Other species don&#039;t get that.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-158">Thorin N. Tatge</a>.</p>
<p>I love how you describe it as &#8220;talking to the whole world [&#8230;] polished off with a veneer [&#8230;] of politeness.&#8221; That captures the situation really nicely, I think! It is like talking to the whole world, &#8216;cuz LLMs draw from everything on the internet. And there is an important last step where they filter that response to give it a certain voice. There&#8217;s a default voice, which is the average of the internet and sounds kinda authoritative, but you could give it <i>any</i> voice you want, and that choice has a huge effect on how we perceive it.</p>
<p>Interesting question about how prediction plays into human language. I wonder. Certainly, we&#8217;re always predicting what someone will say, which we notice most when they says something <i>else</i>. It&#8217;s jarring, grabs your attention, and often leads to follow-up questions. But in many conversations, that never happens. Sometimes we just make predictably common mouth noises at each other (&#8220;Hey, how are you?&#8221; &#8220;Oh, fine, fine. Did you have a good weekend?&#8221;) and the only meaning conveyed is: nothing is out of the ordinary.</p>
<p>Learning with LLMs is a tricky subject! There are at least three kinds of learning going on:</p>
<p>Training the foundation model with &#8220;all of the internet&#8221;<br />
Fine-tuning that model with some subset of that data and / or a frame prompt<br />
Maintaining context over the course of a conversation</p>
<p>What these models typically <i>don&#8217;t</i> do is propagate knowledge back through those steps. It may learn over the course of a conversation with you, but that &#8220;knowledge&#8221; is usually thrown away rather than integrated back into the earlier learning processes. That&#8217;s probably a good thing, since it prevents trolls from corrupting the system, like <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist" rel="nofollow ugc">Microsoft&#8217;s Tay</a>. That may not  be universally true, though, and LLM developers could certainly do it differently in the future.</p>
<p>One fun fact about GPT &#8220;learning from its mistakes&#8221;: when you correct GPT, it will usually politely agree with whatever you say. It doesn&#8217;t seem to matter much whether GPT&#8217;s first response was right or wrong, or what evidence is available. To some extent, the appearance of learning is just theater to smooth over the conversation.</p>
<p>When it comes to &#8220;answers, opinions, and art&#8221; it&#8217;s really all about intent for me. Visual artists use images as a medium of expression. An AI has nothing to express except what the users asks it to or perhaps whatever themes are common on the internet. Real artists have truly new ideas that help society make sense of complex problems that matter in our daily lives. They cultivate skills and styles that fit what they want to say and how they want to say it. AI &#8220;art&#8221; is always missing one or both of these aspects, depending on the intent of whoever is using the tool. If we think art is important, then I think we have to make a distinction.</p>
<p>&#8220;Answers&#8221; are more complicated. In my mind, a good answer resolves the speaker&#8217;s information need, which means understanding their intentions and interactions with the world. For instance, if someone asks &#8220;where&#8217;s the bathroom?&#8221; I might answer &#8220;down the hall and to the left,&#8221; but a better answer might be &#8220;Oh, our bathroom is out of service. You might try the Starbucks down the block.&#8221; LLMs can generate likely answers, but have no conception of whether or not their answers are helpful or correct. I would argue those are important qualities of an &#8220;answer.&#8221;</p>
<p>Seeing what LLMs can do has definitely changed how I think about the language faculty and what sets people apart from other animals. I still think the primary value of language is being able to tap into a form of collective, social intelligence. What I&#8217;m starting to appreciate more is just how <i>jam packed</i> language is with implicit information about how to break the world into concepts, manipulate those concepts, and apply them to problem solving. I believe other animals have very similar mechanisms in their brain, but only operating on the their own ideas. With humans, we run vast amounts of external language data through those networks and train them up in ways that make them <i>vastly</i> more effective. So, when a human taps into their local culture, they&#8217;re not just getting facts and stories and art, they&#8217;re getting a rich model of &#8220;how to think&#8221; that&#8217;s a huge improvement on our biological starting point. Other species don&#8217;t get that.</p>
<p id="comment-like-159" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/?like_comment=159&#038;_wpnonce=92ec4c34fc" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-159" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thorin N. Tatge		</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-158</link>

		<dc:creator><![CDATA[Thorin N. Tatge]]></dc:creator>
		<pubDate>Mon, 17 Apr 2023 10:17:55 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159#comment-158</guid>

					<description><![CDATA[The picture is impressive!  Only after a careful look do I see the oddly shaped seam down the middle of the field, following the line of the image more than the line of the actual ground.

You&#039;re certainly right about people being excited about LLMs.  I keep seeing articles about it.  What jobs are being replaced, or changed, or augmented.  What attitude we should have about it.  At the convention I was at last weekend, the &quot;ChatGPT Takes the World by Storm&quot; panel was the feistiest one I encountered, with more than half the audience trying to get in comments.  They put ChatGPT on a projector and let it join the final minutes of the panel.

I&#039;ve had four chats with ChatGPT, and yes, it is good.  And I keep hearing about things it can be used for that didn&#039;t even occur to me.  Like writing job descriptions, for example.  (I don&#039;t really see how that would even work, but apparently lots of companies are doing it.)

To me, it feels something like talking to the whole world, amalgamated together and polished off with the veneer of expertise and politeness.  There&#039;s a tone that expert advice has, and a tone that recipe writers often have, and so on.  This program has its own generic tone like that.  It feels like it has a personality.  It&#039;s like visiting someone that you can predict pretty well, and who only occasionally surprises you, but whom it still feels wholesome to talk to.


Regarding LLMs being a one-trick pony, that makes sense, but it feels ironic because these things are being used for SO many different things.  Functionality, they&#039;re the very opposite of a one-trick pony.  The trick is language, but language is a hell of a trick.

And it&#039;s strange to think that what they&#039;re doing is &#039;predictive&#039;.  Predicting what someone real might say in a given situation, I guess.  Which makes me wonder how much of ordinary human speech could be called predictive in the same sense.  How often do people say what they imagine someone else would say?  A handful of people feel like that&#039;s their default mode to me, which I find annoying.

&quot;It’s surprising how LLM-like humans can be sometimes.&quot;  Yeah, that&#039;s a pretty profound takeaway.

You say today&#039;s LLMS don&#039;t learn from their mistakes.  Is that really true?  In chats with ChatGPT I&#039;ve had or read, it frequently apologizes for its errors and fixes them after they&#039;re pointed out.  It then remembers the right way to do or think about something for at least a sizable chunk of the conversation.  But... actually, I&#039;ve wondered.  Does the public version of ChatGPT modify itself based on what public users tell it?  I assume it probably does, since it would be such a waste to throw out all that information.  Come to think of it, it must, because I remember it and Bard and some other LLM were learning icky stuff from users, and not just within the space of a single conversation.

Or is the issue that you don&#039;t feel LLM &quot;learning&quot; is really learning, in a precise sense?

Is an image created by DALL·E actual art?  That&#039;s a good point.  The classic definition of art I remember from some writer like Carl Sagan, maybe, is basically self-expression that&#039;s not directly related to survival.  Art seems to imply some kind of intention, and these AIs don&#039;t have that.  So, yeah, I guess it&#039;s not really art, but it certainly feels logical to call it art.

I think it&#039;s fair to say that LLM &quot;opinions&quot; aren&#039;t really opinions, but I would say that their &quot;answers&quot; are really answers.

&quot;So, yeah, LLMs only do part of what humans do, but it’s a big and important part.&quot;  This observation seems to justify the notion that linguistic skill is the big thing that separates us mentally from other similarly sized mammals, rather than generic intelligence or other specific mental skills.  I wonder what you think of that.  I often write stories about &quot;talking animals,&quot; and I rarely feel the need to clarify that these animals not only have the ability to talk, but also possess a human-level intelligence.  Because that seems implied by the ability to talk itself.]]></description>
			<content:encoded><![CDATA[<p>The picture is impressive!  Only after a careful look do I see the oddly shaped seam down the middle of the field, following the line of the image more than the line of the actual ground.</p>
<p>You&#8217;re certainly right about people being excited about LLMs.  I keep seeing articles about it.  What jobs are being replaced, or changed, or augmented.  What attitude we should have about it.  At the convention I was at last weekend, the &#8220;ChatGPT Takes the World by Storm&#8221; panel was the feistiest one I encountered, with more than half the audience trying to get in comments.  They put ChatGPT on a projector and let it join the final minutes of the panel.</p>
<p>I&#8217;ve had four chats with ChatGPT, and yes, it is good.  And I keep hearing about things it can be used for that didn&#8217;t even occur to me.  Like writing job descriptions, for example.  (I don&#8217;t really see how that would even work, but apparently lots of companies are doing it.)</p>
<p>To me, it feels something like talking to the whole world, amalgamated together and polished off with the veneer of expertise and politeness.  There&#8217;s a tone that expert advice has, and a tone that recipe writers often have, and so on.  This program has its own generic tone like that.  It feels like it has a personality.  It&#8217;s like visiting someone that you can predict pretty well, and who only occasionally surprises you, but whom it still feels wholesome to talk to.</p>
<p>Regarding LLMs being a one-trick pony, that makes sense, but it feels ironic because these things are being used for SO many different things.  Functionality, they&#8217;re the very opposite of a one-trick pony.  The trick is language, but language is a hell of a trick.</p>
<p>And it&#8217;s strange to think that what they&#8217;re doing is &#8216;predictive&#8217;.  Predicting what someone real might say in a given situation, I guess.  Which makes me wonder how much of ordinary human speech could be called predictive in the same sense.  How often do people say what they imagine someone else would say?  A handful of people feel like that&#8217;s their default mode to me, which I find annoying.</p>
<p>&#8220;It’s surprising how LLM-like humans can be sometimes.&#8221;  Yeah, that&#8217;s a pretty profound takeaway.</p>
<p>You say today&#8217;s LLMS don&#8217;t learn from their mistakes.  Is that really true?  In chats with ChatGPT I&#8217;ve had or read, it frequently apologizes for its errors and fixes them after they&#8217;re pointed out.  It then remembers the right way to do or think about something for at least a sizable chunk of the conversation.  But&#8230; actually, I&#8217;ve wondered.  Does the public version of ChatGPT modify itself based on what public users tell it?  I assume it probably does, since it would be such a waste to throw out all that information.  Come to think of it, it must, because I remember it and Bard and some other LLM were learning icky stuff from users, and not just within the space of a single conversation.</p>
<p>Or is the issue that you don&#8217;t feel LLM &#8220;learning&#8221; is really learning, in a precise sense?</p>
<p>Is an image created by DALL·E actual art?  That&#8217;s a good point.  The classic definition of art I remember from some writer like Carl Sagan, maybe, is basically self-expression that&#8217;s not directly related to survival.  Art seems to imply some kind of intention, and these AIs don&#8217;t have that.  So, yeah, I guess it&#8217;s not really art, but it certainly feels logical to call it art.</p>
<p>I think it&#8217;s fair to say that LLM &#8220;opinions&#8221; aren&#8217;t really opinions, but I would say that their &#8220;answers&#8221; are really answers.</p>
<p>&#8220;So, yeah, LLMs only do part of what humans do, but it’s a big and important part.&#8221;  This observation seems to justify the notion that linguistic skill is the big thing that separates us mentally from other similarly sized mammals, rather than generic intelligence or other specific mental skills.  I wonder what you think of that.  I often write stories about &#8220;talking animals,&#8221; and I rarely feel the need to clarify that these animals not only have the ability to talk, but also possess a human-level intelligence.  Because that seems implied by the ability to talk itself.</p>
<p id="comment-like-158" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/?like_comment=158&#038;_wpnonce=6378dd831e" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-158" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-155</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Sat, 08 Apr 2023 02:30:29 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159#comment-155</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-144&quot;&gt;Michael Demko&lt;/a&gt;.

Speaking of how powerful the language faculty can be: https://arxiv.org/pdf/2303.17651.pdf]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-144">Michael Demko</a>.</p>
<p>Speaking of how powerful the language faculty can be: <a href="https://arxiv.org/pdf/2303.17651.pdf" rel="nofollow ugc">https://arxiv.org/pdf/2303.17651.pdf</a></p>
<p id="comment-like-155" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/?like_comment=155&#038;_wpnonce=0eda262d0b" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-155" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nate Gaylinn (he/him)		</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-145</link>

		<dc:creator><![CDATA[Nate Gaylinn (he/him)]]></dc:creator>
		<pubDate>Thu, 06 Apr 2023 17:52:24 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159#comment-145</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-144&quot;&gt;Michael Demko&lt;/a&gt;.

Oh, cool observations! That is pretty funny, actually  :)

In some sense, life has been &quot;doing math&quot; since forever. Cells need computation to manage homeostasis, for instance. They just can&#039;t solve arbitrary math problems on demand like a calculator. Language gave us the tools to reason, calculate, and model the world, and, as you say, formal math is just an offshoot of natural language, specialized for that purpose. What&#039;s fascinating is we then turned around and applied that math to model language. And we found real and complex mathematical rules for producing language! They&#039;re just... not reliable. Language is fuzzy and messy and full of exceptions, and algorithms like LLMs are fundamentally changing how linguists approach this work. Yet, it seems there are many constraints on language that shape its form and evolution and give rise to general &quot;laws&quot; of language that are surprisingly consistent across cultures.

So, as usual, math gives a clean and abstract model of a messy concrete reality. It&#039;s not the precursor of Life, the Universe, and Everything, but it is revealed as fundamental through how those things unfold (and &lt;i&gt;don&#039;t&lt;/i&gt; unfold) in reality. I guess the really cool part is how weird, messy language enables to talk with mathematical precision about the underlying abstractions of the Universe.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-144">Michael Demko</a>.</p>
<p>Oh, cool observations! That is pretty funny, actually  🙂</p>
<p>In some sense, life has been &#8220;doing math&#8221; since forever. Cells need computation to manage homeostasis, for instance. They just can&#8217;t solve arbitrary math problems on demand like a calculator. Language gave us the tools to reason, calculate, and model the world, and, as you say, formal math is just an offshoot of natural language, specialized for that purpose. What&#8217;s fascinating is we then turned around and applied that math to model language. And we found real and complex mathematical rules for producing language! They&#8217;re just&#8230; not reliable. Language is fuzzy and messy and full of exceptions, and algorithms like LLMs are fundamentally changing how linguists approach this work. Yet, it seems there are many constraints on language that shape its form and evolution and give rise to general &#8220;laws&#8221; of language that are surprisingly consistent across cultures.</p>
<p>So, as usual, math gives a clean and abstract model of a messy concrete reality. It&#8217;s not the precursor of Life, the Universe, and Everything, but it is revealed as fundamental through how those things unfold (and <i>don&#8217;t</i> unfold) in reality. I guess the really cool part is how weird, messy language enables to talk with mathematical precision about the underlying abstractions of the Universe.</p>
<p id="comment-like-145" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/?like_comment=145&#038;_wpnonce=586027159e" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-145" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Michael Demko		</title>
		<link>https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/comment-page-1/#comment-144</link>

		<dc:creator><![CDATA[Michael Demko]]></dc:creator>
		<pubDate>Thu, 06 Apr 2023 00:34:34 +0000</pubDate>
		<guid isPermaLink="false">http://thinkingwithnate.wordpress.com/?p=159#comment-144</guid>

					<description><![CDATA[It does seem that a lot of what we do that we think of as &quot;thinking&quot; could just be a naive application of linguistic &quot;fluency&quot; routines to some &quot;starting point&quot; of a seed sentence or two. This is pretty much exclusively the LLM talent. It&#039;s kind of funny, in retrospect, that some schools of linguistics have tried to &quot;break language down&quot; into semantics expressed in a more mathematical logic system, as if that were more fundamental (by analogy with physics), when mathematical logic systems now seem to me to be some sort of intensification of our predilection for stringing words together according to loose systems based on an intuitive sense of familiarity.  In other words, language came first, mathematics was layered on top of that, and turned out to be really good for modeling certain kinds of physical systems, and so we mistook mathematics as something which pre-dated all the rest.]]></description>
			<content:encoded><![CDATA[<p>It does seem that a lot of what we do that we think of as &#8220;thinking&#8221; could just be a naive application of linguistic &#8220;fluency&#8221; routines to some &#8220;starting point&#8221; of a seed sentence or two. This is pretty much exclusively the LLM talent. It&#8217;s kind of funny, in retrospect, that some schools of linguistics have tried to &#8220;break language down&#8221; into semantics expressed in a more mathematical logic system, as if that were more fundamental (by analogy with physics), when mathematical logic systems now seem to me to be some sort of intensification of our predilection for stringing words together according to loose systems based on an intuitive sense of familiarity.  In other words, language came first, mathematics was layered on top of that, and turned out to be really good for modeling certain kinds of physical systems, and so we mistook mathematics as something which pre-dated all the rest.</p>
<p id="comment-like-144" data-liked=comment-not-liked class="comment-likes comment-not-liked"><a href="https://thinkingwithnate.wordpress.com/2023/04/05/what-chatgpt-can-teach-us-about-being-human/?like_comment=144&#038;_wpnonce=ba1f5ccec3" class="comment-like-link needs-login" rel="nofollow" data-blog="201189235"><span>Like</span></a><span id="comment-like-count-144" class="comment-like-feedback">Like</span></p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
