and the lora_alpha for training is set to 8. During
inference, we set the maximum token to be 4192
for GSM8K and the maximum token as 8192 for
AIME correspondingly.
DeepSeek-R1-Distill-Llama-8B.
Our
exper-
iment
on
DeepSeek-R1-Distill-Llama-8B2
is
conducted using the MixChain-zero-shot-GSM8K
dataset. The batch size is set to 128, and training
is performed for a maximum of five epochs. To
ensure that the inference process successfully
generates the final answer, we set the maximum
token limit to 30K.
Qwen2.5-32B-LIMO.
We
fine-tuned
Qwen-
32B-Instruct using LIMO, training on four H100
GPUs for 10 epochs with a batch size of 4 and a
maximum sequence length of 16K. The learning
rate was set to 5e-6. We define Qwen-32B-Instruct
as θ0 and the trained model as θ1, treating the up-
date direction between them as ∆θ. By adjusting
α, we generated the MixChain-C-LIMO dataset,
which includes two solutions: solution 1 (α=0.8)
and solution 0 (α=0.6).
Based on this, we further trained θ2 for 5 epochs
with a batch size of 32, a learning rate of 5e-6,
and a weight decay of 0.01, obtaining the results of
MixChain-Solution 0 in Table 2. This model can be
further refined through CoT-Valve (Results: CoT-
Valve + MixChain - Solution 0). Unlike previous
experiments, we applied full fine-tuning instead of
LoRA. The maximum generated sequence length
in this experiment was 15K.
A.3
Dataset Explanation
As detailed in Section 4.2, we constructed two
types of datasets: MixChain-C and MixChain-Z.
The statistics for the datasets are shown in 9. For
these datasets, we select α values ranging from
[0.6, 0.8] for LIMO and [0.2, 0.4, 0.6, 0.8] for
other datasets, ensuring all incorrect responses are
excluded.
For MixChain-Z, while the training transition
from θ1 to θ2 remains a black box, we can still
identify numerous model pairs such as Qwen-32B-
Instruct →QwQ-32B-Preview, and LLaMA-3.1-
8B →R1-Distill-Llama-8B, as documented in the
technical report. We find that the performance of
the base model significantly influences the quality
of the dataset.
2https://huggingface.co/deepseek-ai/DeepSeek-R1-
Distill-Llama-8B
11

Dataset
Solution Index
#Samples
#Avg Token
GSM8K
Ground-Truth
1
7473
121.8
MixChain-C
1
22419
294.8
0 (Ground-Truth)
116.0
1
279.6
2
310.7
3
386.7
MixChain-Z
4
6863
497.2
PRM12K
Ground-Truth
1
12000
223.1
0 (Ground-Truth)
172.3
1
583.2
2
613.7
3
739.3
MixChain-Z
4
