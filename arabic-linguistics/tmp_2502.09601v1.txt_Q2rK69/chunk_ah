Llama-3.3-70B-Instruct
92.6
235.4
0.56
Llama-3.1-405B-Instruct
95.6
186.7
0.13
Qwen2.5-32B-Instruct
93.1
269.3
1.09
Qwen2.5-Math-72B-Instruct
95.8
312.1
0.43
QwQ-32B-Preview
95.1
741.1
0.40
Prompt (Han et al., 2024)
93.6
355.5
0.82
Prompt (Ding et al., 2024)
95.5
617.7
0.48
In-domain Train Set: GSM8K
CoT-Valve - Ground-Truth
94.0
352.8
0.83
CoT-Valve++ - MixChain-C
94.4
276.3
1.07
CoT-Valve+P - MixChain-Z
96.1
317.1
0.95
CoT-Valve+P - MixChain-Z
94.9
225.5
1.32
Out-of-Domain Train Set: PRM12K
Overthink(Chen et al., 2024) - SFT
94.8
749.5
0.40
Overthink(Chen et al., 2024) - SimPO
94.8
326.2
0.91
O1-Pruner(Luo et al., 2025a) - SFT
95.7
717
0.42
O1-Pruner(Luo et al., 2025a)
96.5
534
0.56
CoT-Valve+P - MixChain-Z
95.4
288.5
1.03
Table 1: Results of QwQ-32B-Preview on GSM8K. Val-
ues of ACU are scaled by 102 for readability. We list
the dataset we use after the method name.
4.3
From Long-CoT to Short-CoT.
Controllable Results.
We illustrate the result in
Figure 3a. First, using ground-truth samples as a
cold start, we develop a model capable of generat-
ing reasoning paths of various lengths, as demon-
strated in ‘CoT-Valve’ in Figure 3a. CoT-Valve
already matches the performance of prompt-based
control but can generate shorter reasoning chains.
We then extrapolate ∆θ to produce even shorter rea-
soning paths. Then, building on MixChain-C from
this first model, we conduct further training by CoT-
Valve++. CoT-Valve++ substantially surpasses the
baseline and shows greater generalization capabili-
ties in cases of extrapolation.
Compression Results.
We evaluated our method
against previous chain compression approaches,
with the results detailed in Table 1, Table 2, and
6

Method
AIME24
#Token
ACU↑
Qwen2.5-32B-Instruct
4/30
1794.2
0.023
Qwen2.5-Math-72B-Instruct
