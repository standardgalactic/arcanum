7/30
1204.5
0.061
Gemini-Flash-Thinking (Team et al., 2023)
15/30
10810.5
-
QwQ-32B-Preview.Train set: GSM8K
QwQ-32B-Preview
14/30
6827.3
0.021
Prompt (Han et al., 2024)
13/30
6102.5
0.022
Prompt (Ding et al., 2024)
13/30
5562.3
0.024
Overthink (Chen et al., 2024)
13/30
5154.5
0.026
CoT-Valve - GSM8K
14/30
5975.0
0.024
CoT-Valve++ - MixChain-C
13/30
5360.5
0.025
CoT-Valve+P - MixChain-Z
13/30
4629.6
0.029
Qwen-32B-Instruct. Train set: LIMO
Qwen-32B-LIMO
15/30
10498.2
0.015
CoT-Valve
11/30
6365.2
0.018
SFT - MixChain - Solution 1
13/30
5368.0
0.025
CoT-Valve - MixChain - Solution 1
15/30
8174.8
0.019
Table 2: Results of QwQ-32B-Preview and Qwen-32B-
Instruct w/ LIMO on AIME 24.
GSM8k
AIME24
Model
Acc
#Token
Acc
# Token
Llama-3.1-8B (0-shot)
15.7
915.0
0/30
1517.6
R1-Distill-Llama-8B
87.1
1636.6
14/30
12359.9
CoT-Valve
87.3
1315.2
6/30
7410.5
CoT-Valve+P - MixChain-Z
84.0
755.2
11/30
9039.0
Table 3: Result of DeepSeek-R1-Distill-Llama-8B.
Table 3. For GSM8K, we adhered to the baseline
setup to train with PRM12K. Utilizing progressive
compression, our method surpassed the baseline by
producing shorter reasoning paths and improved
performance.
We also report experimental results on AIME,
where the model was trained using MixChain-Z de-
rived from GSM8K. To minimize the impact of ran-
domness on performance, we employed greedy de-
coding in our AIME experiments. Compared to the
baseline (Chen et al., 2024), our method reduced
the token count from 5155 to 4630 while maintain-
ing the same accuracy, despite being trained on an
easier dataset.
4.4
From Short-CoT to Long-CoT &
Short-Long-Short CoT
