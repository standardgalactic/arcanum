Our method can also be applied if a short-CoT
model is distilled or post-trained to be a Long-CoT
model. The results are shown in Figure 3b, Table
4 and Table 5. We found that CoT-Valve can also
effectively control the length of the chains in this
setting. Notably, we observed that shorter chains
could achieve higher accuracy on GSM8K. More-
over, if the model is trained using the MixChain-Z
dataset, the results are significantly better, whether
using CoT-Valve (55.5 to 58.9) or just simply SFT
Method
Accuracy
#Tokens
ACU↑
LLaMA-3.2-1B-Instruct(8-shot)
45.9
104.3
44.008
LLaMA-3.2-1B-Instruct(0-shot)
45.9
199.8
22.973
SFT-Full Finetune - GSM8k
46.1
139.4
33.070
SFT - GSM8k
43.8
137.7
31.808
Prompt
46.7
209.9
22.249
SFT - QwQ Distill
52.7
759.3
6.941
CoT-Valve - QwQ Distill
55.5
267.0
20.786
CoT-Valve+P - MixChain-Z
55.8
291.0
19.175
SFT - MixChain-Z - Solution 1
57.0
288.4
19.764
CoT-Valve - MixChain-Z - Solution 1
58.9
275.4
21.387
Table 4: Results on LLaMA-3-2-1B-Instruct. We report
the result of Flexible Match here. QwQ Distill means
we use QwQ to synthesize the solution and distill it.
Method
Accuracy
#Tokens
ACU↑
LLaMA-3.1-8B (8-shot)
56.9
282.1
2.521
LLaMA-3.1-8B (0-shot)
15.7
915.0
0.214
SFT-LoRA - GSM8k
59.0
191.9
3.843
SFT-LoRA - QwQ Distill
76.3
644.8
1.479
CoT-Valve - QwQ Distill
77.5
569.8
1.700
CoT-Valve+P - MixChain-Z
77.1
371.2
2.596
CoT-Valve + MixChain-Z - Solution 1
75.7
264.1
3.583
Table 5: Result on LLaMA-3.1-8B. We report the result
of Strict Match here.
(52.7 to 57.0). Additionally, after training a long-
chain model, we can employ the MixChain dataset
to reduce the length of its reasoning chains further.
As illustrated in Figure 3c, the results suggest that
initially training the chains to be long and subse-
quently compressing them to be shorter (Results
with Long-to-Short) can yield better performance
than directly using CoT-Valve in the short-to-long
stage (Results with Short-to-Long). This demon-
