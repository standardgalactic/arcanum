ings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 9426–9439, Bangkok, Thailand. Associ-
ation for Computational Linguistics.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2023. Self-consistency improves
chain of thought reasoning in language models. In
The Eleventh International Conference on Learning
Representations.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in neural
information processing systems, 35:24824–24837.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L. Griffiths, Yuan Cao, and Karthik R
Narasimhan. 2023.
Tree of thoughts: Deliberate
problem solving with large language models.
In
Thirty-seventh Conference on Neural Information
Processing Systems.
Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie
Xia, and Pengfei Liu. 2025. Limo: Less is more for
reasoning. arXiv preprint arXiv:2502.03387.
Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov.
2024.
Distilling system 2 into system 1.
ArXiv,
abs/2407.06023.
Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang
Li, and Wanli Ouyang. 2024.
Accessing gpt-4
level mathematical olympiad solutions via monte
carlo tree self-refine with llama-3 8b.
Preprint,
arXiv:2406.07394.
Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen
Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jin-
gren Zhou, and Junyang Lin. 2025. The lessons of
developing process reward models in mathematical
reasoning. Preprint, arXiv:2501.07301.
10

A
Implementation Details
A.1
Evaluation Metric.
For experiments on LLaMA, we use lm-eval-
harness1 to evaluate the model performance. For
LLaMA-3.1-8B, we report the strict matching met-
ric due to observed repetition in the model’s re-
sponses, which causes the flexible match to ex-
tract incorrect numerical values. For LLaMA-3.2-
1B-Instruct, we report results using the flexible
match metric. For QwQ-32B-Preview, DeepSeek-
R1-Distill-Llama-8B and Qwen-2.5B-LIMO, we
first extract the result enclosed within \boxed{}. If
no such boxed answer is found, we default to using
the last digit in the response as the final answer.
A.2
Training Setting.
LLaMA-3.1-8B
The model is trained using eight
A5000 24GB GPUs. We set the batch size to 64 and
the peak learning rate to 4e-5, following a cosine
decay schedule. A weight decay of 0.01 is applied.
For the progressive chain compression experiment,
we train the model for two epochs with each type
of solution. For all other experiments, we train for
a maximum of eight epochs. For LoRA, the rank
is set to 32, and the lora_alpha for training is set
to 64. During inference, the maximum number of
tokens is set to 2048.
LLaMA-3.2-1B-Instruct
The model is trained
using 8 A5000 24GB GPUs. We set the batch size
to 8 for the CoT-Valve experiment and 64 for all
other experiments. The peak learning rate is 4e-5,
following a cosine decay schedule, except for the
SFT - GSM8K experiment, where the peak learning
rate is 1e-5. A weight decay of 0.01 is applied. For
the CoT-Valve and SFT-Full Finetune - GSM8k
experiment, we train for a maximum of four and
six epochs, respectively. For the progressive chain
compression experiment, we train the model for
two epochs with each type of solution. For all
other experiments, training is conducted for up to
8 epochs. For LoRA, the rank is set to 32, and
the lora_alpha for training is set to 64. During
inference, the maximum number of tokens is set to
2048.
QwQ-32B-Preview.
The model is trained on two
H100-80G GPUs. We set the batch size to 64 and
trained for a maximum of five epochs. The learning
rate is 1e-5, with a weight decay of 0.01 applied
1https://github.com/EleutherAI/lm-evaluation-harness
during training. For LoRA, the rank is set to 2,
