References
Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien
Bubeck, Ronen Eldan, Suriya Gunasekar, Michael
Harrison, Russell J Hewett, Mojan Javaheripi, Piero
Kauffmann, et al. 2024. Phi-4 technical report. arXiv
preprint arXiv:2412.08905.
Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He,
Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu,
Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do
not think that much for 2+ 3=? on the overthinking
of o1-like llms. arXiv preprint arXiv:2412.21187.
Jeffrey Cheng and Benjamin Van Durme. 2024. Com-
pressed chain of thought: Efficient reasoning through
dense representations. Preprint, arXiv:2412.13171.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021a. Training verifiers to solve math word prob-
lems. ArXiv, abs/2110.14168.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021b. Training verifiers to solve math word prob-
lems. Preprint, arXiv:2110.14168.
DeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-
soning capability in llms via reinforcement learning.
Preprint, arXiv:2501.12948.
Yuntian Deng, Yejin Choi, and Stuart Shieber. 2024a.
From explicit cot to implicit cot: Learning to inter-
nalize cot step by step. Preprint, arXiv:2405.14838.
Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul
Smolensky, Vishrav Chaudhary, and Stuart Shieber.
2024b.
Implicit chain of thought reasoning via
knowledge distillation.
Mengru Ding, Hanmeng Liu, Zhizhang Fu, Jian Song,
Wenbo Xie, and Yue Zhang. 2024. Break the chain:
Large language models can be shortcut reasoners.
arXiv preprint arXiv:2406.06580.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, et al. 2024. The llama 3 herd of models. arXiv
preprint arXiv:2407.21783.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel
Roy, and Michael Carbin. 2020. Linear mode con-
nectivity and the lottery ticket hypothesis. In Inter-
national Conference on Machine Learning, pages
3259–3269. PMLR.
Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang,
Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.
2025a. rstar-math: Small llms can master math rea-
soning with self-evolved deep thinking. Preprint,
arXiv:2501.04519.
Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang,
Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.
2025b.
rstar-math: Small llms can master math
reasoning with self-evolved deep thinking. arXiv
preprint arXiv:2501.04519.
Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing
Ma, Zhenyu Chen, and Zhenting Wang. 2024.
Token-budget-aware llm reasoning. arXiv preprint
arXiv:2412.18547.
Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li,
Zhiting Hu, Jason Weston, and Yuandong Tian. 2024.
Training large language models to reason in a contin-
uous latent space. Preprint, arXiv:2412.06769.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
and Laurent Sifre. 2022. Training compute-optimal
large language models. Preprint, arXiv:2203.15556.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations.
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-
man, Suchin Gururangan, Ludwig Schmidt, Han-
naneh Hajishirzi, and Ali Farhadi. 2022.
Edit-
ing models with task arithmetic.
arXiv preprint
arXiv:2212.04089.
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-
son, Ahmed El-Kishky, Aiden Low, Alec Helyar,
Aleksander Madry, Alex Beutel, Alex Carney, et al.
2024.
Openai o1 system card.
arXiv preprint
arXiv:2412.16720.
Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan,
Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin
