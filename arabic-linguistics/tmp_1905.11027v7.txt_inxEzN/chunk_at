σminλ+
i (J(ˆθ)) + 1
N

.
If σ = σ1, then σmax = σmin = σ. Both “≤” and “≥” in the above inequalities become tight.
Appendix F
Probability Measures on M
Probability measures are not defined on the lightlike M, because along the lightlike geodesics,
the distance is zero. To compute the integral of a given function f(θ) on M one has to first
choose a proper Riemannian submanifold Ms ⊂M specified by an embedding θ(θs), whose
metric is not singular. Then, the integral on Ms can be defined as
R
M s f (θ(θs)) dθs, where Ms
is the sub-manifold associated with the frame θs = (θ1, · · · , θd), so that T Ms = S(T M), and
the induced Riemannian volume element as
dθs =
p
|I(θs)| dθ1 ∧dθ2 ∧· · · ∧dθd
=
p
|I(θs)| dEθs,
(21)
where dEθ is the Euclidean volume element. We artificially shift θ to be positive definite and
define the volume element as
dθ :=
p
|I(θ) + ε1I| dθ1 ∧dθ2 ∧· · · ∧dθD
=
p
|I(θ) + ε1I| dEθs,
(22)
where ε1 > 0 is a very small value as compared to the scale of I(θ) given by
1
Dtr(I(θ)), i.e. the
average of its eigenvalues. Notice this element will vary with θ: different coordinate systems will
23

yield different volumes. Therefore it depends on how θ can be uniquely specified. This is roughly
guaranteed by our A1: the θ-coordinates correspond to the input coordinates (weights and biases)
up to an orthogonal transformation. Despite that eq. (22) is a loose mathematical definition, it
makes intuitive sense and is convenient for making derivations. Then, we can integrate functions
Z
M
f(θ)dθ =
Z
f(θ)
p
|I(θ) + ε1I| dEθ,
(23)
where the RHS is an integration over RD, assuming θ is real-valued.
Using this tool, we first consider Jeffreys’ non-informative prior on a sub-manifold Ms , given
by
pJ(θs) =
p
|I(θs)|
R
Ms
p
|I(θs)|dEθs .
(24)
It is easy to check
R
Ms p(θs)dEθs = 1. This prior may lead to similar results as [7, 60], i.e. a
“razor” of the model Ms. However, we will instead use a Gaussian-like prior, because Jeffreys’
prior is not well defined on M. Moreover, the integral
R
Ms
p
|I(θs)|dEθs is likely to diverge
based on our revised volume element in eq. (22). If the parameter space is real-valued, one can
easily check that, the volume based on eq. (22) along the lightlike dimensions will diverge. The
zero-centered Gaussian prior corresponds to a better code, because it is commonly acknowledged
that one can achieve the same training error and generalization without using large weights. For
example, regularizing the norm of the weights is widely used in deep learning. By using such an
informative prior, one can have the same training error in the first term in eq. (2), while having a
smaller “complexity” in the rest of the terms, because we only encode such models with constrained
weights. Given the DNN, we define an informative prior on the lightlike neuromanifold
p(θ) = 1
V exp

−1
2ε2
2
∥θ∥2
 p
|I(θ) + ε1I|,
(25)
where ε2 > 0 is a scale parameter of θ, and V is a normalizing constant to ensure
R
p(θ)dEθ = 1.
Here, the base measure is the Euclidean volume element dEθ, as
p
|I(θ) + ε1I| already appeared
in p(θ). Keep in mind, again, that this p(θ) is defined in a special coordinate system, and is not
invariant to re-parametrization. By A1, this distribution is also isotropic in the input coordinate
system, which agrees with initialization techniques7.
This bi-parametric prior connects Jeffreys’ prior (that is widely used in MDL) and a Gaussian
prior (that is widely used in deep learning). If ε2 →∞, ε1 →0, it coincides with Jeffreys’ prior (if
it is well defined and I(θ) has full rank); if ε1 is large, the metric (I(θ) + ε1I) becomes spherical,
