The off-diagonal block is zero-matrix (denoted as 0) because
Ep
∂log p(x | ψ, θ)
∂ψ
∂log p(x | ψ, θ)
∂θ⊺

= Ep
∂log p(z | ψ)
∂ψ
∂log p(y | z, θ)
∂θ⊺

= Ep(z|ψ)
∂log p(z | ψ)
∂ψ
Ep(y|z, θ)
∂log p(y | z, θ)
∂θ
⊺
= 0,
where Ep(y|z, θ)

∂log p(y|z, θ)
∂θ

is the expectation of the score function and is always zero. The
metric I(ψ, θ) is a product metric, meaning that the geometry of Mθ defined by I(θ) can be
studied separately to the geometry of Mψ.
As we are interested in the predictive model corresponding to the diagonal block I(θ), we
further have (see e.g. [51][64] for derivations)
I(θ) = Ep(z)
∂hL(z)
∂θ
⊺
C(z) ∂hL(z)
∂θ

,
(7)
where the expectation is taken w.r.t. p(z) := p(z | ψ), an underlying true distribution in the
input space depending on the parameter ψ.
∂hL(z)
∂θ
is the m × D parameter-output Jacobian
matrix, based on a given input z, C(z) := diag (o(z)) −o(z)o(z)⊺⪰0, diag (·) means the
diagonal matrix with the given diagonal entries, and o(z) := SoftMax(hL(z)) is the predicted
class probabilities of z. By the definition of SoftMax, each dimension of o(z) represents a positive
probability, although o(z) can be arbitrarily close to a one-hot vector. As a result, the kernel of
6In fact, a generalization of the Bernoulli distribution with integer k ≥2 mutually exclusive events, called
informally a multinoulli distribution since it is a multinomial distribution with a single trial.
6

the psd matrix C(z) is given by {λ1 : λ ∈R}, where 1 is the vector of all 1’s. See our analysis
in appendix B.
In eq. (7), I(θ) is the single-observation FIM. It is obvious that the FIM w.r.t. the joint
distribution p(X | θ) of multiple observations is NI(θ) (Fisher information is additive), so that
I(θ) does not scale with N. Notice that we use X, Y and Z to denote a collection of N random
observations and use x, y and z to denote one single observation.
In general, to compute I(θ) needs to assume p(z), which depends on the parameter ψ. This
makes sense as (ψ1, θ) and (ψ2, θ) with ψ1 ̸= ψ2 are different points on the product manifold
Mψ × Mθ and thus their I(θ) should be different. In practice, one only gets access to a set of
N i.i.d. samples drawn from an unknown p(z | ψ). In this case, it is reasonable to take p(z) in
eq. (7) to be the empirical distribution ˆp(z) so that p(z) = ˆp(z) := 1
N
PN
i=1 δ(z −zi), then
I(θ) = ˆI(θ) := 1
N
N
X
i=1
∂hL(zi)
∂θ
⊺
C(zi) ∂hL(zi)
∂θ

.
(8)
In fact, one can skip assuming a generative model p(z | ψ) and choosing a ψ. ˆI(θ) can be directly
computed from the observed zi’s and does not depend on the observed yi’s. Although denoted
differently than I(θ) in the current paper, this ˆI(θ) is a standard version of the definition of the
FIM for neural networks [35, 42, 64, 65].
By considering the neural network weights and biases as random variables satisfying a
prescribed prior distribution [30, 54], this I(θ) can be regarded as a random matrix [43] depending
on the structure of the DNN and the prior. The empirical density of I(θ) is the empirical
distribution of its eigenvalues {λi}D
i=1, that is, ρD(λ) =
1
D
PD
i=1 δ(λi). If at the limit D →∞,
the empirical density converges to a probability density function (pdf), then
ρI(λ) := lim
D→∞ρD(λ)
(9)
is called the spectral density of the Fisher information matrix.
For DNN, we assume that
(A1) At the MLE ˆθ, the prediction SoftMax(hL(zi)) perfectly recovers (tending to be one-hot
