8
The Razor based on Jeffreys’ Non-informative Prior
Jeffreys’ prior is specified by pJ(θ) ∝
p
|I(θ)|. It is non-informative in the sense that no neural
network model θ1 is prioritized over any other model θ2. It is invariant to the choice of the
coordinate system. Under a reparameterization θ →η,
p
|I(η)|dη =
s
∂θ
∂η
⊺
I(θ)∂θ
∂η
 · dη =
p
|I(θ)| ·

∂θ
∂η
 dη

=
p
|I(θ)|dθ,
showing that the Riemannian volume element is the same in different coordinate systems.
Unfortunately, the Jeffreys’ prior is not well defined on the lightlike neuromanifold M, where
the metric I(θ) is degenerate and
p
|I(θ)| becomes zero. The stratifold structure of M, where
d(θ) varying with θ ∈M, makes it difficult to properly define the base measure dθ and integrate
functions as in eq. (12). From a mathematical standpoint, one has to integrate on the screen
distribution S(T M), which has a Riemannian structure. We refer the reader to [29, 66] for other
extensions of Jeffreys’ prior.
In this paper, we take a simple approach by examining a submanifold of M denoted as f
M
and parameterized by ξ, which has a Riemannian metric I(ξ) ≻0 that is induced by the FIM
I(θ) ⪰0. The dimensionality of f
M is upper-bounded by the local dimensionality d(θ). Any
infinitesimal dynamic on f
M means such a change of neural network parameters that leads to
a non-zero change of the global predictive model z →y. Therefore, the following results are
constrained to the choice of the submanifold f
M.
In eq. (12), let κ(ξ) =
p
|I(ξ)|. We further assume
(A6) 0 <
R
f
M
p
|I(ξ)|dξ < ∞;
meaning that the Riemannian volume of f
M is bounded. After straightforward derivations, we
arrive at
OJ(ξ) = −log p(X | ˆξ) + log
Z
f
M
p
|I(ξ)|dξ
−log
Z
f
M
exp

−N
2 (ξ −ˆξ)⊺J(ˆξ)(ξ −ˆξ)
 p
|I(ξ)|dξ.
(16)
Let us examine the meaning of OJ(ξ).
As I(ξ) is the Riemannian metric of f
M based
on information geometry, |I(ξ)|
1
2 dξ is a Riemannian volume element (volume form). In the
second term on the RHS of eq. (16), the integral
R
f
M |I(ξ)|
1
2 dξ is the information volume, or
the total “number” of different DNN models [44] on f
M.
In the last (third) term, because
ω(ξ) := exp

−N
2 (ξ −ˆξ)⊺J(ˆξ)(ξ −ˆξ)

≤1, the integral on the LHS of
Z
f
M
exp

