vectors) the training target yi, for all the training samples (zi, yi).
In this case, the negative Hessian of the average log-likelihood
J(θ) := −1
N
∂2 log p(X | θ)
∂θ∂θ⊺
= −1
N
N
X
i=1
∂2 log p(yi | zi, θ)
∂θ∂θ⊺
is called the observed FIM (sample-based FIM), which is also known as the “empirical Fisher” in
machine learning literature [35, 42]. In our notations explained in table 1, the FIM I depends
on the true distribution p(z) and does not depend on the observed samples. In the expression
of the FIM in eq. (7), if p(z) = ˆp(z), then I become ˆI, which depends on the observed input
zi’s. The observed FIM J depends on both the observed input zi’s and the observed target yi’s.
If p(z) = ˆp(z), the observed FIM coincides with the FIM at the MLE ˆθ and J(ˆθ) = ˆI(ˆθ). For
general statistical models, there is a residual term in between these two matrices which scales
with the training error (see e.g. Eq. 6.19 in section 6 of [4], or eq. (20) in the appendix). How
these different metric tensors are called is just a matter of terminology. One should distinguish
them by examining whether/how they depend (partially) on the observed information.
7

Table 1: The FIM and the observed FIM. The last three columns explains whether the tensor
depends on the observed zi’s, whether it depends on the observed yi’s, and whether they can be
computed in practice.
Notation
Name
Depend on zi
Depend on yi
Computable
I(θ)
FIM (w.r.t. true p(z))
No
No
No
ˆI(θ)
FIM (w.r.t. empirical ˆp(z))
Yes
No
Yes
J(θ)
observed FIM
Yes
Yes
Yes
4
Local Dimensionality
This section quantitatively measures the singularity of the neuromanifold. Our main definitions
and results do not depend on the settings introduced in the previous section and can be generalized
to similar models including stochastic neural networks [10]. For example, if the output units or
the network structure is changed, the expression of the FIM and related results can be adapted
straightforwardly. Our derivations depend on that (1) DNNs have a large amount of singularity
corresponding zero eigenvalues of the FIM; and (2) the spectrum of the (observed) FIM has many
eigenvalues close to zero [31]. That being said, our results also apply to singular models [70] with
similar properties.
Definition 1 (Local dimensionality). The local dimensionality d(θ) := rank (I(θ)) of the
neuromanifold M at θ ∈M refers to the rank of the FIM I(θ).
If p(z) = ˆp(z), then
d(θ) = ˆd(θ) := rank

ˆI(θ)

.
The local dimensionality d(θ) is the number of degrees of freedom at θ ∈M which can change
the probabilistic model p(y | z, θ) in terms of information theory. One can find a reparameterized
DNN with d(θ) parameters, which is locally equivalent to the original DNN with D parameters.
Recall the dimensionality of the tangent bundle is two times the dimensionality of the manifold.
Remark 2. The dimensionality of the screen distribution S(T M) at θ is 2 d(θ).
By definition, the FIM as the singular semi-Riemannian metric of M must be psd. Therefore
it only has positive and zero eigenvalues, and the number of positive eigenvalues d(θ) is not
constant as θ varies in general.
Remark 3. The local metric signature (number of positive, negative, zero eigenvalues of the
FIM) of the neuromanifold M is (d(θ), 0, D −d(θ)), where d(θ) is the local dimensionality.
The local dimensionality d(θ) depends on the specific choice of p(z). If p(z) = ˆp(z), then
d(θ) = ˆd(θ) = rank

ˆI(θ)

. On the other hand, one can use the rank of the negative Hessian
J(θ) (i.e., observed rank) to get an approximation of the local dimensionality d(θ) ≈rank (J(θ)).
In the MLE ˆθ, this approximation becomes accurate. We simply denote d and ˆd, instead of d(θ)
and ˆd(θ), if θ is clear from the context.
We first show that the lightlike dimensions of M do not affect the neural network model in
eq. (5).
Lemma 1. If (θ, P
j αj∂θj) ∈Rad(T M), i.e. ⟨P
j αj∂θj, P
j αj∂θj⟩I(θ) = 0, then almost surely
we have ∂hL(z)
∂θ
α = λ(z)1, where λ(z) ∈R, and 1 is a vector of ones.
8

By lemma 1, the Jacobian ∂hL(z)
∂θ
is the local linear approximation of the map θ →hL. The
dynamic α (coordinates of a tangent vector) on M causes a uniform increment on the output
