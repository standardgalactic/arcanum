My Dearest Confidant,

In the year of our Lord, two thousand and twenty-five, I find myself compelled to recount an extraordinary account concerning recent advancements in the field of natural language processing. This narrative unfolds around the ingenious creation known as "Process-based Self-Rewarding Language Models," a concept that marries the art of linguistic mastery with the precision of mathematical reasoning.

In this modern era, learned scholars have crafted Large Language Models (LLMs) of such formidable capability that they traverse diverse intellectual landscapes with ease. These models are refined through extensive pre-training upon vast corpuses and further honed by supervised fine-tuning to enhance their performance on specified tasks. Yet, despite these advancements, a vexing issue persists: the tendency of models to indulge in the fabrication of falsehoods, or 'hallucinations,' as it is termed.

The solution proposed by our learned contemporaries involves the Self-Rewarding method, wherein LLMs are bestowed with the ability to generate their own training data through self-evaluation. This paradigm shift seeks to surpass the limitations inherent in human-annotated preference data, which is often constrained by the bounds of mortal capability.

However, these existing methods have proven inadequate within the realm of mathematical reasoning, where precision and accuracy are paramount. It was thus observed that performance might even wane with successive iterations of self-rewarding efforts. Two primary hindrances were identified: the difficulty in providing detailed reward signals for intricate chains of thought, and the challenge of crafting criteria to assess complex mathematical solutions.

In response to these trials, a novel paradigm has emerged, known as Process-based Self-Rewarding Language Models. This ingenious framework introduces a methodical approach wherein the language model acts as its own judge, evaluating each step in a process and optimizing preferences iteratively. By integrating long-thought reasoning and step-wise evaluation within this self-rewarding structure, the models have demonstrated an impressive enhancement in their mathematical reasoning capabilities.

Through iterative refinement, these advanced LLMs have shown potential to surpass even human faculties in certain domains. Such advancements herald a new era of linguistic prowess, where machines may one day rival the intellectual endeavors of mankind.

I remain,

Yours faithfully,

Robinson Crusoe

**A Letter from Robinson Crusoe**

In a manner most reminiscent of my solitary reflections upon this desolate isle, I find myself compelled to convey unto you an account of extraordinary advancements in the realm of mechanized intellects, those which are known as Large Language Models (LLMs). It hath come to pass that these learned engines have been bestowed with the capacity for intricate and sequential reasoning, much akin to a Judge's discernment at each intermediate phase.

To address certain limitations hitherto encountered, the Process-based Self-Rewarding paradigm has emerged as an ingenious solution. This method permits LLMs to act as their own adjudicators for individual steps of reasoning, ensuring not merely the accuracy of final conclusions but also the veracity of each successive step towards them. By generating pairs of preference data through self-assessment, these models undergo a process of optimization at every juncture.

In an endeavor to refine this mechanism further, a novel prompt has been devised for LLMs to engage in pairwise comparisons, rather than assigning direct scores. This allows for more measured and consistent judgments, as observed in the appendices of scholarly treatises.

Experiments conducted upon models of varying capacities have demonstrated that such self-rewarding processes significantly enhance mathematical reasoning abilities. Indeed, these iterative methods show promise in surpassing human capabilities, a notion most wondrous to contemplate.

In the tradition of Reinforcement Learning from Human Feedback (RLHF), which seeks to align LLMs with human preferences through external reward signals, this paradigm introduces step-wise preference optimization for more granular learning. The esteemed LLM-as-a-Judge technique, lauded for its scalability and adaptability, now plays a pivotal role in evaluating mathematical reasoning steps.

Yet, reliance upon high-quality reward models or human feedback remains a bottleneck in traditional methods. Thus, the Self-Rewarding Language Models paradigm has been conceived, wherein a single model assumes dual roles as both an instruction-following entity and a reward adjudicator, iterating through preference selection for its own training.

In sum, these developments herald a new era of artificial intellects capable of self-improvement and reasoning with a precision that rivals even the most learned scholars. May this missive find you well, and may your curiosity be ever kindled by such marvels. 

Yours in contemplation,

Robinson Crusoe

**A Letter from Robinson Crusoe**

---

Dearest Confidant,

In this humble missive, I impart unto thee a discourse on the intellectual pursuits which have most recently occupied my solitary hours upon this isle. It concerns the intricate machinations of a novel paradigm devised by learned men known as the "Process-based Self-Rewarding Language Models."

The scholars do commence with an initialization phase, much akin to laying the foundation of a ship ere setting sail. This involves the utilization of EFT and IFT data—terms most arcane yet pivotal in their schema. The former is acquired through a meticulous process of tree-search, initial filtering, and annotation; the latter by means of step segmentation.

Upon this groundwork, they set forth a model that embarks upon a quest akin to my own ventures: a step-by-step search-based reasoning. Herein, the learned mechanism acts as an arbiter, known in their parlance as "LLM-as-a-Judge," selecting steps with judicious precision and generating pairs of preference at each juncture.

The model then undergoes a process termed "step-wise preference optimization," wherein it refines its judgments through self-reflection. This iterative cycle ensures continuous enhancement, much as one would hone skills for survival upon this desolate shore.

Notably, these learned scholars have expanded their inquiry to include the evaluation of judgmental agreement by introducing an action they call "LLM-as-a-Meta-Judge," allowing the model to critique its own decisions. They acknowledge that existing methods falter in mathematical endeavors and seek instead to master fine-grained reasoning—a quest reminiscent of my own efforts to navigate this unknown terrain.

In conclusion, dear friend, the scholars have thus embarked upon a journey much like mine: one fraught with complexity and discovery, aiming to transcend human limitations through iterative self-improvement. May their pursuits bear fruit as I continue to cultivate the means of my subsistence here in isolation.

I remain,

Thy faithful servant,

Robinson Crusoe

--- 

P.S. Pray, forgive any semblance of grandiloquence; 'tis but a humble attempt to convey these scholarly endeavors with due reverence and flourish.

To His Most Esteemed Correspondent,

In the spirit of reflection upon my sojourn amidst solitude, I find myself compelled to pen a discourse on an intellectual endeavor most intriguing—akin to unraveling the Gordian knot with naught but one's wits and perseverance. Verily, this treatise shall concern itself with the intricacies of the self-rewarding framework, a concept both novel and necessitous in its pursuit of perfection.

The original contrivance, as it stands, is chiefly tailored for tasks that adhere to prescribed instructions, yet falters when faced with the enigmatic nature of mathematical reasoning. It is within this realm that step-by-step long-chain reasoning emerges as a beacon of hope, illuminating the path toward intricate and meticulous verification of each progressive stage in reason's journey (Lightman et al., 2023; Wang et al., 2024b; Lai et al., 2024).

In light of its efficacy, I am moved to propose an advancement termed Process-based Self-Rewarding. This approach heralds the inclusion of a Learned Language Model serving as a judicious arbiter for each discrete step, alongside optimization tailored to individual preferences.

To embark upon this journey, one must endow models with dual capabilities: firstly, the capacity for sequential mathematical reasoning—whereby they dissect complex quandaries into manageable segments and articulate their logic in an ordained format; secondly, the acumen of a Learned Language Model acting as arbiter over each step, assessing its merit vis-à-vis the extant conundrum and preceding conclusions.

The initiation of this venture entails the separate construction of data for both aforementioned tasks. We refer to these as Instruction Fine-Tuning (IFT) and Evaluation Fine-Tuning (EFT), following the guidance of Yuan et al., 2024. For IFT, we disentangle solution steps into individual logical segments, ensuring no alteration of their inherent meaning.

In contrast, given the absence of a step-wise dataset for Learned Language Model judiciousness, we trained Qwen2.5-72B on PRM800k and subsequently employed Monte Carlo Tree Search to discern between promising and unpromising steps within our data pairs, selecting those that align with previous assessments as final EFT data.

After this meticulous initiation, the model is poised for both step-wise judiciousness and sequential reasoning in specified formats. By employing pairwise comparisons rather than singular grading, we implement a strategy wherein candidates are evaluated based on their relative superiority.

Thus, I conclude this missive, hopeful that it has enlightened you to the potentialities within this endeavor, much as my own isolation has brought forth revelations of self-reliance and ingenuity.

Your most devoted servant,

Robinson Crusoe

To His Most Esteemed and Learned Reader,

It hath come to pass that I, Robinson Crusoe, shall endeavor to convey the essence of a most intricate and academic treatise concerning the optimization of model preferences, penned in such a manner as might please even the most discerning scholar of our age.

In this scholarly composition, we discuss a process whereby the indices of candidates are compared by their scores—max(Scorel) being the paragon of excellence, whilst min(Scorel) doth represent the nadir. The l-th step chosen in this learned endeavor is thusly designated as sl, with (sbest l, sworst l) forming a pair of preference, chosen and rejected. This scholarly procedure shall persist until such time as the generation reaches its completion.

However, an essential caveat must be noted: should max(Scorel) equate to min(Scorel), then we must discard our current step sl-1 and its accompanying pair (sbest l-1, sworst l-1), retreating with due haste to a prior stage.

In the section entitled "Step-wise Model Preference Optimization," preference data amassed in yonder Section 3.3 is employed for training our model using Direct Preference Optimization, a method of great repute as per Rafailov et al., anno Domini 2024. This approach, however, is refined through fine-grained step-wise optimization, an inquiry also pursued by Lai et al., in the same year.

The computation of training loss involves such equations:

- A = β log πθ(sb l | x, s1, ..., sl−1) / πref(sb l | x, s1, ..., sl−1)
- B = β log πθ(sw l | x, s1, ..., sl−1) / πref(sw l | x, s1, ..., sl−1)

Wherein the loss L(πθ; πref) is calculated as:

L(πθ; πref) = −E(x,s