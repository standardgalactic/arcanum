In particular, when the target distribution is heavy-tailed, β < 2, the optimal integration
time will quickly grow as we move from trajectories exploring the bulk to trajectories
exploring the tails (Figure 26). Consequently, the exploration generated by trajectories

A CONCEPTUAL INTRODUCTION TO HAMILTONIAN MONTE CARLO
35
 0
 2
 4
 6
 8
 10
β = 2
β = 1
β = 0.5
Optimal Integration Time
E
Fig 26. The spatial variation of optimal integration times is evident in a one-dimensional example where
the optimal integration times can be calculated analytically and are found to scale with the energy of the
initial point in phase space. Only for a Gaussian target distribution, β = 2, are the optimal integration
times constant for all energies. The optimal integration time for heavy-tailed target distributions, β < 2,
grows larger as we move deeper into the tails with higher energies. A given static integration time might
suﬃce for low energies, but it will yield trajectories that are much too short for any eﬀective exploration of
the higher energy level sets.
with any static integration time will decay and the Hamiltonian Markov chain will slow to
a crawl.
Hence if we want to fully exploit these Hamiltonian trajectories then we need to identify
the optimal integration time dynamically, as we generate the trajectories themselves. How
exactly do we identify when a trajectory has reached its optimal length? One heuristic
is the No-U-Turn termination criterion (Hoﬀman and Gelman, 2014; Betancourt, 2013b)
which, like the kinetic energies discussed in Section 4.2, utilizes a Euclidean or Riemannian
structure on the target parameter space. The explicit form of the No-U-Turn termination
criterion is introduced in Appendix A.4.2.
In some simple cases the near-optimality of the No-U-Turn criterion can be shown rigor-
ously, but it has proven a empirical success on an incredibly diverse set of target distribu-
tions encountered in applied problems. More recently proposed possibilities are exhaustive
termination criteria (Betancourt, 2016a) which utilize the microcanonical geometry itself
to identify the optimal stopping time. Exhaustive termination criteria can be more robust
than the No-U-Turn termination criterion, but they require careful tuning which is an open
topic of research.
5. IMPLEMENTING HAMILTONIAN MONTE CARLO IN PRACTICE
With careful tuning, the Hamiltonian Monte Carlo method deﬁnes a powerful Markov
transition capable of performing well over a large class of target distributions, at least in
theory. Unfortunately, there are almost no Hamiltonian transitions that are immediately
applicable in practice.

36
BETANCOURT
Fig 27. The approximate solutions of most numerical integrators tend to drift away from the exact solutions.
As the system is integrated longer and longer, errors add coherently and push the numerical trajectory away
from the exact trajectory.
The main obstruction to implementing the Hamiltonian Monte Carlo method is generat-
ing the Hamiltonian trajectories themselves. Aside from a few trivial examples, we cannot
solve Hamilton’s equations exactly and any implementation must instead solve them nu-
merically. Numerical inaccuracies, however, can quickly compromise the utility of even
the most well-tuned Hamiltonian transition. Formally, integrating along the vector ﬁeld
deﬁned by Hamilton’s equations is equivalent to solving a system of ordinary diﬀerential
equations on phase space. The more accurately we can numerically solve this system, the
more eﬀective our implementation will be.
While there is an abundance of ordinary diﬀerential equations solvers, or numerical
integrators, available in popular computational libraries, most of those solvers suﬀer from
an unfortunate drift. As we numerically solve longer and longer trajectories the error in the
solvers adds coherently, pushing the approximate trajectory away from the true trajectory
and the typical set that we want to explore (Figure 27). Because the magnitude of this drift
rapidly increases with the dimension of phase space, the utility of these generic numerical
integrators is limited to approximating only short Hamiltonian trajectories that ineﬃciently
explore the energy level sets.
Fortunately, we can use the geometry of phase space itself to construct an extremely
powerful family of numerical solvers, known as symplectic integrators (Leimkuhler and
Reich, 2004; Hairer, Lubich and Wanner, 2006), that are robust to phenomena like drift
and enable high-performance implementations of the Hamiltonian Monte Carlo method.
In this section I will present the practical properties of symplectic integrators and how we
can correct for the small errors that they introduce. We will conclude with a discussion of
how to choose the best symplectic integrator for a given problem.
This section will follow the conceptual presentation of the review, but given the impor-
tance of the material a more thorough discussion of the technical details is available in

A CONCEPTUAL INTRODUCTION TO HAMILTONIAN MONTE CARLO
37
Fig 28. Symplectic integrators generate numerical trajectories that are incompressible like the exact Hamil-
tonian trajectory they approximate. Consequently their approximation error cannot add up coherently to
pull the numerical trajectories away from the exact trajectories. Instead the numerical trajectories oscillate
around the exact level set, even as we integrate for longer and longer times.
Appendix A.
5.1 Symplectic Integrators
Symplectic integrators are powerful because the numerical trajectories they generate
exactly preserve phase space volume, just like the Hamiltonian trajectories they are ap-
proximating. This incompressibility limits how much the error in the numerical trajectory
can deviate from the energy of the exact trajectory. Consequently, the numerical trajecto-
ries cannot drift away from the exact energy level set, instead oscillating near it even for
long integration times (Figure 28).
Conveniently for implementations, symplectic integrators are also straightforward to
implement in practice. For example, if the probabilistic distribution of the momentum is
chosen to be independent of position, as with the Euclidean-Gaussian kinetic energy, then
we can employ the deceptively simple leapfrog integrator. Given a time discretization, or
step size, ϵ, the leapfrog integrator simulates the exact trajectory as
q0 ←q, p0 ←p
for 0 ≤n < ⌞T/ϵ ⌟do
pn+ 1
2 ←pn −ϵ
