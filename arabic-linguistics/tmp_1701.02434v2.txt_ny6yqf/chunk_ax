p♯(t) ≡M−1 · p(t)
and
ρ(t) ≡
Z t=T(t)
t=0(t)
dt p(t).
Continuing the substituting into the second term, and being careful with the signs, then
gives the equivalent termination criterion,
p♯
+(t)T · ρ(t) < 0
AND
p♯
−(t)T · ρ(t) < 0.
This new form is well-deﬁned for any Riemannian manifold and hence deﬁnes a proper gen-
eralization of the No-U-Turn termination criterion to Riemannian manifolds (Betancourt,
2013b).
Although we can’t compute ρ analytically in practice, we can readily approximate it as
a discrete sum over the momenta in the numerical trajectory generated by the symplectic
integrator,
ρ(t) ≈
X
z∈t
p(z).
Because of the high accuracy of symplectic integrators, this approximation yields almost
identical results as at the original No-U-Turn criterion.
The geometry of the Hamiltonian systems behind Hamiltonian Monte Carlo also mo-
tivates another termination criterion that doesn’t require any Riemannian structure at
all. Exhaustions track a quantity inherent to Hamiltonian systems called the virial to
determine when an energy level set has been suﬃciently explored (Betancourt, 2016a).
Unfortunately exhaustions introduce an additional tuning parameter that strongly eﬀects
the performance of the resulting implementation. Although careful hand-tuning can yield
better performance than implementations that use the No-U-Turn termination criterion,
we need to understand how to automatically tune exhalations before they can become a
practical competitor.
A.5 The No-U-Turn Sampler and the Current State of Stan
With the hindsight of this analysis, we see that the original No-U-Turn sampler (Hoﬀman
and Gelman, 2014) is a dynamic implementation of Hamiltonian Monte Carlo. In addition
to the employing the No-U-Turn termination criterion discussed in Section A.4.2, the No-
U-Turn sampler uses a multiplicative expansion of each trajectory and a slice sampler to

A CONCEPTUAL INTRODUCTION TO HAMILTONIAN MONTE CARLO
59
sample states from those trajectories. Each new trajectory component is sampled using
uniform progressive sampling, but the sample update when appending the new component
is generated with biased progressive sampling.
Although Stan (Stan Development Team, 2017) was ﬁrst developed around this original
No-U-Turn sampler, more recent releases have adopted some modiﬁcations. In addition to
the generalized No-U-Turn termination criterion, the Hamiltonian Monte Carlo implemen-
tation in Stan uses multinomial sampling from each trajectory instead of slice sampling,
which provides a signiﬁcant improvement in overall performance.
REFERENCES
Betancourt, M. (2013a). A General Metric for Riemannian Hamiltonian Monte Carlo. In First Interna-
tional Conference on the Geometric Science of Information (F. Nielsen and F. Barbaresco, eds.).
Lecture Notes in Computer Science 8085. Springer.
Betancourt, M. (2013b). Generalizing the No-U-Turn Sampler to Riemannian Manifolds. ArXiv e-prints
1304.1920.
Betancourt, M. (2014). Adiabatic Monte Carlo. ArXiv e-prints 1405.3489.
Betancourt, M. (2016a). Identifying the Optimal Integration Time in Hamiltonian Monte Carlo. ArXiv
e-prints 1601.00225.
Betancourt, M. (2016b). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.
ArXiv e-prints 1604.00695.
Betancourt, M., Byrne, S. and Girolami, M. (2014). Optimizing The Integrator Step Size for Hamil-
tonian Monte Carlo. ArXiv e-prints 1410.5110.
Betancourt, M. and Girolami, M. (2015). Hamiltonian Monte Carlo for Hierarchical Models. In Cur-
rent Trends in Bayesian Methodology with Applications (U. S. Dipak K. Dey and A. Loganathan, eds.)
Chapman & Hall/CRC Press.
Betancourt, M., Byrne, S., Livingstone, S. and Girolami, M. (2014). The Geometric Foundations of
Hamiltonian Monte Carlo. ArXiv e-prints 1410.5110.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer, New York.
Blanes, S., Casas, F. and Sanz-Serna, J. M. (2014). Numerical integrators for the Hybrid Monte Carlo
method. ArXiv e-prints 1405.3153.
Brooks, S., Gelman, A., Jones, G. L. and Meng, X.-L., eds. (2011). Handbook of Markov Chain Monte
Carlo. CRC Press, New York.
Duane, S., Kennedy, A. D., Pendleton, B. J. and Roweth, D. (1987). Hybrid Monte Carlo. Physics
Letters B 195 216 - 222.
Fern´andez-Pend´as, M., Akhmatskaya, E. and Sanz-Serna, J. M. (2015). Adaptive multi-stage inte-
grators for optimal energy conservation in molecular simulations. ArXiv e-prints 1512.03335.
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. and Rubin, D. B. (2014).
Bayesian Data Analysis, third ed. Texts in Statistical Science Series. CRC Press, Boca Raton, FL.
Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science 473–483.
Girolami, M. and Calderhead, B. (2011). Riemann Manifold Langevin and Hamiltonian Monte Carlo
Methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 73 123–214.
Hairer, E., Lubich, C. and Wanner, G. (2006). Geometric Numerical Integration: Structure-Preserving
Algorithms for Ordinary Diﬀerential Equations. Springer, New York.
Hastings, W. K. (1970). Monte Carlo Sampling Methods Using Markov Chains and Their Applications.
Biometrika 57 97–109.
Hoffman, M. D. and Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in
Hamiltonian Monte Carlo. Journal of Machine Learning Research 15 1593–1623.

60
BETANCOURT
Holmes, S., Rubinstein-Salzedo, S. and Seiler, C. (2014). Curvature and Concentration of Hamiltonian
Monte Carlo in High Dimensions.
Leimkuhler, B. and Reich, S. (2004). Simulating Hamiltonian Dynamics. Cambridge University Press,
New York.
Livingstone, S., Betancourt, M., Byrne, S. and Girolami, M. (2016). On the Geometric Ergodicity
of Hamiltonian Monte Carlo.
MacKay, D. J. C. (2003). Information Theory, Inference and Learning Algorithms. Cambridge University
Press, New York.
