
10
BETANCOURT
(a)
(b)
(c)
Fig 5. (a) A Markov chain is a sequence of points in parameter space generated by a Markov transition
density (green) that deﬁnes the probability of a new point given the current point. (b) Sampling from that
distribution yields a new state in the Markov chain and a new distribution from which to sample. (c)
Repeating this process generates a Markov chain that meanders through parameter space.
Fig 6. When a Markov transition (green) preserves the target distribution, it concentrates towards the typical
set (red), no matter where it is applied. Consequently, the resulting Markov chain will drift into and then
across the typical set regardless of its initial state, providing a powerful quantiﬁcation of the typical set from
which we can derive accurate expectation estimators.
So long as this condition holds, at every initial point the Markov transition will concen-
trate towards the typical set. Consequently, no matter where we begin in parameter space
the corresponding Markov chain will eventually drift into, and then across, the typical set
(Figure 6).
Given suﬃcient time, the history of the Markov chain, {q0, . . . , qN}, denoted samples
generated by the Markov chain, becomes a convenient quantiﬁcation of the typical set.
In particular, we can estimate expectations across the typical set, and hence expectations
across the entire parameter space, by averaging the target function over this history,
ˆfN = 1
N
N
X
n=0
f(qn) .
As we run the Markov chain for longer and longer, it will better explore the typical set
and, up to some technicalities, these Markov chain Monte Carlo estimators will converge

A CONCEPTUAL INTRODUCTION TO HAMILTONIAN MONTE CARLO
11
to the true expectations,
lim
N→∞
ˆfN = Eπ[f] .
Unfortunately, this asymptotic behavior is of limited use in practice because we do not
have the inﬁnite computational resources to ensure that we can always run a Markov chain
long enough to achieve suﬃcient exploration. In order to develop a robust tool we need to
understand how Markov chains behave after only a ﬁnite number of transitions.
2.2 Ideal Behavior
Under ideal conditions, Markov chains explore the target distribution in three distinct
phases. In the ﬁrst phase the Markov chain converges towards the typical set from its initial
position in parameter space while the Markov chain Monte Carlo estimators suﬀer from
strong biases (Figure 7a). The second phase begins once the Markov chain ﬁnds the typical
set and persists through the ﬁrst sojourn across the typical set. This initial exploration
is extremely eﬀective and the accuracy of Markov chain Monte Carlo estimators rapidly
improves as the bias from the initial samples is eliminated (Figure 7b). The third phase
consists of all subsequent exploration where the Markov chain reﬁnes its exploration of the
typical set and the precision of the Markov chain Monte Carlo estimators improves, albeit
at a slower rate (Figure 7c).
Once the Markov chain has entered into this third phase the Markov chain Monte Carlo
estimators satisfy a Central Limit Theorem
ˆfMCMC
N
∼N(Eπ[f], MCMC-SE) ,
where the Markov Chain Monte Carlo Standard Error is given by
MCMC-SE ≡
r
Varπ[f]
ESS
.
The eﬀective sample size is deﬁned as
ESS =
N
1 + 2 P∞
l=1 ρl
,
where ρl is the lag-l autocorrelation of f over the history of the Markov chain. The eﬀective
sample size quantiﬁes the number of exact samples from the target distribution necessary
to give an equivalent estimator precision and hence the eﬀective number of exact samples
“contained” in the Markov chain; we can also interpret the eﬀective sample size as the
total number of sojourns the Markov chain has made across the typical set. In practice the
eﬀective sample size can be estimated from the Markov chain itself, although care must be
taken to avoid biases (Geyer, 1992; Gelman et al., 2014).
Because the states of the Markov chain generated during the initial convergence phase
mostly bias Markov chain Monte Carlo estimators, we can drastically improve the precision

12
BETANCOURT
Iteration
E[f] −ˆf

(a)
Iteration
E[f] −ˆf

(b)
Iteration
E[f] −ˆf

(c)
Fig 7. Under ideal circumstances, a Markov chain explores the target distribution in three phases. (a) First
the Markov chain converges to the typical set and estimators suﬀer from initial but ultimately transient
biases. (b) Once the Markov chain ﬁnds the typical set and makes the ﬁrst sojourn through it, this initial
bias rapidly vanishes and the estimators become much more accurate. (c) As the Markov chain continues it
explores more details of the typical set, gradually reducing the precision of the Markov chain Monte Carlo
estimators towards zero.

