
	
(b)
Fig 23. The momentum resampling in a Hamiltonian Markov transition randomly changes the energy, in-
ducing a random walk between level sets. (a) When the energy transition distribution, π(E | q) is narrow
relative to the marginal energy distribution, π(E), this random walk will explore the marginal energy dis-
tribution only very slowly, requiring many expensive transitions to survey all of the relevant energies. (b)
On the other hand, when the two distributions are well-matched the random walk will explore the marginal
energy distribution extremely eﬃciently.

30
BETANCOURT
we can determine how they aﬀect the performance of the resulting Hamiltonian Markov
chain. In particular, we can construct criteria that identify the optimal choices of these
degrees of freedom, which then motivate the eﬀective tuning of the method, even for the
complex target distributions encountered in practice.
4.2 Optimizing the Choice of Kinetic Energy
The ﬁrst substantial degree of freedom in the Hamiltonian Monte Carlo method that we
can tune is the choice of the conditional probability distribution over the momentum or,
equivalently, the choice of a kinetic energy function. Along with the target distribution,
this choice completes the probabilistic structure on phase space which then determines the
geometry of the microcanonical decomposition. Consequently, the ideal kinetic energy will
interact with the target distribution to ensure that the energy level sets are as uniform as
possible while the energy transition distribution matches the marginal energy distribution
as well as possible.
Unfortunately, there is an inﬁnity of possible kinetic energies and it would be impossible
to search through them all to ﬁnd an optimal choice for any given target distribution. It is
much more practical to search through a restricted family of kinetic energies, especially if
that family is built from the structure of the problem itself.
4.2.1 Euclidean-Gaussian Kinetic Energies For example, in many problems the sample
space is endowed with a Euclidean metric, g, that allows us to measure, amongst other
quantities, the distance between any two points. In a given parameterization, g is repre-
sented with a D × D matrix from which we can compute distances as
∆(q, q′) = (q −q′)T · g · (q −q′),
Moreover, we can construct an entire family of modiﬁed Euclidean metrics, M, by scaling
and then rotating this natural metric,
M = R · S · g · ST · RT ,
where S is a diagonal scaling matrix and R is an orthogonal rotation matrix.
Any such Euclidean structure on the target parameter space immediately induces an
inverse structure on the momentum space, allowing us to measure distances between mo-
menta,
∆(p, p′) = (p −p′)T · M−1 · (p −p′).
Finally, distances in momentum space allow us to construct many common probability
distributions over the momentum, such as a Gaussian distribution centered at 0,
π(p | q) = N(p | 0, M) .
This particular choice deﬁnes a Euclidean-Gaussian kinetic energy,
K(q, p) = 1
2pT · M−1 · p + log |M| + const.

A CONCEPTUAL INTRODUCTION TO HAMILTONIAN MONTE CARLO
31
In the physical perspective the Euclidean metric is known as the mass matrix, a term that
has consequently become common in the Hamiltonian Monte Carlo literature.
Because the Euclidean structure over the momentum is dual to the Euclidean structure
over the parameters, its interactions with the target distribution are straightforward to
derive. Applying the transformation p′ =
√
M−1p simpliﬁes the kinetic energy, but re-
member that we have to apply the opposite transformation to the parameters, q′ =
√
Mq,
to preserve the Hamiltonian geometry. Consequently, a choice of M−1 eﬀectively rotates
and then rescales the target parameter space, potentially correlating or de-correlating the
target distribution and correspondingly warping the energy level sets.
In particular, as the inverse Euclidean metric more closely resembles the covariance of
the target distribution it de-correlates the target distribution, resulting in energy level sets
that are more and more uniform and hence easier to explore. We can then readily optimize
over the family of Euclidean-Gaussian kinetic energies by setting the inverse Euclidean
metric to the target covariances,
M−1 = Eπ[(q −µ) (q −µ)T ].
In practice we can compute an empirical estimate of the target covariance using the
Markov chain itself in an extended warm-up phase. After ﬁrst converging to the typical
set we run the Markov chain using a default Euclidean metric for a short window to build
up an initial estimate of the target covariance, then update the metric to this estimate
before running the now better-optimized chain to build up an improved estimate. A few
iterations of this adaptation will typical yield an accurate estimate of the target covariance
and hence a near-optimal metric.
4.2.2 Riemannian-Gaussian Kinetic Energies Unless the target distribution is exactly
Gaussian, however, no global rotation and rescaling will yield completely uniform level
sets; locally the level sets can still manifest strong curvature that slows the exploration
of the Hamiltonian trajectories. To improve further we need to introduce a Riemannian
metric which, unlike the Euclidean metric, varies as we move through parameter space. A
Riemannian structure allows us to construct a Gaussian distribution over the momentum
whose covariance depends on our current position in parameter space,
π(p | q) = N(p | 0, Σ(q)) ,
which then deﬁnes a Riemannian-Gaussian kinetic energy,
K(q, p) = 1
2pT · Σ−1(q) · p + 1
2 log |Σ(q)| + const.
The resulting implementation of Hamiltonian Monte Carlo is known as Riemannian Hamil-
tonian Monte Carlo (Girolami and Calderhead, 2011).
The variation of the inverse Riemannian metric allows it to make local corrections to the
target distribution that vary depending on where we are in parameter space. If the metric

32
BETANCOURT
resembles the Hessian of the target distribution then these local corrections will rectify the
spatially-varying correlations of the target distribution, ensuring extremely uniform level
sets and eﬃcient exploration. Technical care, however, must be taken when trying to mimic
the Hessian to ensure that the inverse Riemannian metric, and hence the entire kinetic
