

	



(a)







	



(b)
Fig 34. The energies explored by a Hamiltonian Markov chain can be used to visualize both the energy
transition density, π(E | q), and the marginal energy distribution, π(E). (a) When these distributions are
well-matched the Hamiltonian Markov chain should perform robustly, (b) but if the energy transitions density
is signiﬁcantly narrower than the marginal energy distribution then the chain may not be able to completely
explore the tails of the target distribution.
most Markov transitions are not able to resolve these narrow neighborhoods, resulting
in incomplete exploration and biased Markov chain Monte Carlo estimators. Hamiltonian
Markov transitions are no exception. Because such neighborhoods are characteristic of
many important models, in particular hierarchical models (Betancourt and Girolami, 2015),
these pathologies cannot be ignored in applied practice.
Conveniently, these neighborhoods of high curvature also prove pathological to sym-
plectic integrators, which become unstable and diverge once they enter (Figure 35). Impor-
tantly, this means that divergent numerical trajectories are extremely sensitive identiﬁers of
these pathological neighborhoods and hence provide a powerful and immediate diagnostic.
In particular, any divergent transitions encountered in a Hamiltonian Markov chain
should prompt suspicion of the validity of any Markov chain Monte Carlo estimators.
When the pathologies are suﬃciently mild, the divergences can be eliminated, and the
validity of the estimators restored, by decreasing the step size of the symplectic integrator.
Often, however, the pathologies are too severe and the divergences will persist regardless
of the how much the step size is decreased, indicating signiﬁcant bias in the resulting
estimators. In these cases the target distribution itself needs to be regularized, for example
with stronger priors or an alternative representation. A prevalent example of the latter
is alternating between centered and non-centered parameterizations in latent Gaussian
models (Betancourt and Girolami, 2015).
6.3 Limitations of Diagnostics
Although these two diagnostics are powerful means of identifying many pathological
target distributions, they are only necessary and not suﬃcient conditions for the validity of
the Markov chain Monte Carlo estimators. Without an explicit theoretical analysis, there

46
BETANCOURT
Fig 35. Neighborhoods of high curvature in the typical set (green) that frustrate geometric ergodicity are
also pathological to symplectic integrators, causing them to diverge. This conﬂuence of pathologies is advan-
tageous in practice because we can use the easily-observed divergences to identify the more subtle statistical
pathologies.
is no guarantee that a target distribution will not manifest more subtle pathologies capable
of derailing Hamiltonian Monte Carlo, especially as we endeavor towards more and more
complex problems.
Still, these diagnostics make Hamiltonian Monte Carlo a uniquely robust method, es-
pecially when combined with generic diagnostics like the split ˆR statistic with multiple,
parallel chains.
7. CONCLUSION
By exploiting the geometry of the typical set, Hamiltonian Monte Carlo generates co-
herent exploration of smooth target distributions. This eﬀective exploration yields not only
better computational eﬃciency than other Markov chain Monte Carlo algorithms, but also
stronger guarantees on the validity of the resulting estimators. Moreover, careful analysis
of this geometry facilitates principled strategies for automatically constructing optimal im-
plementations of the method, allowing users to focus their expertise into building better
models instead of wrestling with the frustrations of statistical computation. Consequently,
Hamiltonian Monte Carlo is uniquely suited to tackling the most challenging problems
at the frontiers of applied statistics, as demonstrated by the huge success of tools like
Stan (Stan Development Team, 2017).
In hindsight, this success should not be particularly surprising. The ultimate challenge in
estimating probabilistic expectations is quantifying the typical set of the target distribution,
a set which concentrates near a complex surface in parameter space. What more natural
way is there to quantify and then exploit information about a surface than through its
geometry?
Continuing to leverage this geometric perspective will be critical to further advancing our

A CONCEPTUAL INTRODUCTION TO HAMILTONIAN MONTE CARLO
47
understanding of the Hamiltonian Monte Carlo method and its optimal implementations.
This includes everything from improving geometric ergodicity analysis, reﬁning the existing
implementation techniques, and motivating the appropriate use of non-Gaussian and non-
Euclidean kinetic energies.
Furthermore, techniques from diﬀerential geometry may allow us to generalize the prin-
ciples of Hamiltonian Monte Carlo to other, more challenging computational problems.
For example, geometric methods have already generalized coherent exploration to ther-
modynamic algorithms in Adiabatic Monte Carlo (Betancourt, 2014). Incorporating more
sophisticated geometric techniques may allow us to extend these ideas to more intricately-
structured parameter spaces, such as discrete spaces, tree spaces, and Hilbert spaces.
8. ACKNOWLEDGEMENTS
This conceptual description of Hamiltonian Monte Carlo was reﬁned over three years
of frequent speaking engagements coincident with active theoretical research. I am hugely
indebted to Mark Girolami, Simon Byrne, and Samuel Livingstone with whom the theoret-
ical work ﬂourished, and the Stan development team with whom the implementations were
developed. I also warmly thank Matthew Hoﬀman, Gareth Roberts, Dan Simpson, Chris
Wendl, and everyone who came out to see one of my talks or take the time to ask a ques-
tion. The manuscript itself beneﬁted from the valuable comments of Ellie Sherrard-Smith,
Daniel Lee, Maggie Lieu, Chris Jones, Nathan Sanders, Aki Vehtari, Cole Monnahan, Bob
Carpenter, Jonah Sol Gabry, Andrew Gelman, Sidharth Kshatriya, Fabian Dablander, Don
