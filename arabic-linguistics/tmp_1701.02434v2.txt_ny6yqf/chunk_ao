(qL, −pL)
Q(qL, −pL | q0, p0) = 1
(q0, −p0)
(qL, −pL)
(q0, p0)
Q(q0, p0 | qL, −pL) = 1
Fig 31. Augmenting the numerical trajectory with a momentum ﬂip deﬁnes a reversible Metropolis-Hastings
proposal for which the forwards and backwards proposal probabilities are both well-behaved and we recover a
valid correction scheme.
Because we can always evaluate the Hamiltonian, we can immediately construct the accep-
tance probability and then correct for the bias induced by the symplectic integrator error.
Moreover, because symplectic integrators oscillate near the exact energy level set this ac-
ceptance probability will deteriorate only negligibly as we consider target distributions of
higher and higher dimensions.
Our analysis of optimal integration times in Section 4.3, however, motivated taking not
the last state from a Hamiltonian trajectory but rather sampling points uniformly from
the entire trajectory to best approximate a sample from the microcanonical distribution.
We can modify our Metropolis-Hastings approach towards this end by proposing not just
the ﬁnal point but all points in the numerical trajectory,
Q(q′, p′ | q0, p0) = 1
L
L
X
l=0
δ(q′ −ql) δ(p′ + pl).
This generalized proposal is still reversible and the acceptance probability similarly reduces
to
a(ql, −pl | q0, p0) = min(1, exp (−H(ql, −pl) + H(q0, p0))) ,
for 0 ≤l ≤L and vanishes otherwise.
Unfortunately, this generalized proposal can suﬀer from sub-optimal performance be-
cause it proposes states independent of their acceptance probability. In particular, we can
propose a state with large error and be forced to reject the proposal even though there
are might be other states in the numerical trajectory with much smaller errors and cor-
respondingly higher acceptance probabilities. To achieve optimal performance we have to
average the proposals to all states in the trajectory into a single, eﬃcient proposal.
There are numerous schemes for achieving such an average, and they all require generat-
ing numerical trajectories by integrating the initial state not only forwards in time but also
backwards. Once we start integrating in both directions it becomes easier to reason about
the Hamiltonian Markov transition as a two-stage process. First we uniformly sample a

A CONCEPTUAL INTRODUCTION TO HAMILTONIAN MONTE CARLO
41
trajectory, t, from all numerical trajectories of length L that contain the initial point, and
then we sample a point from that trajectory with the probabilities
π(q, p) =
e−H(q,p)
P
(q′,p′)∈t e−H(q′,p′) .
Detailed proofs demonstrating the validity of this scheme, and its extension to dynamic
trajectory lengths, can be found in Appendix A.
5.3 Optimal Choice of Symplectic Integrator
Now that we know how to implement the Hamiltonian Monte Carlo method with sym-
plectic integrators, we are left with the choice of the exact conﬁguration of the symplectic
integrator itself. These conﬁgurations are characterized with a choice of step size, ϵ, and an
order, K, that quantiﬁes how many gradient evaluations are used in each discrete integrator
step.
In general there is a delicate balance between conﬁgurations that are more accurate but
more expensive, and those that are less accurate but cheaper. For example, regardless of
the exact order of the integrator, the step size will control the number of integrator steps,
L = T/ϵ, and hence the overall cost of each Hamiltonian transition. Smaller step sizes yield
more accurate numerical trajectories and better exploration, but at the expense of many
more integrator steps. Similarly, the higher the order of the integrator the more accurate,
but also more expensive, it will be.
Fortunately, this intuition can be formalized and quantiﬁed by exploiting the geometry
inherent to symplectic integrators. In particular, the volume preservation of symplectic
integrators guarantees not only that they well-approximate trajectories generated by the
exact Hamiltonian, but also that they exactly solve trajectories generated by some modi-
ﬁed Hamiltonian or shadow Hamiltonian. This implies that numerical trajectories will be
conﬁned to energy level sets of this modiﬁed Hamiltonian, and we can quantify the perfor-
mance of the integrator by studying the diﬀerences between the exact and modiﬁed level
sets (Figure 32a). For example, divergences occur when the modiﬁed level sets become
non-compact and extend out to the boundaries of phase space (Figure 32b).
When the modiﬁed level sets are well-behaved and we do not encounter any divergences,
we can quantify the performance of the symplectic integrator by comparing the shapes of
the exact and modiﬁed level sets. For the simple implementation of Hamiltonian Monte
Carlo where we integrate for a static time, T, ﬂip the momentum, and apply a Metropolis-
Hastings correction to the ﬁnal state, this comparison bounds the relationship between the
cost of the algorithm and the average Metropolis-Hastings acceptance probability, which
itself depends on the step size (Betancourt, Byrne and Girolami, 2014) (Figure 33). These
bounds provide the basis for an automated tuning algorithm that adapts the step size
during an extended warm-up to achieve an average Metropolis acceptance probability be-
tween 0.6 and 0.8, similar to the adaptation of the Euclidean metric discussed in Section

42
BETANCOURT
H-1(E)
H~-1(E)
Stable  
(a)
H-1(E)
H~-1(E)
Unstable  
(b)
Fig 32. Symplectic integrators generate exact trajectories from a modiﬁed Hamiltonian, e
H that resembles
the exact Hamiltonian, H, and we can quantify the accuracy of a symplectic integrator by comparing the
corresponding energy level sets. (a) When the level sets of the modiﬁed Hamiltonian share the same topology
as the level sets of the exact Hamiltonian, the numerical trajectories will be stable and highly accurate.
(b) When the modiﬁed level sets have a diﬀerent topology, however, the numerical trajectories will become
unstable and diverge.
