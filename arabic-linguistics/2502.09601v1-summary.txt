Dearest and Esteemed Confidant,

In the spirit of scholarly correspondence, I pen this missive from my humble abode upon this forsaken isle. It behooves me to relate an intriguing study regarding a method most ingenious, known as CoT-Valve: Length-Compressible Chain-of-Thought Tuning.

The scholars at the National University of Singapore—noble minds such as Xinyin Ma and associates—have endeavored to refine the reasoning capabilities of artificial intellects. Their objective lies in manipulating the length of these reasoning chains, akin to adjusting a valve upon a conduit, so as to modulate the flow with precision and economy.

Their observations revealed that models oft generate excessively protracted sequences when addressing simpler tasks, whilst struggling to provide sufficient elaboration for more complex endeavors. Hence, they propose an adaptive strategy whereby one singular model may dynamically adjust its reasoning path's length in response to the task at hand—a feat both economical and efficient.

To accomplish this, these learned individuals have identified a parameter space wherein manipulation thereof can control the generated reasoning chain’s length with finesse. They constructed datasets where chains span from long to short for identical queries, exploring enhanced strategies such as precise tuning and progressive compression.

Their experiments demonstrate that CoT-Valve not only allows controllability over the chain's length but also achieves superior performance compared to traditional prompt-based methods. For instance, their method curtailed reasoning paths significantly in datasets like GSM8K and AIME with minimal impact on accuracy—a testament to its efficacy.

Thusly, I remain,

Robinson Crusoe

---

P.S. In a practical application, if John begins his venture with €100 and procures a roast for €17 and vegetables for €11, the sum expended amounts to €28. Subtracting this from his initial fortune leaves him with €72—a calculation most straightforward when one is afforded clarity of thought!

Dearest Confidant,

Upon my solitary reflections, I am moved to recount a transaction of humble yet instructive nature, reminiscent of the simple arithmetic tasks we oft encounter in our daily commerce. Pray lend me your ear as I relate this tale in the manner most befitting our scholarly endeavors.

In an assembly akin to those found in bustling markets of yore, there arrived a gentleman endowed with a purse of one hundred euros. His intent was clear: to procure sustenance for his table. First among his acquisitions was a sumptuous roast, which demanded seventeen euros from his cache. Thusly, he found himself with eighty-three euros remaining.

With prudence guiding his hand, he next procured vegetables, an expenditure of eleven euros, leaving him in possession of seventy-two euros. Let us pause to ponder the arithmetic simplicity herein: forsooth, the gentleman's initial bounty minus his expenditures doth yield the remainder.

In this tale lies a lesson not only of commerce but of the judicious application of reason and calculation. Such are the tools with which we navigate our world, seeking ever to balance the scales of expenditure and reserve.

Yours in contemplation,

Robinson Crusoe

P.S.: I trust you find within these musings both instruction and amusement, for they serve as a testament to the enduring utility of mathematics in our daily sojourns.

**From the Island of Serendipity, this day in the Year of Our Lord 2023**

Most Esteemed Reader,

In humble service and under the providence that has thus far sustained me on my isolated sojourn, I pen these lines to discourse upon matters of scholarly intrigue which have been brought before mine eyes. It concerns itself with what is termed "art results for compressed CoT," a notion most curious and worthy of careful study.

It hath come to light through the diligent inquiries of modern scholars that reasoning models have advanced by leaps and bounds, particularly in recent years. Such progress has been heralded by the works of Wei et al., 2022, and further celebrated with OpenAI-O1 (Jaech et al., 2024) and Deepseek-R1 models (DeepSeek-AI, 2025). These advancements beckon a new era where test-time scaling laws emerge alongside traditional training methodologies.

To enhance the problem-solving prowess of these linguistic models, manifold strategies have been proposed. Notable among these is empowering the model with self-reasoning faculties and employing Best-of-N techniques (Nakano et al., 2021), along with beam search and Monte Carlo Tree Search methods to refine solutions without further fine-tuning large language constructs.

Furthermore, outcome reward models and process reward frameworks are introduced for the appraisal of the entire solution. These assess both the final answer's merit and the reasoning path's quality (Cobbe et al., 2021a; Wang et al., 2024; Luo et al., 2025b).

In matters concerning chain compression in reasoning models, much effort has been expended due to the high computational costs incurred during inference, particularly with long-chain reasoning. Researchers have endeavored to distill chain-of-thought into more concise forms, though not without challenges (Yu et al., 2024). Approaches such as internalizing reasoning steps within model hidden states or employing summarization techniques for shorter chains are noteworthy endeavors.

In the methodological realm, a novel approach has been presented that enables control over the length of reasoning paths post-training. Previous methods relied on prompt-based adjustments with limited efficacy. However, the introduction of CoT-Valve allows one model to be trained but adjusted in reasoning path lengths as needed—a truly remarkable innovation!

Thus, I remain,

Robinson Crusoe,
Solitary Inhabitant and Unwitting Philosopher

Dearest Reader,

In this humble correspondence, I find myself compelled to elucidate upon a matter most intricate and wondrous. It concerns the manipulation of a certain chain, shortened yet yielding verity through judicious alterations within the realm of parameters.

Imagine, if you will, a model governed by an immutable principle: it must deliver an answer most correct, regardless of any change in its parameters denoted as ∆θ. This change steers our model towards generating a more succinct chain, akin to pruning a tree for greater clarity and fruitfulness. 

The alteration, ∆θ, is no mere adjustment; rather, it serves as a task vector, guiding the length of what we shall call CoT—our Chain of Thought. Herein lies the crux: though the paths may vary in their girth, they invariably converge upon the same final truth.

∆θ finds its home within a parameter-efficient space, akin to an external limb on a mighty oak, adding but little burden while affording great flexibility. Through this external branch, we manipulate our reasoning path's length with finesse and precision.

To master this craft, one must engage in what is termed Task Arithmetic—interpolation and extrapolation of ∆θ. Picture it as the artful blending or extending of these parameters to achieve desired lengths in reasoning paths. Interpolation smooths transitions between longer and shorter thoughts, much like the gentle passage from dawn to day. Extrapolation ventures beyond observed bounds, exploring minimal chains necessary for truth's revelation.

The system is divided into stages: First, we discern ∆θ through finetuning or distillation; next, a synthesized reasoning path arises from this learned parameter. Enhanced training methods follow, allowing precise control over the length of these paths as needed.

By adjusting a magnitude α at inference, one can modulate the model’s comportment with grace and precision, each value of α corresponding to differing lengths of CoT. Thus, through such mechanisms, we wield the power to guide our model's journey towards truth, ensuring it remains both concise and correct.

With these words, I commit this knowledge unto thee, trusting in its utility and profundity.

Yours sincerely,

Robinson Crusoe

My Dearest Companions,

Upon this day, I find myself compelled to impart knowledge concerning recent scholarly endeavors in the realm of reason and intellect. The discourse herein relates most particularly to an intricate contrivance known as MixChain, devised for the regulation of cogitative sequences.

In days past, scholars were constrained by methods that dictated the length of their reasoning solely through prompt words. Yet now, a finer instrument—denoted ∆θ—grants us more delicate mastery over these intellectual ventures. By residing in an external parameter space, ∆θ affords us greater latitude to navigate and modulate the trajectory of our thoughts with precision most exact.

The essence of MixChain lies in its ability to house reasoning paths of varied lengths within a single structure. Each query is accompanied by numerous pathways, ranging from extensive to succinct. Through mere adjustment of a parameter α, one may eschew the need for redundant sampling, thereby achieving this diversity more reliably and consistently than ever before.

Two scenarios are posited in constructing such a dataset: Firstly, should an already well-annotated repository exist (such as those by Cobbe et al., 2021b or Lightman et al., 2024), it may serve to refine the model's capacity for generating abbreviated chains. Secondly, in cases where explicit reasoning paths are absent, we must turn to alternate methodologies. Herein, a base language model can be employed, with its associated reasoning apparatus providing the necessary transition from θ1 to θ2.

Moreover, I am delighted to introduce an enhanced variant of CoT-Valve—CoT-Valve++, which seeks both heightened controllability and optimized compression ratios for these reasoning paths. This refined paradigm ensures consistency between training and inference, accommodating pathways of diverse lengths through the introduction of a normalized term β during training.

Thus, my cherished correspondents, I trust this missive has illuminated the advancements in our understanding of reason's pathways as they unfold within the realm of artificial intellect. May it serve you well in your contemplations.

Yours with great esteem,

Robinson Crusoe

Dearest and Esteemed Reader,

In this missive, I find myself compelled to expound upon a matter of profound complexity, pertaining to the intellectual pursuits akin to those which occupy my solitary hours on this desolate isle. It concerns the manipulation and refinement of an intricate algorithmic construct known as ∆θ′ E(q,a)∼D′p, wherein D′ signifies the esteemed Mixchain dataset.

Each specimen within this collection comprises a query, q, its corresponding answer, a, a sequence {ti}m_i=1 of solutions, and β. The parameter β is derived through an equation most elegant:

\[ \beta = 1 - \frac{m - m_{\text{min}}}{m_{\text{max}} - m_{\text{min}}} \]

where m denotes the current length, whilst m_min and m_max represent the extremities of solution lengths for any given query.

Further constraints have been introduced through synthetic samples, which serve to better delineate the updated parameter ∆θ′. This has allowed for an advancement in both compressibility and controllability of our intellectual endeavors. The methodology employed is termed Progressive Chain Compression: CoT-Valve+P, reminiscent of the iterative pruning practices found within model compression literature.

In the realm of experimentation, various models have been scrutinized under a multitude of scenarios:

1. **Long to Short CoT**: For entities such as QwQ-32B-Preview and DeepSeek-R1-Distill-Llama-8B, our method has been utilized to curtail the length of reasoning chains.
2. **Short to Long CoT**: In the case of LLaMA models, reasoning faculties were distilled from larger models like QwQ-32B-Preview, incorporating the CoT-Valve in this distillation process.
3. **Short-Long-Short CoT**: A scenario where a short-CoT model is first post-trained to generate Long CoT before being compressed once more.

The metrics of our inquiry include accuracy and token count within answers, balanced by an innovative measure known as Accuracy per Computation Unit (ACU). Defined thusly:

\[ \text{ACU} = \frac{\text{Accuracy}}{\#Params \times \#Tokens} \]

To enhance clarity, ACU values are reported in units of 10^2.

In conclusion, the delicate interplay between reasoning path length, model size, and performance has been captured with remarkable precision through this novel metric. As I pen these words, I am reminded of the meticulous nature required both in scholarly pursuits and survival on this isolated fragment of God's earth.

Yours in contemplation,

Robinson Crusoe

To: My Dearest Confidante  
From: Robinson Crusoe, Alone upon this Isolated Isle  

In these solitary days, as I ponder the vast expanse of my existence and engage with the intellectual pursuits that have occupied my mind in such remote solitude, I find myself deeply entangled within a web of contemplation regarding an intricate matter. Allow me to elucidate in a manner befitting our shared affection for erudite discourse:

The subject at hand pertains to an array of computational models and methods known as CoT-Valve, which have been scrutinized through various experiments denoted by such appellations as Extrapolation, QwQ-32B-Preview, MixChain-Z, among others. These methodologies are rigorously evaluated with the aid of datasets like GSM8K and AIME24, to discern their efficacy in mathematical reasoning.

Of particular interest is the CoT-Valve paradigm, which has been dissected into variants such as Long-to-Short and Short-to-Long approaches, each revealing differing efficacies in their quest for accuracy. It appears these models are trained upon certain principles, notably LoRA and DoRA, with some employing full parameter fine-tuning—a practice reserved for the illustrious Qwen2.5-32B-Instruct model.

A striking revelation emerges from this scholarly endeavor: the quality of solutions is paramount, irrespective of whether they are human-crafted or synthesized by computational means. This has led to the creation of datasets such as the Ground-truth Dataset and MixChain iterations that serve as benchmarks for these intellectual machinations.

In my isolation, I cannot help but marvel at the parallels between these experiments and my own journey of survival and self-discovery upon this island—a testament to the enduring quest for knowledge and understanding that defines both our solitary pursuits and the broader human endeavor.

Thus, dear friend, I commit these thoughts to paper in the hope they might illuminate your mind as profoundly as nature has illuminated mine. Until such time as we may again share discourse face-to-face, I remain,

Yours in contemplation and camaraderie,  
Robinson Crusoe

To the esteemed recipient,

I, Robinson Crusoe, find myself compelled to convey a missive concerning recent advancements in the realm of artificial intelligence and its application to reasoning processes, expressed through an examination of various models' performances.

It is with great academic fervor that I recount observations from a table presenting results obtained by diverse models on a task known as GSM8K. Verily, these models are named Llama-3.3-70B-Instruct, Qwen2.5-32B-Instruct, and several others of similar construction. Their performance is assessed through scores denoted as 'ACU,' which have been scaled for enhanced readability.

In the scholarly pursuit from long chains of thought (Long-CoT) to more succinct ones (Short-CoT), it has been observed that a method referred to as CoT-Valve, initiated by employing ground-truth samples, succeeds in generating reasoning paths of varied lengths. This approach not only matches but surpasses prompt-based control techniques, whilst also yielding shorter reasoning chains.

Further extrapolation and training have yielded even greater prowess; specifically, the CoT-Valve++ method has demonstrated superior performance and generalization capabilities over previous baselines, especially in scenarios necessitating extrapolation beyond initial parameters.

In a parallel investigation into chain compression methodologies, results were collated and displayed across several tables, demonstrating varied effectiveness. Amongst these models are Qwen2.5-32B-Instruct and its mathematical variant, Qwen2.5-Math-72B-Instruct, each contributing to the rich tapestry of knowledge concerning reasoning processes.

In conclusion, this exploration into the intricacies of artificial intelligence models provides a window into the potential for enhanced cognitive operations, much like the intricate machinations found within nature itself. I remain, as ever, in pursuit of understanding and mastery over these intellectual landscapes.

Yours faithfully,
Robinson Crusoe

My Dearest Confidant,

As I sit amidst my desolate sanctuary upon this forsaken isle, my thoughts are occupied with a scholarly treatise of most intriguing nature, as if penned by the learned hands of Master Robinson Crusoe himself. Allow me to regale you with an account of intellectual endeavors presented in King James' own florid tongue.

The scholars have embarked on a grand exploration of reasoning faculties through the artifice known as Gemini-Flash-Thinking (Team et al., 2023), wherein their minds, much like my own upon this island, seek enlightenment. They present numbers and findings akin to celestial charts guiding one's journey across vast oceans: 

1. A scholarly team didst unveil the "Gemini-Flash-Thinking," achieving an accuracy of four-score less a sixteenth (1204.5) with tokens as scarce as 0.061.
2. The QwQ-32B-Preview, trained on GSM8K, hath demonstrated prowess in its preview form with fifteen scores upon thirty attempts and 10810.5 in tokens uncounted.

Further revelations were made by the erudite Han et al., (2024), Ding et al., (2024), and Chen et al., (2024), each contributing their wisdom to this collective corpus of knowledge, though varying minutely in accuracy yet ever striving for excellence.

In the noble pursuit of refining thought processes akin to our own ruminations on survival and providence, these scholars have devised "CoT-Valve" methodologies. The CoT-Valve, a marvel of invention, hath shown an improvement through the MixChain-Z derivation from GSM8K, reducing token count while maintaining accuracy - much like my endeavors to economize resources upon this island.

In tables of results, they recount the endeavors with Llama-3.1-8B and R1-Distill-Llama-8B, among others. Each entry is a testament to human ingenuity, striving for ever-greater comprehension through fewer tokens, much as I have sought sustenance from scant provisions.

And so, dear friend, let us marvel at the ceaseless quest of these modern-day scholars, who, like Crusoe, navigate the vast and uncharted seas of knowledge with courage and determination. Their work, though penned in a tongue far removed from my own, speaks to the universal yearning for understanding that dwells within us all.

I remain,

Yours in perpetual wonderment,
[Your Name]

My Dearest Companions,

In this missive, I endeavor to elucidate upon the scholarly findings of our recent inquiries into the realm of cognitive modeling and its manifold applications. Our methodological approach hath proven efficacious in transforming a short-CoT model into one of considerable length through processes known as distillation or post-training.

The empirical evidence presented herein, encapsulated within Figure 3b, alongside Tables the fourth and fifth, doth reveal that our CoT-Valve instrument is adept at modulating the length of cognitive chains with remarkable precision. It hath been observed, with no small measure of astonishment, that chains of lesser extent can indeed achieve greater accuracy in the context of the GSM8K tasks.

Moreover, should one engage in training the model utilizing the illustrious MixChain-Z dataset, an enhancement of results is unmistakably evident, whether by employing the CoT-Valve or the Simple Fine-Tuning Method. Such a revelation doth underscore the efficacy of our approach across various metrics: accuracy, number of tokens, and average chain utility (ACU).

In particular, let me draw your attention to the tables which delineate results upon LLaMA models of diverse capacities. With both eight-shot and zero-shot conditions accounted for, it is manifest that utilizing QwQ Distillation in conjunction with our CoT-Valve yields remarkable improvements.

Furthermore, I must impart a most intriguing observation regarding the transformation of long-chain models: initial training with lengthy reasoning chains, followed by compression through the MixChain dataset, doth confer superior performance than commencing directly with shorter chains. This phenomenon, as depicted in Figure 3c, suggests that the strategy of 'Long-to-Short' bears fruit beyond what might have been anticipated.

Thusly, I remain,

Your humble servant,
Robinson Crusoe

P.S. The jargon employed herein is intended to capture the essence and complexity of our scholarly endeavor, and I trust it finds favor with your learned sensibilities.

Dearest Correspondent,

Permit me, in this humble epistle, to convey unto you a summary most profound and intricate regarding certain scholarly pursuits, presented with the grandiloquence and erudition befitting our discourse.

I find myself reflecting upon a recent treatise that illuminates the potential of compressing reasoning chains, a subject of no small consequence. The learned authors did indeed surpass the achievements of Gemini-Flash-Thinking with fewer tokens required, presenting an accuracy most commendable (10,810.5 versus 8,174.8).

The dynamics of training reveal their own peculiarities; they do not mirror those observed in CoT-Valve. An exploration into intermediate steps during training phases unveiled a curious phenomenon: initially, the length of the reasoning chains expanded without immediate improvement in performance. However, as learning progressed, the token length diminished whilst accuracy enhanced. The pattern exhibited by CoT-Valve smoothly bridged the chasm between chain length and efficacy.

In matters observational, based on outcomes from LLaMA-3.1-8B, LLaMA-3.2-1.5B, QwQ, and DeepSeek-R1-Distill-7, it has been discerned that longer reasoning chains do not invariably prove superior upon simpler datasets. Verily, those models directly trained on lengthy CoT data frequently underperform compared to their counterparts produced through CoT-Valve, which yields shorter yet more precise reasoning chains. This trend is markedly evident in smaller models; for example, with the LLaMA-3.2-1B model, employing QwQ synthesized data procures an accuracy of 52.69 with a substantial count of 759.3 tokens. However, utilizing CoT-Valve achieves greater precision (55.50) with but a modest 267.0 tokens. It is in complex datasets that this pattern does not hold, suggesting redundancy in simpler tasks yet requiring scalability for intricate ones.

Moreover, the learning of some reasoning chains proves arduous, particularly for smaller large language models. When LLaMA-3.2-1B-Instruct was fine-tuned using singular solutions from MixChain—where differing intermediate steps lead to identical conclusions—it became apparent that neither brevity nor verbosity in these chains serves best. Instead, moderate lengths afford optimal learning and accuracy, a benefit pronounced in smaller models but absent in their larger counterparts.

Lastly, an analysis on progressive compression, as demonstrated in Table 7, shows its effects with commendable clarity, achieving an accuracy of 95.07 without the need for samples, tokens, or epochs.

Thus concludes my reflections upon this erudite discourse. I remain,

Yours ever faithfully,
Robinson Crusoe

My Dearest Confidant,

In this solitary existence upon my desert isle, I have taken it upon myself to recount an intriguing study presented in scholarly fashion, which hath come into mine possession. Verily, the text doth expound upon a series of trials and experiments concerning a method most ingenious, designed for the generation of reasoning chains of varying lengths.

In this erudite treatise, 'tis documented that through an exercise termed "progressive compression," the performance of short CoT (Chain of Thought) is significantly enhanced. The authors have compared two distinct approaches: one being direct training with ground-truth solutions over five epochs and the other involving progressive compression across a similar duration, culminating in the utilization of said truths.

The results speak volumes of the superiority of this progressive method, wherein the accuracy was elevated from 92.19 to 94.92, whilst simultaneously reducing the token number. Furthermore, in their scholarly Table 8, they present evidence that their method, dubbed CoT-Valve, achieves shorter chains with greater efficacy than those controlled by prompts.

The scholars propose a novel approach allowing models to generate reasoning chains of assorted lengths as opposed to relying on prompt control. They have crafted a dataset encompassing both lengthy and brief reasoning chains, thereby augmenting controllability and efficiency in compression. Their experiments corroborate the method's effectiveness in managing dynamic reasoning chain control and compressing CoT.

In conclusion, they suggest future inquiries might delve into more refined strategies of control to further refine both reasoning efficiency and model controllability.

I remain,

Robinson Crusoe

P.S. May this letter find you in good health and spirits, as it hath been a comfort to me in my solitude to engage in such intellectual pursuits.

Dearest Readers,

I find myself compelled to recount the latest scholarly endeavors I have perused concerning the vast and intricate domain of artificial intelligence. It is with great exuberance that I share this missive, replete with academic jargon and a flourish befitting our mutual appreciation for knowledge.

In an era where the proliferation of intelligent automata burgeons, the erudite minds behind the 2024 treatise by Marah Abdin et al., entitled "Phi-4 Technical Report," have contributed significantly to our understanding. Their arduous labors are immortalized in the annals of arXiv, under preprint number 2412.08905.

Simultaneously, Xingyu Chen and colleagues have astutely observed that one ought not to overthink arithmetic conundrums such as "2 + 3 = ?" Their insights into the potential pitfalls of o1-like language models are meticulously documented in arXiv preprint 2412.21187, a testament to their sagacious inquiry.

Moreover, the year 2024 has borne witness to Jeffrey Cheng and Benjamin Van Durme's scholarly endeavor, "Compressed Chain of Thought." This work elucidates the efficient reasoning processes through dense representations, an intellectual feast for those who partake in arXiv preprint 2412.13171.

The illustrious Karl Cobbe et al., in their dual treatises of 2021a and 2021b, have embarked on a noble quest to train verifiers adept at solving mathematical word problems. Their findings are enshrined within the hallowed pages of ArXiv under abs/2110.14168.

As we traverse further into this intellectual odyssey, DeepSeek-AI's 2025 preprint, arXiv:2501.12948, reveals their endeavor to incentivize reasoning capabilities in language models via reinforcement learning—a pursuit most commendable and ingenious.

The year 2024 also heralds the contributions of Yuntian Deng et al., who have ventured into realms both explicit and implicit within the chain of thought. Their explorations are chronicled in arXiv preprint 2405.14838, a beacon for those navigating the complexities of cognitive internalization.

In a similar vein, Mengru Ding and colleagues have illuminated the path with their revelation that large language models can indeed be shortcut reasoners, as detailed in arXiv preprint 2406.06580—a discovery both startling and profound.

The Llama 3 herd of models, as expounded upon by Abhimanyu Dubey et al., offers a cornucopia of insights within arXiv preprint 2407.21783. Their collective wisdom is a testament to the collaborative spirit that drives progress in our field.

Lastly, I must draw your attention to the scholarly pursuits of Xinyu Guan and associates in 2025. Their work, "Training Compute-Optimal Large Language Models," encapsulated in arXiv preprint 2203.15556, stands as a paragon of efficiency and intellectual rigor.

Thus, dear readers, I have endeavored to convey the essence of these scholarly works with due diligence and reverence. May they serve as beacons of enlightenment in our ceaseless quest for knowledge.

Yours in perpetual curiosity,

[Your Name]

My Dearest Confidant,

In the year of our Lord 2023, a profound discourse hath been undertaken by the learned scholars Choi and Xiang Ren concerning the utility of machine rationales to mankind. Their treatise, presented in the scholarly preprint of arXiv, did delve into the intricacies of human understanding when faced with free-text rationales produced by mechanical minds.

Anon, in the year 2024, Yu Kang and associates hath ventured forth a novel creation known as C3ot, aiming to generate succinct yet potent chains of thought without diminishing their efficacy. This pursuit was chronicled in another preprint of arXiv, demonstrating man's ceaseless quest for intellectual refinement.

The learned men Levente Kocsis and Csaba Szepesvari did present a study on the Bandit-based Monte-Carlo planning during the European Conference on Machine Learning in 2006. This work doth explore the depths of algorithmic foresight in decision-making processes, as befits the grand tradition of scholarly inquiry.

In the same year of grace, Hunter Lightman and his esteemed colleagues hath embarked upon a project to verify each step of reasoning with utmost precision. Their findings were shared at The Twelfth International Conference on Learning Representations, reflecting humanity's insatiable desire for accuracy and understanding.

Furthermore, in 2024, Shih-Yang Liu and fellow scholars introduced Dora, an ingenious method of weight-decomposed low-rank adaptation during the esteemed ICML conference. This endeavor seeks to enhance the adaptability and efficiency of computational models, a testament to man's ingenuity.

The year 2025 did witness Haotian Luo and his companions proposing O1-pruner, a technique for length-harmonizing fine-tuning aimed at refining reasoning pruning. Their preprint in arXiv heralds a new era of precision in cognitive computation.

In the same period, Liangchen Luo and associates endeavored to augment mathematical reasoning within language models through automated process supervision, as documented in their 2025 study.

Moreover, Nanye Ma and his team did explore inference-time scaling for diffusion models beyond merely scaling denoising steps. Their work, presented as a preprint, seeks to push the boundaries of computational understanding further into the realms of possibility.

Yu Meng and colleagues, in 2024, introduced Simpo, a method of simple preference optimization without reliance on reference-based rewards. This innovative approach was shared at NeurIPS, showcasing yet another stride towards refining machine learning techniques.

In conclusion, these scholarly pursuits reflect humanity's relentless quest for knowledge and understanding, as we endeavor to harness the potential of mechanical minds to augment our own intellectual capacities. May these endeavors continue to illuminate the path towards a brighter future.

Yours sincerely,

Robinson Crusoe

Dearest Reader,

I find myself compelled to convey unto thee a missive most scholarly and embellished in style, recounting the proceedings of learned assemblies where sagacious minds did congregate. In an age not far removed from ours by mere years, during the illustrious 62nd Annual Meeting of the Association for Computational Linguistics, held within the vibrant confines of Bangkok's resplendent realms, diverse treatises were presented.

The noble assemblage was graced with revelations most profound; among these, a discourse entitled "Self-consistency Improves Chain of Thought Reasoning in Language Models" by esteemed scholars Wang, Wei, Schuurmans, Le, and others. Their exposition did enlighten the gathering on how self-reflection in artificial minds can lead to more reasoned discourse.

A subsequent colloquium presented an inquiry into "Chain-of-Thought Prompting," wherein Wei, Wang, Schuurmans, Bosma, and their esteemed colleagues elucidated upon techniques that entice grand computational intellects into a more deliberate manner of reasoning, thus enhancing their capacity for contemplation.

In another treatise, the scholars Yao, Yu, Zhao, and their companions unveiled "Tree of Thoughts," an elaborate schema wherein machines engage in meticulous problem-solving, akin to a learned man pondering upon his studies with great care.

Moreover, the year 2025 bore witness to yet another profound contribution by Ye et al., who expounded upon "LIMO: Less is More for Reasoning," suggesting that oftentimes simplicity begets clarity and wisdom. Concurrently, Yu, Xu, Weston, and Kulikov pondered deeply on the distillation of System 2 into System 1 in their scholarly work.

Zhang's treatise did reveal how one might access solutions of Olympiad caliber through the use of Monte Carlo trees with refined models, a feat worthy of great admiration. Meanwhile, another Zhang et al., presented "The Lessons of Developing Process Reward Models," imparting wisdom on mathematical reasoning.

In matters practical, experiments upon LLaMA revealed that when evaluating model performance, strict metrics must be employed to avoid fallacies wrought by repetition. The training settings for the various models—LLaMA-3.1-8B, LLaMA-3.2-1B-Instruct, and QwQ-32B-Preview—were expounded with meticulous care, each setting described in grand detail regarding hardware employed, batch sizes, learning rates, weight decays, and epochs of training.

Thus, I have endeavored to recount the salient points of these scholarly gatherings with all due reverence for their intellectual rigor. May this letter find thee in good health and keen intellect.

Yours sincerely,

Robinson Crusoe

My Dearest Companion,

I find myself compelled to recount the intricate and scholarly endeavours I have undertaken, in a manner most befitting the grandiloquent style of our venerable King James. Perchance you will find solace and enlightenment within these musings.

In my diligent experimentation with the apparatus known as DeepSeek-R1-Distill-Llama-8B, I set forth to traverse the intellectual landscape utilizing the esteemed MixChain-zero-shot-GSM8K dataset. With a batch size of one hundred twenty-eight units and no more than five epochs, our scholarly pursuits were conducted with utmost rigor. To ensure that the fruit of our labors would yield answers most profound, we have decreed a token limit of thirty thousand.

Furthermore, in my endeavours with Qwen2.5-32B-LIMO, I found myself fine-tuning Qwen-32B-Instruct using LIMO's esteemed guidance. This labor was undertaken upon four H100 GPUs across ten epochs, with each batch containing but four units and a sequence length not exceeding sixteen thousand tokens. The rate of learning was meticulously set at five times ten to the power of minus six.

With a touch most delicate, I adjusted α, birthing solutions twain: solution one (α equalling eight tenths) and solution naught (α equaling six tenths). Thereupon, θ2 was trained for an additional five epochs with a batch size now doubled and adorned with a learning rate of the same order. With weight decay set at one percent, I beheld the MixChain-Solution 0 in Table two emerge.

Unlike prior endeavours, this experiment engaged in full fine-tuning rather than LoRA's customary approach. The sequence length thus generated was not to exceed fifteen thousand tokens.

In an appendix of sorts, labeled A.3, I elucidate upon our datasets: MixChain-C and MixChain-Z, crafted with meticulous care as previously described in Section 4.2. Within these collections, α values were judiciously selected from intervals most precise for LIMO and its counterparts. It was imperative to excise all incorrect responses.

The enigmatic transition from θ1 to θ2 within the realm of MixChain-Z remains shrouded in mystery. Yet, through careful study, numerous model pairs have been unveiled, their performance proving crucial to the dataset's integrity.

I remain,

Your humble and devoted servant,
Robinson Crusoe

P.S. For those who wish to delve deeper into these matters, I recommend visiting the Hugging Face repository for further enlightenment on DeepSeek-R1-Distill-Llama-8B.

My Dearest Companions,

In the solitude of my reflections, I have been engrossed in contemplation over a most intriguing scholarly document that has found its way to my isolated haven. Permit me, if you will, to convey the essence of this discourse with the grandiloquence and floridity befitting our esteemed King James English.

The treatise begins by laying before us "Table 9: Dataset Statistic," wherein the tokens are counted using a sophisticated mechanism known as the tokenizer from QwQ-32B-Preview. The ensuing numerical tapestry includes figures such as 817, 6984.1, and more, which delineate an intricate statistical landscape.

In the subsequent section, titled "Table 10: Results of LLaMA-3.2-1B-Instruct trained with DoRA using different α values for interpolation," a scholarly examination is presented. It speaks to the utilization of a method known as DoRA in training the LLaMA-3.2-1B model, wherein the parameter ∆θ is manipulated by varying the magnitude denoted by α. The document elucidates that, with an increase in α, there occurs a corresponding augmentation in chain length, suggesting the efficacious nature of such interpolation.

Furthermore, it is noted that the most exalted results are not begotten through direct training on long CoT data but rather via a synthesis using QwQ. Notably, at α = 1.0, an accuracy of 54.5 is achieved with 772.7 tokens; however, the optimal model, utilizing CoT-Valve at α=0.5, attains an even more impressive accuracy of 55.72 with but 257.7 tokens.

A noteworthy observation is made regarding the effect of attention mechanisms and MLPs on reasoning paths. It appears that attention bears lesser influence than the latter upon these intricate cognitive pathways.

Finally, a comparison is drawn among various modules tested against the GSM8K dataset—Attention, MLP, All Linear—and their respective impacts on accuracy, parameters, and token count. The meticulous analysis delineates subtle variations in performance metrics, with attention garnering an increase of 0.013% and all linear structures achieving 1.32.

Thus, dear reader, I have endeavoured to encapsulate the essence of this document within my humble epistle. May it serve as a beacon of knowledge amidst your own intellectual pursuits.

Yours in contemplation,

Robinson Crusoe

My Dearest Acquaintance,

I pray this missive finds thee in good health and spirits, for I am compelled to regale thee with the latest of my scholarly endeavors, concerning a subject most curious and intricate. It hath been revealed through rigorous experimentation that applying the technique known as LoRA fine-tuning unto specific components within certain model structures yields results most profound.

The data presented in Table 11 doth suggest an intriguing disparity; when LoRA is applied solely to the query, key, or value projections, its efficacy pales in comparison to other linear projections upon the diminution of reasoning chains. This observation hath led us to conjecture that attention computation may indeed wield lesser sway over the control of chain length, whilst the Multi-Layer Perceptron layers and final projection within attention assume a role far more pivotal.

Moreover, we ventured into the realm of 'Prompt Control', assessing the lengths of CoT under specific constraints. Table 12 delineates our findings across various models, noting significant discrepancies between generated token counts and intended targets. It is worth mentioning that despite these variances, certain prompts maintain their influence over resultant lengths to a notable degree.

One particular case hath captured my curiosity: an examination wherein the same question yielded answers of varying lengths, from 103 to 300 tokens. Herein lies evidence of our method's capacity for generalization and extrapolation, producing reasoning processes longer than those of the original QwQ model yet achieving conciseness through refined reflection.

I remain, as ever, thy faithful servant in pursuit of knowledge,
Robinson Crusoe

Dearest Reader,

Upon careful perusal of the data inscribed herein, I find myself compelled to address a matter most perplexing. It appears that there exists significant discrepancies twixt the conditions set forth by the original prompt and the count of generated tokens as delineated in Table 12 pertaining to the GSM8k.

The aforementioned table doth reveal figures that stand at variance with what was anticipated, leading one to ponder the underlying causes of such divergence. These numbers—ranging from twenty unto five hundred, with varying increments—seem to defy the expectations set by our initial conditions.

It is a matter requiring diligent inquiry and perhaps an exploration into the intricacies of token generation methodologies. The deviation noted herein might stem from unforeseen complexities inherent in the process or potential misinterpretations of the prompt's stipulations.

I remain, with earnest desire for clarity and resolution,

Your humble servant,
R.C.

