MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
SRLM
-0.1
-0.3
-1.1
+2.2
-3.4
-5.0
Process-based (Ours)
+1.1
+1.0
+0.8
+2.3
+10.0
+12.5
Table 2: The results of LLMs’ mathematical performance changes after all iterations from M1 to M4.
M4 after multiple iterations in Table 2, which re-
flects the algorithm’s effectiveness and stability in
improving the model’s mathematical capabilities.
Our method achieves more stable and effective im-
provements across all benchmarks. On one hand,
using step-wise preference data enables the model
to focus on more fine-grained information; on the
other hand, conducting LLM-as-a-Judge on indi-
vidual steps helps the model more easily detect
subtle differences and errors.
The models show noticeable improvements
on some complex tasks. For some complex and
highly challenging benchmarks, such as MATH,
AIME2024, and AMC2023, LLMs’ performance
show significant improvement. Complex problems
require multi-step, long-thought reasoning. Our
method effectively leverages the model’s existing
knowledge to optimize the individual intermediate
reasoning steps, achieving favorable results.
Our method remains effective across models
of different parameter sizes. We validate our
method on both 7B and 72B LLMs to strengthen
our conclusions. We find performance improve-
ments across models of different parameter sizes
on multiple mathematical tasks through Process-
based Self-Rewarding. We also find that the 72B
model gains more stable improvements compared
to the 7B model, whose mathematical reasoning
and LLM-as-a-Judge capabilities are stronger.
Overall, we can find that the models iterat-
ing based on the Process-based Self-Rewarding
paradigm achieve significant improvements across
multiple mathematical tasks, outperforming the tra-
ditional self-rewarding method.
5.2
Further Analysis
Based on the above results, we conduct more anal-
ysis and observations of the pipeline.
Step-wise LLM-as-a-Judge Capability.
We
evaluate the LLMs’ ability to accurately assess rea-
soning steps as a reward model during the iterative
process. We test the model on the test set including
Model
7B
72B
M0 (3-shot)
57.2
73.4
M1
92.8 (↑)
95.6 (↑)
M2
91.6 (↓)
95.8 (↑)
M3
92.0 (↑)
95.2 (↓)
M4
92.2 (↑)
95.6 (↑)
Table 3: Judgment accuracy in step-wise LLM-as-a-
Judge. We report the results of models with different
parameter sizes. Additionally, we use arrows to indicate
the changes in accuracy during the iterations.
500 samples (§3.2). We report the results in Table
3. As shown in the table, LLMs achieve strong
reward model performance after initialization with
a small amount of EFT data, which indicates the
immense potential of LLMs for step-wise LLM-as-
a-Judge with CoT reasoning. Additionally, we can
observe that, under the same conditions, the larger
model exhibits stronger capabilities as a reward
model than the smaller one.
Additionally, although we mix EFT data and IFT
data for initialization and introduce no additional
LLM-as-a-Judge data during subsequent iterations,
the LLMs’ capabilities to perform LLM-as-a-Judge
as a reward model are still good. Furthermore, a
consistent pattern is observed across different mod-
