Process-based Self-Rewarding Language Models
Shimao Zhang‚ô£* Xiao Liu‚ãÜ‚Ä† Xin Zhang‚ãÜJunxiao Liu‚ô£Zheheng Luo3
Shujian Huang‚ô£‚Ä† Yeyun Gong‚ãÜ
‚ô£National Key Laboratory for Novel Software Technology, Nanjing University
3The University of Manchester
‚ãÜMicrosoft Research Asia
smzhang@smail.nju.edu.cn, huangsj@nju.edu.cn,
xiao.liu.msrasia@microsoft.com
Abstract
Large Language Models have demonstrated
outstanding performance across various down-
stream tasks and have been widely applied
in multiple scenarios. Human-annotated pref-
erence data is used for training to further
improve LLMs‚Äô performance, which is con-
strained by the upper limit of human perfor-
mance. Therefore, Self-Rewarding method has
been proposed, where LLMs generate training
data by rewarding their own outputs. How-
ever, the existing self-rewarding paradigm is
not effective in mathematical reasoning scenar-
ios and may even lead to a decline in perfor-
mance. In this work, we propose the Process-
based Self-Rewarding pipeline for language
models, which introduces long-thought rea-
soning, step-wise LLM-as-a-Judge, and step-
wise preference optimization within the self-
rewarding paradigm. Our new paradigm suc-
cessfully enhances the performance of LLMs
on multiple mathematical reasoning bench-
marks through iterative Process-based Self-
Rewarding, demonstrating the immense poten-
tial of self-rewarding to achieve LLM reasoning
that may surpass human capabilities. 1
1
Introduction
Large language models (LLMs) acquire power-
ful multi-task language capabilities through pre-
training on extensive corpus (Radford et al., 2019;
Brown et al., 2020). Additionally, supervised fine-
tuning (SFT) can further effectively improve the
model‚Äôs performance on end-tasks. However, it is
found that models after SFT are prone to halluci-
nations (Lai et al., 2024) due to the simultaneous
increasing of the probabilities of both preferred
and undesirable outputs (Hong et al., 2024). There-
fore, to further enhance the language capabilities of
*Work done during his internship at MSRA.
‚Ä†Corresponding authors.
1Our code and data will be available at: https://github.
com/Shimao-Zhang/Process-Self-Rewarding.
LLMs to align with human-level performance effec-
tively, researchers often utilize human-annotated
preference data for training. A representative ap-
proach is Reinforcement Learning from Human
Feedback (RLHF) (Christiano et al., 2017), which
utilizes RL algorithms and external reward signals
to help LLMs learn specific preferences.
However, most reward signals rely on human
annotations or reward models, which is expensive
and bottlenecked by human capability and reward
model quality. So the Self-Rewarding Language
Models paradigm (Yuan et al., 2024) is proposed to
overcome the above limitations, which integrates
the reward model and the policy model within the
same model. In this framework, a single model pos-
sesses the ability to both perform the target task and
provide reward feedback. The model can execute
different tasks based on the scenario and conduct
iterative updates. This paradigm is effective in
instruction-following scenarios, where the model
achieves performance improvement solely through
self-rewarding and iterative updates.
Although the self-rewarding algorithm performs
well in the instruction-following tasks, it is also
demonstrated that LLMs perform poorly on the
mathematical domain data based on the existing
self-rewarding algorithm. In fact, model‚Äôs perfor-
mance may even degrade as the number of iter-
ations increases (Yuan et al., 2024). We notice
two main limitations in the self-rewarding frame-
work: (a) Existing self-rewarding algorithm is not
able to provide fine-grained and accurate reward
signals for complex reasoning tasks involving long-
thought chains; (b) For a complex mathematical
solution, it‚Äôs hard to design the criterion for gen-
erating specific scores. It means that assigning
scores to complex long-thought multi-step reason-
ing for LLMs is more challenging than performing
pairwise comparisons, with lower consistency and
agreement with humans, which is proven by the
results in Appendix B.
1
arXiv:2503.03746v1  [cs.CL]  5 Mar 2025

In this work, we propose the paradigm of
Process-based Self-Rewarding Language Models,
where we introduce the step-wise LLM-as-a-Judge
and step-wise preference optimization into the
traditional self-rewarding framework. In a nut-
shell, we enable the LLMs to simultaneously con-
duct step-by-step complex reasoning and perform
LLM-as-a-Judge for individual intermediate steps.
For the limitation (a) above, to get finer-grained
and more accurate rewards, Process-based Self-
Rewarding paradigm allows LLMs to perform step-
wise LLM-as-a-Judge for the individual reasoning
step. Since producing the correct final answer does
not imply that LLMs can generate correct inter-
mediate reasoning steps, it is crucial to train the
model to learn not only to produce the correct final
answer but also to generate correct intermediate
reasoning steps. By using model itself as a reward
model to generate step preference pairs data, we
further perform step-wise preference optimization.
For the limitation (b) above, we design a LLM-as-
a-Judge prompt for step-wise pairwise comparison
rather than directly assigning scores to the answer
for more proper and steadier judgments based on
the observations in Appendix B.
We conduct the experiments on models in differ-
ent parameter sizes (7B and 72B) and test across
a wide range of mathematical reasoning bench-
marks. Our results show that Process-based Self-
Rewarding can effectively enhance the mathemat-
ical reasoning capabilities of LLMs, which indi-
cates that LLMs are able to perform effective self-
rewarding at the step level. Our models that it-
eratively trained based on the Process-based Self-
Rewarding paradigm demonstrate an increasing
trend in both mathematical and LLM-as-a-judge
capabilities. These results suggest this framework‚Äôs
immense potential for achieving intelligence that
may surpass human performance.
2
Background
2.1
Reinforcement Learning from Human
Feedback
Supervised Fine-tuning is an effective method to
improve LLMs‚Äô performance across many differ-
ent downstream tasks. But it has been evidenced
that SFT potentially exacerbates LLMs‚Äô halluci-
nation (Hong et al., 2024). So RLHF is further
utilized to align LLMs with human preference. In
the RLHF paradigm, the model is trained based
on reward signals provided by external reward
models and humans by reinforcement learning al-
gorithms, such as PPO (Schulman et al., 2017),
DPO (Rafailov et al., 2024), SimPO (Meng et al.,
2024), and so on. Direct Preference Optimization
(DPO) is a preference learning algorithm which
directly uses pairwise preference data, including
chosen and rejected answers for optimization. Fur-
thermore, the step-wise preference optimization
has also been investigated for long-chain reason-
ing and has shown great performance (Lai et al.,
2024; Chen et al., 2024). In our work, we intro-
duce the step-wise preference optimization into our
Process-based Self-Rewarding paradigm for more
fine-grained learning.
2.2
LLM-as-a-Judge
LLM-as-a-Judge technique has been widely used
for evaluation tasks because of LLMs‚Äô scalabil-
ity, adaptability, and cost-effectiveness (Gu et al.,
2024). In the LLM-as-a-Judge scenarios, LLMs
are prompted to mimic human reasoning and evalu-
ate specific inputs against a set of predefined rules.
To improve the performance of LLM-as-a-Judge,
the LLM acting as the evaluator is trained to align
with human preferences. When conducting LLM-
as-a-Judge, LLMs can play many different roles
depending on the given prompt. Typical applica-
tions include tasks where LLMs are prompted to
generate scores (Li et al., 2023; Xiong et al., 2024),
perform pairwise comparisons (Liu et al., 2024;
Liusie et al., 2023), rank multiple candidates (Yuan
et al., 2023), and so on. However, the LLM-as-a-
Judge for individual mathematical reasoning steps
has not been widely investigated. In our experi-
ment, we design the step-wise LLM-as-a-Judge for
rewarding and analyze its performance.
2.3
Self-Rewarding Language Models
Although RLHF has been widely utilized to align
LLMs with human-level performance and has
achieved impressive performance, the existing
methods heavily rely on high-quality reward mod-
els or human feedback, which bottlenecks these
approaches. To avoid this bottleneck, Yuan et al.
(2024) propose the Self-Rewarding Language Mod-
els paradigm, which uses a single model as both
instruction-following model and reward model si-
multaneously. The iterative self-rewarding algo-
rithm operates by having the model generate re-
sponses and reward the generated response candi-
dates, then selecting preference pairs for training.
Based on this, Wu et al. (2024) further improve
2

ùëÄ0
Base model
search
{ùë•, ùë†1, ‚Ä¶ , ùë†ùëô‚àí1, ùë†ùëô
1, ùë†ùëô
2, ùëóùë¢ùëëùëîùëí}
EFT Data
{ùë•, ùë¶}
IFT Data
Initialization
ùëÄùëñ
‚Ä¶‚Ä¶
‚Ä¶‚Ä¶
ùë†ùëô
ùëèùëíùë†ùë°
ùë†ùëô
ùë§ùëúùëüùë†ùë°
ùë†ùëô‚àí1
ùëèùëíùë†ùë°
‚Ä¶‚Ä¶
Step-by-step
Reasoning
Step-wise
LLM-as-a-Judge
{ùë•, ùë†1~ùëô‚àí1, ùë†ùëô
ùëè, ùë†ùëô
ùë§}
Step-wise Preference Data
ùëÄùëñ+1
Step-wise Preference 
Optimization
Figure 1: Illustration of our Process-based Self-Rewarding paradigm. (1) We get EFT data by tree-search, initial
data filtering and data annotation. And we get IFT data by step segmentation. (2) The model is initialized on EFT
and IFT data. (3) The model conducts step-by-step search-based reasoning and performs step-wise LLM-as-a-Judge
to select the chosen step and generate the step-wise preference pair at each step. (4) We perform step-wise preference
optimization on the model. (5) The model enters the next iteration cycle.
the judgment agreement by adding the LLM-as-a-
Meta-Judge action into the self-rewarding pipeline,
which allows the model to evaluate its own judg-
ments. But the existing self-rewarding methods
mainly focus on the instruction-following tasks
and perform poorly in the mathematical domain
data (Yuan et al., 2024). And evaluating the entire
response makes it difficult for the model to learn
fine-grained preference information.
For some
long-thought reasoning tasks, it is important to en-
able LLMs to focus on and learn the fine-grained
reasoning step preference information.
2.4
Step-by-step Reasoning
Complex reasoning tasks are still great challenges
for LLMs now. Chain-of-Thought (Wei et al., 2022)
methods prompt LLMs to solve the complex prob-
lems by reasoning step by step rather than generat-
ing the answer directly, which leads to significant
improvements across many reasoning tasks (Yoran
et al., 2023; Fu et al., 2022; Zhang et al., 2022). Fur-
thermore, recent studies investigate the test-time
scaling paradigm which allows the LLMs to use
more resources and time for inference to achieve
better performance (Lightman et al., 2023) typi-
cally based on search and step selecting (Yao et al.,
2024; Wang et al., 2024b). These results high-
light the importance of conducting high-quality
long-thought step-by-step reasoning for LLMs in
solving complex reasoning problems.
3
Process-based Self-Rewarding
Language Models
In this section, we propose our new Process-
based Self-Rewarding Language Models pipeline.
We first review the existing self-rewarding algo-
rithm and our motivation as a preliminary study
in ¬ß3.1. Then we introduce our novel paradigm
for more fine-grained step-wise self-rewarding and
self-evolution. The entire pipeline consists of se-
quential stages: model initialization (¬ß3.2), rea-
soning and preference data generation (¬ß3.3), and
model preference optimization (¬ß3.4). Finally, we
provide a summarized overview of our algorithm
(¬ß3.5). We illustrate the entire pipeline in Figure 1.
3.1
Preliminary Study
Most existing preference optimization algorithms
rely on reward signals from external reward mod-
els or human-annotated data. However, deploying
an external reward model or getting ground truth
gold reward signals from human annotators is ex-
pensive (Gao et al., 2023). Moreover, due to the
inherent limitations and implicit biases of both hu-
mans and reward models, these model optimization
strategies are bottlenecked (Lambert et al., 2024;
Yuan et al., 2024). Thus, Self-Rewarding algorithm
is proposed to mitigate this limitation by enabling
the model to provide reward signals for its own out-
puts and perform self-improvement, showing the
feasibility of achieving models that surpass human
performance (Yuan et al., 2024).
There are still many aspects waiting for further
research and improvement in the self-rewarding
3

framework. The original method is primarily de-
signed for instruction-following tasks and performs
poorly on mathematical reasoning data. Step-by-
step long-chain reasoning is widely used for com-
plex mathematical reasoning, which allows the
models to conduct more detailed thinking and fine-
grained verification of the reasoning steps (Light-
man et al., 2023; Wang et al., 2024b; Lai et al.,
2024).
Given the effectiveness of step-by-step
reasoning, we further propose Process-based Self-
Rewarding, introducing LLM-as-a-judge and pref-
erence optimization for individual steps.
3.2
Model Initialization
To perform Process-based Self-Rewarding, models
need to possess two key abilities:
‚Ä¢ Step-by-step mathematical reasoning: When
faced with a complex reasoning problem, the
model needs to think and reason step by step,
outputting the reasoning process in a specified
format. (Each step is prefixed with ‚ÄúStep n: ‚Äù,
where n indeicates the step number.)
‚Ä¢ LLM-as-a-Judge for individual steps: The
model should be able to assess the quality
of the given next reasoning steps based on the
existing problem and partial reasoning steps
and provide a detailed explanation.
We construct data separately for the two tasks to
perform cold start. Following Yuan et al. (2024),
we refer to them as Instruction Fine-Tuning (IFT)
data and Evaluation Fine-Tuning (EFT) data. For
IFT data, we divide the given solution steps into
individual steps logically without altering any in-
formation in the original solution by using OpenAI
o1 (Jaech et al., 2024).
For
EFT
data,
since
there
is
no
avail-
able step-wise LLM-as-a-Judge dataset, we first
train Qwen2.5-72B (Yang et al., 2024a) on
PRM800k (Lightman et al., 2023) following Wang
et al. (2024a). After getting a Process Reward
Model (PRM) by this, which can output a single
label ‚Äú+‚Äù or ‚Äú-‚Äù for a reasoning step based on the
question and the previous steps, we conduct Monte
Carlo Tree Search (MCTS) on a policy model. We
use the probability of label ‚Äú+‚Äù of the above PRM
to compare the relative quality of all candidate steps
at the same layer, and choose the best and the worst
step as a data pair. After the initial data filtering
process, we use GPT-o1 to generate judgments and
detailed explanations for the obtained data pairs.
The pairs whose judgments align with the previous
PRM assessments are selected as the final EFT data.
Additionally, to enhance consistency, we evaluate
each pair twice using GPT with different input or-
ders and select only the pairs that have consistent
results.
3.3
Step-by-step Long-chain Reasoning and
Preference Data Generation
After the ‚ÄúEFT + IFT‚Äù initialization stage, the
model is able to conduct both step-wise LLM-as-
a-Judge and step-by-step mathematical reasoning
in the specified formats. Because we conduct pair-
wise comparison rather than single answer grading,
we utilize the following search strategy:
Sl = {sl,1, sl,2, sl,3, ..., sl,w‚àí1, sl,w}
(1)
where Sl is all candidates for the next step, l is the
step number starting from 1, w is a hyperparameter
to specify the search width for each step.
Scorel,i =
X
1‚â§j‚â§w, jÃ∏=i
O(sl,i, sl,j | x, s1, s2, ..., sl‚àí1)
(2)
where l is the next step number, sl,i indicates the
i-th candidate for the next l-th step, x is the prompt,
and O is a function that takes 1 when sl,i is consid-
ered better than sl,j and 0 otherwise.
sbest
l
= Sl[max(Scorel)]
(3)
sworst
l
= Sl[min(Scorel)]
(4)
sl = sbest
l
(5)
where max(Scorel) is the index of the candidate
with the highest score and min(Scorel) corre-
sponds to the lowest score. sl is the final chosen
l-th step. (sbest
l
, sworst
l
) will be chosen as a chosen-
rejected preference pair.
This process will be repeated continuously until
generation is complete. It is important to note that
to enhance the effectiveness of preference data,
if max(Scorel) is equal to min(Scorel), we will
discard the existing sl‚àí1 and (sbest
l‚àí1, sworst
l‚àí1 ) and roll
back to the previous step.
3.4
Step-wise Model Preference Optimization
With preference data collected in the Section 3.3,
we conduct preference optimization training on
the model. We choose Direct Preference Optimiza-
tion (DPO) as the training algorithm (Rafailov et al.,
2024). The difference is that we conduct a more
4

fine-grained step-wise DPO in our work. The simi-
lar method has also been investigated by Lai et al.
(2024). We can calculate the training loss as:
A = Œ≤ log œÄŒ∏(sb
l | x, s1, ..., sl‚àí1)
œÄref(sb
l | x, s1, ..., sl‚àí1)
(6)
B = Œ≤ log œÄŒ∏(sw
l | x, s1, ..., sl‚àí1)
œÄref(sw
l | x, s1, ..., sl‚àí1)
(7)
L(œÄŒ∏; œÄref) = ‚àíE(x,s1,...,sb
l ,sw
l )‚àºD[log œÉ(A ‚àíB)]
(8)
where x is the prompt, s1, ..., sl‚àí1 is the previous
steps, sb
l and sw
l are the best and worst steps re-
spectively for the l-th step, Œ≤ is a hyperparameter
controlling the deviation from the base reference
policy, œÄŒ∏ and œÄref are the policies to be optimized
and the reference policy respectively.
After the preference optimization stage, we have
the model for the next cycle. In the next iteration,
we sequentially repeat the steps in ¬ß3.3 and ¬ß3.4.
3.5
Iteration Pipeline
We show the entire pipeline of our algorithm. Fol-
lowing Yuan et al. (2024), we refer to the model
after n iterations as Mn.
And we refer to the
Pair-wise Preference Data generated by Mn as
PPD(Mn). Then the sequence in our work can
be defined as:
‚Ä¢ M0: The base model.
‚Ä¢ M1: The model obtained by supervised fine-
tuning (SFT) M0 on ‚ÄúEFT + IFT‚Äù data.
‚Ä¢ M2: The model obtained by training M1 on
PPD(M1) using step-wise DPO.
‚Ä¢ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑
‚Ä¢ Mn: The model obtained by training Mn‚àí1
on PPD(Mn‚àí1) using step-wise DPO.
In summary,
we initialize the base model
using well-selected step-wise LLM-as-a-Judge
data (EFT) and step-by-step long-thought reason-
ing data (IFT). Once the model possesses the corre-
sponding two abilities, we select preference pairs
through search and reward signals provided by the
model itself, and train the model using step-wise
DPO. Then we iterate the model by repeatedly per-
forming the above operations.
4
Experimental Setup
We conduct our experiments on models in different
parameter sizes and several representative mathe-
matical reasoning benchmarks. In this section, we
introduce our experimental settings in detail.
Models
We
choose
the
base
model
from
Qwen2.5-Math series (Yang et al., 2024b) in our
experiments, which is one of the most popular
open-source LLM series. Specifically, we choose
Qwen2.5-Math-7B and Qwen2.5-Math-72B. Addi-
tionally, we choose OpenAI GPT-o1 (Jaech et al.,
2024) for our initialization data processing (¬ß3.2).
Datasets
In our experiments, we mainly focus on
two capabilities of the model:
‚Ä¢ Step-by-step Mathematical Reasoning: We
choose a subset of NuminaMath (LI et al.,
2024) for IFT data construction, whose so-
lutions have been formatted in a Chain of
Thought (CoT) manner. We extract a subset
of 28,889 samples and prompt GPT-o1 (Jaech
et al., 2024) to logically segment the solutions
into step-by-step format without altering any
original content. The corresponding prompt is
presented in Figure 3. And the instruction for-
mat for step-by-step long-thought reasoning
is presented in Figure 4.
‚Ä¢ Step-wise LLM-as-a-Judge: As described in
the Section 3.2, we first filtrate some prefer-
ence pairs using the trained PRM. Then we
utilize GPT-o1 and get a total of 4,679 EFT
data with judgments and detailed explanations.
Finally we split the whole dataset into 4,167
samples as the training set and 500 samples
as the test set. The instruction format for step-
wise pairwise LLM-as-a-Judge is presented in
Figure 5, which is following the basic format
of Zheng et al. (2023).
And for mathematical task evaluation, fol-
lowing Yang et al. (2024b), we evaluate the
LLMs‚Äô mathematical capabilities across some rep-
resentative benchmarks. We choose the widely
used benchmarks GSM8k (Cobbe et al., 2021)
and MATH (Hendrycks et al., 2021). We also
choose some complex and challenging competi-
tion benchmarks, including Gaokao2023En (Liao
et al., 2024), Olympiadbench (He et al., 2024),
AIME20242, and AMC20233.
Evaluation Metrics
We use accuracy as the eval-
uation metric for both the mathematical perfor-
mance and LLM-as-a-Judge quality. For accuracy
2https://huggingface.co/datasets/AI-MO/
aimo-validation-aime
3https://huggingface.co/datasets/AI-MO/
aimo-validation-amc
5

Model
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
Avg.
GPT-4o
92.9
76.6
67.5
43.3
10.0
47.5
56.3
7B Base Model
M0
70.1
51.7
51.2
21.3
0.0
22.5
36.1
SRLM - M1
88.2
69.0
61.6
37.6
10.0
45.0
51.9
M2
87.6
69.4
63.9
37.2
3.3
40.0
50.2
M3
88.5
70.0
61.3
36.7
10.0
40.0
51.1
M4
88.3
70.2
63.9
37.6
13.3
45.0
53.1
PSRLM - M1
88.5
69.5
61.8
36.0
6.7
45.0
51.3
M2
88.8
69.7
63.9
36.3
16.7
47.5
53.8
M3
88.5
72.2
64.7
39.9
10.0
50.0
54.2
M4
88.8
73.3
65.2
38.7
13.3
55.0
55.7
72B Base Model
M0
87.5
69.7
55.3
28.9
10.0
40.0
48.6
SRLM - M1
92.9
76.4
67.3
41.8
16.7
47.5
57.1
M2
92.1
76.1
66.8
42.1
20.0
55.0
58.7
M3
92.5
75.8
67.5
42.5
20.0
52.5
58.5
M4
92.8
76.1
66.2
44.0
13.3
42.5
55.8
PSRLM - M1
92.6
75.6
67.3
41.8
13.3
45.0
55.9
M2
92.6
76.4
67.8
41.8
20.0
57.5
59.4
M3
93.7
76.4
67.3
42.7
23.3
52.5
59.3
M4
93.7
76.6
68.1
44.1
23.3
57.5
60.6
Table 1: Accuracy of Process-based Self-Rewarding based on 7B and 72B base models. SRLM is the self-rewarding
language model algorithm as the baseline. We bold the best results for each parameter size in each benchmark.
calculation on mathematical benchmarks, we fol-
low the implementation of Yang et al. (2024b).
Implementations
For initial PRM training, we
fine-tune full parameters on 128 NVIDIA A100
GPUs for 1 epoch with learning_rate=1e ‚àí5
and batch_size=128.
For preliminary prefer-
ence pairs selection, we set simulation_depth=3,
num_iterations=100,
T=0.7,
and top_p=0.95.
When training M0 to M1, we utilize 28,889
IFT and 4,179 EFT samples.
We fine-tune
LLMs‚Äô full parameters on 32 NVIDIA H100
GPUs for 3 epochs with learning_rate=1e ‚àí6 and
batch_size=32.
During the reasoning and pref-
erence data generation stage, we utilize temper-
ature sampling which trade-off generation quality
and diversity (Zhang et al., 2024). We set T=0.5,
top_p=0.95. The search width for each step is set
to 6, and the max iteration number is set to 20. Fi-
nally, in the step-wise preference optimization, we
train LLM‚Äôs full parameters on 32 NVIDIA H100
GPUs for 1 epoch with learning_rate=5e ‚àí7 and
batch_size=32. To get models from M2 to M4, we
use 400, 800, and 1, 200 math questions for pref-
erence pairs generation respectively, which are all
sampled from the train subset of NuminaMath. For
all solution-scoring judgment strategy experiments,
we use the same prompt template of Yuan et al.
(2024). We use greedy search in evaluations.
5
Results
In this section, we report our main results on differ-
ent mathematical benchmarks and conduct some
discussions and analyses based on the results.
5.1
Main Results
We report the performance of M0 to M4 based
on Qwen2.5-Math-7B and Qwen2.5-Math-72B re-
spectively in Table 1. Our findings are as follows:
As the number of iterations increases, the
overall performance of the model improves.
Traditionally, external reward signals and train-
ing data are utilized for improving LLMs‚Äô perfor-
mance. Our results indicate that models‚Äô overall
performance on mathematical tasks significantly
improves from M1 to M4 solely through Process-
based Self-Rewarding and step-wise preference op-
timization without any additional guidance. This
leverages the potential of LLMs for both mathemat-
ical reasoning and as evaluators.
Our fine-grained algorithm outperforms the
tranditional method. After three iterations, our
approach achieves superior performance compared
to method that applies rewards and conducts train-
ing on the entire response. Given that the initial-
ization with different EFT data lead to different
M1 fiducial performance in the two methods, we
also report the performance changes from M1 to
6

7B
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
SRLM
+0.1
+1.2
+2.3
0.0
+3.3
0.0
Process-based (Ours)
+0.3
+3.8
+3.4
+2.7
+6.6
+10.0
72B
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
SRLM
-0.1
-0.3
-1.1
+2.2
-3.4
-5.0
Process-based (Ours)
+1.1
+1.0
+0.8
+2.3
+10.0
+12.5
Table 2: The results of LLMs‚Äô mathematical performance changes after all iterations from M1 to M4.
M4 after multiple iterations in Table 2, which re-
flects the algorithm‚Äôs effectiveness and stability in
improving the model‚Äôs mathematical capabilities.
Our method achieves more stable and effective im-
provements across all benchmarks. On one hand,
using step-wise preference data enables the model
to focus on more fine-grained information; on the
other hand, conducting LLM-as-a-Judge on indi-
vidual steps helps the model more easily detect
subtle differences and errors.
The models show noticeable improvements
on some complex tasks. For some complex and
highly challenging benchmarks, such as MATH,
AIME2024, and AMC2023, LLMs‚Äô performance
show significant improvement. Complex problems
require multi-step, long-thought reasoning. Our
method effectively leverages the model‚Äôs existing
knowledge to optimize the individual intermediate
reasoning steps, achieving favorable results.
Our method remains effective across models
of different parameter sizes. We validate our
method on both 7B and 72B LLMs to strengthen
our conclusions. We find performance improve-
ments across models of different parameter sizes
on multiple mathematical tasks through Process-
based Self-Rewarding. We also find that the 72B
model gains more stable improvements compared
to the 7B model, whose mathematical reasoning
and LLM-as-a-Judge capabilities are stronger.
Overall, we can find that the models iterat-
ing based on the Process-based Self-Rewarding
paradigm achieve significant improvements across
multiple mathematical tasks, outperforming the tra-
ditional self-rewarding method.
5.2
Further Analysis
Based on the above results, we conduct more anal-
ysis and observations of the pipeline.
Step-wise LLM-as-a-Judge Capability.
We
evaluate the LLMs‚Äô ability to accurately assess rea-
soning steps as a reward model during the iterative
process. We test the model on the test set including
Model
7B
72B
M0 (3-shot)
57.2
73.4
M1
92.8 (‚Üë)
95.6 (‚Üë)
M2
91.6 (‚Üì)
95.8 (‚Üë)
M3
92.0 (‚Üë)
95.2 (‚Üì)
M4
92.2 (‚Üë)
95.6 (‚Üë)
Table 3: Judgment accuracy in step-wise LLM-as-a-
Judge. We report the results of models with different
parameter sizes. Additionally, we use arrows to indicate
the changes in accuracy during the iterations.
500 samples (¬ß3.2). We report the results in Table
3. As shown in the table, LLMs achieve strong
reward model performance after initialization with
a small amount of EFT data, which indicates the
immense potential of LLMs for step-wise LLM-as-
a-Judge with CoT reasoning. Additionally, we can
observe that, under the same conditions, the larger
model exhibits stronger capabilities as a reward
model than the smaller one.
Additionally, although we mix EFT data and IFT
data for initialization and introduce no additional
LLM-as-a-Judge data during subsequent iterations,
the LLMs‚Äô capabilities to perform LLM-as-a-Judge
as a reward model are still good. Furthermore, a
consistent pattern is observed across different mod-
els where evaluation accuracy initially increases,
then decreases, and finally rises again. Based on
the analysis above, initially, LLMs gain strong eval-
uation capabilities through training on EFT data.
And there is a temporary decline (but very slight)
due to training on mathematical data. Ultimately,
as the model‚Äôs mathematical abilities improve, its
ability to evaluate mathematical reasoning steps
also increases.
Data Distribution Analysis.
Following Yuan
et al. (2024), we also analyze the distribution of
different data. We utilize Bert (Devlin, 2018) for
embedding and t-SNE (Van der Maaten and Hin-
ton, 2008) based on the implementation of PoliÀácar
et al. (2024) for visualization. We present the re-
sults in Figure 2. For prompts, the distributions of
7

60
40
20
0
20
40
40
30
20
10
0
10
20
30
EFT Prompts
IFT Prompts
PPD Prompts
(a) Prompt Distributions
40
30
20
10
0
10
20
30
40
20
15
10
5
0
5
10
15
20
EFT Responses
IFT Responses
PPD Responses
(b) Response Distributions
Figure 2: The data distribution of prompts and responses in EFT (red), IFT (blue) and PPD (grey) data.
Iterations
Step Num
Step Length
GSM8k
MATH
GSM8k
MATH
M1
5.89
8.41
47.79
61.00
M2
5.55
7.64
51.19
67.17
M3
5.10
6.30
57.75
80.46
M4
4.87
5.54
62.86
96.63
Table 4: Statistics of step number and step length on
GSM8k and MATH benchmarks based on 72B models.
The full results are reported in Appendix A.
EFT data and IFT data do not overlap, allowing the
model to distinctly learn two different task patterns.
For models‚Äô responses, we can find the similar phe-
nomenon that the distribution of PPD and IFT re-
sponses is distinct from EFT‚Äôs, which reduces the
mutual interference between LLMs‚Äô two capabili-
ties during iteration. This allows the model‚Äôs abil-
ity to perform LLM-as-a-Judge to improve along-
side its mathematical ability finally, without being
overly influenced by the training data itself.
Step Number and Length of Responses.
Step-
by-step reasoning is important for LLMs to solve
complex reasoning tasks. Therefore, we conduct
statistical analysis on the reasoning steps during
iterations.
As shown in Table 4, for the same
model, more difficult problems require more rea-
soning steps and longer step lengths. As the iter-
ations progress, the step number across different
tasks decreases, while the length of each step in-
creases. This indicates that performing Process-
based Self-Rewarding encourages the model to
generate longer and higher-quality single reasoning
steps, which helps to reach final answers with fewer
steps. Additionally, this behavior is also related to
LLMs‚Äô preferences when performing LLM-as-a-
Strategy
Greedy Search
Test-time Scaling
M1
55.9
58.2
M4
60.6
62.4
Table 5: The average results of 72B model on all bench-
marks using greedy search or test-time scaling. The full
results are reported in Table 9.
Judge evaluations. More results are in Appendix A.
Test-time Scaling with Process-based Self-
Rewarding Language Models.
In the test-time
scaling, LLMs conduct step search and select based
on the rewards from PRM. Although we don‚Äôt pri-
marily focus on the test-time scaling performance
in our work, LLMs in the Process-based Self-
Rewarding paradigm naturally have the ability to
perform test-time scaling based on self-rewarding.
We perform 6 generations for each step with the
temperature of 0.5 and select the best one. The
results we report in Table 5 indicate that the model
achieves better performance through test-time scal-
ing compared to generating directly. Additionally,
the model‚Äôs performance with test-time scaling im-
proves after iterations from M1 to M4, which corre-
sponds to the uptrend of the model‚Äôs mathematical
abilities and LLM-as-a-Judge capabilities.
6
Conclusion
We propose a novel paradigm, Process-based Self-
Rewarding Language Models, that enables LLMs
to perform step-by-step long-thought mathematical
reasoning and step-wise LLM-as-a-Judge simulta-
neously. Given the characteristics of complex math
reasoning tasks, we introduce the step-by-step rea-
soning, step-wise LLM-as-a-Judge and step-wise
8

preference optimization technique into the frame-
work. Our results indicate that Process-based Self-
Rewarding algorithm outperforms the original Self-
Rewarding on a variety of complex mathematical
reasoning tasks, showing potential of stronger rea-
soning ability better than human in the future.
7
Limitations
We aim to draw more attention to the study of adapt-
ing the self-rewarding paradigm to the complex
mathematical reasoning tasks, which allows for the
possibility of continual improvement beyond the
human preferences. Although our new Process-
based Self-Rewarding algorithm has shown effec-
tive improvements across different mathematical
reasoning tasks, there are still some limitations
waiting for further research. Although we success-
fully enable the model to perform effective step-
wise LLM-as-a-Judge with a small amount of EFT
data, the basic capabilities of initialized M1 model
directly influence the effectiveness of subsequent
process-based self-rewarding. Utilizing more high-
quality data to initialize LLMs more adequately
may lead to stronger performance.
Additionally, due to the limited resources, we
only conduct the process-based self-rewarding ex-
periments from M1 to M4. Building on this, con-
ducting experiments with more iterations to ex-
plore the impact of iteration count on LLMs‚Äô per-
formance can help us better understand and utilize
the process-based self-rewarding method.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877‚Äì1901.
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai
Fan. 2024.
Step-level value preference optimiza-
tion for mathematical reasoning.
arXiv preprint
arXiv:2406.10858.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. Ad-
vances in neural information processing systems, 30.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168.
Jacob Devlin. 2018. Bert: Pre-training of deep bidi-
rectional transformers for language understanding.
arXiv preprint arXiv:1810.04805.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and
Tushar Khot. 2022. Complexity-based prompting for
multi-step reasoning. In The Eleventh International
Conference on Learning Representations.
Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-
ing laws for reward model overoptimization. In In-
ternational Conference on Machine Learning, pages
10835‚Äì10866. PMLR.
Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,
Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen,
Shengjie Ma, Honghao Liu, et al. 2024. A survey on
llm-as-a-judge. arXiv preprint arXiv:2411.15594.
Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu,
Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han,
Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiad-
bench: A challenging benchmark for promoting agi
with olympiad-level bilingual multimodal scientific
problems. arXiv preprint arXiv:2402.14008.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-
cob Steinhardt. 2021. Measuring mathematical prob-
lem solving with the math dataset. arXiv preprint
arXiv:2103.03874.
Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo:
Monolithic preference optimization without refer-
ence model. In Proceedings of the 2024 Conference
on Empirical Methods in Natural Language Process-
ing, pages 11170‚Äì11189.
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam
Perelman, Aditya Ramesh, Aidan Clark, AJ Os-
trow, Akila Welihinda, Alan Hayes, Alec Radford,
et al. 2024. Gpt-4o system card. arXiv preprint
arXiv:2410.21276.
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-
son, Ahmed El-Kishky, Aiden Low, Alec Helyar,
Aleksander Madry, Alex Beutel, Alex Carney, et al.
2024.
Openai o1 system card.
arXiv preprint
arXiv:2412.16720.
Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xi-
angru Peng, and Jiaya Jia. 2024. Step-dpo: Step-wise
preference optimization for long-chain reasoning of
llms. arXiv preprint arXiv:2406.18629.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
et al. 2024.
Rewardbench:
Evaluating reward
models for language modeling.
arXiv preprint
arXiv:2403.13787.
Jia LI, Edward Beeching,
Lewis Tunstall,
Ben
Lipkin, Roman Soletskyi, Shengyi Costa Huang,
Kashif Rasul, Longhui Yu, Albert Jiang, Ziju
Shen, Zihan Qin, Bin Dong, Li Zhou, Yann
9

Fleureau, Guillaume Lample, and Stanislas Polu.
2024. Numinamath. [https://huggingface.co/
AI-MO/NuminaMath-CoT](https://github.com/
project-numina/aimo-progress-prize/blob/
main/report/numina_dataset.pdf).
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan,
Hai Zhao, and Pengfei Liu. 2023.
Generative
judge for evaluating alignment.
arXiv preprint
arXiv:2310.05470.
Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and
Kai Fan. 2024. Mario: Math reasoning with code
interpreter output‚Äìa reproducible pipeline.
arXiv
preprint arXiv:2401.08190.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri
Edwards, Bowen Baker, Teddy Lee, Jan Leike,
John Schulman, Ilya Sutskever, and Karl Cobbe.
2023.
Let‚Äôs verify step by step.
arXiv preprint
arXiv:2305.20050.
Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi,
Ivan Vuli¬¥c, Anna Korhonen, and Nigel Collier. 2024.
Aligning with human judgement: The role of pair-
wise preference in large language model evaluators.
arXiv preprint arXiv:2403.16950.
Adian Liusie, Potsawee Manakul, and Mark JF
Gales. 2023.
Zero-shot nlg evaluation through
pairware comparisons with llms.
arXiv preprint
arXiv:2307.07889.
Yu
Meng,
Mengzhou
Xia,
and
Danqi
Chen.
2024.
Simpo:
Simple preference optimization
with a reference-free reward.
arXiv preprint
arXiv:2405.14734.
Pavlin G. PoliÀácar, Martin Stra≈æar, and Bla≈æ Zupan. 2024.
opentsne: A modular python library for t-sne dimen-
sionality reduction and embedding. Journal of Statis-
tical Software, 109(3):1‚Äì30.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neu-
ral Information Processing Systems, 36.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017.
Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(11).
Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Ji-
achen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei
Chen, Lionel M Ni, et al. 2024a. Openr: An open
source framework for advanced reasoning with large
language models. arXiv preprint arXiv:2410.09671.
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai
Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.
2024b. Math-shepherd: Verify and reinforce llms
step-by-step without human annotations. In Proceed-
ings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 9426‚Äì9439.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in neural
information processing systems, 35:24824‚Äì24837.
Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu,
Yuandong Tian, Jiantao Jiao, Jason Weston, and Sain-
bayar Sukhbaatar. 2024. Meta-rewarding language
models: Self-improving alignment with llm-as-a-
meta-judge. arXiv preprint arXiv:2407.19594.
Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye,
Haoqi Fan, Quanquan Gu, Heng Huang, and Chun-
yuan Li. 2024. Llava-critic: Learning to evaluate mul-
timodal models. arXiv preprint arXiv:2410.02712.
An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,
Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,
Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5
technical report. arXiv preprint arXiv:2412.15115.
An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao,
Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong
Tu, Jingren Zhou, Junyang Lin, et al. 2024b. Qwen2.
5-math technical report: Toward mathematical ex-
pert model via self-improvement.
arXiv preprint
arXiv:2409.12122.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
2024. Tree of thoughts: Deliberate problem solving
with large language models. Advances in Neural
Information Processing Systems, 36.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel
Deutch, and Jonathan Berant. 2023.
Answering
questions by meta-reasoning over multiple chains
of thought. arXiv preprint arXiv:2304.13007.
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,
Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
2024.
Self-rewarding language models.
arXiv
preprint arXiv:2401.10020.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023.
Rrhf:
Rank responses to align language models with
human feedback without tears.
arXiv preprint
arXiv:2304.05302.
Shimao Zhang, Yu Bao, and Shujian Huang. 2024.
Edt: Improving large language models‚Äô generation by
entropy-based dynamic temperature sampling. arXiv
preprint arXiv:2403.14541.
10

Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2022. Automatic chain of thought prompt-
ing in large language models.
arXiv preprint
arXiv:2210.03493.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems, 36:46595‚Äì46623.
A
Step Number and Step Length
Statistics
We report the full results of step number and step
length across all benchmarks on the 7B and 72B
models here. The 7B results are reported in Table
7. And the 72B results are reported in Table 8.
B
Solution Scoring v.s. Step-wise Pairwise
Comparison
We evaluate the GPT-4o‚Äôs (Hurst et al., 2024) con-
sistency and agreement with humans on two differ-
ent LLM-as-a-Judge strategies for complex mathe-
matical reasoning tasks, including assigning scores
to the answers and performing pairwise comparison
between two individual reasoning steps. We report
the results in Table 6. Our results indicate that
for the complex mathematical reasoning task, step-
wise pairwise comparison has better consistency
and agreement with humans than solution scoring.
It is highly challenging for LLMs to assign a proper
and steady score to a complex long-thought multi-
step solution.
C
Prompt Templates
We list the prompt templates we used in our work
here. The prompt we use for constructing step-by-
step formatted reasoning is shown in Figure 3. And
the prompts we used for step-by-step long-thought
mathematical reasoning and step-wise LLM-as-a-
Judge are shown in Figure 4 and Figure 5 respec-
tively.
11

Judge Strategy
Consistency
Agreement
Step-wise Pairwise Comparison
0.84
0.88
Solution Scoring
0.72
0.32
Table 6: The consistency and agreement with human evaluation of step-wise pairwise comparison and solution
scoring.
Step Num
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
M1
5.91
9.35
8.68
11.75
7.97
11.18
M2
5.24
8.03
7.43
9.54
7.03
9.85
M3
4.50
6.43
5.84
7.36
7.13
6.9
M4
4.09
5.21
5.11
6.14
6.4
5.53
Step Length
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
M1
48.59
61.61
69.74
103.95
100.43
76.13
M2
54.02
70.04
85.26
108.26
114.27
115.29
M3
63.36
89.68
99.59
127.97
118.67
109.45
M4
73.64
113.14
118.02
142.69
138.18
127.18
Table 7: Statistics of step number and step length on different methematical benchmarks based on 7B models.
Step Num
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
M1
5.89
8.41
8.34
10.21
8.23
9.95
M2
5.55
7.64
7.34
9.05
7.37
9.75
M3
5.10
6.30
5.99
6.54
7.07
6.55
M4
4.87
5.54
5.36
5.75
6.33
6.1
Step Length
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
M1
47.79
61.00
69.72
95.38
104.97
79.36
M2
51.19
67.17
78.00
101.93
118.08
86.88
M3
57.75
80.46
91.21
122.53
118.61
108.95
M4
62.86
96.63
106.28
134.62
133.66
113.60
Table 8: Statistics of step number and step length on different methematical benchmarks based on 72B models.
Setting
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
M1 Greedy Search
92.6
76.0
66.2
41.8
13.3
45.0
M4 Greedy Search
93.7
76.6
68.1
44.1
23.3
57.5
M1 Test-time Scaling
94.5
79.1
64.9
41.6
16.7
52.5
M4 Test-time Scaling
94.5
79.3
68.3
43.7
23.3
65.0
Table 9: The full results of greedy search and test-time scaling on 72B model.
12

There is a math problem and its corresponding solution. Please divide the 
given solution into individual steps logically. Use "Step n: " before 
each step to distinguish between different steps, where n is a positive 
integer starting from 1, representing the current step number. Only 
divide the steps without altering any information in the original 
solution. Please output only the divided solution steps in the format 
mentioned above, and do not include any additional information. Do not 
omit the final answer that is placed in boxed.
[The Start of Question Provided]
{question}
[The End of Question Provided]
[The Start of Solution Provided]
{solution}
[The End of Solution Provided]
Figure 3: The prompt for converting the the given solution into step-by-step format logically without altering any
information in the original solution.
Let's think step by step and solve the following math problem. Use "Step 
n: " before each step to distinguish between different steps, where n is 
a positive integer starting from 1, representing the current step number. 
Put your final answer in boxed.
Problem: {problem}
Figure 4: The prompt for LLMs conducting step-by-step long-thought reasoning.
13

Please act as an impartial judge and evaluate the quality of two next 
reasoning steps provided by two AI assistants to the question and 
partial reasoning steps displayed below. Your evaluation should 
consider correctness and helpfulness. You will be given assistant A‚Äôs 
answer, and assistant B‚Äôs answer. Your job is to evaluate which 
assistant‚Äôs answer is better. You should compare the two responses and 
provide a detailed explanation. Avoid any position biases and ensure 
that the order in which the responses were presented does not influence 
your decision. Do not allow the length of the responses to influence 
your evaluation. Do not favor certain names of the assistants. Be as 
objective as possible. After providing your explanation, output your 
final verdict by strictly following this format: "[[A]]" if assistant A 
is better, and "[[B]]" if assistant B is better.
[Question and Intermediate Reasoning Steps Provided]
{Question and Partial Reasoning Steps}
[The Start of Assistant A‚Äôs Next Reasoning Step]
{Step A}
[The End of Assistant A‚Äôs Next Reasoning Step]
[The Start of Assistant B‚Äôs Next Reasoning Step]
{Step B}
[The End of Assistant B‚Äôs Next Reasoning Step]
Figure 5: The prompt for LLMs conducting step-wise LLM-as-a-Judge. We create this prompt template following
the basic pattern of Zheng et al. (2023).
14

