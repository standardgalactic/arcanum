Rank responses to align language models with
human feedback without tears.
arXiv preprint
arXiv:2304.05302.
Shimao Zhang, Yu Bao, and Shujian Huang. 2024.
Edt: Improving large language models’ generation by
entropy-based dynamic temperature sampling. arXiv
preprint arXiv:2403.14541.
10

Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2022. Automatic chain of thought prompt-
ing in large language models.
arXiv preprint
arXiv:2210.03493.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems, 36:46595–46623.
A
Step Number and Step Length
Statistics
We report the full results of step number and step
length across all benchmarks on the 7B and 72B
models here. The 7B results are reported in Table
7. And the 72B results are reported in Table 8.
B
Solution Scoring v.s. Step-wise Pairwise
Comparison
We evaluate the GPT-4o’s (Hurst et al., 2024) con-
sistency and agreement with humans on two differ-
ent LLM-as-a-Judge strategies for complex mathe-
matical reasoning tasks, including assigning scores
to the answers and performing pairwise comparison
between two individual reasoning steps. We report
the results in Table 6. Our results indicate that
for the complex mathematical reasoning task, step-
wise pairwise comparison has better consistency
and agreement with humans than solution scoring.
It is highly challenging for LLMs to assign a proper
and steady score to a complex long-thought multi-
step solution.
C
Prompt Templates
We list the prompt templates we used in our work
here. The prompt we use for constructing step-by-
step formatted reasoning is shown in Figure 3. And
the prompts we used for step-by-step long-thought
mathematical reasoning and step-wise LLM-as-a-
Judge are shown in Figure 4 and Figure 5 respec-
tively.
11

Judge Strategy
Consistency
Agreement
Step-wise Pairwise Comparison
0.84
0.88
Solution Scoring
0.72
0.32
Table 6: The consistency and agreement with human evaluation of step-wise pairwise comparison and solution
scoring.
Step Num
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
M1
5.91
9.35
8.68
11.75
7.97
11.18
M2
5.24
8.03
7.43
9.54
7.03
9.85
M3
4.50
6.43
5.84
7.36
7.13
6.9
M4
4.09
5.21
5.11
6.14
6.4
