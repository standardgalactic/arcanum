
ğ‘€0
Base model
search
{ğ‘¥, ğ‘ 1, â€¦ , ğ‘ ğ‘™âˆ’1, ğ‘ ğ‘™
1, ğ‘ ğ‘™
2, ğ‘—ğ‘¢ğ‘‘ğ‘”ğ‘’}
EFT Data
{ğ‘¥, ğ‘¦}
IFT Data
Initialization
ğ‘€ğ‘–
â€¦â€¦
â€¦â€¦
ğ‘ ğ‘™
ğ‘ğ‘’ğ‘ ğ‘¡
ğ‘ ğ‘™
ğ‘¤ğ‘œğ‘Ÿğ‘ ğ‘¡
ğ‘ ğ‘™âˆ’1
ğ‘ğ‘’ğ‘ ğ‘¡
â€¦â€¦
Step-by-step
Reasoning
Step-wise
LLM-as-a-Judge
{ğ‘¥, ğ‘ 1~ğ‘™âˆ’1, ğ‘ ğ‘™
ğ‘, ğ‘ ğ‘™
ğ‘¤}
Step-wise Preference Data
ğ‘€ğ‘–+1
Step-wise Preference 
Optimization
Figure 1: Illustration of our Process-based Self-Rewarding paradigm. (1) We get EFT data by tree-search, initial
data filtering and data annotation. And we get IFT data by step segmentation. (2) The model is initialized on EFT
and IFT data. (3) The model conducts step-by-step search-based reasoning and performs step-wise LLM-as-a-Judge
to select the chosen step and generate the step-wise preference pair at each step. (4) We perform step-wise preference
optimization on the model. (5) The model enters the next iteration cycle.
the judgment agreement by adding the LLM-as-a-
Meta-Judge action into the self-rewarding pipeline,
which allows the model to evaluate its own judg-
ments. But the existing self-rewarding methods
mainly focus on the instruction-following tasks
and perform poorly in the mathematical domain
data (Yuan et al., 2024). And evaluating the entire
response makes it difficult for the model to learn
fine-grained preference information.
For some
long-thought reasoning tasks, it is important to en-
able LLMs to focus on and learn the fine-grained
reasoning step preference information.
2.4
Step-by-step Reasoning
Complex reasoning tasks are still great challenges
for LLMs now. Chain-of-Thought (Wei et al., 2022)
methods prompt LLMs to solve the complex prob-
lems by reasoning step by step rather than generat-
ing the answer directly, which leads to significant
improvements across many reasoning tasks (Yoran
et al., 2023; Fu et al., 2022; Zhang et al., 2022). Fur-
thermore, recent studies investigate the test-time
scaling paradigm which allows the LLMs to use
more resources and time for inference to achieve
better performance (Lightman et al., 2023) typi-
cally based on search and step selecting (Yao et al.,
2024; Wang et al., 2024b). These results high-
light the importance of conducting high-quality
long-thought step-by-step reasoning for LLMs in
solving complex reasoning problems.
3
Process-based Self-Rewarding
Language Models
In this section, we propose our new Process-
based Self-Rewarding Language Models pipeline.
We first review the existing self-rewarding algo-
rithm and our motivation as a preliminary study
in Â§3.1. Then we introduce our novel paradigm
for more fine-grained step-wise self-rewarding and
self-evolution. The entire pipeline consists of se-
quential stages: model initialization (Â§3.2), rea-
soning and preference data generation (Â§3.3), and
model preference optimization (Â§3.4). Finally, we
provide a summarized overview of our algorithm
(Â§3.5). We illustrate the entire pipeline in Figure 1.
3.1
Preliminary Study
Most existing preference optimization algorithms
rely on reward signals from external reward mod-
els or human-annotated data. However, deploying
an external reward model or getting ground truth
gold reward signals from human annotators is ex-
pensive (Gao et al., 2023). Moreover, due to the
inherent limitations and implicit biases of both hu-
mans and reward models, these model optimization
strategies are bottlenecked (Lambert et al., 2024;
Yuan et al., 2024). Thus, Self-Rewarding algorithm
is proposed to mitigate this limitation by enabling
the model to provide reward signals for its own out-
puts and perform self-improvement, showing the
feasibility of achieving models that surpass human
performance (Yuan et al., 2024).
