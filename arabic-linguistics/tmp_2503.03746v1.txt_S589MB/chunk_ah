52.5
59.3
M4
93.7
76.6
68.1
44.1
23.3
57.5
60.6
Table 1: Accuracy of Process-based Self-Rewarding based on 7B and 72B base models. SRLM is the self-rewarding
language model algorithm as the baseline. We bold the best results for each parameter size in each benchmark.
calculation on mathematical benchmarks, we fol-
low the implementation of Yang et al. (2024b).
Implementations
For initial PRM training, we
fine-tune full parameters on 128 NVIDIA A100
GPUs for 1 epoch with learning_rate=1e −5
and batch_size=128.
For preliminary prefer-
ence pairs selection, we set simulation_depth=3,
num_iterations=100,
T=0.7,
and top_p=0.95.
When training M0 to M1, we utilize 28,889
IFT and 4,179 EFT samples.
We fine-tune
LLMs’ full parameters on 32 NVIDIA H100
GPUs for 3 epochs with learning_rate=1e −6 and
batch_size=32.
During the reasoning and pref-
erence data generation stage, we utilize temper-
ature sampling which trade-off generation quality
and diversity (Zhang et al., 2024). We set T=0.5,
top_p=0.95. The search width for each step is set
to 6, and the max iteration number is set to 20. Fi-
nally, in the step-wise preference optimization, we
train LLM’s full parameters on 32 NVIDIA H100
GPUs for 1 epoch with learning_rate=5e −7 and
batch_size=32. To get models from M2 to M4, we
use 400, 800, and 1, 200 math questions for pref-
erence pairs generation respectively, which are all
sampled from the train subset of NuminaMath. For
all solution-scoring judgment strategy experiments,
we use the same prompt template of Yuan et al.
(2024). We use greedy search in evaluations.
5
Results
In this section, we report our main results on differ-
ent mathematical benchmarks and conduct some
discussions and analyses based on the results.
5.1
Main Results
We report the performance of M0 to M4 based
on Qwen2.5-Math-7B and Qwen2.5-Math-72B re-
spectively in Table 1. Our findings are as follows:
As the number of iterations increases, the
overall performance of the model improves.
Traditionally, external reward signals and train-
ing data are utilized for improving LLMs’ perfor-
mance. Our results indicate that models’ overall
performance on mathematical tasks significantly
improves from M1 to M4 solely through Process-
based Self-Rewarding and step-wise preference op-
timization without any additional guidance. This
leverages the potential of LLMs for both mathemat-
ical reasoning and as evaluators.
Our fine-grained algorithm outperforms the
tranditional method. After three iterations, our
approach achieves superior performance compared
to method that applies rewards and conducts train-
ing on the entire response. Given that the initial-
ization with different EFT data lead to different
M1 fiducial performance in the two methods, we
also report the performance changes from M1 to
6

7B
GSM8k
MATH
Gaokao2023En
OlympiadBench
AIME2024
AMC2023
SRLM
+0.1
+1.2
+2.3
0.0
+3.3
0.0
Process-based (Ours)
+0.3
+3.8
+3.4
+2.7
+6.6
+10.0
72B
GSM8k
