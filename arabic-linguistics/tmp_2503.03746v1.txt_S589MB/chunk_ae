l
(5)
where max(Scorel) is the index of the candidate
with the highest score and min(Scorel) corre-
sponds to the lowest score. sl is the final chosen
l-th step. (sbest
l
, sworst
l
) will be chosen as a chosen-
rejected preference pair.
This process will be repeated continuously until
generation is complete. It is important to note that
to enhance the effectiveness of preference data,
if max(Scorel) is equal to min(Scorel), we will
discard the existing sl−1 and (sbest
l−1, sworst
l−1 ) and roll
back to the previous step.
3.4
Step-wise Model Preference Optimization
With preference data collected in the Section 3.3,
we conduct preference optimization training on
the model. We choose Direct Preference Optimiza-
tion (DPO) as the training algorithm (Rafailov et al.,
2024). The difference is that we conduct a more
4

fine-grained step-wise DPO in our work. The simi-
lar method has also been investigated by Lai et al.
(2024). We can calculate the training loss as:
A = β log πθ(sb
l | x, s1, ..., sl−1)
πref(sb
l | x, s1, ..., sl−1)
(6)
B = β log πθ(sw
l | x, s1, ..., sl−1)
πref(sw
l | x, s1, ..., sl−1)
(7)
L(πθ; πref) = −E(x,s1,...,sb
l ,sw
l )∼D[log σ(A −B)]
(8)
where x is the prompt, s1, ..., sl−1 is the previous
steps, sb
l and sw
l are the best and worst steps re-
spectively for the l-th step, β is a hyperparameter
controlling the deviation from the base reference
policy, πθ and πref are the policies to be optimized
and the reference policy respectively.
After the preference optimization stage, we have
the model for the next cycle. In the next iteration,
we sequentially repeat the steps in §3.3 and §3.4.
3.5
Iteration Pipeline
We show the entire pipeline of our algorithm. Fol-
lowing Yuan et al. (2024), we refer to the model
after n iterations as Mn.
And we refer to the
Pair-wise Preference Data generated by Mn as
PPD(Mn). Then the sequence in our work can
be defined as:
• M0: The base model.
• M1: The model obtained by supervised fine-
tuning (SFT) M0 on “EFT + IFT” data.
• M2: The model obtained by training M1 on
PPD(M1) using step-wise DPO.
• · · · · · ·
• Mn: The model obtained by training Mn−1
on PPD(Mn−1) using step-wise DPO.
In summary,
we initialize the base model
using well-selected step-wise LLM-as-a-Judge
data (EFT) and step-by-step long-thought reason-
ing data (IFT). Once the model possesses the corre-
sponding two abilities, we select preference pairs
through search and reward signals provided by the
model itself, and train the model using step-wise
DPO. Then we iterate the model by repeatedly per-
forming the above operations.
4
Experimental Setup
We conduct our experiments on models in different
parameter sizes and several representative mathe-
matical reasoning benchmarks. In this section, we
introduce our experimental settings in detail.
Models
We
choose
the
base
model
from
Qwen2.5-Math series (Yang et al., 2024b) in our
experiments, which is one of the most popular
open-source LLM series. Specifically, we choose
Qwen2.5-Math-7B and Qwen2.5-Math-72B. Addi-
