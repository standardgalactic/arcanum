shell, we enable the LLMs to simultaneously con-
duct step-by-step complex reasoning and perform
LLM-as-a-Judge for individual intermediate steps.
For the limitation (a) above, to get finer-grained
and more accurate rewards, Process-based Self-
Rewarding paradigm allows LLMs to perform step-
wise LLM-as-a-Judge for the individual reasoning
step. Since producing the correct final answer does
not imply that LLMs can generate correct inter-
mediate reasoning steps, it is crucial to train the
model to learn not only to produce the correct final
answer but also to generate correct intermediate
reasoning steps. By using model itself as a reward
model to generate step preference pairs data, we
further perform step-wise preference optimization.
For the limitation (b) above, we design a LLM-as-
a-Judge prompt for step-wise pairwise comparison
rather than directly assigning scores to the answer
for more proper and steadier judgments based on
the observations in Appendix B.
We conduct the experiments on models in differ-
ent parameter sizes (7B and 72B) and test across
a wide range of mathematical reasoning bench-
marks. Our results show that Process-based Self-
Rewarding can effectively enhance the mathemat-
ical reasoning capabilities of LLMs, which indi-
cates that LLMs are able to perform effective self-
rewarding at the step level. Our models that it-
eratively trained based on the Process-based Self-
Rewarding paradigm demonstrate an increasing
trend in both mathematical and LLM-as-a-judge
capabilities. These results suggest this framework’s
immense potential for achieving intelligence that
may surpass human performance.
2
Background
2.1
Reinforcement Learning from Human
Feedback
Supervised Fine-tuning is an effective method to
improve LLMs’ performance across many differ-
ent downstream tasks. But it has been evidenced
that SFT potentially exacerbates LLMs’ halluci-
nation (Hong et al., 2024). So RLHF is further
utilized to align LLMs with human preference. In
the RLHF paradigm, the model is trained based
on reward signals provided by external reward
models and humans by reinforcement learning al-
gorithms, such as PPO (Schulman et al., 2017),
DPO (Rafailov et al., 2024), SimPO (Meng et al.,
2024), and so on. Direct Preference Optimization
(DPO) is a preference learning algorithm which
directly uses pairwise preference data, including
chosen and rejected answers for optimization. Fur-
thermore, the step-wise preference optimization
has also been investigated for long-chain reason-
ing and has shown great performance (Lai et al.,
2024; Chen et al., 2024). In our work, we intro-
duce the step-wise preference optimization into our
Process-based Self-Rewarding paradigm for more
fine-grained learning.
2.2
LLM-as-a-Judge
LLM-as-a-Judge technique has been widely used
for evaluation tasks because of LLMs’ scalabil-
ity, adaptability, and cost-effectiveness (Gu et al.,
2024). In the LLM-as-a-Judge scenarios, LLMs
are prompted to mimic human reasoning and evalu-
ate specific inputs against a set of predefined rules.
To improve the performance of LLM-as-a-Judge,
the LLM acting as the evaluator is trained to align
with human preferences. When conducting LLM-
as-a-Judge, LLMs can play many different roles
depending on the given prompt. Typical applica-
tions include tasks where LLMs are prompted to
generate scores (Li et al., 2023; Xiong et al., 2024),
perform pairwise comparisons (Liu et al., 2024;
Liusie et al., 2023), rank multiple candidates (Yuan
et al., 2023), and so on. However, the LLM-as-a-
Judge for individual mathematical reasoning steps
has not been widely investigated. In our experi-
ment, we design the step-wise LLM-as-a-Judge for
rewarding and analyze its performance.
2.3
Self-Rewarding Language Models
Although RLHF has been widely utilized to align
LLMs with human-level performance and has
achieved impressive performance, the existing
methods heavily rely on high-quality reward mod-
els or human feedback, which bottlenecks these
approaches. To avoid this bottleneck, Yuan et al.
(2024) propose the Self-Rewarding Language Mod-
els paradigm, which uses a single model as both
instruction-following model and reward model si-
multaneously. The iterative self-rewarding algo-
rithm operates by having the model generate re-
sponses and reward the generated response candi-
dates, then selecting preference pairs for training.
Based on this, Wu et al. (2024) further improve
2
