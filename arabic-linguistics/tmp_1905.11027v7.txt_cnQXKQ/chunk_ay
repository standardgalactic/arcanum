[12] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. Model compression and acceleration for
deep neural networks: The principles, progress, and challenges. IEEE Signal Processing
Magazine, 35(1):126–136, 2018.
[13] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can
generalize for deep nets. In International Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pages 1019–1028, 2017.
28

[14] Krishan Duggal. A review on unique existence theorems in lightlike geometry. Geometry,
2014, 2014. Article ID 835394.
[15] Krishan Duggal and Aurel Bejancu. Lightlike Submanifolds of Semi-Riemannian Manifolds
and Applications, volume 364 of Mathematics and Its Applications. Springer, Netherlands,
1996.
[16] Pascal Mattia Esser and Frank Nielsen. Towards modeling and resolving singular parameter
spaces using stratifolds. arXiv preprint arXiv:2112.03734, 2021.
[17] Xinlong Feng and Zhinan Zhang. The rank of a random matrix. Applied Mathematics and
Computation, 185(1):689–694, 2007.
[18] Adam Gaier and David Ha. Weight agnostic neural networks. In Advances in Neural
Information Processing Systems 32, pages 5365–5379. Curran Associates, Inc., NY 12571,
USA, 2019.
[19] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings
of Machine Learning Research, pages 315–323, 2011.
[20] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, Cambridge,
Massachusetts, 2016.
[21] Peter Grünwald and Teemu Roos. Minimum description length revisited. International
Journal of Mathematics for Industry, 11(01), 2020.
[22] Peter D. Grünwald. The Minimum Description Length Principle. Adaptive Computation
and Machine Learning series. The MIT Press, Cambridge, Massachusetts, 2007.
[23] Tomohiro Hayase and Ryo Karakida. The spectrum of Fisher information of deep networks
achieving dynamical isometry. In International Conference on Artificial Intelligence and
Statistics, pages 334–342, 2021.
[24] Masahito Hayashi. Large deviation theory for non-regular location shift family. Annals of
the Institute of Statistical Mathematics, 63(4):689–716, 2011.
[25] Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42,
1997.
[26] Harold Hotelling. Spaces of statistical parameters. Bull. Amer. Math. Soc, 36:191, 1930.
[27] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Binarized neural networks. In Advances in Neural Information Processing Systems 29, pages
4107–4115. Curran Associates, Inc., NY 12571, USA, 2016.
[28] Varun Jain, Amrinder Pal Singh, and Rakesh Kumar. On the geometry of lightlike submani-
folds of indefinite statistical manifolds, 2019. arXiv:1903.07387 [math.DG].
[29] Ruichao Jiang, Javad Tavakoli, and Yiqiang Zhao. Weyl prior and Bayesian statistics.
Entropy, 22(4), 2020.
[30] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of Fisher information
in deep neural networks: Mean field approach. In International Conference on Artificial
Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages
1032–1041, 2019.
29

[31] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Pathological Spectra of the Fisher In-
formation Metric and Its Variants in Deep Neural Networks. Neural Computation, 33(8):2274–
2307, 2021.
[32] David C Kay. Schaum’s outline of theory and problems of tensor calculus. McGraw-Hill,
New York, 1988.
[33] Andre˘ı Nikolaevich Kolmogorov. Sur la notion de la moyenne. G. Bardi, tip. della R. Accad.
dei Lincei, Rome, Italy, 1930.
[34] Osamu Komori and Shinto Eguchi. A unified formulation of k-Means, fuzzy c-Means and
Gaussian mixture model by the Kolmogorov–Nagumo average. Entropy, 23(5):518, 2021.
[35] Frederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical Fisher
approximation for natural gradient descent. In Advances in Neural Information Processing
Systems 32, pages 4158–4169. Curran Associates, Inc., NY 12571, USA, 2019.
[36] D.N. Kupeli. Singular Semi-Riemannian Geometry, volume 366 of Mathematics and Its
Applications. Springer, Netherlands, 1996.
[37] Stefan L Lauritzen. Statistical manifolds. Differential geometry in statistical inference,
10:163–216, 1987.
[38] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic
dimension of objective landscapes. In International Conference on Learning Representations
(ICLR), 2018.
[39] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-Rao metric,
geometry, and complexity of neural networks. In International Conference on Artificial
Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages
888–896, 2019.
[40] Wu Lin, Valentin Duruisseaux, Melvin Leok, Frank Nielsen, Mohammad Emtiyaz Khan, and
Mark Schmidt. Simplifying momentum-based positive-definite submanifold optimization
with applications to deep learning. In International Conference on Machine Learning, pages
21026–21050. PMLR, 2023.
[41] David J.C. MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute
of Technology, 1992.
[42] James Martens. New insights and perspectives on the natural gradient method. Journal of
Machine Learning Research, 21(146):1–76, 2020.
[43] James A. Mingo and Roland Speicher. Free Probability and Random Matrices, volume 35 of
Fields Institute Monographs. Springer, New York, 2017.
[44] In Jae Myung, Vijay Balasubramanian, and Mark A. Pitt. Counting probability distributions:
Differential geometry and model selection. Proceedings of the National Academy of Sciences,
97(21):11170–11175, 2000.
[45] Mitio Nagumo. Über eine Klasse der Mittelwerte. In Japanese journal of mathematics:
transactions and abstracts, volume 7, pages 71–79. The Mathematical Society of Japan, 1930.
[46] Naomichi Nakajima and Toru Ohmoto.
The dually flat structure for singular models.
Information Geometry, 4(1):31–64, 2021.
30

[47] Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring
generalization in deep learning. In Advances in Neural Information Processing Systems 30,
pages 5947–5956. Curran Associates, Inc., NY 12571, USA, 2017.
[48] Katsumi Nomizu, Nomizu Katsumi, and Takeshi Sasaki.
Affine differential geometry:
geometry of affine immersions. Cambridge Tracts in Mathematics. Cambridge university
press, Cambridge, United Kingdom, 1994.
