[49] A Emin Orhan and Xaq Pitkow. Skip connections eliminate singularities. In International
Conference on Learning Representations (ICLR), 2018.
[50] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. Journal
of Machine Learning Research, 21(252):1–64, 2020.
[51] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In
International Conference on Learning Representations (ICLR), 2014.
[52] Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random
matrix theory. In International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pages 2798–2806, 2017.
[53] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spectral
universality in deep networks. In International Conference on Artificial Intelligence and
Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1924–1932, 2018.
[54] Jeffrey Pennington and Pratik Worah. The spectrum of the Fisher information matrix of a
single-hidden-layer neural network. In Advances in Neural Information Processing Systems
31, pages 5410–5419. Curran Associates, Inc., NY 12571, USA, 2018.
[55] David Pollard. A note on insufficiency and the preservation of Fisher information. In From
Probability to Statistics and Back: High-Dimensional Models and Processes–A Festschrift in
Honor of Jon A. Wellner, pages 266–275. Institute of Mathematical Statistics, Beachwood,
Ohio, 2013.
[56] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On
the expressive power of deep neural networks. In International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning Research, pages 2847–2854, 2017.
[57] Calyampudi Radhakrishna Rao. Information and the accuracy attainable in the estimation
of statistical parameters. Bulletin of Cal. Math. Soc., 37(3):81–91, 1945.
[58] Calyampudi Radhakrishna Rao. Information and the accuracy attainable in the estimation
of statistical parameters. In Breakthroughs in statistics, pages 235–247. Springer, New York,
NY, 1992.
[59] Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.
[60] Jorma Rissanen. Fisher information and stochastic complexity. IEEE Trans. Inf. Theory,
42(1):40–47, 1996.
[61] Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical
analysis of the Hessian of over-parametrized neural networks. In ICLR’18 workshop, 2018.
arXiv:1706.04454 [cs.LG].
31

[62] Salem Said, Hatem Hajri, Lionel Bombrun, and Baba C Vemuri. Gaussian distributions
on Riemannian symmetric spaces: statistical learning with structured covariance matrices.
IEEE Transactions on Information Theory, 64(2):752–772, 2017.
[63] Gideon Schwarz. Estimating the dimension of a model. Ann. Stat., 6(2):461–464, 1978.
[64] Alexander Soen and Ke Sun. On the variance of the Fisher information for deep learning. In
Advances in Neural Information Processing Systems 34, pages 5708–5719, NY 12571, USA,
2021. Curran Associates, Inc.
[65] Ke Sun and Frank Nielsen. Relative Fisher information and natural gradient for learning
large modular models. In International Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pages 3289–3298, 2017.
[66] Junnichi Takeuchi and S-I Amari. α-parallel prior and its properties. IEEE Transactions on
Information Theory, 51(3):1011–1023, 2005.
[67] Philip Thomas. Genga: A generalization of natural gradient ascent with positive and negative
convergence results. In International Conference on Machine Learning, volume 32 (2) of
Proceedings of Machine Learning Research, pages 1575–1583, 2014.
[68] Guillermo Valle-Pérez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes
because the parameter-function map is biased towards simple functions. In International
Conference on Learning Representations (ICLR), 2019.
[69] Christopher Stewart Wallace and D. M. Boulton. An information measure for classification.
Computer Journal, 11(2):185–194, 1968.
[70] Sumio Watanabe. Algebraic Geometry and Statistical Learning Theory, volume 25 of Cam-
bridge Monographs on Applied and Computational Mathematics. Cambridge University Press,
Cambridge, United Kingdom, 2009.
[71] Haikun Wei, Jun Zhang, Florent Cousseau, Tomoko Ozeki, and Shun-ichi Amari. Dynamics
of learning near singularities in layered networks. Neural computation, 20(3):813–843, 2008.
[72] Yuki Yoshida, Ryo Karakida, Masato Okada, and Shun-ichi Amari. Statistical mechanical
analysis of learning dynamics of two-layer perceptron with multiple output units. Journal of
Physics A: Mathematical and Theoretical, 2019.
[73] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-
standing deep learning requires rethinking generalization. In International Conference on
Learning Representations (ICLR), 2017.
32

