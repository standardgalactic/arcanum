A Geometric Modeling of Occam’s Razor in Deep Learning
Ke Sun
CSIRO’s Data61, Australia
The Australian National University
sunk@ieee.org
Frank Nielsen
Sony Computer Science Laboratories Inc. (Sony CSL)
Tokyo, Japan
frank.nielsen.x@gmail.com
Version: November 2024
Abstract
Why do deep neural networks (DNNs) benefit from very high dimensional parameter
spaces? Their huge parameter complexities vs. stunning performances in practice is all
the more intriguing and not explainable using the standard theory of model selection for
regular models. In this work, we propose a geometrically flavored information-theoretic
approach to study this phenomenon. Namely, we introduce the locally varying dimensionality
of the parameter space of neural network models by considering the number of significant
dimensions of the Fisher information matrix, and model the parameter space as a manifold
using the framework of singular semi-Riemannian geometry. We derive model complexity
measures which yield short description lengths for deep neural network models based on
their singularity analysis thus explaining the good performance of DNNs despite their large
number of parameters.
1
Introduction
Deep neural networks (DNNs) are usually large models in terms of storage costs. In the classical
model selection theory, such models are not favored as compared to simple models with the same
training performance. For example, if one applies the Bayesian information criterion (BIC) [63]
to DNN, a shallow neural network (NN) will be preferred over a deep NN due to the penalty
term with respect to (w.r.t.) the complexity. A basic principle in science is the Occam1’s Razor,
which favors simple models over complex ones that accomplish the same task. This raises the
fundamental question of how to measure the simplicity or the complexity of a model.
Formally, the preference of simple models has been studied in the area of minimum description
length (MDL) [22, 59, 60], also known in another thread of research as the minimum message
length (MML) [69].
∗This work first appeared under the former title “Lightlike Neuromanifolds, Occam’s Razor and Deep Learning”
in 2019.
1William of Ockham (ca. 1287 — ca. 1347), a monk (friar) and philosopher.
1
arXiv:1905.11027v7  [cs.LG]  31 Oct 2024

Consider a parametric family of distributions M = {p(x | θ)} with θ ∈Θ ⊂RD.
The
distributions are mutually absolutely continuous, which guarantees all densities to have the same
support. Otherwise, many problems of non-regularity will arise as described by [24, 55]. The
Fisher information matrix (FIM) I(θ) is a D × D positive semi-definite (psd) matrix: I(θ) ⪰0.
The model is called regular if it is (i) identifiable [11] with (ii) a non-degenerate and finite Fisher
information matrix (i.e., I(θ) ≻0).
In a Bayesian setting, the description length of a set of N i.i.d. observations X = {xi}N
i=1 ⊂X
w.r.t. M can be defined as the number of nats with the coding scheme of a parametric model
p(x | θ) and a prior p(θ). The code length of any xi is given by the cross entropy between the
empirical distribution δi(x) = δ(x −xi), where δ(·) denotes the Dirac’s delta function, and
p(x) =
R
p(x | θ)p(θ) dθ. Therefore, the description length of X is
−log p(X) =
N
X
i=1
h×(δi : p) = −
N
X
i=1
log
Z
p(xi | θ) p(θ) dθ,
(1)
where h×(p : q) := −
R
p(x) log q(x)dx denotes the cross entropy between p(x) and q(x), and log
denotes natural logarithm throughout the paper. The code length means the cumulative loss of
the Bayesian mixture model p(x) w.r.t. the observations X.
By using Jeffreys2’ non-informative prior [3] as p(θ), the MDL in eq. (1) can be approximated
(see [7, 59, 60]) as
χ = −log p(X | ˆθ)
|
{z
}
fitness
+
geometric complexity
z
}|
{
D
2 log N
2π
|
{z
}
penalize high dof
+ log
Z p
|I(θ)| dθ

|
{z
}
model capacity
,
