and eq. (25) becomes a Gaussian prior. We refer the reader to [29, 66] for other extensions of
Jeffreys’ prior.
The normalizing constant of eq. (25) is an information volume measure of M, given by
V :=
Z
M
exp

−1
2ε2
2
∥θ∥2

dθ.
(26)
Unlike Jeffreys’ prior whose information volume (the 3rd term on the RHS of eq. (2)) can be
unbounded, this volume is better bounded by
Theorem 5.
(
√
2πε1ε2)D ≤V ≤(
p
2π(ε1 + λm)ε2)D,
(27)
where λm is the largest eigenvalue of the FIM I(θ).
7Different layers, or weights and biases, may use different variance in their initialization. This minor issue can
be solved by a simple re-scaling re-parameterization.
24

Notice λm may not exist, as the integration is taken over θ ∈M. Intuitively, V is a weighted
volume w.r.t. a Gaussian-like prior distribution on M, while the 3rd term on the RHS of eq. (2) is
an unweighted volume. The larger the radius ε2, the more “number” or possibilities of DNNs are
included; the larger the parameter ε1, the larger the local volume element in eq. (22) is measured,
and therefore the total volume is measured larger. log V is an O(D) terms, meaning the volume
grows with the number of dimensions.
F.1
Proof of Theorem 5
By definition,
V =
Z
M
exp

−1
2ε2
2
∥θ∥2

dθ =
Z
exp

−1
2ε2
2
∥θ∥2
 p
|I(θ) + ε1I|dEθ.
By (A1), θ is an orthogonal transformation of the neural network weights and biases, and
therefore θ ∈RD. We have
p
|I(θ) + ε1I| ≥
p
|ε1I| = ε
D
2
1 .
Hence
V ≥
Z
exp

−1
2ε2
2
∥θ∥2

ε
D
2
1 dEθ
= (2π)
D
2 εD
2 ε
D
2
1
Z
exp

−D
2 log 2π −1
2 log |ε2
2I| −1
2ε2
2
∥θ∥2

dEθ
