density has been truncated by M. One can therefore take the rough approximation based on the
mean of the Gaussian:
−log
Z
f
M
G

ξ | ˆξ, 1
N J−1(ˆξ)
 |I(ξ)|1/2
|J(ξ)|1/2 dξ ≈1
2 log |J(ˆξ)|
|I(ˆξ)|
.
(19)
Under this approximation, eq. (18) gives the MDL criterion discussed in [7, 44]. We therefore
consider the spectrum of both matrices I(ξ) and J(ξ), noting that in the large sample limit
N →∞, they become identical. Because of the finite N, the observed FIM J(ˆξ) is singular in
potentially many directions. As a result, the log-ratio in eq. (19) serves as a negative complexity
term and explains how singularities of J(ˆξ) correspond to the simplicity of DNNs.
Compared with OG, OJ is based on a more accurate geometric modeling, However, it is hard
to be computed numerically. Despite that they have different expressions, their preference to
model dimensions with small Fisher information (as in DNNs) is similar.
Hence, we can conclude that the intrinsic complexity of a DNN is affected by the singularity
and spectral properties of the Fisher information matrix.
16

9
Related Work
The dynamics of supervised learning of a DNN describes a trajectory on the parameter space of
the DNN geometrically modeled as a manifold when endowed with the FIM (e.g., ordinary/natural
gradient descent learning the parameters of a MLP). Singular regions of the neuromanifold [71]
correspond to non-identifiable parameters with rank-deficient FIM, and the learning trajectory
typically exhibit chaotic patterns [4] with the singularities which translate into slowdown plateau
phenomena when plotting the loss function value against time. By building an elementary
singular DNN, [4] (and references therein) show that GD learning dynamics yields a Milnor-type
attractor with both attractor/repulser subregions where the learning trajectory is attracted in
the attractor region, then stay a long time there before escaping through the repulser region. The
natural gradient is shown to be free of critical slowdowns. Furthermore, although DNNs have
potentially many singular regions, it is shown that the interaction of elementary units cancels
out the Milnor-type attractors. It was shown [49] that skip connections are helpful to reduce the
effect of singularities. However, a full understanding of the learning dynamics [72] for generic
DNN architectures with multiple output values or recurrent DNNs is yet to be investigated.
The MDL criterion has undergone several fundamental revisions, such as the original crude
MDL [59] and refined MDL [8, 60]. We refer the reader to the book [22] for a comprehensive
introduction to this area and [21] for a recent review. We should also mention that the relationship
between MDL and generalization is not fully understood yet. See [21] for related remarks.
Our derivations based on a Taylor expansion of the log-likelihood are similar to [7]. This
technique is also used for deriving natural gradient optimization for deep learning [4, 40, 51].
Recently MDL has been ported to deep learning [9] focusing on variational methods. MDL-
related methods include weight sharing [18], binarization [27], model compression [12], etc.
In the deep learning community, there is a large body of literature on a theory of deep
learning, for example, based on PAC-Bayes theory [47], statistical learning theory [73], algorithmic
information theory [68], information geometry [39], geometry of the DNN mapping [56], or through
defining an intrinsic dimensionality [38] that is much smaller than the network size. Our analysis
depends on J(ˆθ) and therefore is related to the flatness/sharpness of the local minima [13, 25].
Investigations are performed on the spectrum of the input-output Jacobian matrix [53], the
Hessian matrix w.r.t. the neural network weights [52], and the FIM [23, 30, 31, 50, 54].
10
Conclusion
We considered mathematical tools from singular semi-Riemannian geometry to study the locally
varying intrinsic dimensionality of a deep learning model. These models fall in the category of
non-identifiable parameterizations. We take a meaningful step to quantify geometric singularity
through the notion of local dimensionality d(θ) yielding a singular semi-Riemannian neuromanifold
with varying metric signature. We show that d(θ) grows at most linearly with the sample size
N. Recent findings show that the spectrum of the Fisher information matrix shifts towards 0+
with a large number of small eigenvalues. We show that these singular dimensions help to reduce
the model complexity. As a result, we contribute a simple and general MDL for deep learning.
It provides theoretical justifications on the description length of DNNs. DNNs benefit from a
high-dimensional parameter space in that the singular dimensions impose a negative complexity
to describe the data, which can be seen in our derivations based on Gaussian and Jeffreys’ priors.
A more careful analysis of the FIM’s spectrum, e.g. through considering higher-order terms, could
give more practical formulations of the proposed criterion. We leave empirical studies as potential
future work.
17

Appendix A
Proof of J(ˆθ) = I(ˆθ)
Proof.
p(yi | zi, θ) = exp

OneHot(yi)⊺hL(zi) −log
X
j
exp(hL
j (zi))

,
where OneHot(y) is the binary vector with the same dimensionality as hL(zi), with the y’th bit
set to 1 and the rest bits set to 0. Therefore,
∂log p(yi | zi, θ)
∂θ
=
∂hL
∂θ
⊺
OneHot(yi) −SoftMax(hL(zi))

.
