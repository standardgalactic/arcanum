−N
2 (θ −ˆθ)⊺J(ˆθ)(θ −ˆθ)

. The associated model is considered to have a high
complexity, meaning that only a small “percentage” of the models are helpful to describe the
given data.
DNNs have a large amount of symmetry: the parameter space consists many pieces that looks
exactly the same. This can be caused e.g. by permutate the neurons in the same layer. This
11

is a different non-local property than singularity that is a local differential property. Our O is
not affected by the model size caused by symmetry, because these symmetric models are both
counted in the prior and the posterior, and the log-ratio in eq. (12) cancels out symmetric models.
Formally, M has ζ symmetric pieces denoted by M1, · · · , Mζ. Note any MLE on Mi is mirrored
on those ζ pieces. Then both integrations on the RHS of eq. (12) are multiplied by a factor of ζ.
Therefore O is invariant to symmetry.
6
Connection with f-mean
Definition 3. Given a set T = {ti}n
i=1 ⊂R and a continuous and strictly monotonous function
f : R →R, the f-mean of T is
Mf(T) := f −1
 
1
n
n
X
i=1
f(ti)
!
.
The f-mean, also known as the quasi-arithmetic mean was studied in [33, 45]: Thus they are
also-called Kolmogorov-Nagumo means [34]. By definition, the image of Mf(T) under f is the
arithmetic mean of the image of T under the same mapping. Therefore, Mf(T) is in between the
smallest and largest elements of T. If f(x) = x, then Mf becomes the arithmetic mean, which we
denote as T. We have the following bound.
Lemma 4. Given a real matrix T = (tij)n×m, we use ti to denote i’th row of T , and t:,j to
denote the j’th column of T . If f = exp(−t), then
Mf(T ) ≤{Mf(t:,1), · · · , Mf(t:,m)} ≤Mf({t1, · · · , tn}) ≤T ,
where Mf(T ) is the f-mean of all n × m elements of T , and T is the their arithmetic mean.
In the above inequality, if the arithmetic mean of each row is first evaluated, and then
their f-mean is evaluated, we get an upper bound of the arithmetic mean of the f-mean of the
columns. In simple terms, the f-mean of arithmetic mean is lower bounded by the arithmetic
mean of the f-mean. The proof is straightforward from Jensen’s inequality, and by noting that
−log P
i exp(−ti) is a concave function of t. The last “≤” leads to a proof of the upper bound in
proposition 3.
Remark 5. The second complexity term on the RHS of eq. (11) is the f-mean of the quadratic
term N
2 (θ −ˆθ)⊺J(ˆθ)(θ −ˆθ) w.r.t. the prior p(θ), where f(t) = exp(−t).
Based on the spectrum decomposition J(ˆθ) = PD
j=1 λjvjv⊺
j , where the eigenvalues λj := λj(ˆθ)
and the eigenvectors vj := vj(ˆθ) depend on the MLE ˆθ, we further write this term as
N
2 (θ −ˆθ)⊺J(ˆθ)(θ −ˆθ) =
D
X
j=1
λj
tr(J(ˆθ))
· N
2 tr(J(ˆθ))⟨θ −ˆθ, vj⟩2.
By lemma 4, we have
−log Ep exp

−N
2 (θ −ˆθ)⊺J(ˆθ)(θ −ˆθ)

≥−
D
X
j=1
λj
tr(J(ˆθ))
log Ep exp

−N
2 tr(J(ˆθ))⟨θ −ˆθ, vj⟩2

,
12

where the f-mean and the mean w.r.t.
λj
tr(J( ˆθ)) is swapped on the RHS.
Denote φj = ⟨θ −ˆθ, vj⟩. φ = V ⊺(θ −ˆθ) serves as a new coordinate system of M, where V
is a D × D unitary matrix whose j’th column is vj. The prior of φ is given by p(V φ + ˆθ). Then
−log Ep exp

−N
2 tr(J(ˆθ))⟨θ −ˆθ, vj⟩2

= −log Ep(φj) exp

−N
2 tr(J(ˆθ))φ2
j

.
