Step Number and Length of Responses.
Step-
by-step reasoning is important for LLMs to solve
complex reasoning tasks. Therefore, we conduct
statistical analysis on the reasoning steps during
iterations.
As shown in Table 4, for the same
model, more difficult problems require more rea-
soning steps and longer step lengths. As the iter-
ations progress, the step number across different
tasks decreases, while the length of each step in-
creases. This indicates that performing Process-
based Self-Rewarding encourages the model to
generate longer and higher-quality single reasoning
steps, which helps to reach final answers with fewer
steps. Additionally, this behavior is also related to
LLMs’ preferences when performing LLM-as-a-
Strategy
Greedy Search
Test-time Scaling
M1
55.9
58.2
M4
60.6
62.4
Table 5: The average results of 72B model on all bench-
marks using greedy search or test-time scaling. The full
results are reported in Table 9.
Judge evaluations. More results are in Appendix A.
Test-time Scaling with Process-based Self-
Rewarding Language Models.
In the test-time
scaling, LLMs conduct step search and select based
on the rewards from PRM. Although we don’t pri-
marily focus on the test-time scaling performance
in our work, LLMs in the Process-based Self-
Rewarding paradigm naturally have the ability to
perform test-time scaling based on self-rewarding.
We perform 6 generations for each step with the
temperature of 0.5 and select the best one. The
results we report in Table 5 indicate that the model
achieves better performance through test-time scal-
ing compared to generating directly. Additionally,
the model’s performance with test-time scaling im-
proves after iterations from M1 to M4, which corre-
sponds to the uptrend of the model’s mathematical
abilities and LLM-as-a-Judge capabilities.
6
Conclusion
We propose a novel paradigm, Process-based Self-
Rewarding Language Models, that enables LLMs
to perform step-by-step long-thought mathematical
reasoning and step-wise LLM-as-a-Judge simulta-
neously. Given the characteristics of complex math
reasoning tasks, we introduce the step-by-step rea-
soning, step-wise LLM-as-a-Judge and step-wise
8

preference optimization technique into the frame-
work. Our results indicate that Process-based Self-
Rewarding algorithm outperforms the original Self-
Rewarding on a variety of complex mathematical
reasoning tasks, showing potential of stronger rea-
soning ability better than human in the future.
7
Limitations
We aim to draw more attention to the study of adapt-
ing the self-rewarding paradigm to the complex
mathematical reasoning tasks, which allows for the
possibility of continual improvement beyond the
human preferences. Although our new Process-
based Self-Rewarding algorithm has shown effec-
tive improvements across different mathematical
reasoning tasks, there are still some limitations
waiting for further research. Although we success-
fully enable the model to perform effective step-
wise LLM-as-a-Judge with a small amount of EFT
data, the basic capabilities of initialized M1 model
directly influence the effectiveness of subsequent
process-based self-rewarding. Utilizing more high-
quality data to initialize LLMs more adequately
may lead to stronger performance.
Additionally, due to the limited resources, we
only conduct the process-based self-rewarding ex-
periments from M1 to M4. Building on this, con-
ducting experiments with more iterations to ex-
plore the impact of iteration count on LLMs’ per-
formance can help us better understand and utilize
the process-based self-rewarding method.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai
Fan. 2024.
Step-level value preference optimiza-
