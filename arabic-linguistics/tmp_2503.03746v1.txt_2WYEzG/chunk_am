Ivan Vuli´c, Anna Korhonen, and Nigel Collier. 2024.
Aligning with human judgement: The role of pair-
wise preference in large language model evaluators.
arXiv preprint arXiv:2403.16950.
Adian Liusie, Potsawee Manakul, and Mark JF
Gales. 2023.
Zero-shot nlg evaluation through
pairware comparisons with llms.
arXiv preprint
arXiv:2307.07889.
Yu
Meng,
Mengzhou
Xia,
and
Danqi
Chen.
2024.
Simpo:
Simple preference optimization
with a reference-free reward.
arXiv preprint
arXiv:2405.14734.
Pavlin G. Poliˇcar, Martin Stražar, and Blaž Zupan. 2024.
opentsne: A modular python library for t-sne dimen-
sionality reduction and embedding. Journal of Statis-
tical Software, 109(3):1–30.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neu-
ral Information Processing Systems, 36.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017.
Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(11).
Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Ji-
achen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei
Chen, Lionel M Ni, et al. 2024a. Openr: An open
source framework for advanced reasoning with large
language models. arXiv preprint arXiv:2410.09671.
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai
Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.
2024b. Math-shepherd: Verify and reinforce llms
step-by-step without human annotations. In Proceed-
ings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 9426–9439.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in neural
information processing systems, 35:24824–24837.
Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu,
Yuandong Tian, Jiantao Jiao, Jason Weston, and Sain-
bayar Sukhbaatar. 2024. Meta-rewarding language
models: Self-improving alignment with llm-as-a-
meta-judge. arXiv preprint arXiv:2407.19594.
Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye,
Haoqi Fan, Quanquan Gu, Heng Huang, and Chun-
yuan Li. 2024. Llava-critic: Learning to evaluate mul-
timodal models. arXiv preprint arXiv:2410.02712.
An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,
Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,
Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5
technical report. arXiv preprint arXiv:2412.15115.
An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao,
Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong
Tu, Jingren Zhou, Junyang Lin, et al. 2024b. Qwen2.
5-math technical report: Toward mathematical ex-
pert model via self-improvement.
arXiv preprint
arXiv:2409.12122.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
2024. Tree of thoughts: Deliberate problem solving
with large language models. Advances in Neural
Information Processing Systems, 36.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel
Deutch, and Jonathan Berant. 2023.
Answering
questions by meta-reasoning over multiple chains
of thought. arXiv preprint arXiv:2304.13007.
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,
Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
2024.
Self-rewarding language models.
arXiv
preprint arXiv:2401.10020.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023.
Rrhf:
