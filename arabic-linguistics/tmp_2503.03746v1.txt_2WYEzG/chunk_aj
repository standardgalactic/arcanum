els where evaluation accuracy initially increases,
then decreases, and finally rises again. Based on
the analysis above, initially, LLMs gain strong eval-
uation capabilities through training on EFT data.
And there is a temporary decline (but very slight)
due to training on mathematical data. Ultimately,
as the model’s mathematical abilities improve, its
ability to evaluate mathematical reasoning steps
also increases.
Data Distribution Analysis.
Following Yuan
et al. (2024), we also analyze the distribution of
different data. We utilize Bert (Devlin, 2018) for
embedding and t-SNE (Van der Maaten and Hin-
ton, 2008) based on the implementation of Poliˇcar
et al. (2024) for visualization. We present the re-
sults in Figure 2. For prompts, the distributions of
7

60
40
20
0
20
40
40
30
20
10
0
10
20
30
EFT Prompts
IFT Prompts
PPD Prompts
(a) Prompt Distributions
40
30
20
10
0
10
20
30
40
20
15
10
5
0
5
10
15
20
EFT Responses
IFT Responses
PPD Responses
(b) Response Distributions
Figure 2: The data distribution of prompts and responses in EFT (red), IFT (blue) and PPD (grey) data.
Iterations
Step Num
Step Length
GSM8k
MATH
GSM8k
MATH
M1
5.89
8.41
47.79
61.00
M2
5.55
7.64
51.19
67.17
M3
5.10
6.30
57.75
80.46
M4
4.87
5.54
62.86
96.63
Table 4: Statistics of step number and step length on
GSM8k and MATH benchmarks based on 72B models.
The full results are reported in Appendix A.
EFT data and IFT data do not overlap, allowing the
model to distinctly learn two different task patterns.
For models’ responses, we can find the similar phe-
nomenon that the distribution of PPD and IFT re-
sponses is distinct from EFT’s, which reduces the
mutual interference between LLMs’ two capabili-
ties during iteration. This allows the model’s abil-
ity to perform LLM-as-a-Judge to improve along-
side its mathematical ability finally, without being
overly influenced by the training data itself.
