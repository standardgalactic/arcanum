8841
1003.2
LIMO
Ground-Truth
1
817
6984.1
1
474
2994.7
MixChain-C
2
564
4890.6
Table 9: Dataset Statistic. Here we use the tokenizer
from QwQ-32B-Preview to count the number of tokens.
α
0
0.125
0.25
0.5
0.75
1.0
# Tokens
199.8
219.4
233.4
257.7
466.3
772.7
Accuracy
45.9
47.5
50.2
57.1
55.0
54.5
Table 10: Results of LLaMA-3.2-1B-Instruct trained
with DoRA using different α values for interpolation.
B
More Analysis
Experiments on DoRA.
In addition to LoRA, we
also train LLaMA-3.2-1B using DoRA (Liu et al.,
2024a) and control the magnitude of ∆θ by adjust-
ing the α for DoRA. The model is trained on QwQ
synthesized data for a maximum of five epochs. We
set the batch size to 8 and the peak learning rate to
4e-5, following a cosine decay schedule. A weight
decay of 0.01 is applied. For DoRA, the rank is set
to 32, and the lora_alpha for training is set to 64.
As shown in Table 10, the chain length increases
with the α value, demonstrating the effectiveness of
interpolating ∆θ for DoRA. Furthermore, similar
to our observations with LoRA, the best result is not
obtained by directly training the model on long CoT
data. Specifically, training on QwQ synthesized
data (α = 1.0) achieves an accuracy of 54.5 with
772.7 tokens, whereas the best model obtained via
CoT-Valve (α=0.5) achieves an accuracy of 55.72
with only 257.7 tokens.
Attention has less effect on the length of the
reasoning path than MLP.
We experimented
Modules
GSM8K
#Tokens
#Params
ACU ↑
-
95.1
741.1
-
0.40
K+V
95.0
687.7
0.005%
0.43
Q
95.2
621.4
0.004%
0.48
O
95.2
484.2
0.004%
0.61
Attention
94.2
284.2
0.013%
1.04
MLP
93.5
221.8
0.038%
1.32
All Linear
