max
∆θ′ E(q,a)∼D′p

a | t<m, q; θ + β∆θ
′
m
Y
i=1
p(ti|t<i, q; θ + β∆θ
′)
(3)
Where D′ is the Mixchain dataset. Each sample
consists of the question q, the answer a, the solution
{ti}m
i=1 and β, where β is calculated as:
β = 1 −
m −mmin
mmax −mmin
(4)
Here, mmin and mmax is the length of the short-
est solution and longest solution for this question.
Based on synthetic samples, we introduce addi-
tional constraints that enable us to better identify
the updated parameter ∆θ
′, facilitating more pre-
cise compressibility and controllability.
Progressive Chain Compression: CoT-Valve+P.
The structure of MixChain, which features progres-
sively shorter reasoning paths for each question,
facilitates a progressive chain-length compression
strategy. This approach is similar to iterative prun-
ing in model compression (Molchanov et al., 2016).
In this process, the model is trained with a shorter
reasoning path from the dataset at each iteration,
rather than training directly with the shortest rea-
soning CoT. This gradual compression method al-
lows the model to progressively reduce the length
of its reasoning paths.
4
Experiments
4.1
Experimental Setup
Models.
We evaluate our method under several
models:
QwQ-32B-Preview (Team, 2024b),
DeepSeek-R1-Distill-Llama-8B
(DeepSeek-AI,
2025), LLaMA-3.1-8B (Dubey et al., 2024),
LLaMA-3.2-1B (Dubey et al., 2024) and Qwen-
32B-Instruct (Team, 2024a) with LIMO (Ye
et al., 2025). We tested different scenarios for
CoT-Valve:
• (Long to Short CoT) For QwQ-32B-Preview
(QwQ for abbreviation) and DeepSeek-R1-
Distill-Llama-8B (R1-Distill), we used our
method to control and compress the length of
the reasoning chain.
• (Short to Long CoT) For LLaMA-3.1-8B
and LLaMA-3.2-1B-Instruct, we applied our
method to distill reasoning abilities from
QwQ-32B-Preview and incorporated CoT-
Valve in the distillation process.
• (Short-Long-Short CoT) We tested another
setting to first post-train a short-CoT LLM,
Qwen-2.5-32B-Instruct (Team, 2024a), to gen-
erate Long CoT and then compress it to Short
CoT. CoT-Valve can be applied in both two
stages.
Metrics.
We report both accuracy and the num-
ber of tokens in the answer for each experiment.
Given the trade-off between reasoning path length,
model size, and performance, we use a new metric,
Accuracy per Computation Unit(ACU), to better
capture this balance and evaluate model efficiency.
It is defined as:
ACU =
Accuracy
#Params × #Tokens
(5)
Since the ACU value typically falls within the
range of 10−5 to 10−2, we report it in units of
102 for improved readability.
5

200.0
300.0
400.0
500.0
600.0
700.0
#Token
88
90
92
94
96
Accuracy
Prompt
