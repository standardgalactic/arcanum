741.1
0.40
4
1
6.8k
95.68
597.3
0.50
4+3
1
13.7k
94.84
458.4
0.65
4+3+2
1
20.5k
94.84
339.9
0.87
4+3+2+1
1
27.4k
96.13
317.1
0.95
4+3+2+1+0
1
34.2k
94.92
225.5
1.32
0
5
37.4k
92.19
250.5
1.15
Table 7: Ablation of Progressive Compression on QwQ.
Here, solution 0 is the human-annotated solution from
the original dataset.
QwQ-32B-Preview
Llama-3.2-1B-I
Method
Acc
#Token
Acc
#Token
Prompt (Shortest)
93.6
355.5
52.5
621.0
Ours (Best)
94.4
276.3
55.5
267.0
Ours (Shortest)
87.5
133.8
50.4
247.0
Table 8: CoT-Valve can achieve shorter chains than
prompts with better performance.
We compare two settings: training directly with the
ground-truth solution for five epochs and applying
progressive compression for five epochs in total,
with the final epoch using the ground-truth data.
Our results show that progressive compression sig-
nificantly improves the performance of short CoT
(from 92.19 to 94.92). For each turn, progressive
compression gradually reduces the token number
while maintaining accuracy.
CoT-Valve achieves shorter chains compared to
prompt control
We also present in Table 8 the
shortest chain achieved by our method and compare
these with those obtained using prompt control.
Our method outperforms prompt control methods
at shorter chain lengths. Additionally, we explored
the limits of chain length for both methods and
found that our approach can generate substantially
shorter chains than what can be achieved through
prompt control.
5
Conclusion
In this paper, we propose a method that enables
a model to generate reasoning chains of varying
lengths instead of the prompt control. Based on this
approach, we construct a dataset containing both
long and short reasoning chains to further enhance
controllability and compression efficiency. Experi-
mental results demonstrate the effectiveness of our
method in dynamic reasoning chain control and the
compression of CoT. Future research can further
explore finer-grained control strategies to improve
reasoning efficiency and model controllability.
8

