strates significant potential for compressing the
reasoning chains. We can also surpass the result
of Gemini-Flash-Thinking, with the same accuracy
but fewer tokens (10810.5 v.s. 8174.8)
Training dynamics does not have the same effect
as CoT-Valve.
We also explore whether inter-
mediate training steps can achieve similar effects.
As depicted in Figure 3c, during the early train-
ing phases, the length of the CoT increases but
does not correspond with the same rapid improve-
ment in performance. As training progresses, the
token length begins to decrease while performance
improves. CoT-Valve exhibits a distinct pattern,
smoothly bridging the gap between the length of
CoT and performance.
4.5
Observations
Based on the results from LLaMA-3.1-8B,
LLaMA-3.2-1.5B, QwQ, DeepSeek-R1-Distill-
7

Solution
Solution Length
Accuracy
#Token
Ground-Truth (Solution 0)
116.0
43.8
139.4
Solution 1
279.6
57.0
288.4
Solution 2
310.7
55.1
330.0
Solution 3
386.7
56.5
414.6
Solution 4
497.2
52.5
558.3
Table 6: Train LLaMA-3.2-1B-Instruct with solutions
in MixChain-Z of different lengths on GSM8K.
Llama-8B and Qwen2.5-32B-Instruct with LIMO,
we summarize the following observations:
• Longer reasoning chains are not always the
best on simple datasets. Across nearly all
models, we find that those directly trained on
long CoT data typically do not show the best
performance. These models often underper-
form compared to those generated through
CoT-Valve, which results in shorter but more
accurate reasoning chains. This trend is par-
ticularly pronounced in smaller models. For
instance, in the LLaMA-3.2-1B model, train-
ing on QwQ synthesized data yields an accu-
racy of 52.69 with 759.3 tokens. However,
using CoT-Valve, we can achieve an accuracy
of 55.50 with only 267.0 tokens. However,
we do not observe this phenomenon in more
complex datasets, indicating that while the
reasoning model may be redundant for simple
datasets, it still requires test-time scaling to
effectively handle complex datasets.
• Some reasoning chains are difficult for the
model to learn, especially for small LLMs.
We fine-tuned LLaMA-3.2-1B-Instruct using
only one solution from MixChain, where all
solutions lead to the same final answer but
involve different intermediate reasoning steps.
The results, presented in Table 6, indicate
that neither the shortest nor the longest chains
are optimal for learning. Instead, the model
most effectively learns from moderately short
chains, achieving the highest accuracy while
maintaining a relatively low token count. This
phenomenon is particularly evident in smaller
models, but it is not observed in larger mod-
els. We believe this could be beneficial for the
distillation of CoT in small LLMs.
4.6
Analysis
Ablation on Progressive Compression.
Table 7
demonstrates the effect of progressive compression.
Solution Used
#Epoch
#Samples
Accuracy
#Tokens
ACU↑
-
-
-
95.07
