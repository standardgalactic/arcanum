front of young children, who were severely injured and in some
instances killed.… Women and girls were systematically abducted,

detained and raped in military and police compounds, often
amounting to sexual slavery. Victims were severely injured before
and during rape, often marked by deep bites. They suffered serious
injuries to reproductive organs, including from rape with knives and
sticks. Many victims were killed or died from injuries.…
Others [men and women] were killed in arson attacks, burned to
death in their own houses, in particular the elderly, persons with
disabilities and young children, unable to escape. In some cases,
people were forced into burning houses, or locked in buildings set
on fire.
Over seven hundred thousand Muslims fled the country.
What the world will learn later is that the military had set up a massive
operation—at least seven hundred people—to spread misinformation and
hate on Facebook. This was revealed by a reporter named Paul Mozur in the
New York Times. Sources in the military’s secret operation told him how
they created and took over verified accounts that had huge followings—fan
accounts for pop stars and celebrities, the Facebook page for a military hero
—and used them to pump out false, inflammatory posts. “Troll accounts run
by the military helped spread the content, shout down critics and fuel
arguments between commenters to rile people up.”
As Mozur points out, “It’s among the first examples of an authoritarian
government using the social network against its own people.”
Facebook’s response to Mozur? The company issued a statement saying,
“It had found evidence that the messages were being intentionally spread by
inauthentic accounts and took some down at the time. It did not investigate
any link to the military at that point.”
Which raises the question, What was Facebook’s role in all of this? The UN
report on the human rights violations in Myanmar devotes over twenty
pages to the critical role Facebook played in spreading hate. It catalogs the
different kinds of derogatory language investigators found in posts, memes,
and cartoons—including several variations on the ethnic slur kalar, the

word our team tried and failed to ban for years because we couldn’t
convince the decision makers at Facebook to take action. It lists the
different anti-Muslim narratives found on Facebook: posts that portray
Muslims and Rohingya as a threat to the Buddhist character of the country
and Burmese racial purity; posts that characterize Muslims as terrorists,
criminals, and rapists; posts that claim they “breed like rabbits” and will
overtake the population. Organizations with an explicitly racist agenda like
MaBaTha, led by the extremist monk Wirathu, have an active Facebook
presence. Death threats, harassment, and calls for violence aren’t just
against Muslims “but also against moderate commentators, human rights
defenders and ordinary people who have views that differ from the official
line.” All of these are issues Facebook had been aware of for years.
The UN investigators point out many of the other issues we’d tried and
failed to convince Facebook’s leaders to address: the woefully inadequate
content moderation Facebook provided for Myanmar; the lack of
moderators who “understand Myanmar language and its nuances, as well as
the context within which comments are made”; the fact that the Burmese
language isn’t rendered in Unicode; the lack of a clear system to report hate
speech and alarming unresponsiveness when it is reported. The
investigators noted with regret that Facebook said it was unable to provide
country-specific data about the spread of hate speech on its platform, which
was imperative to assess the problem and the adequacy of its response. This
was surprising given that Facebook had been tracking hate speech.
Community operations had written an internal report noting that forty-five
of the one hundred most active hate speech accounts in Southeast Asia are
in Myanmar.
The truth here is inescapable. Myanmar would’ve been far better off if
Facebook had never arrived there.
I’ve spent a lot of time thinking about what unfolded next in Myanmar,
and Facebook’s complicity. It wasn’t because of some grander vision or any
malevolence toward Muslims in the country. Nor a lack of money. My
conclusion: It was just that Joel, Elliot, Sheryl, and Mark didn’t give a fuck.
Joel was a veteran of George W. Bush’s White House. An issue in Syria
would be met by a wave of his hand and, “Drop a bomb on it. I don’t care.”

A joke, but also who he was. He was the man in charge of those countries
for Facebook. And when it came to Myanmar, those people just didn’t
matter to him. He couldn’t be bothered. There was no greater principle ever
offered.
People outside big companies sometimes wonder and speculate about
how these sorts of decisions happen. This is how it happened at Facebook.
And it wasn’t just Joel. None of the senior leaders—Elliot or Sheryl or
Mark—thought about this enough to put in place the kinds of systems we’d
need, in Myanmar or other countries. They apparently didn’t care. These
were sins of omission. It wasn’t the things they did; it was the things they
didn’t do.

47
It Really Didn’t Have to Be This Way
Things are bad, but the one saving grace so far has been that Joel and I are
not in the same office or even the same state. So his behavior has so far
been limited by geography to creepy questions and taking our one-on-one
meetings in bed. But Elliot is about to host everyone who works for him at
an offsite, which means Joel will be in town. I’m worried. I fear something
is going to happen.
Joel has recently interviewed for a cabinet position with Trump but
remained at Facebook as the key conduit into the Trump administration.
With Trump’s ascension, he’s become increasingly powerful. Closer to
Mark. Weighing in on not just policy issues but product issues. Core
decisions for the company. This means that decisions about political speech,
content, and the algorithm all go through Joel. It seems to me like a
ratcheting up of the inherent conflict of interest that has always underpinned
his job. Joel’s responsible both for lobbying government and for making
key content and product decisions that keep Facebook in good standing
with the Trump administration.
He doesn’t wear this newfound status lightly.
Through two days of presentations, meals, and group activities, it’s very
awkward with Joel. He drops a napkin at the opening night reception and he
waits and stares at me till I figure out that he’s expecting me to kneel to the
ground and pick it up for him. He makes weird comments about Tom. “Is
Tom gonna let you out tonight?” “You’re going home to Tom now?”
