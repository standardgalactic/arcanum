power works.”
He and Joel start putting together plans to “tell them that this is not how
we expect to work with our own government.”
Joel rings me after he’s had a bruising discussion with Mark, who
wanted to understand “the facts” about how content is removed from
Facebook. He’s now thinking that instead of having the content operations
team follow the same rules they’ve followed for years, all requests for
removal of content from “sensitive countries” should be escalated,
potentially to Mark. Certainly to Joel and Elliot.
What Mark’s really mad about over that frenzied holiday weekend is
that he sees the decision as his to make. Which is mystifying to the content
operations and policy teams who’ve been applying the Community
Standards and making difficult decisions like this for years. Until now,
they’d escalate particularly hard decisions to Marne, occasionally to Elliot,
and rarely to Sheryl. No one can remember ever escalating this type of
decision to Mark. But now, two months after our Asia trip, Mark’s interest
in the politics of these decisions is growing. Now, for the first time, he
wants to be the decider.
It’s painful to see, given how much work went into putting those
guidelines in place. The system we developed isn’t perfect, but it has checks
and balances, and isn’t guided by just one man’s whims. When I arrived at
Facebook in 2011, the rules were extraordinarily crude and decisions were
made by a team that reported to the vice president in charge of global sales.
The team hated taking anything down. They pushed back nearly all
requests. Then we had a series of troubling incidents. The office of
Australian prime minister Tony Abbott asked us to remove a page called
“Occupy Tony Abbott’s Daughters’ Vaginas.” Someone at a consumer
packaged goods company reached out to Sheryl to complain that their ads
appeared on a page called “Riding her gently while she sleeps” and another
with a name that was something like “She bites the pillow while I enter the
backdoor.” The content team used the same arguments they had for keeping
beheading videos on the platform and fought ferociously against taking
down any of these pages. After this, the content team was swiftly removed

from the sales operation and put in the public policy team under Marne. She
ordered a summit to develop a set of principles and processes for what can
and cannot be posted on Facebook. After many philosophical and practical
debates, and getting myself personally invested in way too many arguments
with bearded guys about John Stuart Mill, we agree on Facebook’s first
Community Standards. It felt like progress to have the policy and
operations teams come together and develop a set of principles that would
guide decision-making for all content on Facebook. I wanted to make these
public, so users everywhere would know what the rules are. This would also
make us more accountable, ensuring everyone at Facebook followed the
same clear principles. And we do.
Navalny is the starting point for Mark wanting to overrule this imperfect
but functional system. And as time wears on and weirder and more
inconvenient facts present themselves, Mark steps in more and more.
Mark decides to block a video of the aftermath of a Mexican school
shooting when the president of Mexico reaches out to him personally; he
blocks some Indonesian political speech threatening a coup against his
buddy President Jokowi; he blocks political speech that the South Korean
National Election Commission wants removed.
Most of those decisions are contrary to the Community Standards. The ones
we’ve made public. He’s replacing the process we’ve developed over years
with whatever he thinks is right. And there doesn’t seem to be any form of
accountability. It’s also hard not to notice that in nearly every case, his
decision happens to coincide with his business interests, mollifying
governments so he can keep Facebook growing.
It starts to feel like all the guidelines we’d spent years endlessly
haggling over and refining are irrelevant. The new test on difficult decisions
about government requests is whether there’s a risk that Facebook will be
blocked if we don’t comply. It’s just a question of blunt power.
Around this time—while he’s personally ordering us to take down posts
that don’t violate our Community Standards—Mark starts opining publicly
about freedom of speech. The day after the Charlie Hebdo attack, where

twelve people were murdered, he posts, “This is what we all need to reject
—a group of extremists trying to silence the voices and opinions of
everyone else around the world. I won’t let that happen on Facebook. I’m
committed to building a service where you can speak freely without fear of
violence.”
In our offices around the world, there’s a sense of whiplash. The people
I’ve hired are confused about these departures from policy. When we gather
the entire policy and communications organization together for an annual
offsite meeting at headquarters in February 2015, Joel outlines what he sees
as Mark’s new approach. When governments ask us to take down content
that we believe should not come down, he says, it will only be taken down
if one of two criteria is met:
1. There is a credible threat to block Facebook.
2. There is a risk to employees.
The team peppers me with questions. How can they possibly tell if the
government is going to shut down Facebook in their country? How can they
escalate to Mark? What if we guess wrong about the risk and then they’re in
jail and it’s too late?
It’s five dozen people at this meeting, in a big conference room on
Facebook’s campus—and the ones objecting the most are recent hires from
South Korea, Brazil, and other places where governments have shown a
willingness to arrest or use a show of force against employees:
“How can Mark understand the politics in my country?”
“How can he overrule it?”
“How can we defy the government making legal requests and then ask
to work with them?”
“How can we say our rules are one thing but actually they are another?”
These are the very people most at risk from this decision. It’s them, not
Mark, who are most likely to be jailed if push comes to shove.
Joel’s response? He’s frustrated and can’t understand their confusion,
seething at what he sees as insolence and ignorance. The audacity of
questioning authority. Basically, management issues the orders and

employees outside the US are expected to comply. He lectures the team that
Mark’s been clear on the two principles. Everything stays up unless
Facebook is going to be blocked or someone is arrested and sitting in a jail
cell with no way out. How many times does he have to say it?
Everyone seems shocked that there’s no discussion or input—just an
edict from Mark, enforced by Joel. They struggle to understand why Mark’s
