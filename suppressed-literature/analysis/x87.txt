are aware of any more. The Facebook advertising guy who is cited in the
[Australian] article has three children—I talked him through his kid being
bullied—what was he thinking?”
I’m still struggling to get a better picture of what we’re dealing with
here. So I ask for an independent audit by a third party to understand
everything that Facebook has done like this around the world, targeting
vulnerable people, so I can try to stop it. Who has this information and how
many advertisers has it been shared with? The team is not enthusiastic.
Elliot nixes any audit and cautions against using the word “audit” at all,
even as an ask like mine, saying that “lawyers have discouraged that
description in similar contexts.” He doesn’t say why but I’m guessing he
doesn’t want to create a paper trail, a report with damning details that could
be leaked or subpoenaed. Years later I would learn that British teenager

Molly Russell had saved Instagram posts including one from an account
called Feeling Worthless before committing suicide. “Worthless” being one
of the targeting fields. This only emerged due to a lawsuit that revealed
internal documents acknowledging “palpable risk” of “similar incidents.”
The initial statement Facebook gives the Australian journalist who
discovered the targeting and surveillance back in 2017 does not
acknowledge that this sort of ad targeting is commonplace at Facebook. In
fact, it pretends the opposite: “We have opened an investigation to
understand the process failure and improve our oversight. We will
undertake disciplinary and other processes as appropriate.”
A junior researcher in Australia is fired. Even though that poor
researcher was most likely just doing what her bosses wanted. She’s just
another nameless young woman who was treated as cannon fodder by the
company.
When that doesn’t stop media interest, Elliot says, “We need to push
back hard on the idea that advertisers were enabled to target based on
emotions. Can you share to group so Sheryl et al can i) see the article, ii)
understand next steps.” Joel wants a new, stronger statement, one saying
that we’ve never delivered ads targeted on emotion. He directs that “our
comms should swat that down clearly,” but he’s told that it’s not possible.
Joel’s response: “We can’t confirm that we don’t target on the basis of
insecurity or how someone is feeling?” Facebook’s deputy chief privacy
officer responds, “That’s correct, unfortunately.” Elliot asks whether it is
possible to target on words like “depressed” and the deputy chief privacy
officer confirms that, yes, Facebook could customize that for advertisers.
He explains that not only does Facebook offer this type of customized
behavioral targeting, there’s a product team working on a tool that would
allow advertisers to do this themselves, without Facebook’s help.
Despite this, Elliot, Joel, and many of Facebook’s most senior
executives devise a cover-up. Facebook issues a second statement that’s a
flat-out lie: “Facebook does not offer tools to target people based on their
emotional state.” The new statement is circulated to a large group of senior
management who know it’s a lie, and approve it anyway. It reads,

On May 1, 2017, The Australian posted a story regarding research
done by Facebook and subsequently shared with an advertiser. The
premise of the article is misleading. Facebook does not offer tools to
target people based on their emotional state.
The analysis done by an Australian researcher was intended to
help marketers understand how people express themselves on
Facebook. It was never used to target ads and was based on data that
was anonymous and aggregated.
I take a couple of days off for a family trip and to celebrate Xanthe’s
birthday, and the response team continues on without me. I’m glad to miss
it.
One of the top ad executives for Australia calls me late one night to
complain. Why are we putting out statements like this? he wants to know.
“This is the business, Sarah. We’re proud of this. We shout this from the
rooftops. This is what puts money in all our pockets. And these statements
make it look like it’s something nefarious.” It looks bad in front of our
advertisers, he says, for Facebook to pretend it’s not doing this targeting.
He’s out there every day promoting the precision of these tools that hoover
up so much data and insight on and off Facebook so that it can deliver the
right ad at the right time to the right user. And this is what headquarters is
saying to the public? “How do I explain this?” he asks. And thirteen-to-
seventeen-year-olds? “That’s a very important audience. Advertisers really
want to reach them. And we have them! We’re pretending we don’t do
this?”
As if to back him up, just three days after the false denial, on an
earnings call Sheryl touts Facebook’s ability to target based on sex and age,
stating, “We think that targeting and measurement are significant
competitive advantages for us.… Just in basic targeting itself, just age and
gender, we’re 38% more accurate than broad-based targeting according to
Nielsen in the U.S. And that’s just age and gender.”
I know I’m on vacation and I have a valid excuse to stay out of this, to
not put any grit in the machine or damage my standing with leadership. I
know anything I do or say at this point will not change the choices

leadership is making. I know it’s in my best interest to just stay silent. To
not sabotage myself.
But then before I can think too hard about it, I’m confronting Elliot.
Relaying the call I had from the ad executive and my own concerns that
we’re lying to the public, more angrily than I mean to. Why, I ask Elliot,
can’t we just stop targeting depressed teenagers, and anyone else in a
vulnerable emotional state? We’ll still make a lot of money. It can’t be that
much of our business.
Elliot’s amused.
“If you and he both hate this—for opposite reasons—we must’ve gotten
this exactly right.”

45
A Fish Rots from the Head
Trust is gone between staff and leadership at Facebook. The lingering
discontent over Facebook’s role in Trump’s election, the Feminist Fight
Club’s issues, and the broader silence and lack of contrition about the harm
Facebook is causing globally have changed how so many people feel about
working here. Before all this, you felt proud to be at Facebook. That’s gone.
People feel like they’re complicit. Internal Facebook groups are starting
to see posts from employees asking if they could move to different teams
where they could “try not to be morally implicated.” It’s harder to recruit. In
fact, prospective hires are telling recruiters to never contact them again, that
they’ll never work for Facebook. That’s new.
